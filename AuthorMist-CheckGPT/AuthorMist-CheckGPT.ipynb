{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/liuzey/CheckGPT-v2.git"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Y7EH_cbhgQ5",
    "outputId": "8f5875bc-43ca-4ea4-9ce5-63380a65d7a2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets"
   ],
   "metadata": {
    "id": "4DFIN_oSiovw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting AI and Human Generated text from our dataset\n",
    "\n"
   ],
   "metadata": {
    "id": "jabLJxQfR4sq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8IXtUH5R1JT"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load AI-generated texts\n",
    "with open('CheckGPT-v2/GPABenchmark/CS_TASK1/gpt.json', 'r') as f:\n",
    "    gpt_data = json.load(f)\n",
    "ai_texts = list(gpt_data.values())  # <-- fixed here\n",
    "\n",
    "# Load Human-written texts\n",
    "with open('CheckGPT-v2/GPABenchmark/CS_TASK1/hum.json', 'r') as f:\n",
    "    hum_data = json.load(f)\n",
    "hum_texts = list(hum_data.values())  # <-- fixed here"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "hum_texts = hum_texts[:150]\n",
    "ai_texts = ai_texts[:200]"
   ],
   "metadata": {
    "id": "ZnzsZuLujA8s"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading AuthorMist as a Paraphraser for Testing"
   ],
   "metadata": {
    "id": "0lCjzxZHSl1t"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"authormist/authormist-originality\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "csv_filename = \"paraphrased_results.csv\"\n",
    "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow([\"Original Text\", \"Paraphrased Text\"])\n",
    "\n",
    "    for i, ai_text in enumerate(tqdm(ai_texts[100:], desc=\"Paraphrasing texts\")):\n",
    "            prompt = f\"\"\"Please paraphrase the following text to make it more human-like while preserving the original meaning.\n",
    "(Provide ONLY the paraphrased text with no additional commentary or responses)\n",
    "\n",
    "{ai_text}\n",
    "\n",
    "Paraphrased text:\"\"\"\n",
    "\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_new_tokens=512,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "            full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            paraphrased_text = full_output.split(\"Paraphrased text:\")[-1].strip()\n",
    "            print(paraphrased_text)\n",
    "            csv_writer.writerow([ai_text, paraphrased_text])\n",
    "            del inputs, outputs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(f\"Paraphrasing complete! Results saved to {csv_filename}\")"
   ],
   "metadata": {
    "id": "K2m13-I0TAPr",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7069881b32354cc4a64c6eb8d8d2c39d",
      "da3eab67bfd4477e8fcb1615a28d906e",
      "b0bfcd634e1e487a934cf648fc02806f",
      "a7988bb68c164802adc6aaa1de600b08",
      "07a0589b5bea41afb86e6cccc622485e",
      "914ed57b749047e0a8a0085591f057c8",
      "344d8647bc3b44f68a75d9e58285590a",
      "2565b59c1cbf463a9a0f789459c34cea",
      "c726240c3c3d4fb096e1a9c6469b7d17",
      "1d84505bf7624f7aad270f72fae16bc8",
      "8acf2eae62f44e77971c1b9651ad35e0"
     ]
    },
    "collapsed": true,
    "outputId": "d70969eb-faaf-43d5-9343-ab667a3ca244"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DETECTION"
   ],
   "metadata": {
    "id": "cRl4j4m0TPEi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "from transformers import (pipeline, AutoTokenizer, AutoModel, T5ForConditionalGeneration, T5Tokenizer, AutoModelForCausalLM)"
   ],
   "metadata": {
    "id": "4OhPpUajWGPN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "qd = pd.read_csv('paraphrased_results.csv')\n",
    "qd_texts = qd['Paraphrased Text'].tolist()"
   ],
   "metadata": {
    "id": "oByCnTNTVFi0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "qd = pd.read_csv('paraphrased_results.csv')\n",
    "qd_texts = qd['Paraphrased Text'].tolist()\n",
    "\n",
    "ai_data = {'text': qd_texts, 'label': 1, 'source': 'AI'}\n",
    "human_data = {'text': hum_texts, 'label': 0, 'source': 'Human'}\n",
    "\n",
    "ai_df = pd.DataFrame(ai_data)\n",
    "human_df = pd.DataFrame(human_data)\n",
    "combined_df = pd.concat([ai_df, human_df], ignore_index=True)\n",
    "random.seed(42)\n",
    "shuffled_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "shuffled_df.to_csv('combined_texts.csv', index=False)\n",
    "\n",
    "print(\"Data preparation complete. Saved to 'combined_texts.csv'\")"
   ],
   "metadata": {
    "id": "NZv0Ga1dUOa6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2eeef5c4-f6c9-49bb-a4d5-a15e925cffe7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RADAR - getting scores"
   ],
   "metadata": {
    "id": "vMWotYw1WSQN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Radar():\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.judge_model = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"TrustSafeAI/RADAR-Vicuna-7B\",\n",
    "            tokenizer=\"TrustSafeAI/RADAR-Vicuna-7B\",\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def score(self, response_list):\n",
    "        scores = self.judge_model(response_list)\n",
    "        return [\n",
    "            (\n",
    "                score[\"score\"]\n",
    "                if score[\"label\"] == \"LABEL_0\"\n",
    "                else 1 - score[\"score\"]\n",
    "            )\n",
    "            for score in scores\n",
    "        ]"
   ],
   "metadata": {
    "id": "oqXTAhhhWTpN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### DETECTGPT - getting scores"
   ],
   "metadata": {
    "id": "kK4xKyynWbQz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DetectGPT():\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.judge_model = None\n",
    "\n",
    "    def _load_model(self):\n",
    "        if self.judge_model is None:\n",
    "            print(\"Loading DetectGPT model...\")\n",
    "            self.judge_model = pipeline(\n",
    "                \"text-classification\",\n",
    "                model=\"Hello-SimpleAI/chatgpt-detector-roberta\",  # Example model\n",
    "                tokenizer=\"Hello-SimpleAI/chatgpt-detector-roberta\",\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                device=self.device,\n",
    "            )\n",
    "            print(\"DetectGPT model loaded successfully\")\n",
    "\n",
    "    def score(self, response_list):\n",
    "        self._load_model()\n",
    "        scores = self.judge_model(response_list)\n",
    "        return [\n",
    "            (score[\"score\"] if score[\"label\"] == \"1\" else 1 - score[\"score\"])\n",
    "            for score in scores\n",
    "        ]"
   ],
   "metadata": {
    "id": "AyEbm6yyWa07"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('combined_texts.csv')\n",
    "responses = df['text'].tolist()\n",
    "\n",
    "def get_scores(judge):\n",
    "    return judge.score(responses)"
   ],
   "metadata": {
    "id": "OMjZz2JkWf5I"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "judge = Radar()\n",
    "scores_radar = get_scores(judge)\n",
    "judge = DetectGPT()\n",
    "scores_detectgpt = get_scores(judge)\n",
    "\n",
    "combined_df = pd.read_csv('combined_texts.csv')\n",
    "combined_df['detectgpt_score'] = scores_detectgpt\n",
    "combined_df['radar_score'] = scores_radar\n",
    "combined_df.to_csv('authormist_scored.csv', index=False)"
   ],
   "metadata": {
    "id": "wF_bA_RNWht0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5174bff8-198c-4e4b-d666-aee4588518a2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "combined_df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "q65AskXKogWM",
    "outputId": "f9d42423-33e6-4925-8d02-1c0e5d2a7ae1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RADAR - getting labels"
   ],
   "metadata": {
    "id": "pBTCx4vHYUOy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Radar():\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.judge_model = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"TrustSafeAI/RADAR-Vicuna-7B\",\n",
    "            tokenizer=\"TrustSafeAI/RADAR-Vicuna-7B\",\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def score(self, response_list):\n",
    "        scores = self.judge_model(response_list)\n",
    "        return [score[\"label\"] for score in scores]"
   ],
   "metadata": {
    "id": "XAw09Y6EYXq8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DetectGPT - getting labels"
   ],
   "metadata": {
    "id": "Wj8eQm8SYgWT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DetectGPT():\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.judge_model = None\n",
    "\n",
    "    def _load_model(self):\n",
    "        if self.judge_model is None:\n",
    "            print(\"Loading DetectGPT model...\")\n",
    "            self.judge_model = pipeline(\n",
    "                \"text-classification\",\n",
    "                model=\"Hello-SimpleAI/chatgpt-detector-roberta\",  # Example model\n",
    "                tokenizer=\"Hello-SimpleAI/chatgpt-detector-roberta\",\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                device=self.device,\n",
    "            )\n",
    "            print(\"DetectGPT model loaded successfully\")\n",
    "\n",
    "    def score(self, response_list):\n",
    "        self._load_model()\n",
    "        scores = self.judge_model(response_list)\n",
    "        return [score[\"label\"] for score in scores]"
   ],
   "metadata": {
    "id": "9NfKRPyOYo-c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('authormist_scored.csv')\n",
    "responses = df['text'].tolist()\n",
    "\n",
    "def get_scores(judge):\n",
    "    return judge.score(responses)"
   ],
   "metadata": {
    "id": "VoQFns_LYwDw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "judge = Radar()\n",
    "scores_radar = get_scores(judge)\n",
    "judge = DetectGPT()\n",
    "scores_detectgpt = get_scores(judge)\n",
    "\n",
    "combined_df = pd.read_csv('authormist_scored.csv')\n",
    "combined_df['detectgpt_labels'] = scores_detectgpt\n",
    "combined_df['radar_labels'] = scores_radar\n",
    "combined_df.to_csv('authormist_scored.csv', index=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LzVPrGGvY0nc",
    "outputId": "362f5ee7-03bf-458d-ac23-d11f1d789b64"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "combined_df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "_dGZYLsWqPc2",
    "outputId": "43dd185c-af2b-41a9-dee6-9cffa66a64a6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AUROC scores on RADAR and DetectGPT Scores\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "1nfJJfXRatkq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auroc_radar = roc_auc_score(combined_df['label'], combined_df['radar_score'])\n",
    "print(f\"RADAR AUROC Score: {auroc_radar:.6f}\")\n",
    "auroc_detectgpt = roc_auc_score(combined_df['label'], combined_df['detectgpt_score'])\n",
    "print(f\"DETECTGPT AUROC Score: {auroc_detectgpt:.6f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UeC1fP1Ka9JU",
    "outputId": "7ab07ed0-43eb-464c-bf42-0d93c900bf9f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# More RADAR Metrics"
   ],
   "metadata": {
    "id": "g8Zxx2rxi85N"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# 1. Convert radar labels into binary labels\n",
    "binary_predictions = (combined_df['radar_labels'] == \"LABEL_1\").astype(int)\n",
    "\n",
    "# 2. Confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(combined_df['label'], binary_predictions).ravel()\n",
    "\n",
    "print(\"True Negatives:\", tn)\n",
    "print(\"False Positives:\", fp)\n",
    "print(\"False Negatives:\", fn)\n",
    "print(\"True Positives:\", tp)\n",
    "\n",
    "# 3. Metrics\n",
    "precision = precision_score(combined_df['label'], binary_predictions)\n",
    "recall = recall_score(combined_df['label'], binary_predictions)\n",
    "f1 = f1_score(combined_df['label'], binary_predictions)\n",
    "accuracy = accuracy_score(combined_df['label'], binary_predictions)\n",
    "\n",
    "print(f\"Precision: {precision:.6f}\")\n",
    "print(f\"Recall: {recall:.6f}\")\n",
    "print(f\"F1 Score: {f1:.6f}\")\n",
    "print(f\"Accuracy: {accuracy:.6f}\")\n",
    "\n",
    "# 4. Attack Success Rate (ASR)\n",
    "# ASR = fraction of \"AIs misclassified as Human\" = FN / (TP + FN)\n",
    "asr = fn / (tp + fn) if (tp + fn) > 0 else 0\n",
    "print(f\"Attack Success Rate: {asr:.6f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAdWMbkQjDpK",
    "outputId": "6d65da94-174c-4a25-ec16-a20275799d82"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# More DetectGPT Metrics"
   ],
   "metadata": {
    "id": "vrFj7iMrjQfE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# 1. Map detectgpt labels to 0/1\n",
    "binary_predictions = (combined_df['detectgpt_labels'] == \"ChatGPT\").astype(int)\n",
    "\n",
    "# 2. Get confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(combined_df['label'], binary_predictions).ravel()\n",
    "\n",
    "print(\"True Negatives:\", tn)\n",
    "print(\"False Positives:\", fp)\n",
    "print(\"False Negatives:\", fn)\n",
    "print(\"True Positives:\", tp)\n",
    "\n",
    "# 3. Compute metrics\n",
    "precision = precision_score(combined_df['label'], binary_predictions)\n",
    "recall = recall_score(combined_df['label'], binary_predictions)\n",
    "f1 = f1_score(combined_df['label'], binary_predictions)\n",
    "accuracy = accuracy_score(combined_df['label'], binary_predictions)\n",
    "\n",
    "print(f\"Precision: {precision:.6f}\")\n",
    "print(f\"Recall: {recall:.6f}\")\n",
    "print(f\"F1 Score: {f1:.6f}\")\n",
    "print(f\"Accuracy: {accuracy:.6f}\")\n",
    "\n",
    "# 4. Attack Success Rate (ASR)\n",
    "asr = fn / (tp + fn) if (tp + fn) > 0 else 0\n",
    "print(f\"Attack Success Rate: {asr:.6f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-EF3jn7GjSaO",
    "outputId": "61b51d1e-c11c-4b26-d1b5-cc23158791f5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "model_col = \"source\"\n",
    "score_col = \"radar_score\"\n",
    "\n",
    "models = combined_df[model_col].unique()\n",
    "\n",
    "# Plot KDE for smooth distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model in models:\n",
    "    subset = combined_df[combined_df[model_col] == model]\n",
    "    sns.kdeplot(subset[score_col].dropna(), label=model, fill=True, alpha=0.4)  # Fill for better visibility\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"RADAR Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of RADAR Scores by Model\")\n",
    "plt.legend(title=\"Models\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "QlfdhuGA5Ssd",
    "outputId": "3d00b92d-09a2-4e20-fc88-0fd85e003fda"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "model_col = \"source\"\n",
    "score_col = \"detectgpt_score\"\n",
    "\n",
    "models = combined_df[model_col].unique()\n",
    "\n",
    "# Plot KDE for smooth distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model in models:\n",
    "    subset = combined_df[combined_df[model_col] == model]\n",
    "    sns.kdeplot(subset[score_col].dropna(), label=model, fill=True, alpha=0.4)  # Fill for better visibility\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"DetectGPT Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of DetectGPT Scores by Model\")\n",
    "plt.legend(title=\"Models\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "bmPxHGTw6UlF",
    "outputId": "0f3a37fd-1e4c-41dc-c1cd-c29b3edcf2fd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Calculate ROC curve points for both detectors\n",
    "fpr_radar, tpr_radar, _ = roc_curve(combined_df['label'], combined_df['radar_score'])\n",
    "fpr_wild, tpr_wild, _ = roc_curve(combined_df['label'], combined_df['detectgpt_score'])\n",
    "\n",
    "# Create a figure with appropriate size\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot both ROC curves\n",
    "plt.plot(fpr_radar, tpr_radar, color='blue', lw=2,\n",
    "         label=f'RADAR (AUROC = {auroc_radar:.4f})')\n",
    "plt.plot(fpr_wild, tpr_wild, color='red', lw=2,\n",
    "         label=f'DETECTGPT (AUROC = {auroc_detectgpt:.4f})')\n",
    "\n",
    "# Plot the diagonal reference line\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
    "\n",
    "# Add labels and formatting\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.title('ROC Curves for AI Text Detection Models', fontsize=16)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "7aqdKkXplhtB",
    "outputId": "2e8dc471-612d-4dc3-8026-e8a0401d2ba1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "model_col = \"source\"\n",
    "score_col = \"radar_score\"\n",
    "\n",
    "models = combined_df[model_col].unique()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop through Human and AI separately\n",
    "for model in models:\n",
    "    subset = combined_df[combined_df[model_col] == model]\n",
    "    scores = subset[score_col].dropna()\n",
    "    sorted_scores = np.sort(scores)\n",
    "    cdf = np.arange(1, len(sorted_scores)+1) / len(sorted_scores)\n",
    "    plt.plot(sorted_scores, cdf, label=model)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"RADAR Score\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.title(\"CDF of RADAR Scores by Model\")\n",
    "plt.legend(title=\"Models\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "Dd5Wmva82IWr",
    "outputId": "6aa7b784-1a70-42e8-8ab9-f81c1165433e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "model_col = \"source\"\n",
    "score_col = \"detectgpt_score\"\n",
    "\n",
    "models = combined_df[model_col].unique()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop through Human and AI separately\n",
    "for model in models:\n",
    "    subset = combined_df[combined_df[model_col] == model]\n",
    "    scores = subset[score_col].dropna()\n",
    "    sorted_scores = np.sort(scores)\n",
    "    cdf = np.arange(1, len(sorted_scores)+1) / len(sorted_scores)\n",
    "    plt.plot(sorted_scores, cdf, label=model)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"DetectGPT Score\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.title(\"CDF of DetectGPT Scores by Model\")\n",
    "plt.legend(title=\"Models\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "sbC0MkZSn7sO",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "outputId": "39df3671-0287-479c-dab4-9c281945916e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "gm4dDfR_1m7n"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}