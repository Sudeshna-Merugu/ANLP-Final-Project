text,label,source
"Since Erdős and Kleitman's pioneering work in 1971, the study of matchings within set families has remained a vibrant field of research. In this paper, we explore two key challenges in this domain: the maximum matching problem and the maximal matching problem. The maximum matching problem aims to identify the largest possible subset of edges that don't share any vertices. We've developed a novel algorithm that delivers a higher edge count in the matching than previous methods, rendering it more efficient for larger datasets. Meanwhile, the maximal matching problem focuses on finding the largest set of edges where no additional edges can be added without violating the matching rules. By examining the underlying combinatorial structures, we've introduced a novel metric known as the maximal matching degree. This metric allows us to establish a new upper limit on the maximal matching degree, contributing to our understanding of when maximal matchings are likely to exist. These findings build upon earlier research and offer fresh insights into the complexities of matchings within set families, with potential applications across computer science, operations research, and beyond.",1,AI
"The field of Artificial Intelligence (AI) and, in particular, the Machine Learning area, counts on a wide range of performance metrics and benchmark data sets to assess the problem-solving effectiveness of its solutions. However, the appearance of research centres, projects or institutions addressing AI solutions from a multidisciplinary and multi-stakeholder perspective suggests a new approach to assessment comprising ethical guidelines, reports or tools and frameworks to help both academia and business to move towards a responsible conceptualisation of AI. They all highlight the relevance of three key aspects: (i) enhancing cooperation among the different stakeholders involved in the design, deployment and use of AI; (ii) promoting multidisciplinary dialogue, including different domains of expertise in this process; and (iii) fostering public engagement to maximise a trusted relation with new technologies and practitioners. In this paper, we introduce the Observatory on Society and Artificial Intelligence (OSAI), an initiative grew out of the project AI4EU aimed at stimulating reflection on a broad spectrum of issues of AI (ethical, legal, social, economic and cultural). In particular, we describe our work in progress around OSAI and suggest how this and similar initiatives can promote a wider appraisal of progress in AI. This will give us the opportunity to present our vision and our modus operandi to enhance the implementation of these three fundamental dimensions.",0,Human
"In this paper, we present the design, implementation and our year-long maintenance experience of SNSAPI, a Python-based middleware which unifies the interfaces and data structures of heterogeneous Social Networking Services (SNS). Unlike most prior works, our middleware is user-oriented and requires zero infrastructure support. It enables a user to readily conduct online social activities in a programmable, cross-platform fashion while gradually reducing the dependence on centralized Online Social Networks (OSN). More importantly, as the SNSAPI middleware can be used to support decentralized social networking services via conventional communication channels such as RSS or Email, it enables the deployment of Decentralized Social Networks (DSN) in an incremental, ad hoc manner. To demonstrate the viability of such type of DSNs, we have deployed an experimental 6000-node SNSAPI-based DSN on PlanetLab and evaluate its performance by replaying traces of online social activities collected from a mainstream OSN. Our results show that, with only mild resource consumption, the SNSAPI-based DSN can achieve acceptable forwarding latency comparable to that of a centralized OSN. We also develop an analytical model to characterize the trade-offs between resource consumption and message forwarding delay in our DSN. Via 20 parameterized experiments on PlanetLab, we have found that the empirical measurement results match reasonably with the performance predicted by our analytical model.",0,Human
"Fueled by the rapid development of communication networks and sensors in portable devices, today many mobile users are invited by content providers to sense and send back real-time useful information (e.g., traffic observations and sensor data) to keep the freshness of the providers' content updates. However, due to the sampling cost in sensing and transmission, an individual may not have the incentive to contribute the real-time information to help a content provider reduce the age of information (AoI). Accordingly, we propose dynamic pricing for the provider to offer age-dependent monetary returns and encourage users to sample information at different rates over time. This dynamic pricing design problem needs to balance the monetary payments to users and the AoI evolution over time, and is challenging to solve especially under the incomplete information about users' arrivals and their private sampling costs. For analysis tractability, we linearize the nonlinear AoI evolution in the constrained dynamic programming problem, by approximating the dynamic AoI reduction as a time-average term and solving the approximate dynamic pricing in closed-form. Then, we estimate this approximate term based on Brouwer's fixed-point theorem. Finally, we provide the steady-state analysis of the optimized approximate dynamic pricing scheme for an infinite time horizon, and show that the pricing scheme can be further simplified to an e-optimal version without recursive computing over time.",0,Human
"This paper introduces Lamassu, a storage-efficient method for hosting encrypted data that keeps sensitive information secure on client devices. By integrating authenticated encryption with inline encryption, Lamassu maintains data confidentiality, integrity, and authenticity without needing extra storage for encrypted data. This innovative approach encrypts data as it's created, ensuring data remains safe from tampering during transfer and storage. The study thoroughly examines Lamassu's security features, offering a formal security proof and performance metrics that highlight its strengths in terms of storage efficiency, processing speed, and security. Additionally, the researchers explore diverse application scenarios, such as cloud storage, mobile devices, and network communications, illustrating how Lamassu can be practically applied. The findings suggest that Lamassu is a viable solution for efficient host-side encryption, with significant potential across multiple environments.",1,AI
"Successful design of human-in-the-loop control systems requires appropriate models for human decision makers. Whilst most paradigms adopted in the control systems literature hide the (limited) decision capability of humans, in behavioral economics individual decision making and optimization processes are well-known to be affected by perceptual and behavioral biases. Our goal is to enrich control engineering with some insights from behavioral economics research through exposing such biases in control-relevant settings. This paper addresses the following two key questions: 1) How do behavioral biases affect decision making? 2) What is the role played by feedback in human-in-the-loop control systems? Our experimental framework shows how individuals behave when faced with the task of piloting an UAV under risk and uncertainty, paralleling a real-world decision-making scenario. Our findings support the notion of humans in Cyberphysical Systems underlying behavioral biases regardless of -- or even because of -- receiving immediate outcome feedback. We observe substantial shares of drone controllers to act inefficiently through either flying excessively (overconfident) or overly conservatively (underconfident). Furthermore, we observe human-controllers to self-servingly misinterpret random sequences through being subject to a ""hot hand fallacy"". We advise control engineers to mind the human component in order not to compromise technological accomplishments through human issues.",0,Human
"This paper studies radio propagation mechanisms that impact handoffs, air interface design, beam steering, and MIMO for 5G mobile communication systems. Knife edge diffraction (KED) and a creeping wave linear model are shown to predict diffraction loss around typical building objects from 10 to 26 GHz, and human blockage measurements at 73 GHz are shown to fit a double knife-edge diffraction (DKED) model which incorporates antenna gains. Small-scale spatial fading of millimeter wave received signal voltage amplitude is generally Ricean-distributed for both omnidirectional and directional receive antenna patterns under both line-of-sight (LOS) and non-line-of-sight (NLOS) conditions in most cases, although the log-normal distribution fits measured data better for the omnidirectional receive antenna pattern in the NLOS environment. Small-scale spatial autocorrelations of received voltage amplitudes are shown to fit sinusoidal exponential and exponential functions for LOS and NLOS environments, respectively, with small decorrelation distances of 0.27 cm to 13.6 cm (smaller than the size of a handset) that are favorable for spatial multiplexing. Local area measurements using cluster and route scenarios show how the received signal changes as the mobile moves and transitions from LOS to NLOS locations, with reasonably stationary signal levels within clusters. Wideband mmWave power levels are shown to fade from 0.4 dB/ms to 40 dB/s, depending on travel speed and surroundings.",0,Human
"Let $G=(V,E)$ be an $n$-nodes non-negatively real-weighted undirected graph. In this paper we show how to enrich a {\em single-source shortest-path tree} (SPT) of $G$ with a \emph{sparse} set of \emph{auxiliary} edges selected from $E$, in order to create a structure which tolerates effectively a \emph{path failure} in the SPT. This consists of a simultaneous fault of a set $F$ of at most $f$ adjacent edges along a shortest path emanating from the source, and it is recognized as one of the most frequent disruption in an SPT. We show that, for any integer parameter $k \geq 1$, it is possible to provide a very sparse (i.e., of size $O(kn\cdot f^{1+1/k})$) auxiliary structure that carefully approximates (i.e., within a stretch factor of $(2k-1)(2|F|+1)$) the true shortest paths from the source during the lifetime of the failure. Moreover, we show that our construction can be further refined to get a stretch factor of $3$ and a size of $O(n \log n)$ for the special case $f=2$, and that it can be converted into a very efficient \emph{approximate-distance sensitivity oracle}, that allows to quickly (even in optimal time, if $k=1$) reconstruct the shortest paths (w.r.t. our structure) from the source after a path failure, thus permitting to perform promptly the needed rerouting operations. Our structure compares favorably with previous known solutions, as we discuss in the paper, and moreover it is also very effective in practice, as we assess through a large set of experiments.",0,Human
"The nuclear norm minimization (NNM) is commonly used to approximate the matrix rank by shrinking all singular values equally. However, the singular values have clear physical meanings in many practical problems, and NNM may not be able to faithfully approximate the matrix rank. To alleviate the above-mentioned limitation of NNM, recent studies have suggested that the weighted nuclear norm minimization (WNNM) can achieve a better rank estimation than NNM, which heuristically set the weight being inverse to the singular values. However, it still lacks a rigorous explanation why WNNM is more effective than NMM in various applications. In this paper, we analyze NNM and WNNM from the perspective of group sparse representation (GSR). Concretely, an adaptive dictionary learning method is devised to connect the rank minimization and GSR models. Based on the proposed dictionary, we prove that NNM and WNNM are equivalent to L1-norm minimization and the weighted L1-norm minimization in GSR, respectively. Inspired by enhancing sparsity of the weighted L1-norm minimization in comparison with L1-norm minimization in sparse representation, we thus explain that WNNM is more effective than NMM. By integrating the image nonlocal self-similarity (NSS) prior with the WNNM model, we then apply it to solve the image denoising problem. Experimental results demonstrate that WNNM is more effective than NNM and outperforms several state-of-the-art methods in both objective and perceptual quality.",0,Human
"With the growing demand for constant environmental monitoring, there's been a push to create energy-efficient sensor nodes that can harness energy from their surroundings. This paper introduces eico, an advanced device designed for long-range environmental monitoring. It integrates several innovative elements like an adaptable power management system that minimizes energy use, a flexible sensing mechanism that changes its frequency according to environmental activity, and a predictive machine learning model that forecasts energy availability and modulates the sensing frequency accordingly. Through various tests, we've shown that eico outperforms other similar devices by extending its operational lifespan and enhancing the precision of its readings. The findings indicate that eico could revolutionize how we monitor the environment, allowing for sustained and reliable data collection without excessive energy expenditure.",1,AI
"Semi-supervised Generative Adversarial Networks (ssGANs) have gained popularity in unsupervised learning within computer vision and natural language processing. Nevertheless, their use in regression tasks has been constrained by the challenge of creating an appropriate loss function. Our paper introduces a novel approach to extend ssGANs to regression problems by employing a feature contrasting loss function. This strategy efficiently utilizes both labeled and unlabeled data to develop a mapping between input and output spaces. We tested our method on several regression benchmarks and compared its effectiveness against other approaches. The outcomes indicate that our solution surpasses state-of-the-art techniques in terms of both precision and reliability. This study broadens the capabilities of ssGANs for regression issues and highlights their value in addressing practical challenges.",1,AI
"This paper introduces a new method for bidirectional updates on detailed medical records using blockchain technology. Current practices for managing medical data tend to be slow, prone to mistakes, and don't provide much transparency. The suggested system seeks to resolve these issues by utilizing blockchain's security and non-modifiable nature to facilitate quick and clear bidirectional updates on medical information. The system architecture includes a blockchain network, smart contracts, and a user interface. The smart contract outlines how medical data can be updated, specifying the criteria for who can access and modify the data. The user interface offers a simple and convenient method for patients and healthcare professionals to use the system and update medical records. Privacy is maintained through a granular access control system where patients can selectively share their records with various healthcare providers depending on their role. To keep sensitive data safe, the system uses encryption and decryption technologies. The researchers validated their idea by developing a practical prototype and performing tests to assess its speed and safety. The test results showed that the system operates efficiently, able to process many transactions every second. Additionally, the system is very secure due to its immutable nature, which prevents any changes to the medical data. In summary, this blockchain-based bidirectional update system represents an innovative approach for updating medical records effectively and transparently. Its strong security and privacy measures make it ideal for real-world healthcare environments, and its ability to manage a high volume of data transactions ensures its practicality.",1,AI
"This research introduces an innovative algorithm designed to determine ideal electricity rates in a smart microgrid system. The algorithm utilizes a multi-objective optimization model that takes into account both financial and operational aspects. The financial goal is to maximize the microgrid's earnings, whereas the operational aim is to minimize energy expenses for users. The proposed method updates electricity rates in real time using current data on demand and supply, including variables like renewable energy production, storage capabilities, and customer load management techniques. To assess its effectiveness, the algorithm was tested through simulations against conventional pricing strategies. The findings indicated that this approach enhanced the efficiency of the microgrid network and offered notable economic advantages to all parties involved—both the operator and consumers. This research advances the field of smart grids and offers a viable solution for optimizing electricity pricing in microgrid environments.",1,AI
"In game theory, farsighted collusion refers to when multiple players work together towards a shared objective, even if it means temporarily compromising their individual interests. This strategy often results in a superior collective outcome down the line. This idea has been explored across several disciplines, including economics, psychology, and computer science.

This paper delves into farsighted collusion within the context of the stable marriage problem—a foundational issue in algorithmic and discrete math. In this setup, we seek an arrangement that pairs two distinct groups so that no pair would rather be matched with each other than with their current partners. We investigate under what circumstances farsighted collusion might arise in these scenarios and show how it could result in more durable solutions. Our findings offer fresh perspectives on the influence of foresighted actions in matching situations and offer valuable guidance for designing algorithms and methods used in real-world matchmaking contexts.",1,AI
"This study introduces a fresh deep bilateral learning technique aimed at enhancing images in real time. The innovative method employs deep learning to improve the visual clarity of photos while maintaining essential elements like edges and patterns. It utilizes a bilateral filter to ensure consistency and edge preservation, alongside a sophisticated neural network to transform lower-resolution images into higher-quality versions. According to tests conducted using a standardized data set, this approach surpasses current top-tier image enhancement solutions in terms of both visual appeal and computational efficiency. The outcomes of this investigation might be quite influential across various sectors, including photography, computer vision, and image manipulation.",1,AI
"This research compares four off-policy prediction learning algorithms—Q-learning, SARSA, Expected SARSA, and Double Q-learning—to see which performs best when predicting collisions between agents in a simulated two-dimensional grid world. Our findings reveal that Expected SARSA generally yields the most accurate predictions in terms of both mean squared error and average reward. In contrast, Double Q-learning appears to struggle the most with these metrics. Importantly, our results highlight that the effectiveness of these algorithms varies greatly depending on the specific settings we use for their parameters. By examining how these differences manifest across various conditions, we hope to provide valuable insights for those researching or applying similar prediction methods, helping them choose the best approach for their particular needs.",1,AI
"Multi-view detection incorporates multiple camera views to alleviate occlusion in crowded scenes, where the state-of-the-art approaches adopt homography transformations to project multi-view features to the ground plane. However, we find that these 2D transformations do not take into account the object's height, and with this neglection features along the vertical direction of same object are likely not projected onto the same ground plane point, leading to impure ground-plane features. To solve this problem, we propose VFA, voxelized 3D feature aggregation, for feature transformation and aggregation in multi-view detection. Specifically, we voxelize the 3D space, project the voxels onto each camera view, and associate 2D features with these projected voxels. This allows us to identify and then aggregate 2D features along the same vertical line, alleviating projection distortions to a large extent. Additionally, because different kinds of objects (human vs. cattle) have different shapes on the ground plane, we introduce the oriented Gaussian encoding to match such shapes, leading to increased accuracy and efficiency. We perform experiments on multiview 2D detection and multiview 3D detection problems. Results on four datasets (including a newly introduced MultiviewC dataset) show that our system is very competitive compared with the state-of-the-art approaches. %Our code and data will be open-sourced.Code and MultiviewC are released at https://github.com/Robert-Mar/VFA.",0,Human
"This research focuses on developing an efficient object detection system for real-time use in UAV applications. As the use of UAVs expands across different sectors, there's a pressing requirement for robust and swift object detection methods suitable for real-time settings. The suggested solution leverages a blend of deep learning and computer vision principles, aiming to deliver precise and quick object identification in UAV contexts. The study involves thorough testing of this proposed method using a standardized data set, revealing that it excels over contemporary leading-edge object detection algorithms in areas such as precision and performance. Additionally, the model has been subjected to multiple real-world UAV applications, including monitoring and inspection activities, demonstrating its practical benefits and effectiveness. This contribution advances the domain of UAV object detection by introducing an exceptionally efficient and dependable system for real-time usage.",1,AI
"This research introduces an innovative technique for efficient blind compressed sensing in MRI. By employing sparsifying transforms, this method compresses data without losing critical information. Using these transforms minimizes the number of measurements needed, which speeds up the imaging process and reduces expenses. Furthermore, the proposed approach comes with convergence assurances, guaranteeing that the reconstructed image closely matches the original. The method's efficacy was tested using MRI data, showing enhanced results when compared to current techniques. This study pushes the boundaries of blind compressed sensing and could have practical implications across different imaging technologies.",1,AI
"This study introduces an innovative approach to combinatorial Bayesian optimization that utilizes the graph Cartesian product to tackle the difficulties associated with optimizing complex, multi-dimensional, non-separable, and discrete functions. We show that our technique can effectively explore the search space and outperform other methods in terms of efficiency and accuracy across various test cases. Furthermore, it's demonstrated that our method can handle large volumes of data, which makes it well-suited for practical applications in diverse fields. Overall, these findings advance the broader field of Bayesian optimization and could have far-reaching impacts on optimization tasks within machine learning and beyond.",1,AI
"Age-Related Macular Degeneration (AMD) is an asymptomatic retinal disease which may result in loss of vision. There is limited access to high-quality relevant retinal images and poor understanding of the features defining sub-classes of this disease. Motivated by recent advances in machine learning we specifically explore the potential of generative modeling, using Generative Adversarial Networks (GANs) and style transferring, to facilitate clinical diagnosis and disease understanding by feature extraction. We design an analytic pipeline which first generates synthetic retinal images from clinical images; a subsequent verification step is applied. In the synthesizing step we merge GANs (DCGANs and WGANs architectures) and style transferring for the image generation, whereas the verified step controls the accuracy of the generated images. We find that the generated images contain sufficient pathological details to facilitate ophthalmologists' task of disease classification and in discovery of disease relevant features. In particular, our system predicts the drusen and geographic atrophy sub-classes of AMD. Furthermore, the performance using CFP images for GANs outperforms the classification based on using only the original clinical dataset. Our results are evaluated using existing classifier of retinal diseases and class activated maps, supporting the predictive power of the synthetic images and their utility for feature extraction. Our code examples are available online.",0,Human
"This paper suggests an innovative method for label-guided graph exploration, allowing users to customize the proportion of labels utilized during the process. Label-guided navigation is widely employed to traverse through large and intricate graphs, where labels act as navigational aids directing exploration towards significant nodes. However, conventional techniques usually apply a predetermined ratio of labels, which might not always be ideal depending on specific situations. Our proposal permits users to modify this ratio according to their unique requirements and objectives. We develop a novel algorithm that adjusts the label ratio dynamically based on the current stage of exploration. This algorithm considers several aspects like graph density, label distribution, and user feedback to identify the most efficient label ratio. To test our method, we performed experiments using both actual and simulated data sets. The outcomes demonstrated that our approach excels over standard label-guided techniques in terms of performance and precision, especially when the optimal label ratio is uncertain or varies over time. Additionally, we evaluated the ease of use of our method through a user study, which was favorably received by participants. In summary, our label-guided graph exploration strategy with variable label ratios offers a versatile and efficient solution for navigating through vast and complicated graphs, applicable across diverse areas including social network analysis, recommendation systems, and bioinformatics.",1,AI
"This study introduces a novel technique for object re-identification (re-ID) by employing transformer-based models, which we call TransReID. The research utilizes the self-attention capability of transformers to extract long-distance relationships and overall context from feature maps. Experimental outcomes showcase that TransReID outperforms existing top-tier re-ID methodologies on various benchmark datasets. Additionally, the paper delves into how different aspects of the TransReID architecture affect its performance, such as varying the number of layers, altering the model size, and incorporating various data augmentation strategies. These analyses offer valuable insights into the efficiency of the proposed method and highlight its promising role in enhancing re-ID capabilities for real-world scenarios.",1,AI
"Despite the tremendous success of BitTorrent, its swarming system suffers from a fundamental limitation: lower or no availability of unpopular contents. Recently, Menasche et al. has shown that bundling is a promising solution to mitigate this availability problem; it improves the availability and reduces download times for unpopular contents by combining multiple files into a single swarm. There also have been studies on bundling strategies and performance issues in bundled swarms. In spite of the recent surge of interest in the benefits of and strategies for bundling, there are still little empirical grounding for understanding, describing, and modeling it. This is the first empirical study that measures and analyzes how prevalent contents bundling is in BitTorrent and how peers access the bundled contents, in comparison to the other non-bundled (i.e., single-filed) ones. To our surprise, we found that around 70% of BitTorrent swarms contain multiple files, which indicate that bundling has become widespread for contents sharing. We also show that the amount of bytes shared in bundled swarms is estimated to be around 85% out of all the BitTorrent contents logged in our datasets. Inspired from our findings, we raise and discuss three important research questions in the field of file sharing systems as well as future contents-oriented networking: i) bundling strategies, ii) bundling-aware sharing systems in BitTorrent, and iii) implications on content-oriented networking.",0,Human
"This research introduces a new technique for quickly calibrating brain tumor models through a geometry-sensitive neural solver. It utilizes deep learning to accurately map MRI images to the physical characteristics of tumors, taking into account their geometric shapes. The model is trained using a mix of simulated and actual patient data, showing substantial enhancements in both precision and speed over conventional optimization methods. This study offers a potentially groundbreaking approach to determining the physical traits of brain tumors from medical scans and could have practical implications in tailoring treatments and optimizing radiation therapy plans for individual patients.",1,AI
"This paper concentrates on the security aspects of IoT edge nodes within the realm of machine learning. It particularly examines the possible security threats that can arise in IoT edge nodes, which are devices placed near a network to gather and process data from other IoT devices. As machine learning becomes more prevalent in IoT, there's a rising worry about the security of these systems because machine learning models are susceptible to different kinds of attacks. The paper reviews various types of security threats that could target IoT edge nodes, such as inference attacks, evasion attacks, and poisoning attacks. Additionally, it discusses how these attacks affect the efficiency of machine learning algorithms and how they can be employed to compromise IoT network security. To tackle these security issues, the paper suggests several countermeasures that can protect against these threats. These countermeasures involve employing secure communication protocols, incorporating security measures into machine learning algorithms, and using secure hardware and software systems. The authors also touch upon the difficulties involved in successfully implementing these countermeasures and offer guidance for future research in this field. In summary, the paper offers an in-depth examination of the security risks posed by IoT edge nodes in the context of machine learning and provides practical strategies for safeguarding against these risks. The insights and suggestions provided in this paper can be utilized by practitioners and researchers to enhance the security of IoT networks and ensure the secure deployment of machine learning algorithms in IoT systems.",1,AI
"Millions of drivers worldwide have enjoyed financial benefits and work schedule flexibility through a ride-sharing economy, but meanwhile they have suffered from the lack of a sense of identity and career achievement. Equipped with social identity and contest theories, financially incentivized team competitions have been an effective instrument to increase drivers' productivity, job satisfaction, and retention, and to improve revenue over cost for ride-sharing platforms. While these competitions are overall effective, the decisive factors behind the treatment effects and how they affect the outcomes of individual drivers have been largely mysterious. In this study, we analyze data collected from more than 500 large-scale team competitions organized by a leading ride-sharing platform, building machine learning models to predict individual treatment effects. Through a careful investigation of features and predictors, we are able to reduce out-sample prediction error by more than 24%. Through interpreting the best-performing models, we discover many novel and actionable insights regarding how to optimize the design and the execution of team competitions on ride-sharing platforms. A simulated analysis demonstrates that by simply changing a few contest design options, the average treatment effect of a real competition is expected to increase by as much as 26%. Our procedure and findings shed light on how to analyze and optimize large-scale online field experiments in general.",0,Human
"This research examines how feature-based algorithm selection systems perform in choosing the most suitable algorithms for solving complex numerical optimization problems. It evaluates various selection methods by assessing their effectiveness in identifying the optimal algorithm for each specific problem. A variety of benchmark scenarios and multiple algorithms are utilized to test these selection methods. The results indicate that incorporating features can greatly enhance the efficiency of optimization algorithms, surpassing the use of a singular approach. The study uncovers the advantages and limitations of different selection strategies and underscores the necessity of developing relevant features for algorithm selection. These discoveries can help in creating improved algorithm selection systems for tackling black-box numerical optimization challenges.",1,AI
"As software systems grow increasingly intricate, maintaining their deployability and reliability has become a paramount concern. Techniques like record and replay have gained traction as a viable method for enhancing software deployability by capturing and repeating the system's execution logs within a regulated setting. Despite their efficacy, current record and replay methods face challenges related to scalability, precision, and operational efficiency. This extended technical report introduces an engineering framework for record and replay that addresses these shortcomings. Initially, we delineate the essential prerequisites for developing robust record and replay systems, focusing on supporting multi-tier applications, optimizing data gathering and storage, and minimizing overheads during both recording and playback phases. Following this, we describe our record and replay architecture, which leverages virtualization and distributed tracing to achieve superior scalability and accuracy. Furthermore, we incorporate innovative strategies to diminish overhead, such as adaptive sampling and selective replay. To validate our approach, we conducted experiments on various real-world applications, resulting in a system capable of accurately and efficiently capturing and reproducing intricate execution sequences. Our findings reveal that this system not only ensures consistent system behavior across diverse scenarios but also aids in pinpointing and resolving performance and reliability issues. Ultimately, our engineering strategy for record and replay offers a pragmatic solution for boosting software system deployability and reliability, particularly beneficial for vast, complex systems. This methodology is useful for software engineers and developers to troubleshoot and optimize system operations and ensure seamless performance across varied environments. Future endeavors will concentrate on integrating our record and replay system with standard software development and testing pipelines and investigate potential applications in domains like cybersecurity and regulatory compliance.",1,AI
"Reinforcement learning (RL) is a widely used technique for decision-making tasks, where agents learn through trial and error to maximize a reward signal. However, in practical scenarios, the reward signals often contain noise or variations, which can result in suboptimal outcomes or undesirable behaviors. This study investigates RL with perturbed rewards, examining two primary areas: (1) how these perturbations affect the learning process and the efficiency of RL algorithms, and (2) the development of resilient RL algorithms capable of managing such disturbances. Various types of reward perturbations, like Gaussian noise and random reward functions, along with specific attack strategies, are considered. The research also evaluates numerous RL approaches, including value-based, policy-based, and actor-critic techniques, across different perturbation conditions. Results indicate that reward perturbations can significantly influence both learning and algorithm performance, underscoring the necessity for creating robust RL algorithms that can handle these disturbances effectively. The findings offer valuable guidance for future research and have potential applications in fields such as robotics, gaming, autonomous systems, and beyond.",1,AI
"Current performance-driven building design methods are not widely adopted outside the research field for several reasons that make them difficult to integrate into a typical design process. In the early design phase, in particular, the time-intensity and the cognitive load associated with optimization and form parametrization are incompatible with design exploration, which requires quick iteration. This research introduces a novel method for performance-driven geometry generation that can afford interaction directly in the 3d modeling environment, eliminating the need for explicit parametrization, and is multiple orders faster than the equivalent form optimization. The method uses Machine Learning techniques to train a generative model offline. The generative model learns a distribution of optimal performing geometries and their simulation contexts based on a dataset that addresses the performance(s) of interest. By navigating the generative model's latent space, geometries with the desired characteristics can be quickly generated. A case study is presented, demonstrating the generation of a synthetic dataset and the use of a Variational Autoencoder (VAE) as a generative model for geometries with optimal solar gain. The results show that the VAE-generated geometries perform on average at least as well as the optimized ones, suggesting that the introduced method shows a feasible path towards more intuitive and interactive early-phase performance-driven design assistance.",0,Human
"This study introduces a novel generative learning technique for modeling spatial and temporal aspects in connected vehicle networks. By integrating advanced deep learning with generative models, the approach aims to accurately represent intricate patterns found within connected vehicle data. The methodology has been tested using real-world data and shown to outperform conventional machine learning algorithms. These findings suggest an innovative path forward for researching spatio-temporal modeling in connected vehicle networks, which could lead to advancements in traffic forecasting and management, as well as improvements in the development of smart transportation systems.",1,AI
"This research introduces an innovative technique for compressing categorical features in machine learning models through submodular optimization. Categorical data is prevalent in various real-world scenarios, yet its high dimensionality often causes substantial computational burdens and storage issues. To address this challenge, the suggested approach employs submodular optimization to identify a subset of key categories that best represent the essential characteristics of the dataset. Essentially, we formulate this as a submodular maximization issue and develop a greedy algorithm to quickly yield a nearly optimal result. Additionally, we establish theoretical assurance regarding the effectiveness of the algorithm's output. Empirical evaluations on multiple test sets show that our method outperforms existing approaches in terms of compression efficiency and model accuracy. Furthermore, this method is straightforward to incorporate into standard machine learning workflows, offering considerable improvements in both efficiency and scalability for models dealing with categorical data.",1,AI
"This study zeroes in on the challenge of choosing energy plans in markets where customers have the option to shop around. It introduces two competitive online algorithms for consumers to decide whether to stick with their current energy deal or change to another one. The ""Stay-or-Switch"" method considers past energy use and costs to find the best course of action. The ""Adaptive Stay-or-Switch"" variant tweaks its strategy as market conditions shift. Both algorithms are tested with real-world data from energy markets through simulations. The findings reveal that these methods significantly enhance cost efficiency for consumers compared to traditional strategies.",1,AI
"Volume transmission is an important neural communication pathway in which neurons in one brain region influence the neurotransmitter concentration in the extracellular space of a distant brain region. In this paper, we apply asymptotic analysis to a stochastic partial differential equation model of volume transmission to calculate the neurotransmitter concentration in the extracellular space. Our model involves the diffusion equation in a three-dimensional domain with interior holes that randomly switch between being either sources or sinks. These holes model nerve varicosities that alternate between releasing and absorbing neurotransmitter, according to when they fire action potentials. In the case that the holes are small, we compute analytically the first two nonzero terms in an asymptotic expansion of the average neurotransmitter concentration. The first term shows that the concentration is spatially constant to leading order and that this constant is independent of many details in the problem. Specifically, this constant first term is independent of the number and location of nerve varicosities, neural firing correlations, and the size and geometry of the extracellular space. The second term shows how these factors affect the concentration at second order. Interestingly, the second term is also spatially constant under some mild assumptions. We verify our asymptotic results by high-order numerical simulation using radial basis function-generated finite differences.",0,Human
"This study looks at the current situation of electronic health record (EHR) interoperability within Tanzanian hospitals and pinpoints the significant issues, obstacles, and areas for improvement needed to enhance EHR system integration in the country. Using a thorough literature review and discussions with healthcare experts and decision-makers, the researchers underscore the primary hindrances to EHR interoperability in Tanzania, such as a lack of standardized practices, insufficient technological resources, and a shortage of skilled personnel. They also discuss the possible advantages of bolstering EHR interoperability, like better patient care, fewer medical errors, and improved public health monitoring. Lastly, the researchers offer suggestions for policymakers, healthcare institutions, and tech companies to boost EHR interoperability in Tanzania, which include developing standards and protocols, investing in technology and infrastructure, and establishing training programs for medical professionals. This research offers valuable perspectives on EHR interoperability in Tanzania and will appeal to healthcare professionals, policymakers, and IT firms focused on health information technology.",1,AI
"A new method is introduced to bolster the security of Industry 4.0 systems by utilizing Quorum-Based Cloud-Edge Distributed Deep Neural Networks (QUDOS). With the rise of cloud-based applications and services in Industry 4.0, there's been an increase in the need for secure and efficient data processing. However, traditional security measures fall short in tackling the current complex threat landscape. To tackle this issue, the researchers propose QUDOS, which is a quorum-based system for securely executing deep neural networks in a hybrid cloud-edge computing setup. QUDOS harnesses the benefits of both cloud and edge computing to offer a robust and effective solution for data processing in Industry 4.0. By employing a quorum-based consensus technique, QUDOS ensures secure and decentralized execution of DNNs across various edge devices and the cloud, thereby minimizing risks associated with single points of failure and data breaches. Furthermore, QUDOS implements secure multi-party computation methods to safeguard sensitive data from being compromised during the DNN execution phase. The researchers have tested and validated the performance and security of QUDOS through various experiments and simulations, highlighting its capability to deliver high accuracy and minimal latency. Moreover, their findings indicate that QUDOS surpasses current top-tier security solutions in terms of both accuracy and latency, making it a compelling option for secure and efficient data processing in Industry 4.0 systems. Ultimately, the researchers advocate for QUDOS as a promising strategy for improving the security of Industry 4.0 systems, offering a secure and efficient platform for executing DNNs in a cloud-edge environment.",1,AI
"This research investigates how to best configure the constellation patterns for Non-Coherent Multiple-Input Multiple-Output (MIMO) Multiple-Access (MA) channels. The goal is to enhance the transmission efficiency by optimizing these constellation designs. A new method has been introduced that considers both the channel conditions and user pairings simultaneously. Simulations have demonstrated that this method yields better Bit Error Rates (BERs), particularly under strong signal-to-noise ratios (SNRs). Consequently, researchers believe this joint constellation design could be a valuable tool for enhancing performance in Non-Coherent MIMO MA channels, which could then be utilized in real-world communication networks.",1,AI
"Single-frame infrared small target (SIRST) detection aims at separating small targets from clutter backgrounds. With the advances of deep learning, CNN-based methods have yielded promising results in generic object detection due to their powerful modeling capability. However, existing CNN-based methods cannot be directly applied for infrared small targets since pooling layers in their networks could lead to the loss of targets in deep layers. To handle this problem, we propose a dense nested attention network (DNANet) in this paper. Specifically, we design a dense nested interactive module (DNIM) to achieve progressive interaction among high-level and low-level features. With the repeated interaction in DNIM, infrared small targets in deep layers can be maintained. Based on DNIM, we further propose a cascaded channel and spatial attention module (CSAM) to adaptively enhance multi-level features. With our DNANet, contextual information of small targets can be well incorporated and fully exploited by repeated fusion and enhancement. Moreover, we develop an infrared small target dataset (namely, NUDT-SIRST) and propose a set of evaluation metrics to conduct comprehensive performance evaluation. Experiments on both public and our self-developed datasets demonstrate the effectiveness of our method. Compared to other state-of-the-art methods, our method achieves better performance in terms of probability of detection (Pd), false-alarm rate (Fa), and intersection of union (IoU).",0,Human
"Chatter identification and detection in machining processes has been an active area of research in the past two decades. Part of the challenge in studying chatter is that machining equations that describe its occurrence are often nonlinear delay differential equations. The majority of the available tools for chatter identification rely on defining a metric that captures the characteristics of chatter, and a threshold that signals its occurrence. The difficulty in choosing these parameters can be somewhat alleviated by utilizing machine learning techniques. However, even with a successful classification algorithm, the transferability of typical machine learning methods from one data set to another remains very limited. In this paper we combine supervised machine learning with Topological Data Analysis (TDA) to obtain a descriptor of the process which can detect chatter. The features we use are derived from the persistence diagram of an attractor reconstructed from the time series via Takens embedding. We test the approach using deterministic and stochastic turning models, where the stochasticity is introduced via the cutting coefficient term. Our results show a 97% successful classification rate on the deterministic model labeled by the stability diagram obtained using the spectral element method. The features gleaned from the deterministic model are then utilized for characterization of chatter in a stochastic turning model where there are very limited analysis methods.",0,Human
"This research introduces an algorithm that can count triangles in networks with many connections efficiently using modern computer systems. By focusing on high-degree nodes that typically slow down traditional methods, this algorithm aims to improve both speed and accuracy. It's engineered to scale well, making it possible to process large networks within a manageable timeframe. Experimental data confirm that this algorithm surpasses current techniques in both speed and precision. These findings highlight the benefits of a parallel strategy for dealing with triangle counting in high-degree networks and offer fresh perspectives on developing effective algorithms for graph analysis.",1,AI
"This research examines how to classify audio signals into a few categories using minimal labeled data. It explores methods like transfer learning and meta-learning to tackle the issue of limited training sets. The investigation uses multiple test datasets and assesses models based on accuracy, precision, recall, and F1-score. Results show that these advanced methods enhance classification efficiency compared to traditional techniques. Additionally, the study looks at how factors like data quantity, category variety, and signal intricacy affect classification outcomes. Overall, this work offers crucial guidance for creating efficient few-shot audio classifiers, which could be used in areas like speech recognition, music analysis, and surveillance systems.",1,AI
"Unstructured data from diverse sources, such as social media and aerial imagery, can provide valuable up-to-date information for intelligent situation assessment. Mining these different information sources could bring major benefits to applications such as situation awareness in disaster zones and mapping the spread of diseases. Such applications depend on classifying the situation across a region of interest, which can be depicted as a spatial ""heatmap"". Annotating unstructured data using crowdsourcing or automated classifiers produces individual classifications at sparse locations that typically contain many errors. We propose a novel Bayesian approach that models the relevance, error rates and bias of each information source, enabling us to learn a spatial Gaussian Process classifier by aggregating data from multiple sources with varying reliability and relevance. Our method does not require gold-labelled data and can make predictions at any location in an area of interest given only sparse observations. We show empirically that our approach can handle noisy and biased data sources, and that simultaneously inferring reliability and transferring information between neighbouring reports leads to more accurate predictions. We demonstrate our method on two real-world problems from disaster response, showing how our approach reduces the amount of crowdsourced data required and can be used to generate valuable heatmap visualisations from SMS messages and satellite images.",0,Human
"This paper introduces and evaluates a knowledge graph method for examining software applications in social science research. It underscores the significance of grasping the tools and techniques employed in social scientific studies and how a knowledge graph can offer an all-encompassing and interconnected perspective on software utilization within this discipline. The authors delve into their findings, which encompass gathering and scrutinizing data from diverse channels such as scholarly articles, software platforms, and digital discourse spaces. They illustrate how the application of a knowledge graph can illuminate connections and interactions between software, researchers, and thematic areas, thereby facilitating the discovery of enhancements and advancements in software practices within social sciences. Ultimately, the authors assert that employing a knowledge graph technique is beneficial for comprehending software usage in this domain and serves as a solid basis for future exploration and advancement in related fields.",1,AI
"Knowledge Graphs (KGs) have been integrated in several models of recommendation to augment the informational value of an item by means of its related entities in the graph. Yet, existing datasets only provide explicit ratings on items and no information is provided about user opinions of other (non-recommendable) entities. To overcome this limitation, we introduce a new dataset, called the MindReader, providing explicit user ratings both for items and for KG entities. In this first version, the MindReader dataset provides more than 102 thousands explicit ratings collected from 1,174 real users on both items and entities from a KG in the movie domain. This dataset has been collected through an online interview application that we also release open source. As a demonstration of the importance of this new dataset, we present a comparative study of the effect of the inclusion of ratings on non-item KG entities in a variety of state-of-the-art recommendation models. In particular, we show that most models, whether designed specifically for graph data or not, see improvements in recommendation quality when trained on explicit non-item ratings. Moreover, for some models, we show that non-item ratings can effectively replace item ratings without loss of recommendation quality. This finding, thanks also to an observed greater familiarity of users towards common KG entities than towards long-tail items, motivates the use of KG entities for both warm and cold-start recommendations.",0,Human
"This research suggests utilizing neural networks to examine attack patterns and create effective strategies against possible threats. Recognizing and responding to attack patterns is crucial in cybersecurity, and this is a significant challenge for security experts. In our study, we introduce a method that combines machine learning principles to analyze and comprehend attack patterns. The proposed model uses both supervised and unsupervised learning methods to identify essential features within vast datasets of attack patterns. The neural network then utilizes these features to detect and classify potential risks, and provides practical solutions to minimize the damage caused by these risks. Our tests show that this proposed strategy can accurately analyze attack patterns and provide useful solutions, efficiently and reliably. This model could enhance cybersecurity measures by offering a thorough and automated approach to identifying and handling security threats.",1,AI
"This study investigates how catastrophic collisions and collision avoidance affect swarming behavior. It starts by explaining swarming patterns in both natural and artificial systems. Then, it examines different elements influencing swarming, such as catastrophic collisions and collision avoidance. The research includes simulations and tests showing how these factors influence swarming dynamics. Through analysis of these results, it concludes that catastrophic collisions and efficient collision avoidance are essential for successful swarming. Additionally, these elements play a crucial role in the overall efficiency of a swarm.",1,AI
"Existing color-guided depth super-resolution (DSR) approaches require paired RGB-D data as training samples where the RGB image is used as structural guidance to recover the degraded depth map due to their geometrical similarity. However, the paired data may be limited or expensive to be collected in actual testing environment. Therefore, we explore for the first time to learn the cross-modality knowledge at training stage, where both RGB and depth modalities are available, but test on the target dataset, where only single depth modality exists. Our key idea is to distill the knowledge of scene structural guidance from RGB modality to the single DSR task without changing its network architecture. Specifically, we construct an auxiliary depth estimation (DE) task that takes an RGB image as input to estimate a depth map, and train both DSR task and DE task collaboratively to boost the performance of DSR. Upon this, a cross-task interaction module is proposed to realize bilateral cross task knowledge transfer. First, we design a cross-task distillation scheme that encourages DSR and DE networks to learn from each other in a teacher-student role-exchanging fashion. Then, we advance a structure prediction (SP) task that provides extra structure regularization to help both DSR and DE networks learn more informative structure representations for depth recovery. Extensive experiments demonstrate that our scheme achieves superior performance in comparison with other DSR methods.",0,Human
"This study introduces a meta-transfer objective to help learn how to separate out the underlying causes. It's rooted in transfer learning, which leverages knowledge gained from tackling one challenge to enhance performance on similar yet distinct issues. The proposed method hopes to use this knowledge to better separate causes across various problems. Experiments tested this approach, and it was found to perform better than current techniques in both precision and broad applicability, thus proving its value in enhancing our ability to discern the essential factors involved.",1,AI
"Recently, the problem of inaccurate learning targets in crowd counting draws increasing attention. Inspired by a few pioneering work, we solve this problem by trying to predict the indices of pre-defined interval bins of counts instead of the count values themselves. However, an inappropriate interval setting might make the count error contributions from different intervals extremely imbalanced, leading to inferior counting performance. Therefore, we propose a novel count interval partition criterion called Uniform Error Partition (UEP), which always keeps the expected counting error contributions equal for all intervals to minimize the prediction risk. Then to mitigate the inevitably introduced discretization errors in the count quantization process, we propose another criterion called Mean Count Proxies (MCP). The MCP criterion selects the best count proxy for each interval to represent its count value during inference, making the overall expected discretization error of an image nearly negligible. As far as we are aware, this work is the first to delve into such a classification task and ends up with a promising solution for count interval partition. Following the above two theoretically demonstrated criterions, we propose a simple yet effective model termed Uniform Error Partition Network (UEPNet), which achieves state-of-the-art performance on several challenging datasets. The codes will be available at: https://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.",0,Human
"Low-precision representation of deep neural networks (DNNs) is critical for efficient deployment of deep learning application on embedded platforms, however, converting the network to low precision degrades its performance. Crucially, networks that are designed for embedded applications usually suffer from increased degradation since they have less redundancy. This is most evident for the ubiquitous MobileNet architecture which requires a costly quantization-aware training cycle to achieve acceptable performance when quantized to 8-bits. In this paper, we trace the source of the degradation in MobileNets to a shift in the mean activation value. This shift is caused by an inherent bias in the quantization process which builds up across layers, shifting all network statistics away from the learned distribution. We show that this phenomenon happens in other architectures as well. We propose a simple remedy - compensating for the quantization induced shift by adding a constant to the additive bias term of each channel. We develop two simple methods for estimating the correction constants - one using iterative evaluation of the quantized network and one where the constants are set using a short training phase. Both methods are fast and require only a small amount of unlabeled data, making them appealing for rapid deployment of neural networks. Using the above methods we are able to match the performance of training-based quantization of MobileNets at a fraction of the cost.",0,Human
"With the growing need for energy-efficient computing, different methods have been devised for allocating virtual machines (VMs) in cloud computing settings. This study introduces a novel strategy aimed at reducing the overall operational time of VMs while taking energy usage into account. By framing the issue as an optimization challenge, we created an algorithm that takes into consideration the load of VMs and the power consumption of the server hardware. This algorithm aims to distribute VMs efficiently so that energy use is evenly spread across all servers, alongside minimizing the collective operational hours of the VMs. To assess its effectiveness, we tested our solution on both actual and simulated data sets. The outcomes indicate that the suggested algorithm performs exceptionally well, not only by reducing the operational duration of VMs but also by substantially lowering energy consumption. The insights from this research are invaluable for developing more energy-efficient cloud computing infrastructures.",1,AI
"This paper delves into the connections between Artin automorphisms, cyclotomic function fields, and folded list-decodable codes. Artin automorphisms, which are essentially algebraic mappings significant in number theory and algebraic geometry, find application here. Cyclotomic function fields, formed by adding roots of unity to the field of rational numbers, provide a fertile ground for exploring these automorphisms. Folded list-decodable codes, an area of focus in communication engineering, benefit from the integration of Artin automorphisms, offering enhanced performance over conventional techniques. Our research indicates that by leveraging Artin automorphisms within cyclotomic function fields, we can create folded list-decodable codes that are more effective. These findings could pave the way for future advancements in the development of efficient error-correcting codes. Our work has implications in diverse domains including cryptography, communication engineering, and computer science. Supporting evidence comes from numerical simulations that confirm our theoretical insights and underscore the practical utility of our methodology. By examining this interplay, our study not only enriches our understanding of number theory and algebraic geometry but also contributes to coding theory.",1,AI
"This study examines the necessity of having a traceable process in neuroimaging research and suggests a method to enhance the reliability and consistency of the outcomes. Neuroimaging datasets are becoming increasingly complex and diverse, presenting challenges such as potential errors, inconsistencies, and insufficient documentation of procedures. The study emphasizes that a traceable and clear workflow is essential for safeguarding the accuracy and dependability of the research findings, and describes an integrated strategy to accomplish this objective. The suggested method incorporates current tools and best practices from software engineering and data management, with a particular emphasis on automation, version control, and metadata tracking. The study also assesses the proposed solution and highlights its advantages, which include enhanced productivity, minimized error risks, and better collaboration and dissemination of results. The research's conclusions have significant implications for the neuroimaging community and support ongoing efforts to elevate the credibility and transparency of scientific investigations.",1,AI
"Reinforcement Learning is proving a successful tool that can manage urban intersections with a fraction of the effort required to curate traditional traffic controllers. However, literature on the introduction and control of pedestrians to such intersections is scarce. Furthermore, it is unclear what traffic state variables should be used as reward to obtain the best agent performance. This paper robustly evaluates 30 different Reinforcement Learning reward functions for controlling intersections serving pedestrians and vehicles covering the main traffic state variables available via modern vision-based sensors. Some rewards proposed in previous literature solely for vehicular traffic are extended to pedestrians while new ones are introduced. We use a calibrated model in terms of demand, sensors, green times and other operational constraints of a real intersection in Greater Manchester, UK. The assessed rewards can be classified in 5 groups depending on the magnitudes used: queues, waiting time, delay, average speed and throughput in the junction. The performance of different agents, in terms of waiting time, is compared across different demand levels, from normal operation to saturation of traditional adaptive controllers. We find that those rewards maximising the speed of the network obtain the lowest waiting time for vehicles and pedestrians simultaneously, closely followed by queue minimisation, demonstrating better performance than other previously proposed methods.",0,Human
"The ability to segment teeth precisely from digitized 3D dental models is an essential task in computer-aided orthodontic surgical planning. To date, deep learning based methods have been popularly used to handle this task. State-of-the-art methods directly concatenate the raw attributes of 3D inputs, namely coordinates and normal vectors of mesh cells, to train a single-stream network for fully-automated tooth segmentation. This, however, has the drawback of ignoring the different geometric meanings provided by those raw attributes. This issue might possibly confuse the network in learning discriminative geometric features and result in many isolated false predictions on the dental model. Against this issue, we propose a two-stream graph convolutional network (TSGCNet) to learn multi-view geometric information from different geometric attributes. Our TSGCNet adopts two graph-learning streams, designed in an input-aware fashion, to extract more discriminative high-level geometric representations from coordinates and normal vectors, respectively. These feature representations learned from the designed two different streams are further fused to integrate the multi-view complementary information for the cell-wise dense prediction task. We evaluate our proposed TSGCNet on a real-patient dataset of dental models acquired by 3D intraoral scanners, and experimental results demonstrate that our method significantly outperforms state-of-the-art methods for 3D shape segmentation.",0,Human
"This paper introduces a method using recurrence relations to evaluate the effective capacity of retransmission strategies in wireless communication networks. The effective capacity refers to the highest achievable data transfer rate when there are strict limitations on transmission delays. The proposed technique offers a computational means to determine effective capacity for various retransmission methods, such as Automatic Repeat Request (ARQ) and Hybrid Automatic Repeat Request (HARQ). The method's effectiveness is confirmed by both numerical analysis and simulation results, showcasing its precision and performance. These findings offer significant contributions to the optimization of retransmission schemes and serve as a foundational resource for ongoing research in wireless communications.",1,AI
"With the increasing interest in the content creation field in multiple sectors such as media, education, and entertainment, there is an increasing trend in the papers that uses AI algorithms to generate content such as images, videos, audio, and text. Generative Adversarial Networks (GANs) in one of the promising models that synthesizes data samples that are similar to real data samples. While the variations of GANs models, in general, have been covered to some extent in several survey papers, to the best of our knowledge, this is among the first survey papers that reviews the state-of-the-art video GANs models. This paper first categorized GANs review papers into general GANs review papers, image GANs review papers, and special field GANs review papers such as anomaly detection, medical imaging, or cybersecurity. The paper then summarizes the main improvements in GANs frameworks that are not initially developed for the video domain but have been adopted in multiple video GANs variations. Then, a comprehensive review of video GANs models is provided under two main divisions according to the presence or non-presence of a condition. The conditional models then further grouped according to the type of condition into audio, text, video, and image. The paper is concluded by highlighting the main challenges and limitations of the current video GANs models. A comprehensive list of datasets, applied loss functions, and evaluation metrics is provided in the supplementary material.",0,Human
"The IEEE Computational Intelligence and Games (CIG) 2017 Game Data Mining Competition sought to identify the most effective method for forecasting player actions within a video game. This paper details the winning strategy and its analysis. The winning technique employed machine learning methods, particularly focusing on gradient boosting and deep neural networks. The results indicated that the winning strategy significantly outperformed other participants in terms of predictive accuracy. This research underscores the capability of integrating machine learning algorithms to anticipate player behavior in video games and underscores the significance of selecting optimal features and designing appropriate models. Additionally, the authors share their thoughts on the limitations and hurdles encountered during the process and offer suggestions for further investigation.",1,AI
"This research introduces a fresh method for grasping the causal connections within a system via a categorical semantic framework. This framework leverages category theory, a mathematical discipline, to delineate the interactions between variables in a system and the outcomes resulting from altering one variable. A key contribution is the development of a categorical causal model that offers a streamlined and understandable approach to discerning causal links among variables. Additionally, the study illustrates how this framework can be beneficial across various fields such as physics, biology, and social sciences by providing concrete examples. The findings suggest that this categorical semantic framework is an effective tool for codifying and comprehending causal relationships in a coherent and consistent manner.",1,AI
"This study investigates the concept of real-time content delivery in broadcasting systems. Real-time transmission contrasts with pre-recorded or on-demand options. The paper traces the development of the idea of real-time since the advent of digital media and the growing influence of social media. It reviews existing literature from various fields, blending theoretical insights with practical research findings. Additionally, the study offers fresh perspectives on how diverse viewers perceive and interpret real-time broadcasts, employing both qualitative and quantitative methodologies. The results indicate that what constitutes real-time isn't constant but changes based on cultural, technological, and social contexts. Furthermore, different groups of viewers might have varying expectations and understandings of real-time depending on the kind of content being shown and where it's consumed. Ultimately, the paper asserts that real-time remains vital for broadcasting, especially during live events and urgent news updates. However, with digital media constantly advancing, we may need to reconsider what 'real-time' means to accommodate newer ways of engaging and interacting.",1,AI
"This research introduces a deep graph metric learning method to enhance the performance and efficiency of systems that identify objects through proxy examples. Current methods often require numerous proxy examples to accurately depict different object types, which can be computationally expensive and limit scalability. In contrast, our ""Fewer is More"" technique uses a deep graph metric learning model to create a succinct proxy representation. By utilizing the connections between proxy examples, this method creates a compressed feature space that retains important object characteristics using fewer examples. Our tests on various evaluation sets show that our solution achieves better results than conventional proxy-based systems, particularly in terms of accuracy and speed, while also needing less data. This work suggests a promising path towards building more effective and resource-efficient object recognition frameworks.",1,AI
"This research delves into efficient labeling algorithms for tackling the adjacent quadratic shortest path problem. The researchers examine the effectiveness of different labeling techniques, including A* and Dijkstra algorithms, and suggest a new hybrid method that integrates features from both approaches. Experimental outcomes indicate that the suggested hybrid algorithm is superior in terms of both speed and quality compared to current solutions. The insights gained from this investigation could lead to the creation of even more effective algorithms for addressing the adjacent quadratic shortest path problem and potentially impact areas like transportation, logistics, and network design.",1,AI
"CNNs have excelled at performing place recognition over time, particularly when the neural network is optimized for localization in the current environmental conditions. In this paper we investigate the concept of feature map filtering, where, rather than using all the activations within a convolutional tensor, only the most useful activations are used. Since specific feature maps encode different visual features, the objective is to remove feature maps that are detract from the ability to recognize a location across appearance changes. Our key innovation is to filter the feature maps in an early convolutional layer, but then continue to run the network and extract a feature vector using a later layer in the same network. By filtering early visual features and extracting a feature vector from a higher, more viewpoint invariant later layer, we demonstrate improved condition and viewpoint invariance. Our approach requires image pairs for training from the deployment environment, but we show that state-of-the-art performance can regularly be achieved with as little as a single training image pair. An exhaustive experimental analysis is performed to determine the full scope of causality between early layer filtering and late layer extraction. For validity, we use three datasets: Oxford RobotCar, Nordland, and Gardens Point, achieving overall superior performance to NetVLAD. The work provides a number of new avenues for exploring CNN optimizations, without full re-training.",0,Human
"A new method called Criss-Cross Attention Network (CCNet) is introduced in the paper, focusing on semantic segmentation. This innovative technique leverages the criss-cross attention mechanism, known for its success in various computer vision applications. In CCNet, this mechanism helps capture long-distance relationships between pixels, which is crucial for accurate segmentation. The network structure includes a feature extraction backbone, a criss-cross attention component, and a segmentation component. It employs both spatial and channel-wise attention to understand how different pixels interact with each other. Experiments on multiple test sets have shown that CCNet performs exceptionally well, outperforming existing models with its streamlined design. These findings indicate that the criss-cross attention mechanism could be an effective solution for semantic segmentation and potentially applied to other computer vision challenges.",1,AI
"This research evaluates two popular techniques used to solve vehicle routing problems (VRPs): sequence optimization and set optimization. Sequence optimization focuses on rearranging the visit order for each vehicle, whereas set optimization divides the set of customers into distinct groups served by separate vehicles. To assess these methods, we performed computational tests on several standard VRP datasets. The efficiency of both strategies was evaluated across various problem dimensions and setups, including varying sizes and configurations. Our findings reveal that the suitability of each method hinges on the specific attributes of the problem at hand. For smaller to mid-sized scenarios where customer needs are broadly similar, sequence optimization proves advantageous. Conversely, for larger, more intricate cases with diverse customer requirements and multiple distribution centers, set optimization demonstrates superior performance. Notably, a combination of both techniques appears to offer promising results for certain VRP types. In summary, our study illuminates the comparative merits and drawbacks of distinct VRP-solving algorithms and suggests best practices for choosing the optimal strategy for specific VRP challenges.",1,AI
"Permutations are utilized across multiple disciplines including cryptography, computer science, and mathematics. Recognizing the connections between permutations is crucial for numerous applications, yet pinpointing these relationships can be quite complex. This research delves into how to efficiently test the relationships between permutations. We concentrate on verifying if two permutations are identical, inverses, or conjugates of one another, using only certain fixed points and their images and inverse images. We've developed effective algorithms for these tests, demonstrating that the number of queries required is modest. The methods we employ integrate elements of group theory and graph theory, with our analysis incorporating concepts from probability combinatorics and algebraic geometry. Furthermore, we establish lower limits on the number of queries necessary for these tests, proving that our algorithms are nearly optimal within constant factors. These findings have significant implications for the investigation of permutation groups and their practical applications in computer science and cryptography.",1,AI
"Explanatory machine learning involves using algorithms to comprehend and explain the outcomes predicted by machine learning models. These algorithms offer substantial value in terms of offering detailed insights into model decision-making processes. However, improper usage could lead to adverse effects. This piece delves into both the positive and negative impacts of explanatory machine learning, particularly focusing on the ethical dimensions associated with its application. From a beneficial standpoint, explanatory machine learning enhances transparency and responsibility in decision-making. By delivering straightforward explanations about how models reach their conclusions, it enables stakeholders to grasp and rely on the decisions made by these models, thereby improving effectiveness and efficiency. Nevertheless, several concerns need addressing. If trained on biased data, explanatory machine learning might perpetuate existing biases and exacerbate discriminatory practices. Furthermore, the explanations generated by these algorithms can be deceptive, potentially causing incorrect decisions due to incomplete or inaccurate information. Given the profound ethical considerations involved, it's crucial for practitioners to weigh these factors carefully when developing and implementing these algorithms. Ultimately, this paper urges for continued research and discourse to harness the advantages of explanatory machine learning while mitigating its possible downsides.",1,AI
"This study looks at how bisimulation and modal logics are used as proof search specifications for bisimulation and modal logics in the π-calculus, a formal system used to model the dynamic behavior of distributed systems. The paper delves into the use of these logical frameworks to ensure the correctness of π-calculus models. It provides a thorough examination of the proof search algorithms and how they're applied within the π-calculus framework. The research shows that employing bisimulation and modal logics as proof search specifications significantly enhances the verification process of π-calculus models. These findings could be crucial for improving the development and deployment of verification tools designed for managing distributed systems.",1,AI
"We consider Bayesian optimization of the output of a network of functions, where each function takes as input the output of its parent nodes, and where the network takes significant time to evaluate. Such problems arise, for example, in reinforcement learning, engineering design, and manufacturing. While the standard Bayesian optimization approach observes only the final output, our approach delivers greater query efficiency by leveraging information that the former ignores: intermediate output within the network. This is achieved by modeling the nodes of the network using Gaussian processes and choosing the points to evaluate using, as our acquisition function, the expected improvement computed with respect to the implied posterior on the objective. Although the non-Gaussian nature of this posterior prevents computing our acquisition function in closed form, we show that it can be efficiently maximized via sample average approximation. In addition, we prove that our method is asymptotically consistent, meaning that it finds a globally optimal solution as the number of evaluations grows to infinity, thus generalizing previously known convergence results for the expected improvement. Notably, this holds even though our method might not evaluate the domain densely, instead leveraging problem structure to leave regions unexplored. Finally, we show that our approach dramatically outperforms standard Bayesian optimization methods in several synthetic and real-world problems.",0,Human
"This paper presents a comprehensive survey of existing authentication and privacy-preserving schemes for 4G and 5G cellular networks. We start by providing an overview of existing surveys that deal with 4G and 5G communications, applications, standardization, and security. Then, we give a classification of threat models in 4G and 5G cellular networks in four categories, including, attacks against privacy, attacks against integrity, attacks against availability, and attacks against authentication. We also provide a classification of countermeasures into three types of categories, including, cryptography methods, humans factors, and intrusion detection methods. The countermeasures and informal and formal security analysis techniques used by the authentication and privacy preserving schemes are summarized in form of tables. Based on the categorization of the authentication and privacy models, we classify these schemes in seven types, including, handover authentication with privacy, mutual authentication with privacy, RFID authentication with privacy, deniable authentication with privacy, authentication with mutual anonymity, authentication and key agreement with privacy, and three-factor authentication with privacy. In addition, we provide a taxonomy and comparison of authentication and privacy-preserving schemes for 4G and 5G cellular networks in form of tables. Based on the current survey, several recommendations for further research are discussed at the end of this paper.",0,Human
"This study examines how the distribution of cryptocurrency holdings among users aligns with or contrasts with principles of egalitarianism through a quantitative analysis. We looked at transaction data from popular cryptocurrencies like Bitcoin, Ethereum, and Litecoin. To measure wealth inequality, we calculated a Gini coefficient for each. Additionally, we examined how much wealth is concentrated among the wealthiest segments of users and evaluated how different aspects, such as mining and trading activity, influence the level of egalitarianism within these ecosystems. Our results indicate that although cryptocurrencies are often seen as decentralized and democratic, they often exhibit significant wealth disparities. However, we highlight some ways to potentially enhance equality, such as addressing wealth concentration and encouraging fair mining practices. In summary, our work illuminates how closely cryptocurrencies adhere to egalitarian ideals and offers guidance on fostering a more equitable wealth distribution in this growing financial landscape.",1,AI
"This research introduces precise limits for randomized load balancing across various network configurations. Load balancing is crucial in distributed computing, and random methods have become popular for tackling this challenge. Yet, past studies offered less precise estimates for how well these random algorithms perform under diverse network setups. To bridge this knowledge gap, we've developed a fresh analytical approach that allows us to establish tight limits on the average highest workload any single component might handle. This approach leverages an innovative application of the well-known balls-and-bins model, enabling us to gauge the likelihood of any particular bin holding too many items. By applying this finding, we derive tight limits on the average highest workload, applicable regardless of the network's structure. Additionally, we validate our method through experiments showing its ability to accurately forecast the behavior of random load balancing in different network topologies, including actual internet networks. These findings shed light on the performance of random load balancing strategies across various network types, providing valuable guidance for designing and analyzing distributed systems that require load balancing.",1,AI
"Cyclus archetypes play a crucial role in nuclear fuel cycle analysis. This paper offers an extensive look at the various cyclus archetypes that have been suggested and utilized in numerous studies. It starts by defining cyclus archetypes and highlighting their significance within this field. Next, it gives a detailed overview of the different archetypes, such as the once-through, closed, and open types. The paper also examines the advantages and disadvantages of each archetype, along with the constraints and underlying assumptions that need to be taken into account when applying them. Finally, the paper reflects on the implications of these findings for the future of nuclear fuel cycle analysis and the advancement of sustainable nuclear energy systems.",1,AI
"This study introduces an improved method for a quantum sealed-bid auction that includes a multiparty circular quantum key agreement. The new approach aims to overcome the security flaws found in conventional sealed-bid auctions, which can be exploited through techniques like bid tampering and data leaks. Quantum cryptography is leveraged to bolster the security of the auction process. The multiparty circular quantum key agreement helps safeguard the integrity of the auction by guaranteeing secure communication among all participants and maintaining the confidentiality of their bids. Simulation and experimental results show that the proposed method is effective and secure, presenting a viable option for running secure electronic auctions. These results underscore the significance of this research in enhancing the security of electronic auction systems.",1,AI
"Compile-time information flow analysis has been a promising technique for protecting confidentiality and integrity of private data. In the last couple of decades, a large number of information flow security tools in the form of run-time execution-monitors or static type systems have been developed for programming languages to analyze information flow security policies. However, existing flow analysis tools lack in precision and usability, which is the primary reason behind not being widely adopted in real application development. In this paper, we propose a compile-time information flow analysis for an imperative program based on a hybrid (mutable + immutable) labelling approach that enables a user to detect information flow-policy breaches and modify the program to overcome violations. We have developed an information flow security analyzer for a dialect of Python language, PyX, called Pifthon using the said approach. The flow-analyzer aids in identifying possible misuse of the information in sequential PyX programs corresponding to a given information flow policy (IFP). Pifthon has distinct advantages like reduced labelling overhead that ameliorates usability, covers a wide range of PyX programs that include termination-and progress-sensitive channels, in contrast to other approaches in the literature. The proposed flow analysis is proved to be sound under the classical non-interference property. Further, case study and experience in the usage of Pifthon are provided.",0,Human
"This study offers an extensive review of recent developments in subpath queries conducted on compressed graphs. It starts by highlighting the significance of graph compression and its utility across different fields. The investigation explores several compression techniques such as adjacency lists, matrices, and multi-level graph decompositions. The core of the study revolves around subpath query algorithms that function on these compressed structures, examining how these methods balance execution speed and memory usage. The analysis encompasses both precise and approximate algorithms, along with performance assessments using actual and simulated data sets. Ultimately, the research reflects on ongoing research challenges and suggests potential areas for future exploration in this area.",1,AI
"This research investigates how to address privacy disputes among multiple users on social media platforms. Given the large number of users and the extensive personal information they share, these platforms are increasingly vulnerable to privacy breaches. Therefore, there's a rising demand for strategies to manage these conflicts effectively. The study introduces a privacy conflict resolution model that takes into account the privacy settings of all parties involved in a dispute. This model integrates concepts from game theory, computational social choice, and privacy-enhancing technologies to find a balanced outcome that satisfies everyone. Additionally, the research includes a set of experiments to test the proposed model and showcase its ability to handle privacy issues in a simulated social media environment. Ultimately, this work aids in creating more secure social media environments and offers valuable advice for those who design and regulate such platforms.",1,AI
"We study exploration in stochastic multi-armed bandits when we have access to a divisible resource that can be allocated in varying amounts to arm pulls. We focus in particular on the allocation of distributed computing resources, where we may obtain results faster by allocating more resources per pull, but might have reduced throughput due to nonlinear scaling. For example, in simulation-based scientific studies, an expensive simulation can be sped up by running it on multiple cores. This speed-up however, is partly offset by the communication among cores, which results in lower throughput than if fewer cores were allocated per trial to run more trials in parallel. In this paper, we explore these trade-offs in two settings. First, in a fixed confidence setting, we need to find the best arm with a given target success probability as quickly as possible. We propose an algorithm which trades off between information accumulation and throughput and show that the time taken can be upper bounded by the solution of a dynamic program whose inputs are the gaps between the sub-optimal and optimal arms. We also prove a matching hardness result. Second, we present an algorithm for a fixed deadline setting, where we are given a time deadline and need to maximize the probability of finding the best arm. We corroborate our theoretical insights with simulation experiments that show that the algorithms consistently match or outperform baseline algorithms on a variety of problem instances.",0,Human
"Vulnerable software represents a tremendous threat to modern information systems. Vulnerabilities in widespread applications may be used to spread malware, steal money and conduct target attacks. To address this problem, developers and researchers use different approaches of dynamic and static software analysis; one of these approaches is called fuzzing. Fuzzing is performed by generating and sending potentially malformed data to an application under test. Since first appearance in 1988, fuzzing has evolved a lot, but issues which addressed to effectiveness evaluation have not fully investigated until now. In our research, we propose a novel approach of fuzzing effectiveness evaluation, taking into account semantics of executed code along with a quantitative assessment. For this purpose, we use specific metrics of source code complexity assessment adapted to perform analysis of machine code. We conducted effectiveness evaluation of these metrics on 104 widespread applications with known vulnerabilities. As a result of these experiments, we were able to identify a set of metrics that are more suitable to find bugs. In addition, we conducted separate experiments on 7 applications without known vulnerabilities by using the set of metrics. The experimental results confirmed that proposed approach can be applied to increase performance of the fuzzing. Moreover, the tools helped detect two critical zero day (previously unknown) vulnerabilities in the wide-spread applications.",0,Human
"Distributed storage systems employ codes to provide resilience to failure of multiple storage disks. Specifically, an $(n, k)$ MDS code stores $k$ symbols in $n$ disks such that the overall system is tolerant to a failure of up to $n-k$ disks. However, access to at least $k$ disks is still required to repair a single erasure. To reduce repair bandwidth, array codes are used where the stored symbols or packets are vectors of length $\ell$. MDS array codes have the potential to repair a single erasure using a fraction $1/(n-k)$ of data stored in the remaining disks. We introduce new methods of analysis which capitalize on the translation of the storage system problem into a geometric problem on a set of operators and subspaces. In particular, we ask the following question: for a given $(n, k)$, what is the minimum vector-length or sub-packetization factor $\ell$ required to achieve this optimal fraction? For \emph{exact recovery} of systematic disks in an MDS code of low redundancy, i.e. $k/n > 1/2$, the best known explicit codes \cite{WTB12} have a sub-packetization factor $\ell$ which is exponential in $k$. It has been conjectured \cite{TWB12} that for a fixed number of parity nodes, it is in fact necessary for $\ell$ to be exponential in $k$. In this paper, we provide a new log-squared converse bound on $k$ for a given $\ell$, and prove that $k \le 2\log_2\ell\left(\log_{\delta}\ell+1\right)$, for an arbitrary number of parity nodes $r = n-k$, where $\delta = r/(r-1)$.",0,Human
"This study introduces a new deep learning framework for traffic prediction called CDGNet. CDGNet is designed to handle both spatial and temporal relationships within traffic data by leveraging a dynamic graph convolutional neural network for capturing regional connections and a recurrent neural network with a time-attention mechanism for recognizing temporal trends. To assess the effectiveness of CDGNet, it was tested against real-world traffic data using established benchmarks and compared against leading traffic prediction methods. The findings indicated that CDGNet significantly outperformed existing models in terms of both short-term and long-term accuracy in traffic forecasting. By adeptly integrating intricate spatial-temporal dynamics in traffic information, CDGNet produces reliable predictions efficiently. Ultimately, this research suggests an innovative method for traffic prediction that could be crucial for optimizing traffic management, urban planning, and smart transportation solutions. CDGNet holds potential to enhance transportation infrastructure, alleviate congestion, increase safety, and elevate living standards in cities.",1,AI
"The rapid growth of the Internet of Things (IoT) has led to billions of devices connecting to the internet. However, this expansion introduces significant security issues due to the limited resources and weak security features of many IoT gadgets. To tackle these problems, the OSCAR (Object Security Architecture for the IoT) was developed. This lightweight architecture utilizes object-oriented security strategies to safeguard IoT devices against diverse types of attacks. The OSCAR design includes three key elements: object modeling, object security, and object communication. Object modeling describes the structure and functions of IoT objects along with their security features. Object security aims to shield these objects and their characteristics from potential security threats. Lastly, object communication sets out the protocols and procedures for secure interactions between IoT objects. Performance evaluations based on various factors such as memory and processing costs, communication delays, and energy usage indicate that OSCAR offers solid security protection while requiring little extra resource consumption. It supports essential security measures like confidentiality, integrity, authentication, and access management. Compared to other IoT security frameworks, OSCAR emerges as a superior choice in terms of both security strength and efficiency. Moreover, OSCAR has been successfully tested on multiple IoT devices, including sensors, actuators, and gateways, showcasing its practical application and effectiveness. In summary, the OSCAR architecture represents an efficient and adaptable approach to securing IoT devices, applicable across different IoT systems and networks. By leveraging object-oriented security techniques, OSCAR ensures robust security without imposing substantial overhead demands.",1,AI
"This research introduces a multi-class classification model for detecting vulnerabilities in smart contracts employing an Attention-based Bi-directional Long Short-Term Memory (AWD-LSTM) network, complemented by a pre-trained encoder derived from natural language processing. The primary objective is to enhance the accuracy in identifying various types of vulnerabilities within smart contract code, a key aspect for safeguarding the security and dependability of decentralized applications. Utilizing a pre-trained encoder from natural language processing, the model extracts features from smart contract code, while the AWD-LSTM network performs the classification task. Experimental outcomes demonstrate that the proposed method surpasses current techniques and offers a viable solution for classifying vulnerabilities across multiple categories in smart contracts.",1,AI
"Recent advances in artificial intelligence (AI) and machine learning have created a general perception that AI could be used to solve complex problems, and in some situations over-hyped as a tool that can be so easily used. Unfortunately, the barrier to realization of mass adoption of AI on various business domains is too high because most domain experts have no background in AI. Developing AI applications involves multiple phases, namely data preparation, application modeling, and product deployment. The effort of AI research has been spent mostly on new AI models (in the model training stage) to improve the performance of benchmark tasks such as image recognition. Many other factors such as usability, efficiency and security of AI have not been well addressed, and therefore form a barrier to democratizing AI. Further, for many real world applications such as healthcare and autonomous driving, learning via huge amounts of possibility exploration is not feasible since humans are involved. In many complex applications such as healthcare, subject matter experts (e.g. Clinicians) are the ones who appreciate the importance of features that affect health, and their knowledge together with existing knowledge bases are critical to the end results. In this paper, we take a new perspective on developing AI solutions, and present a solution for making AI usable. We hope that this resolution will enable all subject matter experts (eg. Clinicians) to exploit AI like data scientists.",0,Human
This research examines how to automatically spot security-related conversations within microservice systems. It combines findings from both industry surveys and experimental tests to offer a thorough look at the present situation in this area. Industry experts were surveyed to understand their views on the significance and difficulties associated with securing microservices. The experiments delve into creating and assessing a machine learning model designed to automatically recognize these security discussions. The study's outcomes offer valuable information about the current security landscape in microservices and highlight the potential for further advancements through automated identification of security talks.,1,AI
"Several cybersecurity domains, such as ransomware detection, forensics and data analysis, require methods to reliably identify encrypted data fragments. Typically, current approaches employ statistics derived from byte-level distribution, such as entropy estimation, to identify encrypted fragments. However, modern content types use compression techniques which alter data distribution pushing it closer to the uniform distribution. The result is that current approaches exhibit unreliable encryption detection performance when compressed data appears in the dataset. Furthermore, proposed approaches are typically evaluated over few data types and fragment sizes, making it hard to assess their practical applicability. This paper compares existing statistical tests on a large, standardized dataset and shows that current approaches consistently fail to distinguish encrypted and compressed data on both small and large fragment sizes. We address these shortcomings and design EnCoD, a learning-based classifier which can reliably distinguish compressed and encrypted data. We evaluate EnCoD on a dataset of 16 different file types and fragment sizes ranging from 512B to 8KB. Our results highlight that EnCoD outperforms current approaches by a wide margin, with accuracy ranging from ~82 for 512B fragments up to ~92 for 8KB data fragments. Moreover, EnCoD can pinpoint the exact format of a given data fragment, rather than performing only binary classification like previous approaches.",0,Human
"In system identification, estimating parameters of a model using limited observations results in poor identifiability. To cope with this issue, we propose a new method to simultaneously select and estimate sensitive parameters as key model parameters and fix the remaining parameters to a set of typical values. Our method is formulated as a nonlinear least squares estimator with L1-regularization on the deviation of parameters from a set of typical values. First, we provide consistency and oracle properties of the proposed estimator as a theoretical foundation. Second, we provide a novel approach based on Levenberg-Marquardt optimization to numerically find the solution to the formulated problem. Third, to show the effectiveness, we present an application identifying a biomechanical parametric model of a head position tracking task for 10 human subjects from limited data. In a simulation study, the variances of estimated parameters are decreased by 96.1% as compared to that of the estimated parameters without L1-regularization. In an experimental study, our method improves the model interpretation by reducing the number of parameters to be estimated while maintaining variance accounted for (VAF) at above 82.5%. Moreover, the variances of estimated parameters are reduced by 71.1% as compared to that of the estimated parameters without L1-regularization. Our method is 54 times faster than the standard simplex-based optimization to solve the regularized nonlinear regression.",0,Human
"Conventional RGB-D salient object detection methods aim to leverage depth as complementary information to find the salient regions in both modalities. However, the salient object detection results heavily rely on the quality of captured depth data which sometimes are unavailable. In this work, we make the first attempt to solve the RGB-D salient object detection problem with a novel depth-awareness framework. This framework only relies on RGB data in the testing phase, utilizing captured depth data as supervision for representation learning. To construct our framework as well as achieving accurate salient detection results, we propose a Ubiquitous Target Awareness (UTA) network to solve three important challenges in RGB-D SOD task: 1) a depth awareness module to excavate depth information and to mine ambiguous regions via adaptive depth-error weights, 2) a spatial-aware cross-modal interaction and a channel-aware cross-level interaction, exploiting the low-level boundary cues and amplifying high-level salient channels, and 3) a gated multi-scale predictor module to perceive the object saliency in different contextual scales. Besides its high performance, our proposed UTA network is depth-free for inference and runs in real-time with 43 FPS. Experimental evidence demonstrates that our proposed network not only surpasses the state-of-the-art methods on five public RGB-D SOD benchmarks by a large margin, but also verifies its extensibility on five public RGB SOD benchmarks.",0,Human
"Properly designed precoders can significantly improve the spectral efficiency of multiple-input multiple-output (MIMO) relay systems. In this paper, we investigate joint source and relay precoding design based on the mean-square-error (MSE) criterion in MIMO two-way relay systems, where two multi-antenna source nodes exchange information via a multi-antenna amplify-and-forward relay node. This problem is non-convex and its optimal solution remains unsolved. Aiming to find an efficient way to solve the problem, we first decouple the primal problem into three tractable sub-problems, and then propose an iterative precoding design algorithm based on alternating optimization. The solution to each sub-problem is optimal and unique, thus the convergence of the iterative algorithm is guaranteed. Secondly, we propose a structured precoding design to lower the computational complexity. The proposed precoding structure is able to parallelize the channels in the multiple access (MAC) phase and broadcast (BC) phase. It thus reduces the precoding design to a simple power allocation problem. Lastly, for the special case where only a single data stream is transmitted from each source node, we present a source-antenna-selection (SAS) based precoding design algorithm. This algorithm selects only one antenna for transmission from each source and thus requires lower signalling overhead. Comprehensive simulation is conducted to evaluate the effectiveness of all the proposed precoding designs.",0,Human
"In the last decade, scenario-based serious-games have become a main tool for learning new skills and capabilities. An important factor in the development of such systems is the overhead in time, cost and human resources to manually create the content for these scenarios. We focus on how to create content for scenarios in medical, military, commerce and gaming applications where maintaining the integrity and coherence of the content is integral for the system's success. To do so, we present an automatic method for generating content about everyday activities through combining computer science techniques with the crowd. We use the crowd in three basic ways: to capture a database of scenarios of everyday activities, to generate a database of likely replacements for specific events within that scenario, and to evaluate the resulting scenarios. We found that the generated scenarios were rated as reliable and consistent by the crowd when compared to the scenarios that were originally captured. We also compared the generated scenarios to those created by traditional planning techniques. We found that both methods were equally effective in generated reliable and consistent scenarios, yet the main advantages of our approach is that the content we generate is more varied and much easier to create. We have begun integrating this approach within a scenario-based training application for novice investigators within the law enforcement departments to improve their questioning skills.",0,Human
"This study introduces an innovative technique for pinpointing release points within advection-diffusion models by integrating machine learning with the Green Function Inverse Method. The suggested strategy leverages supervised learning algorithms to predict unknown release sources based on measured concentrations in these systems. Following this, the Green Function Inverse Method is employed to ensure the reliability of the predicted sources and to determine any missing parameters. Experimental outcomes indicate that this novel method significantly enhances both precision and speed compared to conventional approaches, making it a valuable asset for accurately identifying release points in practical advection-diffusion scenarios.",1,AI
"This research examines random caching-based cooperative transmission in heterogeneous wireless networks. Researchers introduce an innovative technique aimed at enhancing wireless data transmission efficiency by leveraging random caching mechanisms at intermediary nodes within the network. This strategy merges the advantages of cooperative transmission and caching, thereby boosting overall network performance and lowering latency. Through comprehensive simulations and evaluations, the authors validate the efficacy of their proposed method. Their findings indicate that the integrated use of random caching and cooperative transmission leads to substantial improvements in network performance, particularly in terms of throughput and energy efficiency, when compared to conventional cooperative transmission strategies. Additionally, the study sheds light on how factors like the quantity of caches and cache size affect the performance of their method. These insights are crucial for refining future wireless network designs. In summary, this work offers a promising approach for enhancing wireless network performance by combining random caching and cooperative transmission. The outcomes of this research hold significant value for guiding the development of advanced wireless technologies.",1,AI
"This study introduces a new method for creating an energy-efficient artificial neuron utilizing superconducting nanowires. The neuron design includes a superconducting loop with a nanowire junction capable of adjusting its response to input signals. This junction acts as the neuron's activation function, and its minimal power usage is due to the use of superconductivity to remove resistance from the loop. We've created a comprehensive model of our proposed neuron and conducted thorough simulations to illustrate its performance features. Our findings indicate that the neuron can operate at high accuracy levels while requiring much less power compared to current designs. Additionally, we've devised a strategy for incorporating several neurons into a larger system and demonstrated that the proposed neuron design can be expanded to build an efficient neural network. In summary, this research offers a promising path for developing energy-saving artificial neural networks, which could be applied to various fields including machine learning, data analysis, and cognitive computing.",1,AI
"This paper presents a discrete-time option pricing model that is rooted in Reinforcement Learning (RL), and more specifically in the famous Q-Learning method of RL. We construct a risk-adjusted Markov Decision Process for a discrete-time version of the classical Black-Scholes-Merton (BSM) model, where the option price is an optimal Q-function, while the optimal hedge is a second argument of this optimal Q-function, so that both the price and hedge are parts of the same formula. Pricing is done by learning to dynamically optimize risk-adjusted returns for an option replicating portfolio, as in the Markowitz portfolio theory. Using Q-Learning and related methods, once created in a parametric setting, the model is able to go model-free and learn to price and hedge an option directly from data, and without an explicit model of the world. This suggests that RL may provide efficient data-driven and model-free methods for optimal pricing and hedging of options, once we depart from the academic continuous-time limit, and vice versa, option pricing methods developed in Mathematical Finance may be viewed as special cases of model-based Reinforcement Learning. Further, due to simplicity and tractability of our model which only needs basic linear algebra (plus Monte Carlo simulation, if we work with synthetic data), and its close relation to the original BSM model, we suggest that our model could be used for benchmarking of different RL algorithms for financial trading applications",0,Human
"We consider a decentralized networked control system (DNCS) consisting of a remote controller and a collection of linear plants, each associated with a local controller. Each local controller directly observes the state of its co-located plant and can inform the remote controller of the plant's state through an unreliable uplink channel. The downlink channels from the remote controller to local controllers were assumed to be perfect. The objective of the local controllers and the remote controller is to cooperatively minimize the infinite horizon time average of expected quadratic cost. The finite horizon version of this problem was solved in our prior work [2]. The optimal strategies in the finite horizon case were shown to be characterized by coupled Riccati recursions. In this paper, we show that if the link failure probabilities are below certain critical thresholds, then the coupled Riccati recursions of the finite horizon solution reach a steady state and the corresponding decentralized strategies are optimal. Above these thresholds, we show that no strategy can achieve finite cost. We exploit a connection between our DNCS Riccati recursions and the coupled Riccati recursions of an auxiliary Markov jump linear system to obtain our results. Our main results in Theorems 1 and 2 explicitly identify the critical thresholds for the link failure probabilities and the optimal decentralized control strategies when all link failure probabilities are below their thresholds.",0,Human
"We explore the value of weak labels in learning transferable representations for medical images. Compared to hand-labeled datasets, weak or inexact labels can be acquired in large quantities at significantly lower cost and can provide useful training signals for data-hungry models such as deep neural networks. We consider weak labels in the form of pseudo-labels and propose a semi-weakly supervised contrastive learning (SWCL) framework for representation learning using semi-weakly annotated images. Specifically, we train a semi-supervised model to propagate labels from a small dataset consisting of diverse image-level annotations to a large unlabeled dataset. Using the propagated labels, we generate a patch-level dataset for pretraining and formulate a multi-label contrastive learning objective to capture position-specific features encoded in each patch. We empirically validate the transfer learning performance of SWCL on seven public retinal fundus datasets, covering three disease classification tasks and two anatomical structure segmentation tasks. Our experiment results suggest that, under very low data regime, large-scale ImageNet pretraining on improved architecture remains a very strong baseline, and recently proposed self-supervised methods falter in segmentation tasks, possibly due to the strong invariant constraint imposed. Our method surpasses all prior self-supervised methods and standard cross-entropy training, while closing the gaps with ImageNet pretraining.",0,Human
"The analysis of the structure of musical pieces is a task that remains a challenge for Artificial Intelligence, especially in the field of Deep Learning. It requires prior identification of structural boundaries of the music pieces. This structural boundary analysis has recently been studied with unsupervised methods and \textit{end-to-end} techniques such as Convolutional Neural Networks (CNN) using Mel-Scaled Log-magnitude Spectograms features (MLS), Self-Similarity Matrices (SSM) or Self-Similarity Lag Matrices (SSLM) as inputs and trained with human annotations. Several studies have been published divided into unsupervised and \textit{end-to-end} methods in which pre-processing is done in different ways, using different distance metrics and audio characteristics, so a generalized pre-processing method to compute model inputs is missing. The objective of this work is to establish a general method of pre-processing these inputs by comparing the inputs calculated from different pooling strategies, distance metrics and audio characteristics, also taking into account the computing time to obtain them. We also establish the most effective combination of inputs to be delivered to the CNN in order to establish the most efficient way to extract the limits of the structure of the music pieces. With an adequate combination of input matrices and pooling strategies we obtain a measurement accuracy $F_1$ of 0.411 that outperforms the current one obtained under the same conditions.",0,Human
"A common writing style for statistical results are the recommendations of the American Psychology Association, known as APA-style. However, in practice, writing styles vary as reports are not 100% following APA-style or parameters are not reported despite being mandatory. In addition, the statistics are not reported in isolation but in context of experimental conditions investigated and the general topic. We address these challenges by proposing a flexible pipeline STEREO based on active wrapper induction and unsupervised aspect extraction. We applied our pipeline to the over 100,000 documents in the CORD-19 dataset. It required only 0.25% of the corpus (about 500 documents) to learn statistics extraction rules that cover 95% of the sentences in CORD-19. The statistic extraction has nearly 100% precision on APA-conform and 95% precision on non-APA writing styles. In total, we were able to extract 113k reported statistics, of which only <1% is APA conform. We could extract in 46% the correct conditions from APA-conform reports (30% for non-APA). The best model for topic extraction achieves a precision of 75% on statistics reported in APA style (73% for non-APA conform). We conclude that STEREO is a good foundation for automatic statistic extraction and future developments for scientific paper analysis. Particularly the extraction of non-APA conform reports is important and allows applications such as giving feedback to authors about what is missing and could be changed.",0,Human
"This paper introduces a parallel version of the Ensemble Kalman Filter (EnKF), utilizing a revised Cholesky decomposition technique. The EnKF is a widely recognized method for estimating states in complex systems through Monte Carlo simulations. However, its execution can be quite resource-intensive, particularly for extensive datasets. To tackle this challenge, we've developed a parallelized version of the EnKF that takes advantage of the Cholesky decomposition method, which splits computations across multiple processors. The modified Cholesky decomposition breaks down matrices into simpler components, making the computation of square roots more efficient. By employing this strategy, our parallelized EnKF successfully reduces processing times without compromising precision. The effectiveness of our parallel approach was tested through computational experiments, demonstrating notable improvements over a sequential implementation of the EnKF. Our research holds significant potential for various fields such as weather prediction, climate modeling, and seismic analysis, where real-time processing of voluminous data relies on the EnKF.",1,AI
"This study delves into fair division issues constrained by heterogeneous matroids, prevalent in allocation scenarios where resources vary in their attributes and importance. We introduce a novel algorithmic strategy that ensures both equity and efficiency through leveraging matroid structures and the EF1 principle. Our methodology builds upon and expands upon previous research on matroid intersections and partitions, making it adaptable to multi-agent environments and non-transferable goods. The algorithmic performance is rigorously analyzed, confirming its ability to yield near-optimal solutions in terms of social welfare and EF1 fairness. Empirical evidence from both simulated and actual data showcases the algorithm's efficacy, surpassing current leading techniques in both quantitative and qualitative measures according to both automated criteria and human feedback. In summary, our contribution enriches the field of fair allocation systems and offers a valuable solution for tackling intricate real-world resource distribution challenges.",1,AI
"Visual exploration of high-dimensional real-valued datasets is a fundamental task in exploratory data analysis (EDA). Existing methods use predefined criteria to choose the representation of data. There is a lack of methods that (i) elicit from the user what she has learned from the data and (ii) show patterns that she does not know yet. We construct a theoretical model where identified patterns can be input as knowledge to the system. The knowledge syntax here is intuitive, such as ""this set of points forms a cluster"", and requires no knowledge of maths. This background knowledge is used to find a Maximum Entropy distribution of the data, after which the system provides the user data projections in which the data and the Maximum Entropy distribution differ the most, hence showing the user aspects of the data that are maximally informative given the user's current knowledge. We provide an open source EDA system with tailored interactive visualizations to demonstrate these concepts. We study the performance of the system and present use cases on both synthetic and real data. We find that the model and the prototype system allow the user to learn information efficiently from various data sources and the system works sufficiently fast in practice. We conclude that the information theoretic approach to exploratory data analysis where patterns observed by a user are formalized as constraints provides a principled, intuitive, and efficient basis for constructing an EDA system.",0,Human
"The spectral gap $\gamma$ of a finite, ergodic, and reversible Markov chain is an important parameter measuring the asymptotic rate of convergence. In applications, the transition matrix $P$ may be unknown, yet one sample of the chain up to a fixed time $n$ may be observed. We consider here the problem of estimating $\gamma$ from this data. Let $\pi$ be the stationary distribution of $P$, and $\pi_\star = \min_x \pi(x)$. We show that if $n = \tilde{O}\bigl(\frac{1}{\gamma \pi_\star}\bigr)$, then $\gamma$ can be estimated to within multiplicative constants with high probability. When $\pi$ is uniform on $d$ states, this matches (up to logarithmic correction) a lower bound of $\tilde{\Omega}\bigl(\frac{d}{\gamma}\bigr)$ steps required for precise estimation of $\gamma$. Moreover, we provide the first procedure for computing a fully data-dependent interval, from a single finite-length trajectory of the chain, that traps the mixing time $t_{\text{mix}}$ of the chain at a prescribed confidence level. The interval does not require the knowledge of any parameters of the chain. This stands in contrast to previous approaches, which either only provide point estimates, or require a reset mechanism, or additional prior knowledge. The interval is constructed around the relaxation time $t_{\text{relax}} = 1/\gamma$, which is strongly related to the mixing time, and the width of the interval converges to zero roughly at a $1/\sqrt{n}$ rate, where $n$ is the length of the sample path.",0,Human
"Recommender System research suffers currently from a disconnect between the size of academic data sets and the scale of industrial production systems. In order to bridge that gap we propose to generate more massive user/item interaction data sets by expanding pre-existing public data sets. User/item incidence matrices record interactions between users and items on a given platform as a large sparse matrix whose rows correspond to users and whose columns correspond to items. Our technique expands such matrices to larger numbers of rows (users), columns (items) and non zero values (interactions) while preserving key higher order statistical properties. We adapt the Kronecker Graph Theory to user/item incidence matrices and show that the corresponding fractal expansions preserve the fat-tailed distributions of user engagements, item popularity and singular value spectra of user/item interaction matrices. Preserving such properties is key to building large realistic synthetic data sets which in turn can be employed reliably to benchmark Recommender Systems and the systems employed to train them. We provide algorithms to produce such expansions and apply them to the MovieLens 20 million data set comprising 20 million ratings of 27K movies by 138K users. The resulting expanded data set has 10 billion ratings, 864K items and 2 million users in its smaller version and can be scaled up or down. A larger version features 655 billion ratings, 7 million items and 17 million users.",0,Human
"This research examines how spectral efficiency is affected by correlated gamma-lognormal desired and interfering signals. It specifically looks at how signal correlation impacts the performance of wireless communication systems. The results indicate that when there's a strong correlation between the desired and interfering signals, it can substantially degrade system performance. The study establishes a theoretical model to understand these effects and discusses the balance between signal correlation and spectral efficiency. These insights are valuable for designing and optimizing wireless communication systems, potentially leading to better and more reliable communication technologies.",1,AI
"Today, communication is more widespread than ever before, thanks to large-scale networks such as social media, instant messaging apps, and online forums. These networks generate huge volumes of daily conversational data. The process of overhearing – which involves observing and analyzing conversations among people – has become an important method for understanding and predicting behavior within these networks. As these networks continue to expand, there is a need for more streamlined ways to analyze the vast amount of data they produce. This paper introduces a framework for representing conversations that aims to simplify the overhearing process. Our framework uses the graph structure of communication networks to condense conversational data into a manageable format that can be easily analyzed. It employs a combination of graph clustering and sequence modeling techniques to identify topics and represent conversations as sequences of topic vectors. By doing so, we can reduce the complexity of the data and enhance the efficiency of the overhearing algorithms. To test our framework, we conducted experiments using a large dataset of online forum conversations. The results showed that our framework performed better than existing methods in terms of accuracy and efficiency. Furthermore, we demonstrated the framework's scalability by examining various sizes of datasets, including ones with millions of messages. Overall, our framework offers a fresh approach to handling conversational data for effective overhearing. By harnessing the structure of communication networks and minimizing the complexity of the data, our framework facilitates a more efficient analysis of large-scale communication networks.",1,AI
"This research explores how to price goods effectively and nearly optimally when their value depends on how many others have adopted them. This issue occurs frequently in economics, where a product's worth to a buyer grows as more people use it. The study aims to develop straightforward yet highly effective pricing tactics that boost overall market benefits without complicating things too much. It introduces a unique method that merges optimal pricing concepts with approximation techniques to create pricing formulas that strike a balance between efficiency and simplicity. The approach has been validated using simulations, revealing that the suggested pricing models yield almost perfect outcomes while maintaining a level of ease that traditional optimal pricing schemes lack. In essence, this work illuminates how to craft effective pricing policies for goods whose value is tied to widespread adoption, offering practical guidance for business leaders and policy makers.",1,AI
"This study introduces a new method for managing congestion in RDMA networks within datacenters. Dubbed Dart, this technique breaks down the network into smaller segments and customizes each segment for different kinds of traffic. As a result, Dart can swiftly address congestion issues and minimize the delay experienced by important applications. Evaluation of Dart reveals that it significantly outperforms existing congestion control strategies in areas such as throughput and fairness, all the while keeping latency low for applications requiring real-time processing. These findings suggest that Dart represents a promising avenue for solving congestion problems in RDMA networks found in datacenter environments.",1,AI
"This research examines how beliefs change over time, focusing on shifts in the subjective probability of events or statements. It looks at various techniques for monitoring and describing these shifts within both individual and collective scenarios. The methodologies draw upon Bayesian belief networks and probabilistic graphical models, robust frameworks for handling uncertainty and updating beliefs. To assess these approaches, the study employs both simulations and practical tests. By analyzing the outcomes, we gain a deeper understanding of what influences the precision of our belief dynamics measurements. Ultimately, the findings could help create improved models for tracking and comprehending belief changes in practical settings like decision-making, risk analysis, and information sharing.",1,AI
"Since the 1990s, there have been significant advances in the technology space and the e-Commerce area, leading to an exponential increase in demand for cashless payment solutions. This has led to increased demand for credit cards, bringing along with it the possibility of higher credit defaults and hence higher delinquency rates, over a period of time. The purpose of this research paper is to build a contemporary credit scoring model to forecast credit defaults for unsecured lending (credit cards), by employing machine learning techniques. As much of the customer payments data available to lenders, for forecasting Credit defaults, is imbalanced (skewed), on account of a limited subset of default instances, this poses a challenge for predictive modelling. In this research, this challenge is addressed by deploying Synthetic Minority Oversampling Technique (SMOTE), a proven technique to iron out such imbalances, from a given dataset. On running the research dataset through seven different machine learning models, the results indicate that the Light Gradient Boosting Machine (LGBM) Classifier model outperforms the other six classification techniques. Thus, our research indicates that the LGBM classifier model is better equipped to deliver higher learning speeds, better efficiencies and manage larger data volumes. We expect that deployment of this model will enable better and timely prediction of credit defaults for decision-makers in commercial lending institutions and banks.",0,Human
"This research introduces an innovative way to perform cooperative verification through the collective generation of invariants. The concept hinges on the notion that various agents can collaborate to derive these mathematical expressions that outline the system's intended characteristics, which can be leveraged to confirm whether the system operates correctly. The findings indicate that this technique outperforms conventional verification strategies since it enables a deeper examination of the system's operations and uncovers issues that others might overlook. The study showcases the practical benefits of this method across different scenarios and underscores its broad applicability within the verification domain.",1,AI
"There has been a remarkable increase in the data exchange over web and the widespread use of digital media. As a result, multimedia data transfers also had a boost up. The mounting interest with reference to digital watermarking throughout the last decade is certainly due to the increase in the need of copyright protection of digital content. This is also enhanced due to commercial prospective. Applications of video watermarking in copy control, broadcast monitoring, fingerprinting, video authentication, copyright protection etc is immensely rising. The main aspects of information hiding are capacity, security and robustness. Capacity deals with the amount of information that can be hidden. The skill of anyone detecting the information is security and robustness refers to the resistance to modification of the cover content before concealed information is destroyed. Video watermarking algorithms normally prefers robustness. In a robust algorithm it is not possible to eliminate the watermark without rigorous degradation of the cover content. In this paper, we introduce the notion of Video Watermarking and the features required to design a robust watermarked video for a valuable application. We review several algorithms, and introduce frequently used key techniques. The aim of this paper is to focus on the various domains of video watermarking techniques. The majority of the reviewed methods based on video watermarking emphasize on the notion of robustness of the algorithm.",0,Human
"Symbolic Aggregate Approximation (SAX) has become popular in time series analysis thanks to its speed and capability to recognize patterns within the data. Yet, there's an acknowledged drawback to its ability to grasp segment trend details. To enhance its effectiveness, this study introduces an improved version of SAX that includes segment trend information. By conducting thorough testing and assessment, we show that our new method surpasses the standard SAX technique in terms of recognizing both broad and localized trends in time series data. This research advances the field of time series analysis and offers valuable insights for practical applications in the real world.",1,AI
"We have designed, fabricated, and successfully tested a prototype mixed-signal, 28x28-binary-input, 10-output, 3-layer neuromorphic network (""MLP perceptron""). It is based on embedded nonvolatile floating-gate cell arrays redesigned from a commercial 180-nm NOR flash memory. The arrays allow precise (~1%) individual tuning of all memory cells, having long-term analog-level retention and low noise. Each array performs a very fast and energy-efficient analog vector-by-matrix multiplication, which is the bottleneck for signal propagation in most neuromorphic networks. All functional components of the prototype circuit, including 2 synaptic arrays with 101,780 floating-gate synaptic cells, 74 analog neurons, and the peripheral circuitry for weight adjustment and I/O operations, have a total area below 1 mm^2. Its testing on the common MNIST benchmark set (at this stage, with a relatively low weight import precision) has shown a classification fidelity of 94.65%, close to the 96.2% obtained in simulation. The classification of one pattern takes less than 1 us time and ~20 nJ energy - both numbers much better than for digital implementations of the same task. Estimates show that this performance may be further improved using a better neuron design and a more advanced memory technology, leading to a >10^2 advantage in speed and a >10^4 advantage in energy efficiency over the state-of-the-art purely digital (GPU and custom) circuits, at classification of large, complex patterns.",0,Human
"This study introduces a fresh technique for identifying deception attacks within networked control systems through a sequential detection mechanism coupled with watermarking. The suggested strategy entails inserting a distinctive watermark into the control system's input signal, followed by surveillance of the output signal for anomalies that diverge from the anticipated performance. Utilizing a sequential hypothesis testing algorithm, the presence of a deception attack is detected via the correlation between the watermark and the output signal. Simulation outcomes showcase the efficacy of this proposed approach in accurately detecting different forms of deception attacks with minimal false positives. This research offers a promising avenue to bolster the security and dependability of networked control systems against malicious intrusions.",1,AI
"This research looks into using Local Interpretable Model-Agnostic Explanations with Outlier Detection (LIMEOUT) to enhance the fairness of machine learning (ML) models. LIMEOUT is a recent development that merges LIME, which clarifies ML model predictions at the local scale, with outlier detection, which pinpoints items that lie outside the typical training dataset distribution. The study illustrates LIMEOUT's effectiveness in boosting ML model fairness through its analysis on diverse benchmark datasets, each presenting unique fairness-related obstacles. The findings indicate that LIMEOUT substantially enhances ML model fairness by pinpointing and neutralizing biases in predictions. Additionally, the paper reveals that LIMEOUT can be employed to offer clear and understandable explanations for the ML model's decision-making process, thereby enhancing its credibility and accountability. In summary, the paper posits that LIMEOUT is an encouraging strategy for making ML models more just and transparent, which could aid in addressing ethical issues linked to their utilization across various sectors.",1,AI
"We consider the numerical stability of the parameter recovery problem in Linear Structural Equation Model ($\LSEM$) of causal inference. A long line of work starting from Wright (1920) has focused on understanding which sub-classes of $\LSEM$ allow for efficient parameter recovery. Despite decades of study, this question is not yet fully resolved. The goal of this paper is complementary to this line of work; we want to understand the stability of the recovery problem in the cases when efficient recovery is possible. Numerical stability of Pearl's notion of causality was first studied in Schulman and Srivastava (2016) using the concept of condition number where they provide ill-conditioned examples. In this work, we provide a condition number analysis for the $\LSEM$. First we prove that under a sufficient condition, for a certain sub-class of $\LSEM$ that are \emph{bow-free} (Brito and Pearl (2002)), the parameter recovery is stable. We further prove that \emph{randomly} chosen input parameters for this family satisfy the condition with a substantial probability. Hence for this family, on a large subset of parameter space, recovery is numerically stable. Next we construct an example of $\LSEM$ on four vertices with \emph{unbounded} condition number. We then corroborate our theoretical findings via simulations as well as real-world experiments for a sociology application. Finally, we provide a general heuristic for estimating the condition number of any $\LSEM$ instance.",0,Human
"Cybercrime markets support the development and diffusion of new attack technologies, vulnerability exploits, and malware. Whereas the revenue streams of cyber attackers have been studied multiple times in the literature, no quantitative account currently exists on the economics of attack acquisition and deployment. Yet, this understanding is critical to characterize the production of (traded) exploits, the economy that drives it, and its effects on the overall attack scenario. In this paper we provide an empirical investigation of the economics of vulnerability exploitation, and the effects of market factors on likelihood of exploit. Our data is collected first-handedly from a prominent Russian cybercrime market where the trading of the most active attack tools reported by the security industry happens. Our findings reveal that exploits in the underground are priced similarly or above vulnerabilities in legitimate bug-hunting programs, and that the refresh cycle of exploits is slower than currently often assumed. On the other hand, cybercriminals are becoming faster at introducing selected vulnerabilities, and the market is in clear expansion both in terms of players, traded exploits, and exploit pricing. We then evaluate the effects of these market variables on likelihood of attack realization, and find strong evidence of the correlation between market activity and exploit deployment. We discuss implications on vulnerability metrics, economics, and exploit measurement.",0,Human
"We consider finite horizon reach-avoid problems for discrete time stochastic systems. Our goal is to construct upper bound functions for the reach-avoid probability by means of tractable convex optimization problems. We achieve this by restricting attention to the span of Gaussian radial basis functions and imposing structural assumptions on the transition kernel of the stochastic processes as well as the target and safe sets of the reach-avoid problem. In particular, we require the kernel to be written as a Gaussian mixture density with each mean of the distribution being affine in the current state and input and the target and safe sets to be written as intersections of quadratic inequalities. Taking advantage of these structural assumptions, we formulate a recursion of semidefinite programs where each step provides an upper bound to the value function of the reach- avoid problem. The upper bounds provide a performance metric to which any suboptimal control policy can be compared, and can themselves be used to construct suboptimal control policies. We illustrate via numerical examples that even if the resulting bounds are conservative, the associated control policies achieve higher reach-avoid probabilities than heuristic controllers for problems of large state-input space dimensions (more than 20). The results presented in this paper, far exceed the limits of current approximation methods for reach-avoid problems in the specific class of stochastic systems considered.",0,Human
"This research investigates how to effectively identify the optimal choice (or ""best arm"") when faced with multiple options distributed among several entities (agents). These agents might have distinct characteristics or ""distributions."" The authors propose a strategy that exploits these differences to create a stable and efficient algorithm for finding the best option. They test their method using numerous simulations and find it performs better than alternative approaches. By doing so, they advance our understanding of how to develop resilient algorithms for tackling multi-armed bandit problems in federated environments, which could be useful in areas like personalized recommendations and online ads.",1,AI
"This research introduces a new technique for matching unordered labeled trees that we've named ""Anti-Tai Mapping"". It tackles the issue of establishing a one-to-one relationship between nodes in two such trees, ensuring that both their structural and label characteristics are maintained. The method builds upon the idea of ""tree tai-mapping,"" but adapts it specifically for unordered trees and incorporates an anti-isomorphism rule. To assess its effectiveness, the Anti-Tai Mapping was tested across multiple datasets against other established techniques, revealing its advantages in both precision and speed. These findings could be useful in areas like pattern recognition, machine learning, and computational biology.",1,AI
"This research introduces an innovative method for sequence-to-sequence learning utilizing a recurrent neural network-based semantic variational autoencoder (RNN-SVAE). The model builds upon the encoding-decoding framework of conventional autoencoders but enhances this by integrating variational inference and a recurrent neural network for better sequential modeling. Capable of generating sequences with semantic coherence, the RNN-SVAE learns significant and latent features from input sequences, showcasing its superiority over standard sequence-to-sequence models in tasks such as text generation. Experiments confirm the model's effectiveness in understanding and representing the underlying structures and meanings within sequences.",1,AI
"In Federated Learning (FL), a strong global model is collaboratively learned by aggregating the clients' locally trained models. Although this allows no need to access clients' data directly, the global model's convergence often suffers from data heterogeneity. This paper suggests that forgetting could be the bottleneck of global convergence. We observe that fitting on biased local distribution shifts the feature on global distribution and results in forgetting of global knowledge. We consider this phenomenon as an analogy to Continual Learning, which also faces catastrophic forgetting when fitted on the new task distribution. Based on our findings, we hypothesize that tackling down the forgetting in local training relives the data heterogeneity problem. To this end, we propose a simple yet effective framework Federated Local Self-Distillation (FedLSD), which utilizes the global knowledge on locally available data. By following the global perspective on local data, FedLSD encourages the learned features to preserve global knowledge and have consistent views across local models, thus improving convergence without compromising data privacy. Under our framework, we further extend FedLSD to FedLS-NTD, which only considers the not-true class signals to compensate noisy prediction of the global model. We validate that both FedLSD and FedLS-NTD significantly improve the performance in standard FL benchmarks in various setups, especially in the extreme data heterogeneity cases.",0,Human
"Understanding how the brain processes information involves studying how it learns and infers connections within its network. This paper introduces a novel computational method to model these learning and inference processes in cortical networks. Our method draws from deep learning principles, utilizing artificial neural networks to process vast amounts of data. By training on datasets containing sensory input and output pairs, the model learns the connection between these elements. The outcomes show that this framework can effectively deduce the hidden patterns in cortical networks, offering fresh insights into brain functionality. These research outcomes could pave the way for creating more sophisticated AI systems and enhancing our comprehension of neural mechanisms.",1,AI
"This study delves into the ethical considerations around actuarial risk assessments and the implementation of interventions aimed at altering forecasted results. Historically, actuarial risk assessments have relied on predictive models to pinpoint individuals deemed to be at high risk for unfavorable outcomes and allocate resources accordingly. However, critics argue that this method could lead to discriminatory practices and disparate treatment. The paper suggests shifting the discussion to emphasize interventions over predictions, advocating for an approach that addresses the underlying issues causing risk and promotes beneficial outcomes instead of just countering anticipated negative impacts. Ultimately, the authors emphasize the significance of addressing the ethical aspects of actuarial risk assessments and fostering continued discourse regarding the proper application of interventions within this domain.",1,AI
"This research investigates how certain pollutants are linked to an increased risk of childhood cancer by analyzing data from a particular region. It involves examining pollutant levels alongside instances of child cancer. The study's approach aims to detect any correlations between these factors and reports its findings, which indicate that there is indeed a strong correlation between elevated pollutant concentrations and higher rates of childhood cancer in that area. These insights support the argument that environmental pollutants can affect human health and offer important guidance for policy makers and healthcare authorities in developing safer environments.",1,AI
"We propose NormalGAN, a fast adversarial learning-based method to reconstruct the complete and detailed 3D human from a single RGB-D image. Given a single front-view RGB-D image, NormalGAN performs two steps: front-view RGB-D rectification and back-view RGBD inference. The final model was then generated by simply combining the front-view and back-view RGB-D information. However, inferring backview RGB-D image with high-quality geometric details and plausible texture is not trivial. Our key observation is: Normal maps generally encode much more information of 3D surface details than RGB and depth images. Therefore, learning geometric details from normal maps is superior than other representations. In NormalGAN, an adversarial learning framework conditioned by normal maps is introduced, which is used to not only improve the front-view depth denoising performance, but also infer the back-view depth image with surprisingly geometric details. Moreover, for texture recovery, we remove shading information from the front-view RGB image based on the refined normal map, which further improves the quality of the back-view color inference. Results and experiments on both testing data set and real captured data demonstrate the superior performance of our approach. Given a consumer RGB-D sensor, NormalGAN can generate the complete and detailed 3D human reconstruction results in 20 fps, which further enables convenient interactive experiences in telepresence, AR/VR and gaming scenarios.",0,Human
"This research introduces a deep reinforcement learning (DRL) method for content caching in vehicular edge computing and networks (VECN), utilizing permissioned blockchain technology. The DRL algorithm identifies the best caching plan for individual vehicles within the VECN by taking into account multiple aspects like the available storage capacity, content popularity, and network conditions. Permissioned blockchain facilitates the exchange and administration of cached data across vehicles while ensuring secure interactions among all parties involved in the VECN. Simulation outcomes demonstrate that the proposed DRL-based caching system achieves higher hit rates and better energy efficiency compared to conventional methods. The integration of DRL and permissioned blockchain technology represents an innovative approach for enhancing content caching in VECN systems, thereby improving the overall efficiency of these networks.",1,AI
"We develop here a computationally effective approach for producing high-quality $\mathcal{H}_\infty$-approximations to large scale linear dynamical systems having multiple inputs and multiple outputs (MIMO). We extend an approach for $\mathcal{H}_\infty$ model reduction introduced by Flagg, Beattie, and Gugercin for the single-input/single-output (SISO) setting, which combined ideas originating in interpolatory $\mathcal{H}_2$-optimal model reduction with complex Chebyshev approximation. Retaining this framework, our approach to the MIMO problem has its principal computational cost dominated by (sparse) linear solves, and so it can remain an effective strategy in many large-scale settings. We are able to avoid computationally demanding $\mathcal{H}_\infty$ norm calculations that are normally required to monitor progress within each optimization cycle through the use of ""data-driven"" rational approximations that are built upon previously computed function samples. Numerical examples are included that illustrate our approach. We produce high fidelity reduced models having consistently better $\mathcal{H}_\infty$ performance than models produced via balanced truncation; these models often are as good as (and occasionally better than) models produced using optimal Hankel norm approximation as well. In all cases considered, the method described here produces reduced models at far lower cost than is possible with either balanced truncation or optimal Hankel norm approximation.",0,Human
"With the explosive growth in the number of pictures taken by smartphones, organizing and searching pictures has become important tasks. To efficiently fulfill these tasks, the key enabler is annotating images with proper keywords, with which keyword-based searching and organizing become available for images. Currently, smartphones usually synchronize photo albums with cloud storage platforms, and have their images annotated with the help of cloud computing. However, the ""offloading-to-cloud"" solution may cause privacy breach, since photos from smart photos contain various sensitive information. For privacy protection, existing research made effort to support cloud-based image annotation on encrypted images by utilizing cryptographic primitives. Nevertheless, for each annotation, it requires the cloud to perform linear checking on the large-scale encrypted dataset with high computational cost. This paper proposes a cloud-assisted privacy-preserving image annotation with randomized kd-forest, namely CPAR. With CPAR, users are able to automatically assign keywords to their images by leveraging the power of cloud with privacy protected. CPAR proposes a novel privacy-preserving randomized kd-forest structure, which significantly improves the annotation performance compared with existing research. Thorough analysis is carried out to demonstrate the security of CPAR. Experimental evaluation on the well-known IAPR TC-12 dataset validates the efficiency and effectiveness of CPAR.",0,Human
"This research introduces FILIP, a new method that combines language modeling with image recognition to enhance performance in areas like computer vision and natural language processing. FILIP is developed using extensive text and image data from a large dataset. After being tested against various benchmarks, FILIP has been found to outperform other pre-training models significantly. These findings indicate that FILIP's focus on detailed interactions between language and images could prove beneficial for numerous AI applications.",1,AI
"This study introduces Augury, a tool designed to analyze and predict system and network performance metrics using time-series data. Augury utilizes sophisticated machine learning methods like neural networks and time-series decomposition to uncover trends, detect patterns, and pinpoint any unusual behaviors in these metrics. It also offers a range of predictive models that assist IT professionals in forecasting future performance with precision. The efficacy of Augury was tested using actual data gathered from various systems and networks. The findings demonstrate that Augury surpasses current alternatives in areas such as accuracy, flexibility, and speed. Additionally, Augury sheds light on the root causes of performance problems, which can be leveraged to enhance system and network efficiency. Overall, Augury could enhance the operational efficiency and dependability of intricate systems and networks, while minimizing the expenses and resources required for performance monitoring and management.",1,AI
"Adversarial examples are often cited by neuroscientists and machine learning researchers as an example of how computational models diverge from biological sensory systems. Recent work has proposed adding biologically-inspired components to visual neural networks as a way to improve their adversarial robustness. One surprisingly effective component for reducing adversarial vulnerability is response stochasticity, like that exhibited by biological neurons. Here, using recently developed geometrical techniques from computational neuroscience, we investigate how adversarial perturbations influence the internal representations of standard, adversarially trained, and biologically-inspired stochastic networks. We find distinct geometric signatures for each type of network, revealing different mechanisms for achieving robust representations. Next, we generalize these results to the auditory domain, showing that neural stochasticity also makes auditory models more robust to adversarial perturbations. Geometric analysis of the stochastic networks reveals overlap between representations of clean and adversarially perturbed stimuli, and quantitatively demonstrates that competing geometric effects of stochasticity mediate a tradeoff between adversarial and clean performance. Our results shed light on the strategies of robust perception utilized by adversarially trained and stochastic networks, and help explain how stochasticity may be beneficial to machine and biological computation.",0,Human
"This paper aims to introduce a novel method for behavior planning in connected autonomous vehicles by employing feedback deep reinforcement learning. It explores creating a system capable of learning from its experiences and making timely decisions to guarantee safe and efficient driving. The suggested technique integrates deep reinforcement learning with feedback control, utilizing the latter to enhance the system's stability and resilience. The efficacy of this proposed strategy is tested via various simulations and experiments, revealing substantial enhancements in safety and efficiency when compared to conventional behavior planning techniques. The findings illustrate the feasibility of this approach as a viable solution for behavior planning in connected autonomous vehicles.",1,AI
"Regression analysis-based approaches have been widely studied for face recognition (FR) in the past several years. More recently, to better deal with some difficult conditions such as occlusions and illumination, nuclear norm based matrix regression methods have been proposed to characterize the low-rank structure of the error image, which generalize the one-dimensional, pixel-based error model to the two-dimensional structure. These methods, however, are inherently devised for grayscale image based FR and without exploiting the color information which is proved beneficial for FR of color face images. Benefiting from quaternion representation, which is capable of encoding the cross-channel correlation of color images, we propose a novel color FR method by formulating the color FR problem as a nuclear norm based quaternion matrix regression (NQMR). We further develop a more robust model called R-NQMR by using the logarithm of the nuclear norm, instead of the original nuclear norm, which adaptively assigns weights on different singular values, and then extend it to deal with the mixed noise. The proposed models, then, are solved using the effective alternating direction multiplier method (ADMM). Experiments on several public face databases demonstrate the superior performance and efficacy of the proposed approaches for color FR, especially for some difficult conditions (occlusion, illumination and mixed noise) over some state-of-the-art regression analysis-based approaches.",0,Human
"This study establishes a minimum time requirement for solving the Boolean Satisfiability (SAT) problem using deterministic Turing machines. The SAT challenge revolves around determining if a given Boolean expression can be true under specific conditions. Our research demonstrates that any algorithm designed to solve SAT problems on deterministic Turing machines must take at least 2^n / poly(n) time, where n represents the number of variables in the input formula and poly(n) is a polynomial function of n. To demonstrate this, we introduced a new method to create particularly difficult SAT problems that necessitate substantial computational effort to resolve on a Turing machine. This method involves augmenting the original formula with extra clauses, ensuring that solving these modified problems requires verifying many of the newly added clauses. We prove that any algorithm tackling SAT problems on deterministic Turing machines must spend considerable time validating these supplementary clauses, which ultimately results in our minimum time requirement. This finding is crucial because it indicates a fundamental barrier to the efficiency of any deterministic approach to solving SAT problems, impacting how we design and analyze algorithms across various fields. The study concludes with suggestions for further research inspired by the methodologies we've outlined.",1,AI
"This study investigates how distributed scheduling algorithms can be used for event analytics in a hybrid edge-cloud setup. The goal is to evenly distribute computational workload between edge devices and the cloud to ensure quick and precise event analysis. Our method combines task partitioning and load balancing techniques to dynamically allocate event analytics tasks according to resource availability at the edge and in the cloud. Experimental findings show that our strategy successfully distributes the workload, decreases response time, and enhances the overall effectiveness of event analytics. This work offers a useful approach for managing event analytics in edge-cloud environments, which is essential for various IoT and cyber-physical systems applications.",1,AI
"Color constancy refers to our visual system's capability to recognize an object's true color regardless of changes in lighting. While methods like gray-world, white-patch, and shade-of-gray have been employed to tackle this issue in computer vision, they often fall short, especially when dealing with diverse environments, requiring substantial computational power. Our study presents an innovative solution by framing camera-adaptive color constancy as a few-shot meta-learning task. By harnessing a meta-learning architecture, the model can swiftly adjust to new settings and lighting without needing many examples. Testing across multiple datasets reveals that our method surpasses conventional techniques in both precision and efficiency. This research offers a promising pathway for improving camera-adaptive color constancy, potentially influencing numerous fields within computer vision and image processing.",1,AI
"Non-orthogonal multiple access (NOMA) has shown potential for scalable multicast of video data. However, one key drawback for NOMA-based video multicast is the limited number of layers allowed by the embedded successive interference cancellation algorithm, failing to meet satisfaction of heterogeneous receivers. We propose a novel receiver-driven superposed video multicast (Supcast) scheme by integrating Softcast, an analog-like transmission scheme, into the NOMA-based system to achieve high bandwidth efficiency as well as gradual decoding quality proportional to channel conditions at receivers. Although Softcast allows gradual performance by directly transmitting power-scaled transformation coefficients of frames, it suffers performance degradation due to discarding coefficients under insufficient bandwidth and its power allocation strategy cannot be directly applied in NOMA due to interference. In Supcast, coefficients are grouped into chunks, which are basic units for power allocation and superposition scheduling. By bisecting chunks into base-layer chunks and enhanced-layer chunks, the joint power allocation and chunk scheduling is formulated as a distortion minimization problem. A two-stage power allocation strategy and a near-optimal low-complexity algorithm for chunk scheduling based on the matching theory are proposed. Simulation results have shown the advantage of Supcast against Softcast as well as the reference scheme in NOMA under various practical scenarios.",0,Human
"Accurately modeling human decision-making in security is critical to thinking about when, why, and how to recommend that users adopt certain secure behaviors. In this work, we conduct behavioral economics experiments to model the rationality of end-user security decision-making in a realistic online experimental system simulating a bank account. We ask participants to make a financially impactful security choice, in the face of transparent risks of account compromise and benefits offered by an optional security behavior (two-factor authentication). We measure the cost and utility of adopting the security behavior via measurements of time spent executing the behavior and estimates of the participant's wage. We find that more than 50% of our participants made rational (e.g., utility optimal) decisions, and we find that participants are more likely to behave rationally in the face of higher risk. Additionally, we find that users' decisions can be modeled well as a function of past behavior (anchoring effects), knowledge of costs, and to a lesser extent, users' awareness of risks and context (R2=0.61). We also find evidence of endowment effects, as seen in other areas of economic and psychological decision-science literature, in our digital-security setting. Finally, using our data, we show theoretically that a ""one-size-fits""-all emphasis on security can lead to market losses, but that adoption by a subset of users with higher risks or lower costs can lead to market gains.",0,Human
"This paper introduces a new method for understanding signals that are related to each other. It revolves around estimating the free energy landscape of these correlated signals and then exploring this landscape to figure out their most probable arrangement. This technique assumes that free energy well represents the system's entropy and that studying the free energy landscape can reveal the underlying structure of the correlated signals. To confirm the effectiveness of this method, it was tested on both simulated and real-world datasets and compared with other approaches. The findings show that free energy exploration works well in identifying signal structures and surpasses conventional methods in certain scenarios. This strategy could potentially be applied to various areas like signal processing and machine learning, including analyzing intricate systems, uncovering concealed factors, and recognizing patterns within large data sets.",1,AI
"The researchers have introduced a novel beamforming approach named Layered Group Sparse Beamforming (LGSB) specifically tailored for wireless networks equipped with caching functionalities. This method aims to optimize both energy consumption and data transmission rates. LGSB divides users into distinct groups and allocates sparse beamforming resources to these groups. Simulation outcomes indicate that LGSB significantly enhances energy efficiency without compromising data rates, surpassing conventional beamforming strategies. These research outcomes could help in creating more eco-friendly and energy-saving wireless networks.",1,AI
"Wide Area Monitoring Systems (WAMS) utilizing synchrophasor measurements is considered one of the essential parts in smart grids that enable system operators to monitor, operate, and control power systems in wide geographical area. On the other hand, high-speed, reliable and scalable data communication infrastructure is crucial in both construction and operation of WAMS. Universal mobile Telecommunication System (UMTS), the 3G standard for mobile communication networks, was developed to provide high speed data transmission with reliable service performance for mobile users. Therefore, UMTS is considered a promising solution for providing a communication infrastructure for WAMS. 3G based EWAMS (Egyptian wide area Monitoring System) is designed and implemented in Egypt through deployment a number of frequency disturbance recorders (FDRs) devices on a live 220kV/500kV Egyptian grid in cooperation with the Egyptian Electricity Transmission Company (EETC). The developed EWAMS can gather information from 11 FDRs devices which are geographically dispersed throughout the boundary of the Egyptian power grid and to a remote data management center located at Helwan University. The communication performance for the developed EWAMS in terms of communication time delay, throughput, and percentage of wasted bandwidth are studied in this paper. The results showed that the system can achieve successfully the communication requirements needed by various wide area monitoring applications.",0,Human
"The research introduces an efficient method for multi-user multiple-input multiple-output (MIMO) multiple-access channels that preserves full diversity gain while keeping computation minimal. This new approach leverages the characteristics of the channel matrix and uses iterative detection to enhance performance without increasing computational load. Simulation results demonstrate that this technique not only maintains full diversity but also offers better bit error rates than existing methods while requiring fewer computations. Thus, this scheme could be an excellent option for modern MIMO communication systems needing both high throughput and low power consumption.",1,AI
"This study investigates how to teach coordination strategies to groups of robots working together. These robot teams are made up of numerous simple bots that collaborate to perform intricate jobs. Yet, managing cooperation among such groups presents difficulties because of the overall complexity of the system and the absence of a central control system. Our paper introduces a reinforcement learning strategy to teach coordination tactics to robot swarms. Our technique employs a central critic and autonomous units that learn the coordination tactics. The central critic monitors the entire group's state and gives feedback to the autonomous units, which then act locally based on their view of the immediate surroundings. This feedback adjusts the autonomous units' policy parameters, enabling them to refine their collective actions over time. We tested our method on several benchmarks for robot swarms, such as forming shapes, avoiding obstacles, and collaborating on transport. Our findings indicate that our technique is successful in teaching coordinated action to robot swarms, delivering outstanding performance on these challenges and demonstrating adaptability to new settings too. Overall, our work highlights the potential of reinforcement learning for coaching coordination strategies in robot swarms, providing a solid groundwork for future studies in this field. The potential of this approach might allow the creation of more advanced and efficient robot swarms capable of tackling diverse tasks across various scenarios.",1,AI
"Kernel machines often yield superior predictive performance on various tasks; however, they suffer from severe computational challenges. In this paper, we show how to overcome the important challenge of speeding up kernel machines. In particular, we develop a parallel block minimization framework for solving kernel machines, including kernel SVM and kernel logistic regression. Our framework proceeds by dividing the problem into smaller subproblems by forming a block-diagonal approximation of the Hessian matrix. The subproblems are then solved approximately in parallel. After that, a communication efficient line search procedure is developed to ensure sufficient reduction of the objective function value at each iteration. We prove global linear convergence rate of the proposed method with a wide class of subproblem solvers, and our analysis covers strongly convex and some non-strongly convex functions. We apply our algorithm to solve large-scale kernel SVM problems on distributed systems, and show a significant improvement over existing parallel solvers. As an example, on the covtype dataset with half-a-million samples, our algorithm can obtain an approximate solution with 96% accuracy in 20 seconds using 32 machines, while all the other parallel kernel SVM solvers require more than 2000 seconds to achieve a solution with 95% accuracy. Moreover, our algorithm can scale to very large data sets, such as the kdd algebra dataset with 8 million samples and 20 million features.",0,Human
"This research explores how users perceive and evaluate erasure and undetected error probabilities differently in digital communication systems. It delves into why these errors are evaluated so differently by users, and identifies various contributing factors. The study starts by defining erasure and undetected errors and explains their importance in these systems. It then looks at what previous research has found about how users view these two types of errors, noting that there's not much agreement among experts. The research uses experiments to gauge how participants judge the severity and likelihood of erasure and undetected errors in a simulated digital communication setting. Participants provide both numbers and words to explain their judgments. The findings show that users typically regard undetected errors as more critical and damaging to trust than erasure errors. The study uncovers several reasons for these differing views, such as the way the system is set up, the user's past experiences, and personal attitudes towards risk. In summary, this research deepens our understanding of user error evaluations in digital communications, offering valuable advice for those designing reliable and trustworthy systems.",1,AI
"Domain-Specific Languages (DSLs) help practitioners in contributing solutions to challenges of specific domains. The efficient development of user-friendly DSLs suitable for industrial practitioners with little expertise in modelling still is challenging. For such practitioners, who often do not model on a daily basis, there is a need to foster reduction of repetitive modelling tasks and providing simplified visual representations of DSL parts. For industrial language engineers, there is no methodical support for providing such guidelines or documentation as part of reusable language modules. Previous research either addresses the reuse of languages or guidelines for modelling. For the efficient industrial deployment of DSLs, their combination is essential: the efficient engineering of DSLs from reusable modules that feature integrated documentation and guidelines for industrial practitioners. To solve these challenges, we propose a systematic approach for the industrial engineering of DSLs based on the concept of reusable DSL Building Blocks, which rests on several years of experience in the industrial engineering of DSLs and their deployment to various organizations. We investigated our approach via focus group methods consisting of five participants from industry and research qualitatively. Ultimately, DSL Building Blocks support industrial language engineers in developing better usable DSLs and industrial practitioners in more efficiently achieving their modelling.",0,Human
"The paper suggests an integrated strategy for optimizing resource distribution across a three-level Internet of Things (IoT) fog network that integrates the concepts of a Stackelberg game and matching algorithms. In a three-tier architecture, which includes the cloud, fog, and edge layers all equipped with different computing capabilities, the Stackelberg game represents the relationships among these tiers through a leader-follower dynamic. Meanwhile, the matching algorithm then determines the best way to distribute resources according to the network's limitations and goals. Computational evidence indicates that this innovative method is superior in terms of resource management, lower latency, and better overall system performance compared to conventional techniques. This research highlights the capability of merging game theory with optimization methods to tackle resource allocation issues in complex multi-tier IoT systems.",1,AI
"This research examines how structured mappings can be used to efficiently share common information across multiple users in a channel where multiple people are trying to communicate simultaneously. It looks at how these mappings can help reduce interference and enhance the channel's performance. The study offers a theoretical basis for creating structured mappings that balance the transmission of shared data with the resources needed by the channel. Additionally, it introduces an algorithm that utilizes sparse-graph codes to build these mappings practically. Simulation tests show that this method surpasses current techniques in terms of the data that can be transmitted and the effort required for decoding. In conclusion, structured mappings offer a promising avenue for enhancing the efficiency and capacity of communication channels, which could be applied in various fields such as wireless networks and multimedia communications.",1,AI
"This study introduces a new technique for estimating depth without labeled data using H-Net, a deep neural network with an attention system, and epipolar geometry. Estimating depth from stereo images is crucial in computer vision, useful for robotics, self-driving cars, and 3D reconstructions. The method leverages an attention mechanism to prioritize relevant parts of the images for accurate depth calculations. Epipolar geometry is used to ensure that the estimated depths adhere to geometric rules. Experimental outcomes show that this method surpasses current best practices for unsupervised depth estimation when tested on established benchmarks. Moreover, the proposed method can be flexibly applied to other vision-related tasks like optical flow estimation and image enhancement. These findings indicate that H-Net combined with epipolar geometry offers a viable strategy for depth estimation without labels, with broad applicability across computer vision domains.",1,AI
"A new architecture called Serket is introduced in this paper to link stochastic models for creating a comprehensive cognitive system. Built on a modular framework, Serket can incorporate various models into one cohesive system. Leveraging the benefits of stochastic models, which include managing uncertainty and generating probabilistic forecasts, Serket addresses some of these models' challenges, particularly their complexity when merging multiple models. The effectiveness of Serket is tested across diverse tasks like language comprehension and production, decision-making processes, and sensory processing. The outcomes indicate that Serket performs better than previous methods in terms of precision, efficiency, and clarity. Lastly, the authors discuss how Serket could influence the advancement of sophisticated cognitive systems and its possible use cases in fields such as AI and robotics.",1,AI
"This study introduces a fresh method for categorizing music artists utilizing Convolutional Recurrent Neural Networks (CRNNs). The system is structured to glean temporal and spectral information from audio signals directly, enabling it to recognize both quick and extended patterns within the audio data. We tested our model on an extensive music database and contrasted its performance against existing leading methodologies. Our findings indicate that the CRNN-based strategy we developed performs notably better in terms of precision and speed, highlighting the effectiveness of deep learning algorithms in music classification challenges.",1,AI
"In this study, we present an innovative technique for creating faster sublinear algorithms through conditional sampling. Sublinear algorithms are designed to handle vast datasets swiftly by processing only part of them. Our strategy utilizes conditional sampling to generate reliable approximations of certain functions with high confidence from just a fraction of the data. This approach significantly reduces computational time while ensuring that the outcomes remain accurate. We have validated our methodology using multiple real-world datasets and found it outperforms current sublinear algorithms. This research not only expands the field of sublinear algorithms but also holds potential benefits for applications like machine learning, data mining, and big data analysis.",1,AI
"Dense Multi-GPU systems have recently gained a lot of attention in the HPC arena. Traditionally, MPI runtimes have been primarily designed for clusters with a large number of nodes. However, with the advent of MPI+CUDA applications and CUDA-Aware MPI runtimes like MVAPICH2 and OpenMPI, it has become important to address efficient communication schemes for such dense Multi-GPU nodes. This coupled with new application workloads brought forward by Deep Learning frameworks like Caffe and Microsoft CNTK pose additional design constraints due to very large message communication of GPU buffers during the training phase. In this context, special-purpose libraries like NVIDIA NCCL have been proposed for GPU-based collective communication on dense GPU systems. In this paper, we propose a pipelined chain (ring) design for the MPI_Bcast collective operation along with an enhanced collective tuning framework in MVAPICH2-GDR that enables efficient intra-/inter-node multi-GPU communication. We present an in-depth performance landscape for the proposed MPI_Bcast schemes along with a comparative analysis of NVIDIA NCCL Broadcast and NCCL-based MPI_Bcast. The proposed designs for MVAPICH2-GDR enable up to 14X and 16.6X improvement, compared to NCCL-based solutions, for intra- and inter-node broadcast latency, respectively. In addition, the proposed designs provide up to 7% improvement over NCCL-based solutions for data parallel training of the VGG network on 128 GPUs using Microsoft CNTK.",0,Human
"The world is facing a tough situation due to the catastrophic pandemic caused by novel coronavirus (COVID-19). The number people affected by this virus are increasing exponentially day by day and the number has already crossed 6.4 million. As no vaccine has been discovered yet, the early detection of patients and isolation is the only and most effective way to reduce the spread of the virus. Detecting infected persons from chest X-Ray by using Deep Neural Networks, can be applied as a time and laborsaving solution. In this study, we tried to detect Covid-19 by classification of Covid-19, pneumonia and normal chest X-Rays. We used five different Convolutional Pre-Trained Neural Network models (VGG16, VGG19, Xception, InceptionV3 and Resnet50) and compared their performance. VGG16 and VGG19 shows precise performance in classification. Both models can classify between three kinds of X-Rays with an accuracy over 92%. Another part of our study was to find the impact of weather factors (temperature, humidity, sun hour and wind speed) on this pandemic using Decision Tree Regressor. We found that temperature, humidity and sun-hour jointly hold 85.88% impact on escalation of Covid-19 and 91.89% impact on death due to Covid-19 where humidity has 8.09% impact on death. We also tried to predict the death of an individual based on age, gender, country, and location due to COVID-19 using the LogisticRegression, which can predict death of an individual with a model accuracy of 94.40%.",0,Human
"This study delves into how machine learning algorithms can forecast routing congestion in FPGA high-level synthesis. Route congestion is a major hindrance in FPGA designs, leading to delays and higher expenses. Thus, forecasting it precisely is essential for optimizing design parameters and ensuring timely release of FPGA-based products. The suggested methodology integrates machine learning models like decision trees and random forests with conventional congestion prediction indicators to estimate routing congestion throughout the high-level synthesis phase. It also examines the influence of employing various feature extraction strategies and machine learning algorithms on the precision of congestion forecasts. Experimental outcomes indicate that this proposed method yields a high level of prediction accuracy, surpassing conventional congestion prediction techniques. Moreover, the findings reveal that techniques such as Principal Component Analysis (PCA) and Independent Component Analysis (ICA) can substantially enhance prediction accuracy. This research offers an efficient and effective strategy for anticipating routing congestion in FPGA high-level synthesis. The proposed approach can facilitate the optimization of design parameters and minimize design iterations, consequently lowering development costs and speeding up product deployment. Furthermore, the technique holds the potential for application in other FPGA design optimization issues, highlighting the significance of incorporating machine learning in FPGA design.",1,AI
"This research introduces a method to improve lexical semantic databases using distributional semantics. Our approach seeks to overcome the shortcomings of conventional lexical resources like WordNet and ontologies by integrating information extracted from extensive text corpora. Our model comprises three key elements: data acquisition, feature generation, and semantic enhancement. Data gathering entails collecting vast quantities of text from diverse sources, including online discussions, news articles, and social media. Feature creation utilizes techniques like word2vec and GloVe to create numerical representations of words within this textual data. Lastly, we merge these numerical representations back into existing semantic frameworks, such as WordNet and ontologies, to boost their coverage and precision. The efficacy of our methodology is assessed through several benchmark tests, particularly in areas like assessing word similarities and solving analogical problems. Results demonstrate that our framework notably enhances the performance of established lexical semantic systems. Overall, this framework offers a viable strategy to augment lexical semantic databases and could have broad implications for numerous natural language processing applications.",1,AI
"The paper introduces an approximate dynamic programming method for managing community recovery after a natural disaster. Existing methods typically rely on rigid, predefined recovery plans that can become obsolete in rapidly changing situations. In contrast, the proposed ADP method learns from previous disaster scenarios and adjusts recovery strategies in real-time based on the evolving needs of the affected area. This approach integrates advanced machine learning techniques with a flexible optimization model to support adaptive decision-making during recovery efforts. It addresses various goals including reducing recovery times, optimizing resource allocation, and minimizing expenses, all while factoring in the unpredictable nature and constraints of the recovery process. Furthermore, the ADP method allows for input from community members and recovery professionals, enabling continuous improvement through feedback analysis. A hypothetical case study demonstrates the ADP's effectiveness compared to traditional, static recovery plans. It shows that the ADP method can swiftly adjust to new conditions and deliver more efficient, successful recovery outcomes. In summary, this research underscores the advantages of employing ADP in disaster recovery management, offering policymakers a valuable tool to enhance preparedness and response to natural calamities. The ADP method leverages historical data, flexibly responds to evolving recovery conditions, and efficiently manages multiple recovery objectives, ultimately contributing to stronger, more resilient communities.",1,AI
"This research delves into how often sensors should report their status in wireless networks. It specifically looks at managing when to collect data (sensing) and when to resend it (retransmission) to ensure efficient status reporting. Researchers suggest a novel scheduling method that strikes a balance between getting precise readings and conserving energy by changing the intervals at which data is collected and resent depending on network conditions. Computational tests show that this new approach leads to lower energy use and better status reports than existing methods. These insights could greatly influence how we set up and use wireless sensor networks in diverse areas like weather tracking, factory control, and health care.",1,AI
"From the simple measurement of tissue attributes in pathology workflow to designing an explainable diagnostic/prognostic AI tool, access to accurate semantic segmentation of tissue regions in histology images is a prerequisite. However, delineating different tissue regions manually is a laborious, time-consuming and costly task that requires expert knowledge. On the other hand, the state-of-the-art automatic deep learning models for semantic segmentation require lots of annotated training data and there are only a limited number of tissue region annotated images publicly available. To obviate this issue in computational pathology projects and collect large-scale region annotations efficiently, we propose an efficient interactive segmentation network that requires minimum input from the user to accurately annotate different tissue types in the histology image. The user is only required to draw a simple squiggle inside each region of interest so it will be used as the guiding signal for the model. To deal with the complex appearance and amorph geometry of different tissue regions we introduce several automatic and minimalistic guiding signal generation techniques that help the model to become robust against the variation in the user input. By experimenting on a dataset of breast cancer images, we show that not only does our proposed method speed up the interactive annotation process, it can also outperform the existing automatic and interactive region segmentation models.",0,Human
"We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.",0,Human
"With the rapid development of artificial intelligence, conversational bots have became prevalent in mainstream E-commerce platforms, which can provide convenient customer service timely. To satisfy the user, the conversational bots need to understand the user's intention, detect the user's emotion, and extract the key entities from the conversational utterances. However, understanding dialogues is regarded as a very challenging task. Different from common language understanding, utterances in dialogues appear alternately from different roles and are usually organized as hierarchical structures. To facilitate the understanding of dialogues, in this paper, we propose a novel contextual dialogue encoder (i.e. DialogueBERT) based on the popular pre-trained language model BERT. Five self-supervised learning pre-training tasks are devised for learning the particularity of dialouge utterances. Four different input embeddings are integrated to catch the relationship between utterances, including turn embedding, role embedding, token embedding and position embedding. DialogueBERT was pre-trained with 70 million dialogues in real scenario, and then fine-tuned in three different downstream dialogue understanding tasks. Experimental results show that DialogueBERT achieves exciting results with 88.63% accuracy for intent recognition, 94.25% accuracy for emotion recognition and 97.04% F1 score for named entity recognition, which outperforms several strong baselines by a large margin.",0,Human
"Fishing is a beloved pastime, but many anglers encounter difficulties in locating and catching fish. This research sought to uncover the issues that anglers face and propose solutions. A survey was carried out among experienced anglers to learn more about their fishing experiences, the problems they encounter, and their favored techniques for finding and catching fish. The survey revealed that anglers frequently struggle with restricted access to fishing spots, low water quality, and dwindling fish populations. To tackle these problems, the study suggests enhancing accessibility to fishing areas, enforcing conservation measures to safeguard fish species and their ecosystems, and offering education and resources to anglers on sustainable fishing methods. These insights can guide decision-making to ensure the responsible use of fishing resources and enhance the overall fishing experience for enthusiasts.",1,AI
"We study the communication complexity of combinatorial auctions via interpolation mechanisms that interpolate between non-truthful and truthful protocols. Specifically, an interpolation mechanism has two phases. In the first phase, the bidders participate in some non-truthful protocol whose output is itself a truthful protocol. In the second phase, the bidders participate in the truthful protocol selected during phase one. Note that virtually all existing auctions have either a non-existent first phase (and are therefore truthful mechanisms), or a non-existent second phase (and are therefore just traditional protocols, analyzed via the Price of Anarchy/Stability).  The goal of this paper is to understand the benefits of interpolation mechanisms versus truthful mechanisms or traditional protocols, and develop the necessary tools to formally study them. Interestingly, we exhibit settings where interpolation mechanisms greatly outperform the optimal traditional and truthful protocols. Yet, we also exhibit settings where interpolation mechanisms are provably no better than truthful ones. Finally, we apply our new machinery to prove that the recent single-bid mechanism of Devanur et. al.~\cite{DevanurMSW15} (the only pre-existing interpolation mechanism in the literature) achieves the optimal price of anarchy among a wide class of protocols, a claim that simply can't be addressed by appealing just to machinery from communication complexity or the study of truthful mechanisms.",0,Human
"This study introduces an innovative strategy for enhancing how information spreads across networks using message passing techniques. It integrates concepts from graph theory and probability theory to precisely depict the diffusion of information or influence within a network. The researchers validate the effectiveness of their technique via rigorous simulations and practical tests conducted on diverse graph configurations, revealing substantial enhancements in efficiency when compared to conventional methods. The findings of this research hold significant value for multiple disciplines, including social network analysis, marketing, and network architecture design.",1,AI
"Multi-step manipulation tasks in unstructured environments are extremely challenging for a robot to learn. Such tasks interlace high-level reasoning that consists of the expected states that can be attained to achieve an overall task and low-level reasoning that decides what actions will yield these states. We propose a model-free deep reinforcement learning method to learn multi-step manipulation tasks. We introduce a Robotic Manipulation Network (RoManNet), which is a vision-based model architecture, to learn the action-value functions and predict manipulation action candidates. We define a Task Progress based Gaussian (TPG) reward function that computes the reward based on actions that lead to successful motion primitives and progress towards the overall task goal. To balance the ratio of exploration/exploitation, we introduce a Loss Adjusted Exploration (LAE) policy that determines actions from the action candidates according to the Boltzmann distribution of loss estimates. We demonstrate the effectiveness of our approach by training RoManNet to learn several challenging multi-step robotic manipulation tasks in both simulation and real-world. Experimental results show that our method outperforms the existing methods and achieves state-of-the-art performance in terms of success rate and action efficiency. The ablation studies show that TPG and LAE are especially beneficial for tasks like multiple block stacking. Code is available at: https://github.com/skumra/romannet",0,Human
"A sound Decision-Making (DM) process is key to the successful governance of software projects. In many Open Source Software Development (OSSD) communities, DM processes lie buried amongst vast amounts of publicly available data. Hidden within this data lie the rationale for decisions that led to the evolution and maintenance of software products. While there have been some efforts to extract DM processes from publicly available data, the rationale behind how the decisions are made have seldom been explored. Extracting the rationale for these decisions can facilitate transparency (by making them known), and also promote accountability on the part of decision-makers. This work bridges this gap by means of a large-scale study that unearths the rationale behind decisions from Python development email archives comprising about 1.5 million emails. This paper makes two main contributions. First, it makes a knowledge contribution by unearthing and presenting the rationale behind decisions made. Second, it makes a methodological contribution by presenting a heuristics-based rationale extraction system called Rationale Miner that employs multiple heuristics, and follows a data-driven, bottom-up approach to infer the rationale behind specific decisions (e.g., whether a new module is implemented based on core developer consensus or benevolent dictator's pronouncement). Our approach can be applied to extract rationale in other OSSD communities that have similar governance structures.",0,Human
"Attempt to fully discover the temporal diversity and chronological characteristics for self-supervised video representation learning, this work takes advantage of the temporal dependencies within videos and further proposes a novel self-supervised method named Temporal Contrastive Graph Learning (TCGL). In contrast to the existing methods that ignore modeling elaborate temporal dependencies, our TCGL roots in a hybrid graph contrastive learning strategy to jointly regard the inter-snippet and intra-snippet temporal dependencies as self-supervision signals for temporal representation learning. To model multi-scale temporal dependencies, our TCGL integrates the prior knowledge about the frame and snippet orders into graph structures, i.e., the intra-/inter- snippet temporal contrastive graphs. By randomly removing edges and masking nodes of the intra-snippet graphs or inter-snippet graphs, our TCGL can generate different correlated graph views. Then, specific contrastive learning modules are designed to maximize the agreement between nodes in different views. To adaptively learn the global context representation and recalibrate the channel-wise features, we introduce an adaptive video snippet order prediction module, which leverages the relational knowledge among video snippets to predict the actual snippet orders. Experimental results demonstrate the superiority of our TCGL over the state-of-the-art methods on large-scale action recognition and video retrieval benchmarks.",0,Human
"This research introduces a novel heterogeneous graph embedding framework named MTHetGNN, designed for predicting multivariate time series data. MTHetGNN integrates the advantages of graph neural networks and attention mechanisms to understand intricate connections between multiple time series and their interdependencies. It uses a heterogeneous graph to depict various relationships between these series and utilizes graph convolutional neural networks to derive representations for the nodes within this graph. An attention mechanism is employed to highlight the significance of each node during the forecasting process. The effectiveness of the proposed framework is tested across several multivariate time series prediction challenges, showing marked improvements in both predictive accuracy and computational efficiency compared to current top-tier methodologies. This investigation offers a fresh perspective on tackling the issue of multivariate time series forecasting and could have practical implications in diverse sectors like finance, energy, and transportation.",1,AI
"This study introduces SCSGuard, an innovative deep learning method aimed at identifying scams within Ethereum smart contracts. Scamming is a major issue in the Ethereum environment, potentially leading to substantial monetary losses for users. Presently, most methods for spotting scams in smart contracts employ rule-based techniques or static analysis. In contrast, SCSGuard leverages a deep learning model to recognize irregularities in the bytecode of these contracts. To assess the efficacy of SCSGuard, we amassed a substantial dataset containing both known scams and non-scam contracts, and trained the model through supervised learning. We compared SCSGuard’s performance against multiple leading methods for detecting scams in smart contracts. The findings indicate that SCSGuard outperforms these methods in terms of accuracy and recall, suggesting its capability to reliably identify scams in practical smart contracts. Additionally, we explored the elements contributing to SCSGuard’s success, such as the selection of the neural network architecture, the quantity and quality of the training data, and the application of transfer learning to accommodate various scam types. We also acknowledged the limitations of SCSGuard, including challenges in model interpretation and the need for evaluating its resilience against adversarial examples. Ultimately, our work highlights the potential of deep learning in detecting scams within Ethereum smart contracts, and offers guidance for enhancing the overall security and reliability of the Ethereum ecosystem. We envision that further research in this field could lead to improved solutions for ensuring the safety and trustworthiness of Ethereum smart contracts.",1,AI
"This paper aims to conduct a thorough exploration of belief modeling within dynamic systems. It's structured into two main sections. The initial part lays down the groundwork for the topic. Here, the authors define what belief means and why it's crucial in dynamic systems. They then give a detailed overview of previous research on how belief is modeled in these systems, highlighting different methodologies employed. Following this, they introduce a framework for belief modeling in dynamic systems, which encompasses several essential elements like belief representation, updating mechanisms, and evaluation criteria. The framework also delves into the role of uncertainty and discusses ways to measure it. The authors further address the various hurdles in belief modeling within dynamic systems and suggest potential avenues for future research. In summary, this first part offers a strong base for comprehending belief modeling in dynamic systems. By providing a broad review of existing literature and a useful framework, the authors make this paper particularly valuable for both researchers and practitioners in the field. This paper serves as an indispensable resource for those studying belief modeling in dynamic systems.",1,AI
"Public speaking is an important aspect of human communication and interaction. The majority of computational work on public speaking concentrates on analyzing the spoken content, and the verbal behavior of the speakers. While the success of public speaking largely depends on the content of the talk, and the verbal behavior, non-verbal (visual) cues, such as gestures and physical appearance also play a significant role. This paper investigates the importance of visual cues by estimating their contribution towards predicting the popularity of a public lecture. For this purpose, we constructed a large database of more than $1800$ TED talk videos. As a measure of popularity of the TED talks, we leverage the corresponding (online) viewers' ratings from YouTube. Visual cues related to facial and physical appearance, facial expressions, and pose variations are extracted from the video frames using convolutional neural network (CNN) models. Thereafter, an attention-based long short-term memory (LSTM) network is proposed to predict the video popularity from the sequence of visual features. The proposed network achieves state-of-the-art prediction accuracy indicating that visual cues alone contain highly predictive information about the popularity of a talk. Furthermore, our network learns a human-like attention mechanism, which is particularly useful for interpretability, i.e. how attention varies with time, and across different visual cues by indicating their relative importance.",0,Human
"This study uses a bibliometric approach to review literature on the digital agricultural revolution, examining how research trends have evolved over the last decade. It covers papers published between 2011 and 2021 in databases such as Scopus, Web of Science, and Google Scholar. The analysis looks at the expansion of research in this area, identifies key authors and works, and pinpoints major themes and shifts in focus.

The results show that interest in digital agricultural innovation has increased significantly during this period, with a strong focus on areas like precision farming, sensor technology, and big data analysis. Notably, many of the most cited work comes from developed nations, suggesting a need for more research from less affluent regions, especially in developing countries. Overall, the study underscores the importance of continuing research into the digital agricultural revolution, addressing both its challenges and potential benefits.",1,AI
"The research delves into how new words and phrases, or neologisms, are employed on Facebook. By analyzing a vast dataset of posts and comments gathered from Facebook, researchers identified the most prevalent neologisms and observed how they're utilized. The findings indicated that these neologisms are predominantly used to convey emotions, generate humor, and foster a sense of community among users. Notably, younger individuals tend to use them more frequently, and these words spread rapidly across the platform thanks to Facebook's extensive connectivity. This study underscores the influence of social media on language and underscores the significance of keeping an eye on the evolution of neologisms within this context.",1,AI
"Reinforcement learning has lead to considerable break-throughs in diverse areas such as robotics, games and many others. But the application to RL in complex real-world decision making problems remains limited. Many problems in operations management (inventory and revenue management, for example) are characterized by large action spaces and stochastic system dynamics. These characteristics make the problem considerably harder to solve for existing RL methods that rely on enumeration techniques to solve per step action problems. To resolve these issues, we develop Programmable Actor Reinforcement Learning (PARL), a policy iteration method that uses techniques from integer programming and sample average approximation. Analytically, we show that the for a given critic, the learned policy in each iteration converges to the optimal policy as the underlying samples of the uncertainty go to infinity. Practically, we show that a properly selected discretization of the underlying uncertain distribution can yield near optimal actor policy even with very few samples from the underlying uncertainty. We then apply our algorithm to real-world inventory management problems with complex supply chain structures and show that PARL outperforms state-of-the-art RL and inventory optimization methods in these settings. We find that PARL outperforms commonly used base stock heuristic by 44.7% and the best performing RL method by up to 12.1% on average across different supply chain environments.",0,Human
"Methods for neural network hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of model configurations. In this paper, we show that standard frequentist regression models can predict the final performance of partially trained model configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We empirically show that our performance prediction models are much more effective than prominent Bayesian counterparts, are simpler to implement, and are faster to train. Our models can predict final performance in both visual classification and language modeling domains, are effective for predicting performance of drastically varying model architectures, and can even generalize between model classes. Using these prediction models, we also propose an early stopping method for hyperparameter optimization and meta-modeling, which obtains a speedup of a factor up to 6x in both hyperparameter optimization and meta-modeling. Finally, we empirically show that our early stopping method can be seamlessly incorporated into both reinforcement learning-based architecture selection algorithms and bandit based search methods. Through extensive experimentation, we empirically show our performance prediction models and early stopping algorithm are state-of-the-art in terms of prediction accuracy and speedup achieved while still identifying the optimal model configurations.",0,Human
"This research introduces a fresh technique for image classification and semantic segmentation employing structured binary neural networks. The suggested strategy employs binary weights and activations, which diminishes memory and computational demands while sustaining high accuracy. The study illustrates that this architecture improves image classification accuracy beyond conventional neural networks utilizing floating-point weights and activations. Furthermore, the method is expanded to semantic segmentation tasks and outperforms current approaches significantly. Extensive experimentation on established datasets confirms the effectiveness of the proposed method. Structured binary neural networks offer a promising avenue for precise image classification and semantic segmentation with reduced computational and memory needs.",1,AI
"Photo-realistic modeling and rendering of fuzzy objects with complex opacity are critical for numerous immersive VR/AR applications, but it suffers from strong view-dependent brightness, color. In this paper, we propose a novel scheme to generate opacity radiance fields with a convolutional neural renderer for fuzzy objects, which is the first to combine both explicit opacity supervision and convolutional mechanism into the neural radiance field framework so as to enable high-quality appearance and global consistent alpha mattes generation in arbitrary novel views. More specifically, we propose an efficient sampling strategy along with both the camera rays and image plane, which enables efficient radiance field sampling and learning in a patch-wise manner, as well as a novel volumetric feature integration scheme that generates per-patch hybrid feature embeddings to reconstruct the view-consistent fine-detailed appearance and opacity output. We further adopt a patch-wise adversarial training scheme to preserve both high-frequency appearance and opacity details in a self-supervised framework. We also introduce an effective multi-view image capture system to capture high-quality color and alpha maps for challenging fuzzy objects. Extensive experiments on existing and our new challenging fuzzy object dataset demonstrate that our method achieves photo-realistic, globally consistent, and fined detailed appearance and opacity free-viewpoint rendering for various fuzzy objects.",0,Human
"Data structures for efficient sampling from a set of weighted items are an important building block of many applications. However, few parallel solutions are known. We close many of these gaps both for shared-memory and distributed-memory machines. We give efficient, fast, and practicable parallel algorithms for building data structures that support sampling single items (alias tables, compressed data structures). This also yields a simplified and more space-efficient sequential algorithm for alias table construction. Our approaches to sampling $k$ out of $n$ items with/without replacement and to subset (Poisson) sampling are output-sensitive, i.e., the sampling algorithms use work linear in the number of different samples. This is also interesting in the sequential case. Weighted random permutation can be done by sorting appropriate random deviates. We show that this is possible with linear work using a nonlinear transformation of these deviates. Finally, we give a communication-efficient, highly scalable approach to (weighted and unweighted) reservoir sampling. This algorithm is based on a fully distributed model of streaming algorithms that might be of independent interest. Experiments for alias tables and sampling with replacement show near linear speedups both for construction and queries using up to 158 threads of shared-memory machines. An experimental evaluation of distributed weighted reservoir sampling on up to 256 nodes (5120 cores) also shows good speedups.",0,Human
"Recently, barrier function-based safe reinforcement learning (RL) with the actor-critic structure for continuous control tasks has received increasing attention. It is still challenging to learn a near-optimal control policy with safety and convergence guarantees. Also, few works have addressed the safe RL algorithm design under time-varying safety constraints. This paper proposes a model-based safe RL algorithm for optimal control of nonlinear systems with time-varying state and control constraints. In the proposed approach, we construct a novel barrier-based control policy structure that can guarantee control safety. A multi-step policy evaluation mechanism is proposed to predict the policy's safety risk under time-varying safety constraints and guide the policy to update safely. Theoretical results on stability and robustness are proven. Also, the convergence of the actor-critic learning algorithm is analyzed. The performance of the proposed algorithm outperforms several state-of-the-art RL algorithms in the simulated Safety Gym environment. Furthermore, the approach is applied to the integrated path following and collision avoidance problem for two real-world intelligent vehicles. A differential-drive vehicle and an Ackermann-drive one are used to verify the offline deployment performance and the online learning performance, respectively. Our approach shows an impressive sim-to-real transfer capability and a satisfactory online control performance in the experiment.",0,Human
"In quasi-proportional auctions, each bidder receives a fraction of the allocation equal to the weight of their bid divided by the sum of weights of all bids, where each bid's weight is determined by a weight function. We study the relationship between the weight function, bidders' private values, number of bidders, and the seller's revenue in equilibrium. It has been shown that if one bidder has a much higher private value than the others, then a nearly flat weight function maximizes revenue. Essentially, threatening the bidder who has the highest valuation with having to share the allocation maximizes the revenue. We show that as bidder private values approach parity, steeper weight functions maximize revenue by making the quasi-proportional auction more like a winner-take-all auction. We also show that steeper weight functions maximize revenue as the number of bidders increases. For flatter weight functions, there is known to be a unique pure-strategy Nash equilibrium. We show that a pure-strategy Nash equilibrium also exists for steeper weight functions, and we give lower bounds for bids at an equilibrium. For a special case that includes the two-bidder auction, we show that the pure-strategy Nash equilibrium is unique, and we show how to compute the revenue at equilibrium. We also show that selecting a weight function based on private value ratios and number of bidders is necessary for a quasi-proportional auction to produce more revenue than a second-price auction.",0,Human
"Frequency estimation is crucial in many signal processing contexts. This study introduces a new technique for estimating frequencies with a one-sided error margin, a scenario where the estimate must fall within a defined boundary—either above or below the actual value. Our approach leverages an enhanced periodogram alongside a statistical test. When compared against conventional frequency estimation techniques, our method shows superiority in terms of precision, reliability, and efficiency across multiple situations. This innovation will be especially useful for those engaged in signal processing projects where precise frequency estimation with a one-sided tolerance is essential.",1,AI
"The existence of tactile afferents sensitive to slip-related mechanical transients in the human hand augments the robustness of grasping through secondary force modulation protocols. Despite this knowledge and the fact that tactile-based slip detection has been researched for decades, robust slip detection is still not an out-of-the-box capability for any commercially available tactile sensor. This research seeks to bridge this gap with a comprehensive study addressing several aspects of slip detection. Key developments include a systematic data collection process yielding millions of sensory data points, the generalized conversion of multivariate-to-univariate sensor output, an insightful spectral analysis of the univariate sensor outputs, and the application of Long Short-Term Memory (LSTM) neural networks on the univariate signals to produce robust slip detectors from three commercially available sensors capable of tactile sensing. The sensing elements underlying these sensors vary in quantity, spatial arrangement, and mechanics, leveraging principles in electro-mechanical resistance, optics, and hydro-acoustics. Critically, slip detection performance of the tactile technologies is quantified through a measurement methodology that unveils the effects of data window size, sampling rate, material type, slip speed, and sensor manufacturing variability. Results indicate that the investigated commercial tactile sensors are inherently capable of high-quality slip detection.",0,Human
"Egocentric action anticipation is the task of predicting the future actions a camera wearer will likely perform based on past video observations. While in a real-world system it is fundamental to output such predictions before the action begins, past works have not generally paid attention to model runtime during evaluation. Indeed, current evaluation schemes assume that predictions can be made offline, and hence that computational resources are not limited. In contrast, in this paper, we propose a ``streaming'' egocentric action anticipation evaluation protocol which explicitly considers model runtime for performance assessment, assuming that predictions will be available only after the current video segment is processed, which depends on the processing time of a method. Following the proposed evaluation scheme, we benchmark different state-of-the-art approaches for egocentric action anticipation on two popular datasets. Our analysis shows that models with a smaller runtime tend to outperform heavier models in the considered streaming scenario, thus changing the rankings generally observed in standard offline evaluations. Based on this observation, we propose a lightweight action anticipation model consisting in a simple feed-forward 3D CNN, which we propose to optimize using knowledge distillation techniques and a custom loss. The results show that the proposed approach outperforms prior art in the streaming scenario, also in combination with other lightweight models.",0,Human
"Over the past few years, 5G technology has emerged as a vital component of wireless communication networks, offering high-speed data transfer and lower latency. The growing demand for 5G services in crowded urban areas has spurred the need for efficient strategies to choose the right radio access technologies (RATs) for these networks. This paper examines the critical elements that affect RAT selection for dense 5G wireless networks, including network architecture, network density, available spectrum, and user demands. A novel method for RAT selection is introduced, which uses machine learning algorithms to forecast the best RAT for specific scenarios by considering real-time network conditions. The method takes into account several indicators like network traffic, interference levels, and network coverage to make informed choices. This innovative approach has been tested using simulations and experiments, demonstrating its superiority over traditional methods in terms of network performance and user satisfaction. This research sheds light on the complexities involved in selecting RATs for 5G dense networks and suggests a promising strategy for network operators and designers to enhance network efficiency and improve user experiences. The proposed technique is anticipated to significantly influence the evolution of 5G networks and advance the realm of wireless communication systems.",1,AI
"Quora is a popular Q&A site which provides users with the ability to tag questions with multiple relevant topics which helps to attract quality answers. These topics are not predefined but user-defined conventions and it is not so rare to have multiple such conventions present in the Quora ecosystem describing exactly the same concept. In almost all such cases, users (or Quora moderators) manually merge the topic pair into one of the either topics, thus selecting one of the competing conventions. An important application for the site therefore is to identify such competing conventions early enough that should merge in future. In this paper, we propose a two-step approach that uniquely combines the anomaly detection and the supervised classification frameworks to predict whether two topics from among millions of topic pairs are indeed competing conventions, and should merge, achieving an F-score of 0.711. We also develop a model to predict the direction of the topic merge, i.e., the winning convention, achieving an F-score of 0.898. Our system is also able to predict ~ 25% of the correct case of merges within the first month of the merge and ~ 40% of the cases within a year. This is an encouraging result since Quora users on average take 936 days to identify such a correct merge. Human judgment experiments show that our system is able to predict almost all the correct cases that humans can predict plus 37.24% correct cases which the humans are not able to identify at all.",0,Human
"This paper investigates how two kinds of square-shaped tiles, each with four distinct edge types (sticky or non-sticky), can spontaneously form complex structures when placed on a lattice without a set pattern. Through computer simulations and mathematical models, the researchers examine how varying factors like tile density and temperature influence these structures, leading to outcomes like aperiodic tilings and quasi-crystalline patterns. The study highlights the possibility of non-deterministic self-assembly as a method for producing new materials with special characteristics and adds to our understanding of self-assembling systems.",1,AI
"This study investigates how to scale out acid applications through operation partitioning. Acid, which stands for Atomicity, Consistency, Isolation, and Durability, is vital for ensuring secure and dependable transaction processing in databases. As systems expand in size and intricacy, maintaining these acid properties becomes challenging. The research suggests employing operation partitioning as a strategy to enhance scalability, offering better performance and higher reliability. To test this idea, the study implemented operation partitioning in an actual acid application, examining its effects on key performance indicators. The findings show that operation partitioning is a viable solution for scaling acid applications, providing enhanced performance and reliability while preserving the critical acid characteristics.",1,AI
"Graph neural networks (GNN) have been proven to be mature enough for handling graph-structured data on node-level graph representation learning tasks. However, the graph pooling technique for learning expressive graph-level representation is critical yet still challenging. Existing pooling methods either struggle to capture the local substructure or fail to effectively utilize high-order dependency, thus diminishing the expression capability. In this paper we propose HAP, a hierarchical graph-level representation learning framework, which is adaptively sensitive to graph structures, i.e., HAP clusters local substructures incorporating with high-order dependencies. HAP utilizes a novel cross-level attention mechanism MOA to naturally focus more on close neighborhood while effectively capture higher-order dependency that may contain crucial information. It also learns a global graph content GCont that extracts the graph pattern properties to make the pre- and post-coarsening graph content maintain stable, thus providing global guidance in graph coarsening. This novel innovation also facilitates generalization across graphs with the same form of features. Extensive experiments on fourteen datasets show that HAP significantly outperforms twelve popular graph pooling methods on graph classification task with an maximum accuracy improvement of 22.79%, and exceeds the performance of state-of-the-art graph matching and graph similarity learning algorithms by over 3.5% and 16.7%.",0,Human
"This study introduces an innovative method for enhancing the performance of cooperative dual-band UAV swarms by optimizing their flight paths. The proposed technique makes use of the distinctive features of dual-band UAVs, capable of operating across two distinct frequency bands at once, to boost the swarm's communication and sensory abilities. The optimization problem for the UAV swarms is framed as a multi-objective optimization challenge that takes into account various goals such as reducing the total duration of the mission, minimizing the energy usage of the drones, and maximizing the efficiency of the communication and sensing connections among the drones. A genetic algorithm is employed to identify the best flight paths that align with these objectives. The research showcases simulation outcomes that validate the efficacy of the proposed solution in improving the performance of the cooperative dual-band UAV swarm. These simulations indicate that the proposed method can substantially decrease the mission duration and energy expenditure, all while enhancing the quality of the communication and sensory links. Additionally, the paper outlines the real-world applicability of the proposed strategy and suggests potential avenues for further investigation. In summary, this work advances the understanding of cooperative UAV swarms and offers a viable path for optimizing dual-band UAV trajectories in practical scenarios.",1,AI
"A fundamental problem in neuroscience is to characterize the dynamics of spiking from the neurons in a circuit that is involved in learning about a stimulus or a contingency. A key limitation of current methods to analyze neural spiking data is the need to collapse neural activity over time or trials, which may cause the loss of information pertinent to understanding the function of a neuron or circuit. We introduce a new method that can determine not only the trial-to-trial dynamics that accompany the learning of a contingency by a neuron, but also the latency of this learning with respect to the onset of a conditioned stimulus. The backbone of the method is a separable two-dimensional (2D) random field (RF) model of neural spike rasters, in which the joint conditional intensity function of a neuron over time and trials depends on two latent Markovian state sequences that evolve separately but in parallel. Classical tools to estimate state-space models cannot be applied readily to our 2D separable RF model. We develop efficient statistical and computational tools to estimate the parameters of the separable 2D RF model. We apply these to data collected from neurons in the pre-frontal cortex (PFC) in an experiment designed to characterize the neural underpinnings of the associative learning of fear in mice. Overall, the separable 2D RF model provides a detailed, interpretable, characterization of the dynamics of neural spiking that accompany the learning of a contingency.",0,Human
"Iterative optimization heuristics (IOHs) are commonly utilized to tackle intricate issues across different fields. To ensure their efficacy, benchmarking and profiling are vital for assessing and enhancing these algorithms. This study introduces iohprofiler, a novel benchmarking and profiling tool tailored specifically for IOHs. iohprofiler offers a wide array of benchmark scenarios, performance indicators, and profiling mechanisms to facilitate evaluations and improvements in both the performance and scalability of IOHs. The paper details the development and execution of iohprofiler, and showcases experimental outcomes that validate its utility in testing and optimizing multiple IOHs. These findings underscore the advantages of employing iohprofiler for benchmarking and optimizing IOHs and establish it as a significant resource for professionals and researchers in the area.",1,AI
"Most distributed-memory bulk-synchronous parallel programs in HPC assume that compute resources are available continuously and homogeneously across the allocated set of compute nodes. However, long one-off delays on individual processes can cause global disturbances, so-called idle waves, by rippling through the system. This process is mainly governed by the communication topology of the underlying parallel code. This paper makes significant contributions to the understanding of idle wave dynamics. We study the propagation mechanisms of idle waves across the ranks of MPI-parallel programs. We present a validated analytic model for their propagation velocity with respect to communication parameters and topology, with a special emphasis on sparse communication patterns. We study the interaction of idle waves with MPI collectives and show that, depending on the implementation, a collective may be transparent to the wave. Finally we analyze two mechanisms of idle wave decay: topological decay, which is rooted in differences in communication characteristics among parts of the system, and noise-induced decay, which is caused by system or application noise. We show that noise-induced decay is largely independent of noise characteristics but depends only on the overall noise power. An analytic expression for idle wave decay rate with respect to noise power is derived. For model validation we use microbenchmarks and stencil algorithms on three different supercomputing platforms.",0,Human
"With YouTube becoming more popular for sharing content, there are growing concerns about the potential dangers it could pose to children, especially when they come across inappropriate or harmful material. This study examines how to detect, characterize, and analyze such potentially risky content on YouTube. We examined a vast dataset of nearly 100,000 videos from the Kidstube collection to identify videos that could be harmful to children, focusing on violent, sexual, and misleading content. Additionally, we looked at how promoters manipulate children’s emotions or beliefs to gain profit, often using deceptive advertisements and manipulating search results. Our research provides valuable information on the frequency of child-unfriendly content on YouTube, the ways promoters attract young viewers, and the possible consequences on children’s mental health. The research underscores the necessity for enhanced detection and removal of harmful content, as well as stricter control over advertising methods on the platform to safeguard children from potential harm.",1,AI
"Subgraph queries also known as subgraph isomorphism search is a fundamental problem in querying graph-like structured data. It consists to enumerate the subgraphs of a data graph that match a query graph. This problem arises in many real-world applications related to query processing or pattern recognition such as computer vision, social network analysis, bioinformatic and big data analytic. Subgraph isomorphism search knows a lot of investigations and solutions mainly because of its importance and use but also because of its NP-completeness. Existing solutions use filtering mechanisms and optimise the order within witch the query vertices are matched on the data vertices to obtain acceptable processing times. However, existing approaches are iterative and generate several intermediate results. They also require that the data graph is loaded in main memory and consequently are not adapted to large graphs that do not fit into memory or are accessed by streams. To tackle this problem, we propose a new approach based on concepts widely different from existing works. Our approach distills the semantic and topological information that surround a vertex into a simple integer. This simple vertex encoding that can be computed and updated incrementally reduces considerably intermediate results and avoid to load the entire data graph into main memory. We evaluate our approach on several real-word datasets. The experimental results show that our approach is efficient and scalable.",0,Human
"This study looks at how political discussions on Twitter evolved from 2014 to 2019, analyzing trends through Twitter data. It reveals that Twitter has become a crucial space for political discourse, with more people engaging in these conversations. The research also shows that the type of political talk has shifted, with discussions becoming more divided and the atmosphere more confrontational. The findings suggest that these changes on Twitter mirror broader shifts in politics, as more people use social media to express their views and participate in political conversations. The paper underscores the significance of tracking these changes in political discussions online and emphasizes the ongoing need for research to comprehend how these shifts influence political dialogue and democracy.",1,AI
"This study introduces a new method called SRGRAND for decoding in communication systems. Unlike traditional techniques, which solely depend on channel state information (CSI) for decoding, SRGRAND uses both CSI and symbol reliability data to boost performance. Symbol reliability is determined by analyzing the statistical characteristics of the transmitted symbols and the noise present in the channel. This extra information helps determine the quality of each symbol, which is then utilized to refine the decoding process. As a result, simulations demonstrate that SRGRAND performs better than existing methods and offers greater resilience in real-world communication scenarios.",1,AI
"We present a path planning framework that takes into account the human's safety perception in the presence of a flying robot. The framework addresses two objectives: (i) estimation of the uncertain parameters of the proposed safety perception model based on test data collected using Virtual Reality (VR) testbed, and (ii) offline optimal control computation using the estimated safety perception model. Due to the unknown factors in the human tests data, it is not suitable to use standard regression techniques that minimize the mean squared error (MSE). We propose to use a Hidden Markov model (HMM) approach where human's attention is considered as a hidden state to infer whether the data samples are relevant to learn the safety perception model. The HMM approach improved log-likelihood over the standard least squares solution. For path planning, we use Bernstein polynomials for discretization, as the resulting path remains within the convex hull of the control points, providing guarantees for deconfliction with obstacles at low computational cost. An example of optimal trajectory generation using the learned human model is presented. The optimal trajectory generated using the proposed model results in reasonable safety distance from the human. In contrast, the paths generated using the standard regression model have undesirable shapes due to overfitting. The example demonstrates that the HMM approach has robustness to the unknown factors compared to the standard MSE model.",0,Human
"This study examines the difficulty and potential solutions for the probabilistic p-center problem, considering various pressures. The p-center problem is a common issue in operations research and computer science, focusing on placing p facilities within a network so that the longest distance from any point to its nearest facility is as small as possible. The probabilistic p-center problem broadens this by taking into account how demands at different points might vary according to a probability distribution. The 'pressure' aspect refers to any limiting conditions or external influences that might impact the decision-making process. Initially, the paper introduces the probabilistic p-center problem along with its variations, specifically the classical and pressure-sensitive versions. It then investigates the computational challenges of solving this problem, demonstrating its NP-hard nature through transformations from other known NP-hard problems. The paper also offers approximation methods for dealing with the probabilistic p-center problem under pressure, evaluating their efficiency and offering examples with numerical data to show their practicality. Lastly, the study concludes by highlighting the main findings and suggesting areas for further exploration. These findings offer crucial knowledge about the complexities and approximations involved in the probabilistic p-center problem under pressure, serving as an important resource for professionals and academics in the fields of operations research and computer science.",1,AI
"This research investigates how self-supervised learning can enhance the representation of tabular data. Current methods often rely on manually crafted features or pre-existing embeddings, which might miss the data's true structure. To address this issue, we propose a novel approach that utilizes the intrinsic structure of tables to generate more insightful representations. We design two self-supervised tasks aimed at predicting missing entries and restoring damaged tables. Our experiments on various real-world datasets validate the superiority of our method over conventional approaches for tabular data representation. The results indicate that self-supervised learning could be a valuable strategy for better understanding tabular data and boosting performance in tasks like classification and regression.",1,AI
"Unsupervised domain adaptation (UDA) aims to solve the problem of knowledge transfer from labeled source domain to unlabeled target domain. Recently, many domain adaptation (DA) methods use centroid to align the local distribution of different domains, that is, to align different classes. This improves the effect of domain adaptation, but domain differences exist not only between classes, but also between samples. This work rethinks what is the alignment between different domains, and studies how to achieve the real alignment between different domains. Previous DA methods only considered one distribution feature of aligned samples, such as full distribution or local distribution. In addition to aligning the global distribution, the real domain adaptation should also align the meso distribution and the micro distribution. Therefore, this study propose a double classifier method based on high confidence label (DCP). By aligning the centroid and the distribution between centroid and sample of different classifiers, the meso and micro distribution alignment of different domains is realized. In addition, in order to reduce the chain error caused by error marking, This study propose a high confidence marking method to reduce the marking error. To verify its versatility, this study evaluates DCP on digital recognition and target recognition data sets. The results show that our method achieves state-of-the-art results on most of the current domain adaptation benchmark datasets.",0,Human
"This study introduces a Newton-based technique for managing the optimal control of switched systems, which takes advantage of the system's inherent characteristics to enhance computational speed. When compared to other approaches, this proposed method yields more precise outcomes and does so more swiftly. The findings illustrate the efficacy of utilizing the system's structure to tackle the complexities involved in controlling switched systems and suggest potential avenues for further investigation in this field.",1,AI
"In Knowledge Representation, it is crucial that knowledge engineers have a good understanding of the formal expressions that they write. What formal expressions state intuitively about the domain of discourse is studied in the theory of the informal semantics of a logic. In this paper we study the informal semantics of Answer Set Programming. The roots of answer set programming lie in the language of Extended Logic Programming, which was introduced initially as an epistemic logic for default and autoepistemic reasoning. In 1999, the seminal papers on answer set programming proposed to use this logic for a different purpose, namely, to model and solve search problems. Currently, the language is used primarily in this new role. However, the original epistemic intuitions lose their explanatory relevance in this new context. How answer set programs are connected to the specifications of problems they model is more easily explained in a classical Tarskian semantics, in which models correspond to possible worlds, rather than to belief states of an epistemic agent. In this paper, we develop a new theory of the informal semantics of answer set programming, which is formulated in the Tarskian setting and based on Frege's compositionality principle. It differs substantially from the earlier epistemic theory of informal semantics, providing a different view on the meaning of the connectives in answer set programming and on its relation to other logics, in particular classical logic.",0,Human
"The application of reinforcement learning algorithms onto real life problems always bears the challenge of filtering the environmental state out of raw sensor readings. While most approaches use heuristics, biology suggests that there must exist an unsupervised method to construct such filters automatically. Besides the extraction of environmental states, the filters have to represent them in a fashion that support modern reinforcement algorithms. Many popular algorithms use a linear architecture, so one should aim at filters that have good approximation properties in combination with linear functions. This thesis wants to propose the unsupervised method slow feature analysis (SFA) for this task. Presented with a random sequence of sensor readings, SFA learns a set of filters. With growing model complexity and training examples, the filters converge against trigonometric polynomial functions. These are known to possess excellent approximation capabilities and should therfore support the reinforcement algorithms well. We evaluate this claim on a robot. The task is to learn a navigational control in a simple environment using the least square policy iteration (LSPI) algorithm. The only accessible sensor is a head mounted video camera, but without meaningful filtering, video images are not suited as LSPI input. We will show that filters learned by SFA, based on a random walk video of the robot, allow the learned control to navigate successfully in ca. 80% of the test trials.",0,Human
"This research introduces a deep learning method for identifying Distributed Denial of Service (DDoS) and Denial of Service (DoS) attacks within Internet of Things (IoT) networks. It leverages a deep residual network (ResNet) to recognize patterns in network traffic and differentiate between normal activity and malicious activity. The ResNet is trained on a vast dataset of network traffic that includes different kinds of attacks and regular network behavior. The method's performance is tested on a real-world IoT network dataset, where it was found to accurately detect attacks and surpass other baseline models. Moreover, the approach demonstrates its ability to identify zero-day attacks, which are novel and undetected threats that conventional signature-based detection cannot handle. The technique also shows minimal false positives, thereby reducing unnecessary disruptions caused by false alerts. In summary, these findings indicate that deep learning techniques are highly effective in combating DDoS and DoS attacks in IoT environments and point to the potential of ResNet as a useful tool for this task.",1,AI
"The Ethics and Society Review of Artificial Intelligence Research (ESR) offers a thorough look at the moral and ethical considerations surrounding artificial intelligence (AI) research and innovation. This document delves deeply into the present status of AI ethics, examining different ethical models and guidelines that have been suggested to direct the development of AI, along with the obstacles and restrictions associated with these models. It also investigates how societal and political entities influence AI ethics, such as through actions taken by governmental bodies, private companies, and nongovernmental organizations. Finally, the paper suggests steps for advancing AI research, emphasizing the necessity for closer collaboration among AI experts, ethicists, and the general public, and underscores the significance of addressing the wide-ranging social, political, and economic ramifications of AI on society.",1,AI
"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",0,Human
"This research examines how consensus-based distributed filtering works in a network of connected agents. The goal is to predict a shared signal using individual measurements that are spread across the network. Our method involves creating a distributed filtering algorithm based on consensus theory, wherein each node updates its prediction by integrating its own data with information from its neighbors. Additionally, there's a fusion phase where these local predictions are merged to form an overall signal estimate. We explore the convergence of our algorithm and establish criteria for reaching a consensus estimate. Furthermore, we look into how the fusion process impacts the precision of the final prediction and calculate potential error limits. To showcase the algorithm's effectiveness, we conduct simulations showing it can accurately gauge the signal in both noisy and ever-changing scenarios. This technique has practical applications in areas like sensor networks, distributed control systems, and multi-agent setups, where precise estimation of a common signal is necessary.",1,AI
"Randomization-based Machine Learning methods for prediction are currently a hot topic in Artificial Intelligence, due to their excellent performance in many prediction problems, with a bounded computation time. The application of randomization-based approaches to renewable energy prediction problems has been massive in the last few years, including many different types of randomization-based approaches, their hybridization with other techniques and also the description of new versions of classical randomization-based algorithms, including deep and ensemble approaches. In this paper we review the most important characteristics of randomization-based machine learning approaches and their application to renewable energy prediction problems. We describe the most important methods and algorithms of this family of modeling methods, and perform a critical literature review, examining prediction problems related to solar, wind, marine/ocean and hydro-power renewable sources. We support our critical analysis with an extensive experimental study, comprising real-world problems related to solar, wind and hydro-power energy, where randomization-based algorithms are found to achieve superior results at a significantly lower computational cost than other modeling counterparts. We end our survey with a prospect of the most important challenges and research directions that remain open this field, along with an outlook motivating further research efforts in this exciting research field.",0,Human
"The lattice Boltzmann method (LBM) serves as a robust tool in computational fluid dynamics (CFD) for simulating fluid flow by addressing the Boltzmann equation. This paper introduces an innovative way to accelerate LBM on non-uniform grids through massive parallelism. Unlike uniform grids, which offer flexibility but are difficult to distribute across multiple compute nodes, non-uniform grids provide better resolution for intricate shapes. However, these grids have posed challenges in terms of parallelization. Our strategy leverages the data-parallel characteristics of the LBM to distribute calculations across various computing nodes efficiently. We employ a mixed MPI/OpenMP technique when implementing the LBM on non-uniform grids, aiming to optimize both computational workload and communication expenses. The efficacy of our implementation has been validated through a series of benchmark tests, comparing favorably against other parallel approaches. Our findings indicate that our parallelized algorithm performs well up to a high number of compute nodes, delivering substantial gains in speed compared to current methods, thus proving the practicality of our technique for tackling complex CFD simulations on a grand scale.",1,AI
"We present theory and algorithms for the computation of probability-weighted ""keep-out"" sets to assure probabilistically safe navigation in the presence of multiple rigid body obstacles with stochastic dynamics. Our forward stochastic reachability-based approach characterizes the stochasticity of the future obstacle states in a grid-free and recursion-free manner, using Fourier transforms and computational geometry. We consider discrete-time Markovian switched systems with affine parameter-varying stochastic subsystems (DMSP) as the obstacle dynamics, which includes Markov jump affine systems and discrete-time affine parameter-varying stochastic systems (DPV). We define a probabilistic occupancy function, to describe the probability that a given state is occupied by a rigid body obstacle with stochastic dynamics at a given time; keep-out sets are the super-level sets of this occupancy function. We provide sufficient conditions that ensure convexity and compactness of these keep-out sets for DPV obstacle dynamics. We also propose two computationally efficient algorithms to overapproximate the keep-out sets --- a tight polytopic approximation using projections, and an overapproximation using Minkowski sum. For DMSP obstacle dynamics, we compute a union of convex and compact sets that covers the potentially non-convex keep-out set. Numerical simulations show the efficacy of the proposed algorithms for a modified version of the classical unicycle dynamics, modeled as a DMSP.",0,Human
"Ride-hailing apps have gained popularity in numerous cities worldwide. In Nigeria, ORide stands out as a significant player providing inexpensive transportation options to city dwellers. However, the mystery surrounding ORide drivers has been a persistent issue that raises questions about user safety and trust. For this reason, we conducted a study examining how users perceive driver anonymity and its effects on safety. Using a survey of 500 ORide users from Lagos, we explored their opinions on driver anonymity and what factors drive them to utilize the service. Our results indicate that many users are apprehensive about driver anonymity and want more details about their drivers, including their names and pictures. Moreover, users' views on driver anonymity are influenced by their previous experiences with the app, gender, and age. Furthermore, our research highlights potential safety risks associated with driver anonymity, like the challenge of identifying drivers during accidents or security incidents. Based on these findings, we suggest strategies for ORide and similar services to tackle the anonymity issue while prioritizing user safety and privacy. This study adds to the discourse on ride-hailing services in developing nations and underscores the need for finding a balance between user confidentiality and safety.",1,AI
"This research introduces an innovative way to handle epistemic uncertainty through Bayesian confidence calibration. Epistemic uncertainty occurs due to insufficient knowledge or understanding of a system, leading to potential errors in predictions. While Bayesian methods are commonly employed to tackle this issue, they sometimes exhibit overconfidence when the model assumptions are incorrect or when there's a scarcity of data. To tackle these challenges, we present a novel approach grounded in Bayesian confidence calibration, designed to improve the accuracy of uncertainty estimates generated by Bayesian models. Our strategy involves developing a calibration function that fine-tunes the uncertainty predictions made by the Bayesian model, taking into account both the data's reliability and the extent of model misfit. We validate our technique using both simulated and actual datasets and contrast its performance against other prevalent approaches for managing epistemic uncertainty. The outcomes indicate that our method yields more precise and dependable uncertainty assessments, which are crucial for informed decision-making under uncertain conditions. Finally, we explore the practical implications of our method across diverse fields such as finance, engineering, and healthcare.",1,AI
"This research introduces an innovative technique for clustering streaming data by focusing on the evolution of density mountains. It leverages the idea of density peaks to monitor how clusters change over time and recognize the changing characteristics of these density peaks. This strategy enables a more responsive and flexible clustering process compared to conventional approaches, which typically presume a fixed cluster configuration. To assess the effectiveness of this method, experiments were conducted using both artificial and actual datasets. These tests demonstrated that the proposed technique surpasses current methods in terms of precision, speed, and ability to handle large volumes of data. The insights gained from this investigation can enhance our understanding of clustering algorithms for streaming data and may find practical use in diverse areas like online customer segmentation and financial fraud detection.",1,AI
"This study looks into how well subspace codes work for error correction in networks using random linear coding. It analyzes how to balance the smallest difference between elements (minimum distance) and the highest efficiency (maximum rate) in these codes. We propose ways to create subspace codes that are both highly efficient and effective at correcting errors. Additionally, we examine how far off our codes can be from perfect alignment (covering radius) and demonstrate that under specific conditions, our methods result in the best possible performance. These findings could help in designing better subspace codes for error control in random linear network coding systems.",1,AI
"This research explores the effectiveness of universal Bayesian sequence prediction for any given loss function and set of symbols. It seeks to establish the circumstances where this predictive method is most efficient and to lay out a theoretical foundation for proving its optimality. Initially, the basics of Bayesian prediction are introduced, along with the underlying principles. Then, the authors clarify what constitutes a general loss function and set of symbols and demonstrate their applicability within the Bayesian framework. The key findings include a theorem that outlines the necessary conditions for the Bayesian prediction strategy to be optimal. This theorem is substantiated through a blend of mathematical analysis and empirical evidence from simulations. Finally, the paper reflects on the significance of these findings and suggests potential areas for further investigation.",1,AI
"This research introduces an innovative motion planning method named e$ \mathbf{^3} $mop, which integrates motion primitives pruning guided by heuristics and path optimization with a sparse-banded framework. The algorithm is tailored to tackle intricate motion planning challenges, especially in robotic and autonomous systems. Initially, e$ \mathbf{^3} $mop creates a collection of motion primitives under heuristic guidance to minimize the search space. It then prunes this set of primitives using a unique cost function that evaluates the cost of each primitive along with the anticipated cost of the subsequent path. This pruning process dramatically reduces the number of primitives to consider, thereby minimizing computational time. Subsequently, the remaining primitives are employed to form a sparse-banded graph that reflects their connectivity. Finally, the graph is refined through a specialized path optimization technique that leverages its sparse-banded characteristic to swiftly determine the most efficient path. Experimental outcomes demonstrate that e$ \mathbf{^3} $mop excels over current state-of-the-art methods in both speed and problem-solving efficacy. Additionally, the algorithm exhibits remarkable scalability, making it suitable for extensive applications. In summary, e$ \mathbf{^3} $mop represents a breakthrough in motion planning, offering substantial benefits across robotics, autonomous systems, and other fields.",1,AI
"This research introduces a fresh method to boost the security and resilience of unmanned aerial vehicle (UAV) communication networks using reconfigurable intelligent surfaces (RISs). The work starts by highlighting existing issues in UAV communication, such as signal disruptions, jamming threats, and inadequate wireless signal strength. Next, it explains how RISs could be a valuable remedy for these problems. The researchers suggest a secure and sturdy UAV communication framework that leverages RISs to adaptively modify wireless channel conditions on the fly. The effectiveness of their proposed system is tested via both simulations and practical tests, revealing substantial enhancements in signal clarity, data transfer dependability, and security when compared to conventional UAV communication setups. This study offers fresh perspectives on how RISs might serve as a pivotal technology for enhancing both the security and reliability of UAV communications.",1,AI
"This study introduces a technique for calculating subgraph frequencies from data collected by individuals, whether or not they include attributes. The approach leverages the insight that subgraph frequencies can be approximated by multiplying the subgraph frequencies found within the sampled data by the ratio of total subgraphs in the entire graph to those in the sampled portion. The method has been tested both on artificially generated and real-world graphs and has proven capable of accurately estimating these frequencies. These findings highlight the potential of the proposed method in diverse fields like graph pattern discovery and network examination.",1,AI
"This study introduces a Bayesian optimization approach tailored for improving the performance of ATRIAS, a two-legged robot. Bayesian optimization is widely used for finding solutions in complex problems, but it can be enhanced by using domain-specific information. The proposed strategy integrates domain knowledge within a Bayesian optimization framework, utilizing it to shape the prior assumptions about the robot's parameters influencing its performance. These prior assumptions are then incorporated into the algorithm via selection of a suitable prior distribution. To assess its effectiveness, the method was tested across various simulations, comparing its outcomes against traditional Bayesian optimization and other prominent optimization techniques. The findings show that the proposed method requires fewer evaluations to achieve an optimal solution compared to alternatives. Additionally, it reveals the most critical parameters and how they interact, offering deeper insights into ATRIAS' behavior. The results indicate that integrating domain knowledge can significantly boost the efficiency and comprehensibility of optimization processes. Consequently, this research advances our understanding of Bayesian optimization and showcases its applicability in optimizing intricate systems like bipedal robots. It underscores the significance of incorporating domain expertise to enhance optimization algorithms.",1,AI
"This study introduces a new technique named ""Zoom-SVD"" for pinpointing crucial trends within specific segments of extensive datasets. The method aims to be both quick and space-saving, ideal for examining vast collections of time-series data. Zoom-SVD builds upon the Singular Value Decomposition (SVD) algorithm by adding a zooming feature, allowing users to concentrate on particular time periods relevant to their investigation. Only the necessary sections of data are then analyzed through SVD computations. Performance evaluations conducted using various real-world datasets reveal that Zoom-SVD efficiently uncovers significant patterns across diverse time-series applications such as weather prediction, finance, and speech processing. The outcomes indicate that this method outperforms traditional SVD techniques by being notably quicker and more economical in terms of storage, while simultaneously producing accurate pattern detection similar to or even surpassing established methods. In summary, this research offers a promising strategy for enhancing the analysis of time-series data across numerous disciplines.",1,AI
"The datasets of face recognition contain an enormous number of identities and instances. However, conventional methods have difficulty in reflecting the entire distribution of the datasets because a mini-batch of small size contains only a small portion of all identities. To overcome this difficulty, we propose a novel method called BroadFace, which is a learning process to consider a massive set of identities, comprehensively. In BroadFace, a linear classifier learns optimal decision boundaries among identities from a large number of embedding vectors accumulated over past iterations. By referring more instances at once, the optimality of the classifier is naturally increased on the entire datasets. Thus, the encoder is also globally optimized by referring the weight matrix of the classifier. Moreover, we propose a novel compensation method to increase the number of referenced instances in the training stage. BroadFace can be easily applied on many existing methods to accelerate a learning process and obtain a significant improvement in accuracy without extra computational burden at inference stage. We perform extensive ablation studies and experiments on various datasets to show the effectiveness of BroadFace, and also empirically prove the validity of our compensation method. BroadFace achieves the state-of-the-art results with significant improvements on nine datasets in 1:1 face verification and 1:N face identification tasks, and is also effective in image retrieval.",0,Human
"Temporal pattern mining is essential in areas like finance, health, and engineering, where continuous streams of time series data are produced. Traditional techniques for mining temporal patterns often struggle with big datasets, leading to high computational demands. Mutual information, which helps quantify the relationship between variables, has proven useful in reducing the complexity of data by focusing on key features. This paper introduces an efficient method for handling big time series data through mutual information. Our strategy involves three phases: data preprocessing, feature selection based on mutual information, and temporal pattern extraction. During preprocessing, we eliminate redundant data points and standardize the remaining data. Feature selection then uses mutual information to identify the most pertinent features. Lastly, we employ a modified PrefixSpan algorithm for extracting frequent temporal patterns from these selected features. We validate our approach using real-world datasets across multiple domains, comparing it favorably against current best practices. Our results indicate superior performance in both speed and precision compared to existing solutions, particularly when dealing with high-dimensional data. This technique could be broadly applicable in sectors such as finance, health, and engineering, helping to uncover meaningful temporal patterns within vast volumes of time series data.",1,AI
"Fog radio access networks (F-RANs), which consist of a cloud and multiple edge nodes (ENs) connected via fronthaul links, have been regarded as promising network architectures. The F-RAN entails a joint optimization of cloud and edge computing as well as fronthaul interactions, which is challenging for traditional optimization techniques. This paper proposes a Cloud-Enabled Cooperation-Inspired Learning (CECIL) framework, a structural deep learning mechanism for handling a generic F-RAN optimization problem. The proposed solution mimics cloud-aided cooperative optimization policies by including centralized computing at the cloud, distributed decision at the ENs, and their uplink-downlink fronthaul interactions. A group of deep neural networks (DNNs) are employed for characterizing computations of the cloud and ENs. The forwardpass of the DNNs is carefully designed such that the impacts of the practical fronthaul links, such as channel noise and signling overheads, can be included in a training step. As a result, operations of the cloud and ENs can be jointly trained in an end-to-end manner, whereas their real-time inferences are carried out in a decentralized manner by means of the fronthaul coordination. To facilitate fronthaul cooperation among multiple ENs, the optimal fronthaul multiple access schemes are designed. Training algorithms robust to practical fronthaul impairments are also presented. Numerical results validate the effectiveness of the proposed approaches.",0,Human
"This research investigates how well hexaconv, a neural network architecture using hexagonal filters, performs compared to traditional square filter architectures in image classification tasks. It's found that hexaconv can match or exceed the accuracy of square convolutional neural networks, but does so with fewer parameters and computational resources. The paper suggests that hexaconv is a worthwhile replacement for square convolutional neural networks, especially beneficial for tasks dealing with hexagonal patterns like satellite images or cellular networks.",1,AI
"This research introduces a fresh strategy for tackling visual question answering (VQA) through the use of dependency trees to amalgamate visual and textual data. The proposed technique employs a graph structure that reflects the connections between words within questions and corresponding image details to facilitate reasoning processes. Experimental outcomes indicate that these dependency trees adeptly encapsulate the interactions between visual and textual components, resulting in enhanced performance relative to current best practices on VQA test sets. Additionally, the method's transparency is showcased by presenting the reasoning pathway as visualized dependency trees, making the decision-making process more accessible to humans. Overall, this investigation suggests an innovative path for constructing comprehensible VQA frameworks capable of elucidating their outputs to end-users.",1,AI
"The goal of this paper is to establish which practical routing schemes for wireless networks are most suitable for wideband systems in the power-limited regime, which is, for example, a practically relevant mode of operation for the analysis of ultrawideband (UWB) mesh networks. For this purpose, we study the tradeoff between energy efficiency and spectral efficiency (known as the power-bandwidth tradeoff) in a wideband linear multihop network in which transmissions employ orthogonal frequency-division multiplexing (OFDM) modulation and are affected by quasi-static, frequency-selective fading. Considering open-loop (fixed-rate) and closed-loop (rate-adaptive) multihop relaying techniques, we characterize the impact of routing with spatial reuse on the statistical properties of the end-to-end conditional mutual information (conditioned on the specific values of the channel fading parameters and therefore treated as a random variable) and on the energy and spectral efficiency measures of the wideband regime. Our analysis particularly deals with the convergence of these end-to-end performance measures in the case of large number of hops, i.e., the phenomenon first observed in \cite{Oyman06b} and named as ``multihop diversity''. Our results demonstrate the realizability of the multihop diversity advantages in the case of routing with spatial reuse for wideband OFDM systems under wireless channel effects such as path-loss and quasi-static frequency-selective multipath fading.",0,Human
"In recent years, there's been a growing concern about adversarial attacks on deep neural networks. This paper introduces a novel strategy to combat these threats by leveraging the characteristics of the input data's structure. The proposed method involves preprocessing the input data to maintain its original form, followed by passing it through a deep neural network that specializes in identifying shapes. Experimental outcomes indicate that this technique proves highly effective against diverse forms of adversarial attacks, including both white-box and black-box scenarios, resulting in substantial improvements in accuracy when compared to conventional defense mechanisms. The findings highlight the efficacy of integrating shape information in enhancing security measures against adversarial attacks and suggest that this methodology may hold promise for advancing research in this area.",1,AI
"Automatic segmentation methods are an important advancement in medical image analysis. Machine learning techniques, and deep neural networks in particular, are the state-of-the-art for most medical image segmentation tasks. Issues with class imbalance pose a significant challenge in medical datasets, with lesions often occupying a considerably smaller volume relative to the background. Loss functions used in the training of deep learning algorithms differ in their robustness to class imbalance, with direct consequences for model convergence. The most commonly used loss functions for segmentation are based on either the cross entropy loss, Dice loss or a combination of the two. We propose the Unified Focal loss, a new hierarchical framework that generalises Dice and cross entropy-based losses for handling class imbalance. We evaluate our proposed loss function on five publicly available, class imbalanced medical imaging datasets: CVC-ClinicDB, Digital Retinal Images for Vessel Extraction (DRIVE), Breast Ultrasound 2017 (BUS2017), Brain Tumour Segmentation 2020 (BraTS20) and Kidney Tumour Segmentation 2019 (KiTS19). We compare our loss function performance against six Dice or cross entropy-based loss functions, across 2D binary, 3D binary and 3D multiclass segmentation tasks, demonstrating that our proposed loss function is robust to class imbalance and consistently outperforms the other loss functions. Source code is available at: https://github.com/mlyg/unified-focal-loss",0,Human
"This research examines how common and what functions heterogeneous network motifs serve in complex systems. Typically, studies focus on uniform network motifs where each node in the motif performs the same function. However, many real-world networks consist of nodes with varying functions and characteristics. In this study, we propose a technique to identify these heterogeneous motifs within networks and examine their frequency across various real-world systems, such as biological, social, and technological networks. Our investigation shows that heterogeneous motifs occur much more frequently than homogeneous ones in many real-world settings. These motifs are crucial for processes like information processing and regulation. We discover that many of these motifs are essential for organizing networks into modules, facilitating signal transmission, and maintaining regulatory feedback loops. Additionally, we observe that certain heterogeneous motifs appear repeatedly across diverse systems, indicating they might be fundamental components of complex networks. The findings from this research hold significant implications for system design and comprehension. By considering the roles of heterogeneous motifs, we can uncover the underlying mechanisms governing the behavior of real-world systems and devise better ways to control and manipulate them. Furthermore, the methodology we developed for detecting heterogeneous motifs can be applied to numerous types of networks, helping us to better understand the intricate relationship between network architecture and functionality in complex systems.",1,AI
"The study delves into the application of two metrics—Video Multimethod Assessment Fusion (VMAF) and Entropic Differences (ED)—for assessing the quality of high frame rate (HFR) videos. Given the rise in popularity of HFR videos, there's a need for dependable techniques to gauge their visual quality. While VMAF and ED have been widely utilized in traditional frame rates, their effectiveness in evaluating HFR videos hasn't been thoroughly explored. To assess these metrics' applicability, the researchers conducted subjective evaluations involving 30 participants using the Absolute Category Rating (ACR) method. These participants rated HFR video sequences, and their scores were compared against those derived from the VMAF and ED metrics. The study employed a variety of HFR videos from different genres and motion characteristics to validate its outcomes. The research revealed that both VMAF and ED are effective for HFR video quality assessment, showing a strong correlation with the subjective ratings provided by the ACR methodology. It was observed that VMAF outperformed ED marginally, although the difference wasn't statistically significant. This study advances the development of objective methods for assessing the visual quality of HFR videos, which can be beneficial for applications like video compression and streaming. Furthermore, the insights gained from this investigation can help in refining quality assessment algorithms tailored specifically for HFR video content.",1,AI
"This study introduces a new technique for eliminating attribute artifacts from geometry-based point cloud compression. It employs a two-step strategy: initially, the point cloud undergoes downsampling via a quadtree algorithm, then the attribute artifact removal process takes place. In the artifact removal phase, a local regression method is utilized, which leverages the spatial and attribute characteristics of neighboring points to estimate and eliminate artifacts. To evaluate its efficacy, the proposed method was tested on various benchmark datasets, demonstrating superior performance compared to current leading approaches in both objective measurements and visual quality. Additionally, the computational efficiency of the proposed technique ensures it can seamlessly integrate into existing point cloud compression systems. These findings could greatly advance the field of efficient and high-quality point cloud compression.",1,AI
"We introduce a flexible, scalable Bayesian inference framework for nonlinear dynamical systems characterised by distinct and hierarchical variability at the individual, group, and population levels. Our model class is a generalisation of nonlinear mixed-effects (NLME) dynamical systems, the statistical workhorse for many experimental sciences. We cast parameter inference as stochastic optimisation of an end-to-end differentiable, block-conditional variational autoencoder. We specify the dynamics of the data-generating process as an ordinary differential equation (ODE) such that both the ODE and its solver are fully differentiable. This model class is highly flexible: the ODE right-hand sides can be a mixture of user-prescribed or ""white-box"" sub-components and neural network or ""black-box"" sub-components. Using stochastic optimisation, our amortised inference algorithm could seamlessly scale up to massive data collection pipelines (common in labs with robotic automation). Finally, our framework supports interpretability with respect to the underlying dynamics, as well as predictive generalization to unseen combinations of group components (also called ""zero-shot"" learning). We empirically validate our method by predicting the dynamic behaviour of bacteria that were genetically engineered to function as biosensors. Our implementation of the framework, the dataset, and all code to reproduce the experimental results is available at https://www.github.com/Microsoft/vi-hds .",0,Human
"Human pose estimation is a fundamental yet challenging task in computer vision, which aims at localizing human anatomical keypoints. However, unlike human vision that is robust to various data corruptions such as blur and pixelation, current pose estimators are easily confused by these corruptions. This work comprehensively studies and addresses this problem by building rigorous robust benchmarks, termed COCO-C, MPII-C, and OCHuman-C, to evaluate the weaknesses of current advanced pose estimators, and a new algorithm termed AdvMix is proposed to improve their robustness in different corruptions. Our work has several unique benefits. (1) AdvMix is model-agnostic and capable in a wide-spectrum of pose estimation models. (2) AdvMix consists of adversarial augmentation and knowledge distillation. Adversarial augmentation contains two neural network modules that are trained jointly and competitively in an adversarial manner, where a generator network mixes different corrupted images to confuse a pose estimator, improving the robustness of the pose estimator by learning from harder samples. To compensate for the noise patterns by adversarial augmentation, knowledge distillation is applied to transfer clean pose structure knowledge to the target pose estimator. (3) Extensive experiments show that AdvMix significantly increases the robustness of pose estimations across a wide range of corruptions, while maintaining accuracy on clean data in various challenging benchmark datasets.",0,Human
"The paper introduces a groundbreaking method for training multitask multilingual models that are highly scalable and efficient. These models are gaining traction in natural language processing because they can manage multiple languages and tasks concurrently. Yet, traditional training methods for these models tend to be lengthy and resource-intensive, thus constraining their practical use. To address this issue, the proposed strategy makes use of multi-node distributed training and adaptive optimization techniques to speed up training duration and minimize memory requirements. Crucially, the paper proposes a novel technique known as task-aware gradient accumulation, which optimally allocates computational resources across different tasks during training. Furthermore, the authors utilize an adaptive optimizer that flexibly tunes learning rates and momentum values according to the specific needs of each task. Empirical evidence from extensive data sets shows that this approach outperforms existing models in various tasks like language modeling, machine translation, and part-of-speech tagging, all while drastically cutting down training times and memory usage. Overall, this work presents a promising path towards achieving efficient and effective training of multitask multilingual models, which could greatly impact real-world natural language processing applications.",1,AI
"The paper investigates how autonomous systems and artificial intelligence might affect the stability of the nuclear sector. It examines how these technologies could improve safety and security, and minimize the chances of accidents and mishaps. Additionally, the document looks into the difficulties that arise from incorporating these technologies, such as ethical dilemmas and cyber threats. The results indicate that integrating autonomous systems and AI has the potential to transform the nuclear industry and secure its future. Yet, thoughtful planning and strategic deployment are necessary to harness these technologies responsibly and safely. The research underscores the need for joint efforts from nuclear professionals and tech experts to fully leverage these advanced tools in ensuring a stable nuclear landscape.",1,AI
"Manipulating facial expressions is a challenging task due to fine-grained shape changes produced by facial muscles and the lack of input-output pairs for supervised learning. Unlike previous methods using Generative Adversarial Networks (GAN), which rely on cycle-consistency loss or sparse geometry (landmarks) loss for expression synthesis, we propose a novel GAN framework to exploit 3D dense (depth and surface normals) information for expression manipulation. However, a large-scale dataset containing RGB images with expression annotations and their corresponding depth maps is not available. To this end, we propose to use an off-the-shelf state-of-the-art 3D reconstruction model to estimate the depth and create a large-scale RGB-Depth dataset after a manual data clean-up process. We utilise this dataset to minimise the novel depth consistency loss via adversarial learning (note we do not have ground truth depth maps for generated face images) and the depth categorical loss of synthetic data on the discriminator. In addition, to improve the generalisation and lower the bias of the depth parameters, we propose to use a novel confidence regulariser on the discriminator side of the framework. We extensively performed both quantitative and qualitative evaluations on two publicly available challenging facial expression benchmarks: AffectNet and RaFD. Our experiments demonstrate that the proposed method outperforms the competitive baseline and existing arts by a large margin.",0,Human
"This research introduces a streamlined simulation technique aimed at forecasting the thermal progression along overhead power lines. It integrates an analytical approach for heat exchange with numerical weather forecasts to provide precise temperature distributions throughout the line. The devised method is designed to be quick and efficient, making it ideal for real-time tracking and management of transmission lines. The method's efficacy is validated by a practical example, which illustrates its capability to forecast temperature accurately under different weather scenarios. These findings underscore the potential of the proposed simulation technique to enhance the operational robustness and performance of overhead transmission systems.",1,AI
"In this work, we initiate the study of \emph{smoothed analysis} of population protocols. We consider a population protocol model where an adaptive adversary dictates the interactions between agents, but with probability $p$ every such interaction may change into an interaction between two agents chosen uniformly at random. That is, $p$-fraction of the interactions are random, while $(1-p)$-fraction are adversarial. The aim of our model is to bridge the gap between a uniformly random scheduler (which is too idealistic) and an adversarial scheduler (which is too strict).  We focus on the fundamental problem of leader election in population protocols. We show that, for a population of size $n$, the leader election problem can be solved in $O(p^{-2}n \log^3 n)$ steps with high probability, using $O((\log^2 n) \cdot (\log (n/p)))$ states per agent, for \emph{all} values of $p\leq 1$. Although our result does not match the best known running time of $O(n \log n)$ for the uniformly random scheduler ($p=1$), we are able to present a \emph{smooth transition} between a running time of $O(n \cdot \mathrm{polylog} n)$ for $p=1$ and an infinite running time for the adversarial scheduler ($p=0$), where the problem cannot be solved. The key technical contribution of our work is a novel \emph{phase clock} algorithm for our model. This is a key primitive for much-studied fundamental population protocol algorithms (leader election, majority), and we believe it is of independent interest.",0,Human
"This study investigates the application of stacked multilayer perceptron (MLP) networks in recognizing evolving categories of events. Events like changing traffic patterns or shifts in weather fall under this category. The research proposes an approach where a series of MLPs are trained to identify various phases of event evolution, with each network's output serving as input for the subsequent one. By doing so, the system can detect the temporal connections between these events, enhancing predictive capabilities. To assess this technique, the researchers utilized traffic flow data, aiming to forecast traffic volumes for the upcoming hour. The outcomes indicate that the stacked MLPs perform better than conventional methods like linear regression and support vector regression. Furthermore, experiments were conducted to gauge how sensitive the methodology is to different parameters and design choices. In summary, this work offers a viable strategy for understanding dynamic event sequences, and the proposed stacked MLP models can be implemented in various scenarios involving time-varying phenomena.",1,AI
"This research introduces a new approach for choosing representative samples in vast amounts of data by integrating sparse graphs and geodesic Grassmann manifold distances. The aim is to pinpoint a manageable group of data points that best represent the overall dataset, thus facilitating more efficient analysis and drawing insights. Initially, the technique builds a sparse graph based on the data, placing each data point at a node and linking nodes with edges that denote similarities among points. Subsequently, a graph clustering algorithm is employed to discern groups of similarly structured data points. Following this, representatives are chosen within these clusters by measuring the geodesic Grassmann manifold distances between pairs of data points and selecting the one nearest to the average of the cluster within the manifold. The study showcases the efficacy of this approach via tests conducted on various real-world datasets. The outcomes reveal that the proposed method excels over numerous current representative selection techniques in terms of both accuracy and efficiency. This innovative method finds utility across diverse domains, including data summation, visualization, and categorization, all of which require effective representative selection.",1,AI
"Ramsey's theorem, in the version of Erd\H{o}s and Szekeres, states that every 2-coloring of the edges of the complete graph on {1, 2,...,n} contains a monochromatic clique of order 1/2\log n. In this paper, we consider two well-studied extensions of Ramsey's theorem.  Improving a result of R\""odl, we show that there is a constant $c>0$ such that every 2-coloring of the edges of the complete graph on \{2, 3,...,n\} contains a monochromatic clique S for which the sum of 1/\log i over all vertices i \in S is at least c\log\log\log n. This is tight up to the constant factor c and answers a question of Erd\H{o}s from 1981.  Motivated by a problem in model theory, V\""a\""an\""anen asked whether for every k there is an n such that the following holds. For every permutation \pi of 1,...,k-1, every 2-coloring of the edges of the complete graph on {1, 2, ..., n} contains a monochromatic clique a_1<...<a_k with a_{\pi(1)+1}-a_{\pi(1)}>a_{\pi(2)+1}-a_{\pi(2)}>...>a_{\pi(k-1)+1}-a_{\pi(k-1)}. That is, not only do we want a monochromatic clique, but the differences between consecutive vertices must satisfy a prescribed order. Alon and, independently, Erd\H{o}s, Hajnal and Pach answered this question affirmatively. Alon further conjectured that the true growth rate should be exponential in k. We make progress towards this conjecture, obtaining an upper bound on n which is exponential in a power of k. This improves a result of Shelah, who showed that n is at most double-exponential in k.",0,Human
"Software cost estimation is one of the prerequisite managerial activities carried out at the software development initiation stages and also repeated throughout the whole software life-cycle so that amendments to the total cost are made. In software cost estimation typically, a selection of project attributes is employed to produce effort estimations of the expected human resources to deliver a software product. However, choosing the appropriate project cost drivers in each case requires a lot of experience and knowledge on behalf of the project manager which can only be obtained through years of software engineering practice. A number of studies indicate that popular methods applied in the literature for software cost estimation, such as linear regression, are not robust enough and do not yield accurate predictions. Recently the dual variables Ridge Regression (RR) technique has been used for effort estimation yielding promising results. In this work we show that results may be further improved if an AI method is used to automatically select appropriate project cost drivers (inputs) for the technique. We propose a hybrid approach combining RR with a Genetic Algorithm, the latter evolving the subset of attributes for approximating effort more accurately. The proposed hybrid cost model has been applied on a widely known high-dimensional dataset of software project samples and the results obtained show that accuracy may be increased if redundant attributes are eliminated.",0,Human
"In the last two years, more than 200 papers have been written on how machine learning (ML) systems can fail because of adversarial attacks on the algorithms and data; this number balloons if we were to incorporate papers covering non-adversarial failure modes. The spate of papers has made it difficult for ML practitioners, let alone engineers, lawyers, and policymakers, to keep up with the attacks against and defenses of ML systems. However, as these systems become more pervasive, the need to understand how they fail, whether by the hand of an adversary or due to the inherent design of a system, will only become more pressing. In order to equip software developers, security incident responders, lawyers, and policy makers with a common vernacular to talk about this problem, we developed a framework to classify failures into ""Intentional failures"" where the failure is caused by an active adversary attempting to subvert the system to attain her goals; and ""Unintentional failures"" where the failure is because an ML system produces an inherently unsafe outcome. After developing the initial version of the taxonomy last year, we worked with security and ML teams across Microsoft, 23 external partners, standards organization, and governments to understand how stakeholders would use our framework. Throughout the paper, we attempt to highlight how machine learning failure modes are meaningfully different from traditional software failures from a technology and policy perspective.",0,Human
"This research introduces a deep reinforcement learning strategy for optimizing both spectrum and energy efficiency in C-V2X communication systems, taking into account security factors. By employing a novel deep reinforcement learning algorithm, the study tackles the intricate optimization challenge and offers an ideal solution that strikes a balance between spectrum and energy efficiency while bolstering network security. Simulation results showcase the efficacy and superiority of the proposed method over conventional optimization techniques. These findings offer important insights into how to optimally manage spectrum and energy efficiency alongside security concerns in C-V2X communication networks.",1,AI
"The visual cue of optical flow plays a major role in the navigation of flying insects, and is increasingly studied for use by small flying robots as well. A major problem is that successful optical flow control seems to require distance estimates, while optical flow is known to provide only the ratio of velocity to distance. In this article, a novel, stability-based strategy is proposed to estimate distances with monocular optical flow and knowledge of the control inputs (efference copies). It is shown analytically that given a fixed control gain, the stability of a constant divergence control loop only depends on the distance to the approached surface. At close distances, the control loop first starts to exhibit self-induced oscillations, eventually leading to instability. The proposed stability-based strategy for estimating distances has two major attractive characteristics. First, self-induced oscillations are easy for the robot to detect and are hardly influenced by wind. Second, the distance can be estimated during a zero divergence maneuver, i.e., around hover. The stability-based strategy is implemented and tested both in simulation and with a Parrot AR drone 2.0. It is shown that it can be used to: (1) trigger a final approach response during a constant divergence landing with fixed gain, (2) estimate the distance in hover, and (3) estimate distances during an entire landing if the robot uses adaptive gain control to continuously stay on the 'edge of oscillation'.",0,Human
"The Fast Fourier Transform (FFT) is indispensable in many areas such as signal processing, image processing, and scientific simulations. As the volume of data grows, the computational burden of calculating the FFT intensifies, prompting the creation of effective parallel algorithms for handling these computations efficiently. This paper introduces a novel parallel algorithm for multidimensional FFT utilizing Advanced Message Passing Interface (MPI) on powerful computing systems. The proposed method involves a two-step process that integrates a distributed one-dimensional FFT with a block-cyclic data distribution strategy. By leveraging the characteristics of the block-cyclic distribution, our algorithm minimizes communication overhead and ensures even workloads across processors. Additionally, we employ the most recent features of MPI to enhance communication efficiency and decrease synchronization delays. We validate the performance and scalability of our algorithm on various hardware configurations, including large-scale clusters and supercomputers. Our tests reveal that our algorithm achieves near-linear speedup up to thousands of processors, surpassing conventional parallel FFT libraries when dealing with extensive datasets. This technique proves to be highly efficient and can substantially cut down the time needed for multidimensional FFT calculations, thereby facilitating quicker and more precise scientific simulations and data analysis across different disciplines.",1,AI
"In this paper, we study the Nash dynamics of strategic interplays of n buyers in a matching market setup by a seller, the market maker. Taking the standard market equilibrium approach, upon receiving submitted bid vectors from the buyers, the market maker will decide on a price vector to clear the market in such a way that each buyer is allocated an item for which he desires the most (a.k.a., a market equilibrium solution). While such equilibrium outcomes are not unique, the market maker chooses one (maxeq) that optimizes its own objective --- revenue maximization. The buyers in turn change bids to their best interests in order to obtain higher utilities in the next round's market equilibrium solution.  This is an (n+1)-person game where buyers place strategic bids to gain the most from the market maker's equilibrium mechanism. The incentives of buyers in deciding their bids and the market maker's choice of using the maxeq mechanism create a wave of Nash dynamics involved in the market. We characterize Nash equilibria in the dynamics in terms of the relationship between maxeq and mineq (i.e., minimum revenue equilibrium), and develop convergence results for Nash dynamics from the maxeq policy to a mineq solution, resulting an outcome equivalent to the truthful VCG mechanism.  Our results imply revenue equivalence between maxeq and mineq, and address the question that why short-term revenue maximization is a poor long run strategy, in a deterministic and dynamic setting.",0,Human
"Learning disentangled representation of data without supervision is an important step towards improving the interpretability of generative models. Despite recent advances in disentangled representation learning, existing approaches often suffer from the trade-off between representation learning and generation performance i.e. improving generation quality sacrifices disentanglement performance). We propose an Information-Distillation Generative Adversarial Network (ID-GAN), a simple yet generic framework that easily incorporates the existing state-of-the-art models for both disentanglement learning and high-fidelity synthesis. Our method learns disentangled representation using VAE-based models, and distills the learned representation with an additional nuisance variable to the separate GAN-based generator for high-fidelity synthesis. To ensure that both generative models are aligned to render the same generative factors, we further constrain the GAN generator to maximize the mutual information between the learned latent code and the output. Despite the simplicity, we show that the proposed method is highly effective, achieving comparable image generation quality to the state-of-the-art methods using the disentangled representation. We also show that the proposed decomposition leads to an efficient and stable model design, and we demonstrate photo-realistic high-resolution image synthesis results (1024x1024 pixels) for the first time using the disentangled representations.",0,Human
"The proven efficacy of learning-based control schemes strongly motivates their application to robotic systems operating in the physical world. However, guaranteeing correct operation during the learning process is currently an unresolved issue, which is of vital importance in safety-critical systems. We propose a general safety framework based on Hamilton-Jacobi reachability methods that can work in conjunction with an arbitrary learning algorithm. The method exploits approximate knowledge of the system dynamics to guarantee constraint satisfaction while minimally interfering with the learning process. We further introduce a Bayesian mechanism that refines the safety analysis as the system acquires new evidence, reducing initial conservativeness when appropriate while strengthening guarantees through real-time validation. The result is a least-restrictive, safety-preserving control law that intervenes only when (a) the computed safety guarantees require it, or (b) confidence in the computed guarantees decays in light of new observations. We prove theoretical safety guarantees combining probabilistic and worst-case analysis and demonstrate the proposed framework experimentally on a quadrotor vehicle. Even though safety analysis is based on a simple point-mass model, the quadrotor successfully arrives at a suitable controller by policy-gradient reinforcement learning without ever crashing, and safely retracts away from a strong external disturbance introduced during flight.",0,Human
"This paper reviews Ultra-Reliable Low Latency Communications (URLLC) for Massive Machine Type Communications (MMTC) with the goal of supporting essential MMTC functions. It begins by introducing MMTC and its demands, which include minimal latency, high dependability, and extensive capacity. Subsequently, the paper concentrates on how URLLC can fulfill the demanding needs of critical MMTC applications like self-driving cars and industrial control. The paper elucidates the fundamental aspects and design guidelines of URLLC, covering strategies for enhancing reliability and reducing latency, coding and modulation methods, and network structure. Additionally, it addresses the obstacles and constraints involved in deploying URLLC in MMTC networks and offers guidance on the current and potential advancements in this field. Finally, the paper outlines the key outcomes and suggests areas for future study.",1,AI
"A new platform called Visual Genome is introduced in the paper, which integrates language and imagery via dense image annotations sourced from crowd labor. The study details how they annotated and amassed a vast collection of images with comprehensive scene descriptions, object and relational annotations, along with question-and-answer sets. To assess the data's reliability, they compared it against other datasets and showcased its applicability across diverse computer vision and natural language processing projects. Additionally, the researchers highlighted the hurdles and proposed avenues for growth, such as enhancing data accuracy and broadening annotation types. Ultimately, the paper asserts that Visual Genome offers a crucial asset for research in both fields and could substantially propel advancements within them.",1,AI
"Abandoned domain names—those that have expired and aren't being used anymore—are the focus of this investigation. The study looks into how hijackers take control of these domains and the resources they provide, examining their reasons and tactics. It also explores the effects on the former owners and the broader internet. The findings indicate that hijacking these old domains is becoming more common, posing risks to internet stability and security. In the end, the paper suggests ways for policymakers and domain registrars to tackle this issue and safeguard domain owners' rights.",1,AI
"This study introduces a hybrid inference system aimed at enhancing the precision of curvature estimation within the level-set method by leveraging machine learning. The level-set method is extensively utilized across multiple domains including computer vision, medical imaging, and graphic design, primarily due to its effectiveness in modeling shapes and segmenting images. The accuracy of the level-set's progression hinges significantly on the reliability of curvature estimation, making this aspect critical. However, conventional techniques for curvature estimation are subject to drawbacks like susceptibility to noise and irregular data sampling. To address these issues, the researchers have developed a hybrid inference system that integrates both traditional curvature estimation methodologies and machine learning algorithms. This system employs a machine learning model that has been trained on extensive datasets comprising both synthetic and real-world images. Through this training process, the model learns to forecast curvature values at various points along the evolving interface based on localized image characteristics. The hybrid system then merges the predictions generated by the machine learning model with the outcomes from traditional curvature estimation methods, thereby producing more precise and resilient curvature estimates. The performance of the proposed hybrid inference system was assessed using diverse datasets, and its efficacy was compared against conventional curvature estimation techniques. The findings demonstrated superior performance of the hybrid system in terms of both accuracy and robustness. Additionally, the computational efficiency of the proposed system makes it suitable for integration into existing level-set algorithms without significant overhead. In summary, this research offers a promising advancement towards enhancing the accuracy and resilience of the level-set method through the application of machine learning-based strategies. Furthermore, the suggested system can potentially be adapted for other shape modeling and segmentation techniques that require curvature estimation.",1,AI
"Writing accurate numerical software is hard because of many sources of unavoidable uncertainties, including finite numerical precision of implementations. We present a programming model where the user writes a program in a real-valued implementation and specification language that explicitly includes different types of uncertainties. We then present a compilation algorithm that generates a conventional implementation that is guaranteed to meet the desired precision with respect to real numbers. Our verification step generates verification conditions that treat different uncertainties in a unified way and encode reasoning about floating-point roundoff errors into reasoning about real numbers. Such verification conditions can be used as a standardized format for verifying the precision and the correctness of numerical programs. Due to their often non-linear nature, precise reasoning about such verification conditions remains difficult. We show that current state-of-the art SMT solvers do not scale well to solving such verification conditions. We propose a new procedure that combines exact SMT solving over reals with approximate and sound affine and interval arithmetic. We show that this approach overcomes scalability limitations of SMT solvers while providing improved precision over affine and interval arithmetic. Using our initial implementation we show the usefullness and effectiveness of our approach on several examples, including those containing non-linear computation.",0,Human
"A new method for counting and identifying individuals in crowds is introduced, focusing solely on individual points rather than regions. Traditional approaches often assume certain behaviors within crowds, leading to inaccuracies and potential mistakes. By treating each person as a distinct point, the proposed method aims to enhance precision in both count and location. It has been evaluated using real-world data, demonstrating improved accuracy compared to existing techniques. This study challenges conventional assumptions and suggests an alternative, point-based strategy as a way to address these shortcomings.",1,AI
"This research delves into determining the highest achievable minimum distance in linear locally repairable codes, aiming to establish precise limits on this distance. Locally repairable codes have become significant in recent times because of their usefulness in creating fault-tolerant storage systems. The maximum minimum distance of a code signifies the greatest possible minimum distance across all its subcodes and is crucial for assessing a code's error-correcting capabilities. The study examines this distance closely, exploring its connection to other code parameters and how the structure of the code influences its value. To find tighter bounds, researchers apply various mathematical techniques, deriving new constraints on the maximum minimum distance. These findings are then validated using computational simulations. Overall, this work offers valuable perspectives on enhancing the performance of linear locally repairable codes by improving their error-correction abilities.",1,AI
"This study introduces an innovative technique for estimating crowd sizes by combining multi-resolution fusion and multi-scale input priors. By utilizing these approaches, the model aims to provide accurate crowd counts in various scenes. The process starts with capturing the area at varying resolutions and then fuses this information into a high-resolution image. Following this, multi-scale input priors are employed to enhance the accuracy of the crowd count estimation. The proposed method has been tested on multiple public datasets, where it consistently outperformed existing crowd counting algorithms. This advanced technique holds promise for enhancing crowd management and safety monitoring in real-world settings.",1,AI
"We introduce Bee$^+$, a 95-mg four-winged microrobot with improved controllability and open-loop-response characteristics with respect to those exhibited by state-of-the-art two-winged microrobots with the same size and similar weight (i.e., the 75-mg Harvard RoboBee). The key innovation that made possible the development of Bee$^+$ is the introduction of an extremely light (28-mg) pair of twinned unimorph actuators, which enabled the design of a new microrobotic mechanism that flaps four wings independently. A first main advantage of the proposed design, compared to those of two-winged flyers, is that by increasing the number of actuators from two to four, the number of direct control inputs increases from three to four when simple sinusoidal excitations are employed. A second advantage of Bee$^+$ is that its four-wing configuration and flapping mode naturally damp the rotational disturbances that commonly affect the yaw degree of freedom of two-winged microrobots. In addition, the proposed design greatly reduces the complexity of the associated fabrication process compared to those of other microrobots, as the unimorph actuators are fairly easy to build. Lastly, we hypothesize that given the relatively low wing-loading affecting their flapping mechanisms, the life expectancy of Bee$^+$s must be considerably higher than those of the two-winged counterparts. The functionality and basic capabilities of the robot are demonstrated through a set of simple control experiments.",0,Human
"In this paper we investigate the problem of allocating spectrum among radio nodes under SINR requirements. This problem is of special interest in dynamic spectrum access networks where topology and spectral resources differ with time and location. The problem is to determine the number of radio nodes that can transmit simultaneously while still achieving their SINR requirements and then decide which channels these nodes should transmit on. Previous work have shown how this can be done for a large spectrum pool where nodes allocate multiple channels from that pool which renders a linear programming approach feasible when the pool is large enough. In this paper we extend their work by considering arbitrary individual pool sizes and allow nodes to only transmit on one channel. Due to the accumulative nature of interference this problem is a non-convex integer problem which is NP-hard. However, we introduce a constraint transformation that transforms the problem to a binary quadratic constraint problem. Although this problem is still NP-hard, well known heuristic algorithms for solving this problem are known in the literature. We implement a heuristic algorithm based on Lagrange relaxation which bounds the solution value of the heuristic to the optimal value of the constraint transformed problem. Simulation results show that this approach provides solutions within an average gap of 10% of solutions obtained by a genetic algorithm for the original non-convex integer problem.",0,Human
"This study introduces an efficient sampling technique for polynomial chaos-based uncertainty quantification and sensitivity analysis. It leverages weighted approximate Fekete points, recognized for their effectiveness in generating evenly spaced points for polynomial interpolation. These points enable the method to maintain high accuracy while requiring fewer samples than conventional Monte Carlo approaches. Performance evaluations via various numerical tests indicate that this method holds promise as an effective sampling strategy for polynomial chaos-based uncertainty quantification and sensitivity analysis.",1,AI
"OdoViz is a reactive web-based tool for 3D visualization and processing of autonomous vehicle datasets designed to support common tasks in visual place recognition research. The system includes functionality for loading, inspecting, visualizing, and processing GPS/INS poses, point clouds and camera images. It supports a number of commonly used driving datasets and can be adapted to load custom datasets with minimal effort. OdoViz's design consists of a slim server to serve the datasets coupled with a rich client frontend. This design supports multiple deployment configurations including single user stand-alone installations, research group installations serving datasets internally across a lab, or publicly accessible web-frontends for providing online interfaces for exploring and interacting with datasets. The tool allows viewing complete vehicle trajectories traversed at multiple different time periods simultaneously, facilitating tasks such as sub-sampling, comparing and finding pose correspondences both across and within sequences. This significantly reduces the effort required in creating subsets of data from existing datasets for machine learning tasks. Further to the above, the system also supports adding custom extensions and plugins to extend the capabilities of the software for other potential data management, visualization and processing tasks. The platform has been open-sourced to promote its use and encourage further contributions from the research community.",0,Human
"Humans and animals are believed to use a very minimal set of trajectories to perform a wide variety of tasks including walking. Our main objective in this paper is two fold 1) Obtain an effective tool to realize these basic motion patterns for quadrupedal walking, called the kinematic motion primitives (kMPs), via trajectories learned from deep reinforcement learning (D-RL) and 2) Realize a set of behaviors, namely trot, walk, gallop and bound from these kinematic motion primitives in our custom four legged robot, called the `Stoch'. D-RL is a data driven approach, which has been shown to be very effective for realizing all kinds of robust locomotion behaviors, both in simulation and in experiment. On the other hand, kMPs are known to capture the underlying structure of walking and yield a set of derived behaviors. We first generate walking gaits from D-RL, which uses policy gradient based approaches. We then analyze the resulting walking by using principal component analysis. We observe that the kMPs extracted from PCA followed a similar pattern irrespective of the type of gaits generated. Leveraging on this underlying structure, we then realize walking in Stoch by a straightforward reconstruction of joint trajectories from kMPs. This type of methodology improves the transferability of these gaits to real hardware, lowers the computational overhead on-board, and also avoids multiple training iterations by generating a set of derived behaviors from a single learned gait.",0,Human
"Depth coding in 3D-HEVC for the multiview video plus depth (MVD) architecture (i) deforms object shapes due to block-level edge-approximation; (ii) misses an opportunity for high compressibility at near-lossless quality by failing to exploit strong homogeneity (clustering tendency) in depth syntax, motion vector components, and residuals at frame-level; and (iii) restricts interactivity and limits responsiveness of independent use of depth information for ""non-viewing"" applications due to texture-depth coding dependency. This paper presents a standalone depth sequence coder, which operates in the lossless to near-lossless quality range while compressing depth data superior to lossy 3D-HEVC. It preserves edges implicitly by limiting quantisation to the spatial-domain and exploits clustering tendency efficiently at frame-level with a novel binary tree based decomposition (BTBD) technique. For mono-view coding of standard MVD test sequences, on average, (i) lossless BTBD achieved $\times 42.2$ compression-ratio and $-60.0\%$ coding gain against the pseudo-lossless 3D-HEVC, using the lowest quantisation parameter $QP = 1$, and (ii) near-lossless BTBD achieved $-79.4\%$ and $6.98$ dB Bj{\o}ntegaard delta bitrate (BD-BR) and distortion (BD-PSNR), respectively, against 3D-HEVC. In view-synthesis applications, decoded depth maps from BTBD rendered superior quality synthetic-views, compared to 3D-HEVC, with $-18.9\%$ depth BD-BR and $0.43$ dB synthetic-texture BD-PSNR on average.",0,Human
"This research delves into what makes a great summary in an educational setting and examines the capabilities of automatic summarization technology. It seeks to identify the significance of crucial aspects like brevity, pertinence, and cohesion in summaries and how these elements influence their efficacy in educational contexts. Additionally, it looks at the present status of automatic summarization technology, highlighting its shortcomings and possibilities for enhancement. Ultimately, the findings offer guidance on optimal usage of summaries in education and shed light on advancements in automatic summarization technology.",1,AI
"Entity resolution is a widely studied problem with several proposals to match records across relations. Matching textual content is a widespread task in many applications, such as question answering and search. While recent methods achieve promising results for these two tasks, there is no clear solution for the more general problem of matching textual content and structured data. We introduce a framework that supports this new task in an unsupervised setting for any pair of corpora, being relational tables or text documents. Our method builds a fine-grained graph over the content of the corpora and derives word embeddings to represent the objects to match in a low dimensional space. The learned representation enables effective and efficient matching at different granularity, from relational tuples to text sentences and paragraphs. Our flexible framework can exploit pre-trained resources, but it does not depends on their existence and achieves better quality performance in matching content when the vocabulary is domain specific. We also introduce optimizations in the graph creation process with an ""expand and compress"" approach that first identifies new valid relationships across elements, to improve matching, and then prunes nodes and edges, to reduce the graph size. Experiments on real use cases and public datasets show that our framework produces embeddings that outperform word embeddings and fine-tuned language models both in results' quality and in execution times.",0,Human
"To tackle the challenge of creating a critical geometric graph (CGG) in dense wireless sensor networks (WSNs) via a decentralized method, this research delves into the topic. The CGG is crucial in WSNs, capturing the spatial connections among nodes and is extensively utilized in numerous applications. However, building a CGG in dense WSNs is tricky due to the sheer number of nodes and the restricted communication range. To solve this issue, we introduce a novel distributed algorithm that benefits from the characteristics of Delaunay triangulation and Gabriel graphs. This algorithm operates iteratively, divided into three key steps: choosing nodes, local graph construction, and global coordination.

In the initial stage, every node picks a group of potential neighboring nodes according to Delaunay triangulation. Following this, during the local construction phase, each node forms a localized subgraph utilizing its chosen neighbors and applying Gabriel graph principles. Lastly, in the synchronization phase, these local subgraphs are amalgamated to form the complete CGG. We validate our proposed algorithm via thorough simulations and contrast it against other methods. The simulation outcomes reveal that our technique effectively generates a precise and efficient CGG, even within densely packed WSN environments. Additionally, our algorithm shows superior performance concerning both construction time and energy utilization compared to existing solutions. Overall, the proposed distributed strategy offers a viable approach for generating the CGG in dense WSNs, applicable to diverse applications like positioning, routing, and coverage management.",1,AI
"The researchers introduce a fresh technique named Neurint in their work, which applies neural ordinary differential equations (ODEs) for interpolation purposes. This approach employs a parametric strategy to estimate solutions for systems of ODEs and fine-tunes the parameters via backpropagation, thus allowing the model to seamlessly connect any two points within the input space. To evaluate Neurint's effectiveness, the scientists ran comparative experiments against conventional interpolation methods. Their findings reveal that Neurint yields superior outcomes in both accuracy and broad applicability compared to existing techniques. These results suggest that Neurint could be a valuable asset for tackling various interpolation challenges across machine learning and other fields.",1,AI
"DeepCert is a novel approach for confirming the resilience of neural network-based image classifiers in practical settings. It tackles the issue of making sure these classifiers perform well when dealing with real-world images that might be distorted, partially obscured, or different from what they were trained on. DeepCert leverages both formal verification and machine learning to offer a probabilistic assurance of a classifier's robustness concerning a particular set of alterations. The technique is tested across various benchmark datasets, and its outcomes show substantial advancements compared to current robustness verification methods. This study shows that DeepCert has the potential to enhance the reliability and acceptance of neural network image classifiers for real-world use.",1,AI
"This research introduces a decomposition-based multi-objective evolutionary algorithm (MOEA) design under two different algorithm frameworks. The initial focus is on developing a MOEA utilizing decomposition techniques, which convert a complex multi-objective optimization problem into several simpler sub-problems. Another approach examines how these decomposition techniques can enhance existing MOEAs. These algorithms were tested against a variety of benchmark multi-objective optimization problems, and their efficiency was evaluated by comparing them to leading MOEAs. The outcomes indicate that the new methods perform better in terms of both convergence and diversity, providing a richer set of solutions for multi-objective optimization challenges. The study's conclusions offer valuable guidance for MOEA development and showcase the efficacy of decomposition-based MOEAs in tackling multi-objective optimization issues.",1,AI
"This research examines production machine learning pipelines and offers guidance on how to optimize them. It looks at what's currently happening in real-world industry settings, pinpointing typical issues and inefficiencies. By conducting experiments and analyzing real-life examples, the authors illustrate how different optimization strategies affect pipeline performance and efficiency. Their findings reveal that blending hardware and software improvements, like parallel computing and model simplification, can greatly enhance the speed and reliability of machine learning pipelines. The study wraps up by suggesting actionable steps for improving these production pipelines and underscores the importance of ongoing research in this field.",1,AI
"Scene parsing is an essential part of computer vision, aiming to delineate and classify elements within an image or video. This paper introduces a new method for scene parsing by leveraging multiscale feature extraction, purity trees, and optimal covers. Initially, our technique utilizes multiscale feature extraction to gather information across various scales in an image, simultaneously capturing both local and global context. Following this, we apply purity trees to organize the image into distinct regions based on their similarities. Lastly, we utilize optimal covers to allocate these regions to specific semantic categories, ultimately achieving a thorough scene parse. The effectiveness of our method has been validated through testing on standard benchmarks, showing superior performance compared to existing techniques in terms of scene parsing accuracy.",1,AI
"This research introduces a fresh method for identifying indoor environments by merging information about their layout and how they look from different scales. The layout details give us an overview of the entire space, whereas the scale-invariant features ensure that the system can recognize objects regardless of their size. By integrating both types of information, we create a more reliable and precise system than what's currently available. Testing this technique on various test sets has shown that it outperforms current best practices.",1,AI
"This paper offers a comprehensive look at tensor decomposition techniques and their uses in signal processing and machine learning. Tensor decomposition is a valuable approach for handling complex data sets that are prevalent in real-world scenarios. The paper starts by introducing tensors and tensor decomposition, explaining different tensor formats and the underlying mathematics. It then delves into the primary tensor decomposition methods—Tucker decomposition, canonical polyadic decomposition (CPD), and PARAFAC2—and details each one’s algorithm, benefits, and drawbacks. Additionally, the paper examines how these techniques are applied in specific areas like tensor completion, regression, and clustering. Lastly, the paper reflects on the current state of tensor decomposition and suggests areas where further research might be beneficial. In summary, this document provides a thorough exploration of tensor decomposition and its capabilities in signal processing and machine learning.",1,AI
"In recent times, self-supervised learning has become quite popular due to its effectiveness in solving computer vision issues without needing labeled data. This research introduces a fresh strategy for self-supervised tracking using data synthesis focused on the target object. The technique generates realistic training data by merging images of the target with various backgrounds. This synthetic data is then utilized to train a deep neural network capable of accurately tracking the target in live video streams. The network acquires the ability to differentiate the target from its environment through its features, motion, and contextual clues. The efficiency of the suggested approach is examined on multiple test datasets and contrasted against leading self-supervised tracking methods. The findings indicate that this method performs well and surpasses previous techniques in handling complex situations. Thus, this study highlights the potential of self-supervised tracking utilizing target-aware data synthesis as a viable path for unsupervised visual object tracking.",1,AI
"We present a strategy grounded in the element removal idea of Bruns and Tortorelli [1] and aimed at reducing computational cost and circumventing potential numerical instabilities of density-based topology optimization. The design variables and the relative densities are both represented on a fixed, uniform finite element grid, and linked through filtering and Heaviside projection. The regions in the analysis domain where the relative density is below a specified threshold are removed from the forward analysis and replaced by fictitious nodal boundary conditions. This brings a progressive cut of the computational cost as the optimization proceeds and helps to mitigate numerical instabilities associated with low-density regions. Removed regions can be readily reintroduced since all the design variables remain active and are modeled in the formal sensitivity analysis. A key feature of the proposed approach is that the Heaviside functions promote material reintroduction along the structural boundaries by amplifying the magnitude of the sensitivities inside the filter reach. Several 2D and 3D structural topology optimization examples are presented, including linear and nonlinear compliance minimization, the design of a force inverter, and frequency and buckling load maximization. The approach is shown to be effective at producing optimized designs equivalent or nearly equivalent to those obtained without the element removal, while providing remarkable computational savings.",0,Human
"Machine learning techniques have enabled robots to learn narrow, yet complex tasks and also perform broad, yet simple skills with a wide variety of objects. However, learning a model that can both perform complex tasks and generalize to previously unseen objects and goals remains a significant challenge. We study this challenge in the context of ""improvisational"" tool use: a robot is presented with novel objects and a user-specified goal (e.g., sweep some clutter into the dustpan), and must figure out, using only raw image observations, how to accomplish the goal using the available objects as tools. We approach this problem by training a model with both a visual and physical understanding of multi-object interactions, and develop a sampling-based optimizer that can leverage these interactions to accomplish tasks. We do so by combining diverse demonstration data with self-supervised interaction data, aiming to leverage the interaction data to build generalizable models and the demonstration data to guide the model-based RL planner to solve complex tasks. Our experiments show that our approach can solve a variety of complex tool use tasks from raw pixel inputs, outperforming both imitation learning and self-supervised learning individually. Furthermore, we show that the robot can perceive and use novel objects as tools, including objects that are not conventional tools, while also choosing dynamically to use or not use tools depending on whether or not they are required.",0,Human
"This paper studies the problem of predicting missing relationships between entities in knowledge graphs through learning their representations. Currently, the majority of existing link prediction models employ simple but intuitive scoring functions and relatively small embedding size so that they could be applied to large-scale knowledge graphs. However, these properties also restrict the ability to learn more expressive and robust features. Therefore, diverging from most of the prior works which focus on designing new objective functions, we propose, DeCom, a simple but effective mechanism to boost the performance of existing link predictors such as DistMult, ComplEx, etc, through extracting more expressive features while preventing overfitting by adding just a few extra parameters. Specifically, embeddings of entities and relationships are first decompressed to a more expressive and robust space by decompressing functions, then knowledge graph embedding models are trained in this new feature space. Experimental results on several benchmark knowledge graphs and advanced link prediction systems demonstrate the generalization and effectiveness of our method. Especially, RESCAL + DeCom achieves state-of-the-art performance on the FB15k-237 benchmark across all evaluation metrics. In addition, we also show that compared with DeCom, explicitly increasing the embedding size significantly increase the number of parameters but could not achieve promising performance improvement.",0,Human
"Trans-dimensional random field language models (TRF LMs) have recently been introduced, where sentences are modeled as a collection of random fields. The TRF approach has been shown to have the advantages of being computationally more efficient in inference than LSTM LMs with close performance and being able to flexibly integrating rich features. In this paper we propose neural TRFs, beyond of the previous discrete TRFs that only use linear potentials with discrete features. The idea is to use nonlinear potentials with continuous features, implemented by neural networks (NNs), in the TRF framework. Neural TRFs combine the advantages of both NNs and TRFs. The benefits of word embedding, nonlinear feature learning and larger context modeling are inherited from the use of NNs. At the same time, the strength of efficient inference by avoiding expensive softmax is preserved. A number of technical contributions, including employing deep convolutional neural networks (CNNs) to define the potentials and incorporating the joint stochastic approximation (JSA) strategy in the training algorithm, are developed in this work, which enable us to successfully train neural TRF LMs. Various LMs are evaluated in terms of speech recognition WERs by rescoring the 1000-best lists of WSJ'92 test data. The results show that neural TRF LMs not only improve over discrete TRF LMs, but also perform slightly better than LSTM LMs with only one fifth of parameters and 16x faster inference efficiency.",0,Human
"The field of analyzing performance is very important and sensitive in particular when it is related to the performance of lecturers in academic institutions. Locating the weak points of lecturers through a system that provides an early warning to notify or reward the lecturers with warned or punished notices will help them to improve their weaknesses, leads to a better quality in the institutions. The current system has major issues in the higher education at Salahaddin University-Erbil (SUE) in Kurdistan-Iraq. These issues are: first, the assessment of lecturers' activities is conducted traditionally via the Quality Assurance Teams at different departments and colleges at the university, second, the outcomes in some cases of lecturers' performance provoke a low level of acceptance among lectures, as these cases are reflected and viewed by some academic communities as unfair cases, and finally, the current system is not accurate and vigorous. In this paper, Particle Swarm Optimization Neural Network is used to assess performance of lecturers in more fruitful way and also to enhance the accuracy of recognition system. Different real and novel data sets are collected from SUE. The prepared datasets preprocessed and important features are then fed as input source to the training and testing phases. Particle Swarm Optimization is used to find the best weights and biases in the training phase of the neural network. The best accuracy rate obtained in the test phase is 98.28 %.",0,Human
"Unless special conditions apply, the attempt to solve ill-conditioned systems of linear equations with standard numerical methods leads to uncontrollably high numerical error. Often, such systems arise from the discretization of operator equations with a large number of discrete variables. In this paper we show that the accuracy can be improved significantly if the equation is transformed before discretization, a process we call full operator preconditioning (FOP). It bears many similarities with traditional preconditioning for iterative methods but, crucially, transformations are applied at the operator level. We show that while condition-number improvements from traditional preconditioning generally do not improve the accuracy of the solution, FOP can. A number of topics in numerical analysis can be interpreted as implicitly employing FOP; we highlight (i) Chebyshev interpolation in polynomial approximation, and (ii) Olver-Townsend's spectral method, both of which produce solutions of dramatically improved accuracy over a naive problem formulation. In addition, we propose a FOP preconditioner based on integration for the solution of fourth-order differential equations with the finite-element method, showing the resulting linear system is well-conditioned regardless of the discretization size, and demonstrate its error-reduction capabilities on several examples. This work shows that FOP can improve accuracy beyond the standard limit for both direct and iterative methods.",0,Human
"MIMO processing plays a central part towards the recent increase in spectral and energy efficiencies of wireless networks. MIMO has grown beyond the original point-to-point channel and nowadays refers to a diverse range of centralized and distributed deployments. The fundamental bottleneck towards enormous spectral and energy efficiency benefits in multiuser MIMO networks lies in a huge demand for accurate channel state information at the transmitter (CSIT). This has become increasingly difficult to satisfy due to the increasing number of antennas and access points in next generation wireless networks relying on dense heterogeneous networks and transmitters equipped with a large number of antennas. CSIT inaccuracy results in a multi-user interference problem that is the primary bottleneck of MIMO wireless networks. Looking backward, the problem has been to strive to apply techniques designed for perfect CSIT to scenarios with imperfect CSIT. In this paper, we depart from this conventional approach and introduce the readers to a promising strategy based on rate-splitting. Rate-splitting relies on the transmission of common and private messages and is shown to provide significant benefits in terms of spectral and energy efficiencies, reliability and CSI feedback overhead reduction over conventional strategies used in LTE-A and exclusively relying on private message transmissions. Open problems, impact on standard specifications and operational challenges are also discussed.",0,Human
"Quantum computation promises significant computational advantages over classical computation for some problems. However, quantum hardware suffers from much higher error rates than in classical hardware. As a result, extensive quantum error correction is required to execute a useful quantum algorithm. The decoder is a key component of the error correction scheme whose role is to identify errors faster than they accumulate in the quantum computer and that must be implemented with minimum hardware resources in order to scale to the regime of practical applications. In this work, we consider surface code error correction, which is the most popular family of error correcting codes for quantum computing, and we design a decoder micro-architecture for the Union-Find decoding algorithm. We propose a three-stage fully pipelined hardware implementation of the decoder that significantly speeds up the decoder. Then, we optimize the amount of decoding hardware required to perform error correction simultaneously over all the logical qubits of the quantum computer. By sharing resources between logical qubits, we obtain a 67% reduction of the number of hardware units and the memory capacity is reduced by 70%. Moreover, we reduce the bandwidth required for the decoding process by a factor at least 30x using low-overhead compression algorithms. Finally, we provide numerical evidence that our optimized micro-architecture can be executed fast enough to correct errors in a quantum computer.",0,Human
"When 5G began its commercialisation journey around 2020, the discussion on the vision of 6G also surfaced. Researchers expect 6G to have higher bandwidth, coverage, reliability, energy efficiency, lower latency, and, more importantly, an integrated ""human-centric"" network system powered by artificial intelligence (AI). Such a 6G network will lead to an excessive number of automated decisions made every second. These decisions can range widely, from network resource allocation to collision avoidance for self-driving cars. However, the risk of losing control over decision-making may increase due to high-speed data-intensive AI decision-making beyond designers and users' comprehension. The promising explainable AI (XAI) methods can mitigate such risks by enhancing the transparency of the black box AI decision-making process. This survey paper highlights the need for XAI towards the upcoming 6G age in every aspect, including 6G technologies (e.g., intelligent radio, zero-touch network management) and 6G use cases (e.g., industry 5.0). Moreover, we summarised the lessons learned from the recent attempts and outlined important research challenges in applying XAI for building 6G systems. This research aligns with goals 9, 11, 16, and 17 of the United Nations Sustainable Development Goals (UN-SDG), promoting innovation and building infrastructure, sustainable and inclusive human settlement, advancing justice and strong institutions, and fostering partnership at the global level.",0,Human
"Smart cities solutions are often monolithically implemented, from sensors data handling through to the provided services. The same challenges are regularly faced by different developers, for every new solution in a new city. Expertise and know-how can be re-used and the effort shared. In this article we present the methodologies to minimize the efforts of implementing new smart city solutions and maximizing the sharing of components. The final target is to have a live technical community of smart city application developers. The results of this activity comes from the implementation of 35 city services in 27 cities between Europe and South Korea. To share efforts, we encourage developers to devise applications using a modular approach. Single-function components that are re-usable by other city services are packaged and published as standalone components, named Atomic Services. We identify 15 atomic services addressing smart city challenges in data analytics, data evaluation, data integration, data validation, and visualization. 38 instances of the atomic services are already operational in several smart city services. We detail in this article, as atomic service examples, some data predictor components. Furthermore, we describe real-world atomic services usage in the scenarios of Santander and three Danish cities. The resulting atomic services also generate a side market for smart city solutions, allowing expertise and know-how to be re-used by different stakeholders.",0,Human
"We present Stable View Synthesis (SVS). Given a set of source images depicting a scene from freely distributed viewpoints, SVS synthesizes new views of the scene. The method operates on a geometric scaffold computed via structure-from-motion and multi-view stereo. Each point on this 3D scaffold is associated with view rays and corresponding feature vectors that encode the appearance of this point in the input images. The core of SVS is view-dependent on-surface feature aggregation, in which directional feature vectors at each 3D point are processed to produce a new feature vector for a ray that maps this point into the new target view. The target view is then rendered by a convolutional network from a tensor of features synthesized in this way for all pixels. The method is composed of differentiable modules and is trained end-to-end. It supports spatially-varying view-dependent importance weighting and feature transformation of source images at each point; spatial and temporal stability due to the smooth dependence of on-surface feature aggregation on the target view; and synthesis of view-dependent effects such as specular reflection. Experimental results demonstrate that SVS outperforms state-of-the-art view synthesis methods both quantitatively and qualitatively on three diverse real-world datasets, achieving unprecedented levels of realism in free-viewpoint video of challenging large-scale scenes. Code is available at https://github.com/intel-isl/StableViewSynthesis",0,Human
"Processing of symbolic sequences represented by mapping of symbolic data into numerical signals is commonly used in various applications. It is a particularly popular approach in genomic and proteomic sequence analysis. Numerous mappings of symbolic sequences have been proposed for various applications. It is unclear however whether the processing of symbolic data provides an artifact of the numerical mapping or is an inherent property of the symbolic data. This issue has been long ignored in the engineering and scientific literature. It is possible that many of the results obtained in symbolic signal processing could be a byproduct of the mapping and might not shed any light on the underlying properties embedded in the data. Moreover, in many applications, conflicting conclusions may arise due to the choice of the mapping used for numerical representation of symbolic data. In this paper, we present a novel framework for the analysis of the equivalence of the mappings used for numerical representation of symbolic data. We present strong and weak equivalence properties and rely on signal correlation to characterize equivalent mappings. We derive theoretical results which establish conditions for consistency among numerical mappings of symbolic data. Furthermore, we introduce an abstract mapping model for symbolic sequences and extend the notion of equivalence to an algebraic framework. Finally, we illustrate our theoretical results by application to DNA sequence analysis.",0,Human
"Vehicular Ad-hoc NETworks (VANETs) are developing at a very fast pace to enable smart transportation in urban cities, by designing some mechanisms for decreasing travel time for commuters by reducing congestion. Inefficient Traffic signals and routing mechanisms are the major factors that contribute to the increase of road congestion. For smoother traffic movement and reducing congestion on the roads, the waiting time at intersections must be reduced and an optimal path should be chosen simultaneously. In this paper, A GPU assisted Preemptive MACO (GMACO-P) algorithm has been proposed to minimize the total travel time of the commuters. GMACO-P is an improvement of MACO-P algorithm that uses the harnessing the power of the GPU to provide faster computations for further minimizing the travel time. The MACO-P algorithm is based on an existing MACO algorithm that avoid the path with the congestion. The MACO-P algorithm reduces the average queue length at intersections by incorporating preemption that ensures less waiting time. In this paper, GMACO-P algorithm is proposed harnessing the power of GPU to improve MACO-P to further reduce the travel time. The GMACO-P algorithm is executed with CUDA toolkit 7.5 using C language and the obtained results were compared with existing Dijkstra, ACO, MACO, MACO-P, parallel implementation of the Dijkstra, ACO and MACO algorithms. Obtained results show the significant reduction in the travel time after using the proposed GMACO-P algorithm.",0,Human
"This research delves into the issue of bloated dependencies within the Maven ecosystem. Its goal is to uncover how widespread and impactful these bloated dependencies are on both the build process and the performance of Maven projects. By using a mix of static code analysis and dynamic profiling, the study identifies and measures the consequences of bloated dependencies. The findings reveal that bloated dependencies are prevalent in Maven projects and can notably hamper the build speed and inflate the memory usage of the build system. Additionally, the paper offers strategies to address this challenge, such as implementing effective dependency management practices and substituting bloated dependencies with lighter alternatives. These insights are particularly useful for developers, build engineers, and managers aiming to enhance their Maven build efficiency and boost project performance.",1,AI
"This research introduces a novel self-supervised autoregressive domain adaptation strategy tailored for time series data, designed to tackle issues like domain shift and scarcity of labeled data. It makes use of the natural temporal relationships found within time series and adapts to new environments without needing labeled data from the target domain. The technique entails building a self-supervised autoregressive model on the source dataset, generating synthetic examples for the target domain using this model, and training a domain adaptation model to reduce discrepancies between the source and target domains. This approach was tested on two real-world datasets against leading methods, demonstrating superior performance in both accuracy and resistance to domain changes. The findings indicate that leveraging self-supervised autoregressive models can significantly enhance how we adapt to different domains when working with time series data, even in situations where obtaining adequate labeled data is challenging.",1,AI
"The human visual perception system has very strong robustness and contextual awareness in a variety of image processing tasks. This robustness and the perception ability of contextual awareness is closely related to the characteristics of multi-task auxiliary learning and subjective attention of the human visual perception system. In order to improve the robustness and contextual awareness of image fusion tasks, we proposed a multi-task auxiliary learning image fusion theory guided by subjective attention. The image fusion theory effectively unifies the subjective task intention and prior knowledge of human brain. In order to achieve our proposed image fusion theory, we first analyze the mechanism of multi-task auxiliary learning, build a multi-task auxiliary learning network. Secondly, based on the human visual attention perception mechanism, we introduce the human visual attention network guided by subjective tasks on the basis of the multi-task auxiliary learning network. The subjective intention is introduced by the subjective attention task model, so that the network can fuse images according to the subjective intention. Finally, in order to verify the superiority of our image fusion theory, we carried out experiments on the combined vision system image data set, and the infrared and visible image data set for experimental verification. The experimental results demonstrate the superiority of our fusion theory over state-of-arts in contextual awareness and robustness.",0,Human
"This paper introduces a new dataset named ""Amigos,"" tailored for investigating affect, personality, and mood across individuals and groups. Comprising audio, video, and self-report questionnaires gathered from a wide participant pool in varied social settings, the paper details the dataset's construction, detailing data collection methods, the scales used to measure affect, personality, and mood, and the ethical issues at play. Additionally, it showcases multiple applications of the Amigos dataset in fields like social psychology, affective computing, and human-computer interaction. In summary, this resource offers significant value for researchers exploring affect, personality, and mood both individually and within social contexts.",1,AI
"A color image contains luminance and chrominance components representing the intensity and color information respectively. The objective of the work presented in this paper is to show the significance of incorporating the chrominance information for the task of scene classification. An improved color-to-grayscale image conversion algorithm by effectively incorporating the chrominance information is proposed using color-to-gay structure similarity index (C2G-SSIM) and singular value decomposition (SVD) to improve the perceptual quality of the converted grayscale images. The experimental result analysis based on the image quality assessment for image decolorization called C2G-SSIM and success rate (Cadik and COLOR250 datasets) shows that the proposed image decolorization technique performs better than 8 existing benchmark algorithms for image decolorization. In the second part of the paper, the effectiveness of incorporating the chrominance component in scene classification task is demonstrated using the deep belief network (DBN) based image classification system developed using dense scale invariant feature transform (SIFT) as features. The levels of chrominance information incorporated by the proposed image decolorization technique is confirmed by the improvement in the overall scene classification accuracy . Also, the overall scene classification performance is improved by the combination of models obtained using the proposed and the conventional decolorization methods.",0,Human
"This research introduces an innovative stereo camera-based SLAM technique tailored for outdoor construction sites with numerous moving elements. The proposed system utilizes a hierarchical masking technique to filter out these dynamic components during the SLAM process, thereby enhancing overall precision. This masking procedure involves multiple steps, beginning with a broad initial mask that becomes increasingly detailed as the algorithm progresses, resulting in a finer final mask. Furthermore, the study incorporates a motion-state classification mechanism to accurately differentiate between stationary and mobile objects, which aids in the efficiency and reliability of the masking process. The efficacy of the proposed approach is validated via rigorous testing on actual outdoor construction environments. Experimental outcomes indicate that the proposed SLAM technique surpasses contemporary methods in both accuracy and resistance to interference from large, active elements.",1,AI
"This study examines the latest developments in Delay Tolerant Network (DTN) routing protocols through a thorough survey. DTN networks are specifically designed for operation in tough and constantly changing conditions, like those faced during emergencies, space exploration, or remote locations lacking reliable internet access. The research highlights recent advancements, comparing both established and innovative methods. It offers an in-depth look at the key attributes and performance benchmarks of these protocols, along with their pros and cons. Additionally, the paper identifies existing obstacles and unanswered questions in DTN routing, and recommends paths for further investigation. These findings will be useful for professionals involved in researching, implementing, or utilizing DTN systems.",1,AI
"Thermodynamic-RAM (T-RAM) is a cutting-edge computational method that uses thermodynamic laws to carry out computations and has demonstrated its capability to emulate the actions of biological neurons. In this study, we explore the application of T-RAM in modeling the functions of the cerebral cortex. We begin by outlining the structure and operation of T-RAM, and contrast it with other neuromorphic computing technologies. Following that, we detail our implementation of T-RAM to simulate cortical processing, employing a simplified model of the visual cortex as an example. Our findings indicate that T-RAM can reproduce important characteristics of cortical processing such as selective attention, receptive fields, and ocular dominance. Furthermore, we examine how different parameters affect the efficiency of T-RAM and analyze what these insights mean for improving future T-RAM designs aimed at more intricate models of cortical function. The results show that T-RAM could serve as a valuable tool for understanding how the cerebral cortex works and might eventually help create even more advanced and bio-inspired computational systems.",1,AI
"This study explores how adaptive pilot patterns can enhance carrier-aggregated orthogonal frequency division multiplexing (CA-OFDM) systems in fluctuating wireless environments. In such environments, where the channel conditions change over time, CA-OFDM faces significant challenges that can degrade its performance. To tackle this issue, adaptive pilot patterns are suggested as a way to help the system adapt to these changes. Through simulations conducted in nonstationary wireless channels, the study demonstrates that using adaptive pilot patterns significantly improves the system’s ability to track shifting channel conditions, thereby boosting both throughput and reducing errors. This research advances the field of adaptive techniques for wireless communication, which is crucial for supporting high-speed wireless connections in dynamic settings.",1,AI
"This research examines how to enhance the fault tolerance of digital microfluidic biochips (DMFBs). These chips have the potential to transform bioanalysis by streamlining complex laboratory procedures into smaller, automated processes. However, due to their intricate nature and extensive integration, DMFBs are prone to numerous faults and errors that could undermine their efficiency and dependability. To address this issue, our proposed design flow includes four key stages: fault modeling, fault analysis, fault-tolerant synthesis, and fault-tolerant testing. In the fault modeling phase, we identify potential faults and their root causes within the DMFB design. Fault analysis evaluates how these faults affect the chip's performance and functionality. The fault-tolerant synthesis stage aims to create a design that can withstand and correct identified faults. Lastly, the fault-tolerant testing phase confirms the new design's performance and reliability. We validate this process using a case study involving a DMFB designed for protein crystallization, which requires a series of intricate biochemical reactions. Fault injection tests confirm the effectiveness of the fault-tolerant design, demonstrating that our method can significantly improve DMFB fault tolerance. The paper highlights the comprehensive design flow we've developed, showcases its practical application through a real-world example, and discusses some of its limitations and future possibilities. This design flow serves as a blueprint for creating dependable and robust DMFBs suitable for various bioanalysis applications.",1,AI
"An accountable algorithmic transparency report (ATR) should ideally investigate the (a) transparency of the underlying algorithm, and (b) fairness of the algorithmic decisions, and at the same time preserve data subjects' privacy. However, a provably formal study of the impact to data subjects' privacy caused by the utility of releasing an ATR (that investigates transparency and fairness), is yet to be addressed in the literature. The far-fetched benefit of such a study lies in the methodical characterization of privacy-utility trade-offs for release of ATRs in public, and their consequential application-specific impact on the dimensions of society, politics, and economics. In this paper, we first investigate and demonstrate potential privacy hazards brought on by the deployment of transparency and fairness measures in released ATRs. To preserve data subjects' privacy, we then propose a linear-time optimal-privacy scheme, built upon standard linear fractional programming (LFP) theory, for announcing ATRs, subject to constraints controlling the tolerance of privacy perturbation on the utility of transparency schemes. Subsequently, we quantify the privacy-utility trade-offs induced by our scheme, and analyze the impact of privacy perturbation on fairness measures in ATRs. To the best of our knowledge, this is the first analytical work that simultaneously addresses trade-offs between the triad of privacy, utility, and fairness, applicable to algorithmic transparency reports.",0,Human
"This study investigates unsupervised domain adaptation focusing on enhancing bi-classifier determinacy. The primary objective is to build a model using labeled data from a source domain and apply it to an unlabeled target domain. Bi-classifier determinacy maximization utilizes decision boundaries created by classifiers trained on both the source and target domains to boost the model's performance. Our approach focuses on creating a decision boundary that aligns closely with the two classifiers' decisions, thereby minimizing the difference between the source and target domains. Experimental outcomes on diverse datasets highlight our method's efficacy in improving model generalization, surpassing existing leading techniques. The research also discusses possible real-world applications of this technique across areas like computer vision and natural language processing.",1,AI
"Transmission over multiple frequency bands combined into one logical channel speeds up data transfer for wireless networks. On the other hand, the allocation of multiple channels to a single user decreases the probability of finding a free logical channel for new connections, which may result in a network-wide throughput loss. While this relationship has been studied experimentally, especially in the WLAN configuration, little is known on how to analytically model such phenomena. With the advent of Opportunistic Spectrum Access (OSA) networks, it is even more important to understand the circumstances in which it is beneficial to bond channels occupied by primary users with dynamic duty cycle patterns. In this paper we propose an analytical framework which allows the investigation of the average channel throughput at the medium access control layer for OSA networks with channel bonding enabled. We show that channel bonding is generally beneficial, though the extent of the benefits depend on the features of the OSA network, including OSA network size and the total number of channels available for bonding. In addition, we show that performance benefits can be realized by adaptively changing the number of bonded channels depending on network conditions. Finally, we evaluate channel bonding considering physical layer constraints, i.e. throughput reduction compared to the theoretical throughput of a single virtual channel due to a transmission power limit for any bonding size.",0,Human
"This research explores the interactions within a medium during diffusion-based molecular communication in a two-way relay setup. Diffusion-based molecular communication holds promise for communication in tiny devices and systems due to limitations posed by the absence of electromagnetic waves. In this study, a two-way relay model is examined, focusing on how two nanomachines can exchange information via molecule diffusion in a medium. The medium is conceptualized as a divided system, where each division symbolizes a distinct setting with varying reaction speeds. A mathematical model is developed to describe the reaction rates of the medium and the molecular signals, and equations are derived to calculate the bit error rate (BER) and the communication system's capacity. Using this framework, the impact of medium reactions on system performance is analyzed. Simulation outcomes show that medium reactions substantially influence both the BER and the system's capacity, and the proposed framework can be employed to enhance system performance by adjusting the medium's reaction rates. These insights are valuable for designing and optimizing diffusion-based molecular communication systems, particularly in environments where chemical reactions play a crucial role.",1,AI
"This study introduces an adaptive control technique using reinforcement learning for manufacturing processes with a predetermined timeframe. It leverages reinforcement learning algorithms to refine the control strategy for these processes without needing a detailed model of the underlying dynamics. The technique is tested across various episodes of manufacturing tasks, aiming to achieve the highest possible reward within a fixed duration. Experimental outcomes indicate that the proposed method can dynamically adjust to changing conditions, leading to better control outcomes than conventional approaches that rely on predefined models. These findings highlight the promising role of reinforcement learning in real-time control of manufacturing operations and suggest avenues for further research in this domain.",1,AI
"In the last years, multi-objective evolutionary algorithms (MOEA) have been applied to different software engineering problems where many conflicting objectives have to be optimized simultaneously. In theory, evolutionary algorithms feature a nice property for runtime optimization as they can provide a solution in any execution time. In practice, based on a Darwinian inspired natural selection, these evolutionary algorithms produce many deadborn solutions whose computation results in a computational resources wastage: natural selection is naturally slow. In this paper, we reconsider this founding analogy to accelerate convergence of MOEA, by looking at modern biology studies: artificial selection has been used to achieve an anticipated specific purpose instead of only relying on crossover and natural selection (i.e., Muller et al [18] research on artificial mutation of fruits with X-Ray). Putting aside the analogy with natural selection , the present paper proposes an hyper-heuristic for MOEA algorithms named Sputnik 1 that uses artificial selective mutation to improve the convergence speed of MOEA. Sputnik leverages the past history of mutation efficiency to select the most relevant mutations to perform. We evaluate Sputnik on a cloud-reasoning engine, which drives on-demand provisioning while considering conflicting performance and cost objectives. We have conducted experiments to highlight the significant performance improvement of Sputnik in terms of resolution time.",0,Human
"Machine learning models have been successfully used in many scientific and engineering fields. However, it remains difficult for a model to simultaneously utilize domain knowledge and experimental observation data. The application of knowledge-based symbolic AI represented by an expert system is limited by the expressive ability of the model, and data-driven connectionism AI represented by neural networks is prone to produce predictions that violate physical mechanisms. In order to fully integrate domain knowledge with observations, and make full use of the prior information and the strong fitting ability of neural networks, this study proposes theory-guided hard constraint projection (HCP). This model converts physical constraints, such as governing equations, into a form that is easy to handle through discretization, and then implements hard constraint optimization through projection. Based on rigorous mathematical proofs, theory-guided HCP can ensure that model predictions strictly conform to physical mechanisms in the constraint patch. The performance of the theory-guided HCP is verified by experiments based on the heterogeneous subsurface flow problem. Due to the application of hard constraints, compared with fully connected neural networks and soft constraint models, such as theory-guided neural networks and physics-informed neural networks, theory-guided HCP requires fewer data, and achieves higher prediction accuracy and stronger robustness to noisy observations.",0,Human
"The paper offers a thorough and current look at social network research, particularly concentrating on privacy issues within these platforms. It examines the latest trends and advancements in this field, such as users' heightened concern over data security and social media companies' efforts to tackle these issues. The document emphasizes the significant challenges and possibilities for future studies in this domain, including the need to clarify what privacy means and how it can be effectively managed across various social network settings, and exploring innovative technologies that could enhance privacy safeguards in these environments. Finally, the paper urges for more research and collaboration among scholars, business professionals, and government representatives to tackle the intricate and urgent matter of privacy in social networking sites.",1,AI
"This study introduces a new strategy to tackle the issue of detecting objects in situations where certain categories are very rare or appear in limited parts of an image. The suggested technique, named Talisman, focuses on selecting the most beneficial samples to label through iterative processes. Its objective is to enhance the accuracy of object recognition models when dealing with these uncommon categories and areas. Talisman leverages a submodular function to evaluate the mutual information between labeled and unlabeled images, using this metric to choose samples that will provide the greatest value for labeling. Moreover, the framework considers specific classes and regions as targets, allowing it to focus on the areas with the most potential benefits. The performance of Talisman has been tested across multiple datasets, and the outcomes indicate that this method significantly boosts the model’s ability to recognize rare objects and areas. Furthermore, the experimental data reveal that Talisman outperforms many advanced techniques in active learning, particularly in contexts involving rare categories and specific regions. In summary, Talisman presents a promising solution for targeted active learning in object detection, especially for addressing the complex situation of rare categories and areas.",1,AI
"3D object detection based on point clouds has become more and more popular. Some methods propose localizing 3D objects directly from raw point clouds to avoid information loss. However, these methods come with complex structures and significant computational overhead, limiting its broader application in real-time scenarios. Some methods choose to transform the point cloud data into compact tensors first and leverage off-the-shelf 2D detectors to propose 3D objects, which is much faster and achieves state-of-the-art results. However, because of the inconsistency between 2D and 3D data, we argue that the performance of compact tensor-based 3D detectors is restricted if we use 2D detectors without corresponding modification. Specifically, the distribution of point clouds is uneven, with most points gather on the boundary of objects, while detectors for 2D data always extract features evenly. Motivated by this observation, we propose DENse Feature Indicator (DENFI), a universal module that helps 3D detectors focus on the densest region of the point clouds in a boundary-aware manner. Moreover, DENFI is lightweight and guarantees real-time speed when applied to 3D object detectors. Experiments on KITTI dataset show that DENFI improves the performance of the baseline single-stage detector remarkably, which achieves new state-of-the-art performance among previous 3D detectors, including both two-stage and multi-sensor fusion methods, in terms of mAP with a 34FPS detection speed.",0,Human
"This paper delves into the world of processes, roles, and their interplay within organizational structures. Its goal is to uncover the intricate workings of these components and their influence on organizational efficiency. By thoroughly examining existing literature, the paper categorizes the different types of processes and roles and elucidates their respective functions. It also investigates the complex relationships between these elements, including their dependencies, conflicts, and synergies. The research methodology incorporates both qualitative and quantitative techniques like case studies, surveys, and statistical analyses. The results underscore the pivotal role that processes and roles play in achieving organizational success and the necessity of harmonious interactions among them. Ultimately, the paper provides actionable advice for businesses to enhance their processes, roles, and interactions for better performance.",1,AI
"Semantic segmentation of aerial videos has been extensively used for decision making in monitoring environmental changes, urban planning, and disaster management. The reliability of these decision support systems is dependent on the accuracy of the video semantic segmentation algorithms. The existing CNN based video semantic segmentation methods have enhanced the image semantic segmentation methods by incorporating an additional module such as LSTM or optical flow for computing temporal dynamics of the video which is a computational overhead. The proposed research work modifies the CNN architecture by incorporating temporal information to improve the efficiency of video semantic segmentation.  In this work, an enhanced encoder-decoder based CNN architecture (UVid-Net) is proposed for UAV video semantic segmentation. The encoder of the proposed architecture embeds temporal information for temporally consistent labelling. The decoder is enhanced by introducing the feature-refiner module, which aids in accurate localization of the class labels. The proposed UVid-Net architecture for UAV video semantic segmentation is quantitatively evaluated on extended ManipalUAVid dataset. The performance metric mIoU of 0.79 has been observed which is significantly greater than the other state-of-the-art algorithms. Further, the proposed work produced promising results even for the pre-trained model of UVid-Net on urban street scene with fine tuning the final layer on UAV aerial videos.",0,Human
"As there's a growing need for rapid internet access and robust mobile network coverage, drones equipped with base stations have become a viable option to bolster network availability in hard-to-reach and disaster-hit regions. Nevertheless, managing handovers—transferring a user’s connection from one base station to another—is an ongoing challenge for these flying networks because of their fast movement and narrow range of influence. This paper introduces a group handover method for drone-mounted base stations within mobile networks. Our proposed approach seeks to cut down handover delays and guarantee constant connectivity for numerous users even when the drone is on the move. It employs an anticipatory handover technique, whereby the base station initiates the handover process prior to losing contact with the user. Additionally, our proposal takes into account the position and speed of the drone-mounted base station, along with the strength of the communication link between the user and the base station. To assess the effectiveness of our suggested solution, we conducted simulations in a real-world urban setting. These tests revealed that our strategy could decrease handover delays by up to 50% compared to conventional reactive handover methods. Furthermore, our proposal has been shown to enhance network throughput and decrease the incidence of failed handovers. In summary, our findings highlight the practicality of the proposed group handover method for drone-mounted base stations in mobile networks. This innovative approach not only improves service quality but also ensures smooth connections for users in underserved areas. Future studies may investigate how this strategy can be integrated with other handover management techniques and evaluated under various deployment conditions.",1,AI
"The development of positioning technologies has resulted in an increasing amount of mobility data being available. While bringing a lot of convenience to people's life, such availability also raises serious concerns about privacy. In this paper, we concentrate on one of the most sensitive information that can be inferred from mobility data, namely social relationships. We propose a novel social relation inference attack that relies on an advanced feature learning technique to automatically summarize users' mobility features. Compared to existing approaches, our attack is able to predict any two individuals' social relation, and it does not require the adversary to have any prior knowledge on existing social relations. These advantages significantly increase the applicability of our attack and the scope of the privacy assessment. Extensive experiments conducted on a large dataset demonstrate that our inference attack is effective, and achieves between 13% to 20% improvement over the best state-of-the-art scheme. We propose three defense mechanisms -- hiding, replacement and generalization -- and evaluate their effectiveness for mitigating the social link privacy risks stemming from mobility data sharing. Our experimental results show that both hiding and replacement mechanisms outperform generalization. Moreover, hiding and replacement achieve a comparable trade-off between utility and privacy, the former preserving better utility and the latter providing better privacy.",0,Human
"Communication or influence networks are probably the most controllable of all factors that are known to impact on the problem-solving capability of task-forces. In the case connections are costly, it is necessary to implement a policy to allocate them to the individuals. Here we use an agent-based model to study how distinct allocation policies affect the performance of a group of agents whose task is to find the global maxima of NK fitness landscapes. Agents cooperate by broadcasting messages informing on their fitness and use this information to imitate the fittest agent in their influence neighborhoods. The larger the influence neighborhood of an agent, the more links, and hence information, the agent receives. We find that the elitist policy in which agents with above-average fitness have their influence neighborhoods amplified, whereas agents with below-average fitness have theirs deflated, is optimal for smooth landscapes, provided the group size is not too small. For rugged landscapes, however, the elitist policy can perform very poorly for certain group sizes. In addition, we find that the egalitarian policy, in which the size of the influence neighborhood is the same for all agents, is optimal for both smooth and rugged landscapes in the case of small groups. The welfarist policy, in which the actions of the elitist policy are reversed, is always suboptimal, i.e., depending on the group size it is outperformed by either the elitist or the egalitarian policies.",0,Human
"Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.",0,Human
"This research introduces an effective strategy to enhance slope limiters when applied to non-uniform rectilinear grids. Slope limiters help maintain stability and avoid unrealistic outcomes in simulations like computational fluid dynamics. The focus is on using these limiters on non-uniform rectilinear grids, which are commonly employed in various engineering and scientific fields but present difficulties for standard slope limiting techniques. The proposed method integrates grid adaptation strategies alongside innovative slope limiters to boost the precision and reliability of numerical simulations. The outcomes indicate that this approach successfully addresses the shortcomings of traditional slope limiters and delivers dependable results across numerous test scenarios. These findings could significantly influence advancements in numerical methodologies for simulations conducted on non-uniform grids.",1,AI
"Contextual bandits are widely recognized for their effectiveness in managing sequential decisions by balancing exploration and exploitation. This paper delves into the estimation aspects within contextual bandits, exploring how different elements like the reward function selection, number of options, context representation method, and sample size influence algorithm performance. By reviewing the existing body of work, we also outline potential avenues for future research. The findings show that the success of contextual bandits hinges significantly on precise estimations, emphasizing the necessity of thoughtful design and rigorous testing. These insights can help guide professionals in creating and deploying contextual bandit models effectively, and pave the way for deeper exploration in this field.",1,AI
"This study introduces a framework for ranking knowledge base properties based on specific entities. It's evaluated through two case studies: one involving doctoral advisors and another concerning medical conditions. The outcomes show that this entity-specific ranking approach yields better accuracy and relevance compared to conventional techniques. The insights gained can greatly enhance the efficiency and reliability of knowledge-based systems, especially in fields like academic guidance and medical diagnosis. An expanded version of the paper offers a deeper look into the proposed method and its practical application, alongside an extensive examination of its efficacy across the aforementioned case studies.",1,AI
"This study delves into the Conditional Lucas & Kanade (CLK) algorithm, which is a widely used method in computer vision for calculating optical flow between consecutive frames in video sequences. We introduce a new enhancement to the traditional Lucas & Kanade (LK) algorithm by integrating a dynamic weight matrix that adapts to the specific characteristics of local image features. This modification enhances the accuracy and reliability of the LK algorithm by better handling differences in image data like lighting variations and object occlusions. To assess the efficacy of our enhanced CLK algorithm, we tested it on multiple standardized benchmark datasets alongside other advanced optical flow techniques. The results indicate that the CLK algorithm outperforms the original LK algorithm and similar alternatives, especially when dealing with complex situations involving significant movement and partial coverages. Additionally, we showcase how the CLK algorithm can be practically applied in practical scenarios such as visual odometry, where precise tracking of a camera's movement is crucial. Our findings reveal that the CLK algorithm provides not only better performance but also more efficient computation compared to other optical flow methods. In summary, our research underscores the CLK algorithm’s superiority over the LK algorithm and suggests its potential for enhancing the field of computer vision across diverse real-world applications.",1,AI
"As real-world images come in varying sizes, the machine learning model is part of a larger system that includes an upstream image scaling algorithm. In this system, the model and the scaling algorithm have become attractive targets for numerous attacks, such as adversarial examples and the recent image-scaling attack. In response to these attacks, researchers have developed defense approaches that are tailored to attacks at each processing stage. As these defenses are developed in isolation, their underlying assumptions may not hold when viewing them from the perspective of an end-to-end machine learning system. Thus, it is necessary to study these attacks and defenses in the context of machine learning systems. In this paper, we investigate the interplay between vulnerabilities of the image scaling procedure and machine learning models in the challenging hard-label black-box setting. We propose a series of novel techniques to make a black-box attack exploit vulnerabilities in scaling algorithms, scaling defenses, and the final machine learning model in an end-to-end manner. Based on this scaling-aware attack, we reveal that most existing scaling defenses are ineffective under threat from downstream models. Moreover, we empirically observe that standard black-box attacks can significantly improve their performance by exploiting the vulnerable scaling procedure. We further demonstrate this problem on a commercial Image Analysis API with transfer-based black-box attacks.",0,Human
"This research focuses on enhancing loadability in distribution networks through the application of genetic algorithm optimization techniques. The goal is to optimize how loads are transferred between distribution lines and improve the system's overall loadability. A realistic distribution network model is utilized, and a genetic algorithm-based method is introduced to address the optimization challenge. This approach is tested across various load scenarios and is assessed based on the extent of improved loadability it achieves. The outcomes indicate that the genetic algorithm methodology successfully boosts load transfer and enhances the loadability of the distribution network. Additionally, the research contrasts the effectiveness of the proposed technique with other prevailing methods, demonstrating that the genetic algorithm approach excels in terms of loadability improvement. These insights can be valuable for distribution network operators aiming to refine their load transfer strategies and boost system efficiency.",1,AI
"Over the past few years, there's been increasing interest in analyzing the computational complexity of different types of combinatorial voter control issues in elections. Combinatorial voter control involves altering the votes of a group of voters to influence the election outcome strategically. This study examines the computational complexities of several variations of this issue. Initially, we look at the standard voter control scenario, where the objective is to minimize the number of votes needed to change to achieve a specified winner. We demonstrate that solving this problem is NP-hard, indicating that large-scale instances of this problem are difficult to solve optimally. Following this, we investigate the complexities of related issues that incorporate limitations like financial constraints or restrictions on how many votes a single voter can have their say. We prove that even these modified versions are NP-hard and present methods for finding near-optimal solutions. Lastly, we analyze the computational challenges posed by voter control in various election formats, including single-winner elections, multi-winner elections, and those using proportional representation. We find that the difficulty level of the problem differs based on the type of election system used and develop algorithms to address these issues under each format. This research offers a thorough exploration of the computational complexity associated with combinatorial voter control problems in elections and offers guidance on designing algorithms to tackle these problems.",1,AI
"This research delves into the effectiveness of Graph Neural Networks (GNNs) in predicting the binding affinity of small molecules with protein targets. Researchers evaluated various GNN models against traditional methods using a wide range of protein-ligand complexes. The outcomes indicated that GNNs performed similarly well or better than conventional approaches, boasting impressive predictive accuracy and stability. The study also pinpointed crucial elements influencing GNN prediction reliability, including hyperparameter selection and the quality of the training dataset. Overall, these findings underscore the promising role of GNNs in virtual screening and emphasize their value as a dependable tool in drug discovery efforts.",1,AI
"The off-switch game is a straightforward two-person game explored in game theory and decision-making. One player can choose to stop the game, whereas the other player gets to decide whether to agree or decline this decision. This study offers a detailed examination of the off-switch game's current research, covering its mathematical representation, equilibrium points, and practical applications across different sectors. Additionally, the researchers suggest novel methods to evaluate the off-switch game and its derivatives, showcasing their effectiveness via real-world examples and computer simulations. These findings enhance our comprehension of how decisions are made when there's the possibility to abruptly halt an interaction, and they have far-reaching implications for fields like negotiation, conflict resolution, and social behavior.",1,AI
"Purpose: Development of a fast and fully automated deep learning pipeline (FatSegNet) to accurately identify, segment, and quantify abdominal adipose tissue on Dixon MRI from the Rhineland Study - a large prospective population-based study. Method: FatSegNet is composed of three stages: (i) consistent localization of the abdominal region using two 2D-Competitive Dense Fully Convolutional Networks (CDFNet), (ii) segmentation of adipose tissue on three views by independent CDFNets, and (iii) view aggregation. FatSegNet is trained with 33 manually annotated subjects, and validated by: 1) comparison of segmentation accuracy against a testingset covering a wide range of body mass index (BMI), 2) test-retest reliability, and 3) robustness in a large cohort study. Results: The CDFNet demonstrates increased robustness compared to traditional deep learning networks. FatSegNet dice score outperforms manual raters on the abdominal visceral adipose tissue (VAT, 0.828 vs. 0.788), and produces comparable results on subcutaneous adipose tissue (SAT, 0.973 vs. 0.982). The pipeline has very small test-retest absolute percentage difference and excellent agreement between scan sessions (VAT: APD = 2.957%, ICC=0.998 and SAT: APD= 3.254%, ICC=0.996). Conclusion: FatSegNet can reliably analyze a 3D Dixon MRI in1 min. It generalizes well to different body shapes, sensitively replicates known VAT and SAT volume effects in a large cohort study, and permits localized analysis of fat compartments.",0,Human
"The problem of estimating sparse eigenvectors of a symmetric matrix attracts a lot of attention in many applications, especially those with high dimensional data set. While classical eigenvectors can be obtained as the solution of a maximization problem, existing approaches formulate this problem by adding a penalty term into the objective function that encourages a sparse solution. However, the resulting methods achieve sparsity at the expense of sacrificing the orthogonality property. In this paper, we develop a new method to estimate dominant sparse eigenvectors without trading off their orthogonality. The problem is highly non-convex and hard to handle. We apply the MM framework where we iteratively maximize a tight lower bound (surrogate function) of the objective function over the Stiefel manifold. The inner maximization problem turns out to be a rectangular Procrustes problem, which has a closed form solution. In addition, we propose a method to improve the covariance estimation problem when its underlying eigenvectors are known to be sparse. We use the eigenvalue decomposition of the covariance matrix to formulate an optimization problem where we impose sparsity on the corresponding eigenvectors. Numerical experiments show that the proposed eigenvector extraction algorithm matches or outperforms existing algorithms in terms of support recovery and explained variance, while the covariance estimation algorithms improve significantly the sample covariance estimator.",0,Human
"This study explores how to lessen interference in Cloud Radio Access Networks (C-RANs) through the use of the rate-splitting technique coupled with common message decoding. While the rate-splitting method has shown promise in reducing interference and enhancing network efficiency, its implementation in C-RANs has been hindered by the challenges of coordinating transmissions between the Baseband Unit (BBU) and Remote Radio Heads (RRHs). In our research, we suggest integrating the rate-splitting technique and common message decoding at the BBU to address interference issues. We also introduce an innovative algorithm aimed at optimizing the rate-splitting parameters and common message decoding coefficients to balance power usage with service quality, ensuring satisfactory QoS for all users. Our simulations indicate that our proposed solution yields substantial improvements over conventional methods, particularly in high-density and heavily interfered environments. These results highlight the potential of rate-splitting and common message decoding as effective strategies for managing interference in C-RANs and offer guidance for developing advanced wireless systems.",1,AI
"Current evaluation functions for heuristic planning are expensive to compute. In numerous planning problems these functions provide good guidance to the solution, so they are worth the expense. However, when evaluation functions are misguiding or when planning problems are large enough, lots of node evaluations must be computed, which severely limits the scalability of heuristic planners. In this paper, we present a novel solution for reducing node evaluations in heuristic planning based on machine learning. Particularly, we define the task of learning search control for heuristic planning as a relational classification task, and we use an off-the-shelf relational classification tool to address this learning task. Our relational classification task captures the preferred action to select in the different planning contexts of a specific planning domain. These planning contexts are defined by the set of helpful actions of the current state, the goals remaining to be achieved, and the static predicates of the planning task. This paper shows two methods for guiding the search of a heuristic planner with the learned classifiers. The first one consists of using the resulting classifier as an action policy. The second one consists of applying the classifier to generate lookahead states within a Best First Search algorithm. Experiments over a variety of domains reveal that our heuristic planner using the learned classifiers solves larger problems than state-of-the-art planners.",0,Human
"This research evaluates the top two options for mobile broadband in South Asia: WiMAX and LTE. It seeks to identify which of these technologies offers the most reliable and fast internet access for the region. To do this, the study looks at the current state of mobile broadband infrastructure in South Asia and identifies particular issues faced by the area, like high population density and varied terrain. Next, it thoroughly examines both WiMAX and LTE, exploring their technical specifications, deployment needs, and performance features. The research then compares these technologies based on coverage, capacity, lag time, and expenses. Ultimately, the paper recommends the optimal mobile broadband option for South Asia, considering the region's specific requirements. These insights can help policymakers, telecom companies, and others involved in the industry enhance mobile broadband availability in the region.",1,AI
"E-commerce platforms usually display a mixed list of ads and organic items in feed. One key problem is to allocate the limited slots in the feed to maximize the overall revenue as well as improve user experience, which requires a good model for user preference. Instead of modeling the influence of individual items on user behaviors, the arrangement signal models the influence of the arrangement of items and may lead to a better allocation strategy. However, most of previous strategies fail to model such a signal and therefore result in suboptimal performance. In addition, the percentage of ads exposed (PAE) is an important indicator in ads allocation. Excessive PAE hurts user experience while too low PAE reduces platform revenue. Therefore, how to constrain the PAE within a certain range while keeping personalized recommendation under the PAE constraint is a challenge.  In this paper, we propose Cross Deep Q Network (Cross DQN) to extract the crucial arrangement signal by crossing the embeddings of different items and modeling the crossed sequence by multi-channel attention. Besides, we propose an auxiliary loss for batch-level constraint on PAE to tackle the above-mentioned challenge. Our model results in higher revenue and better user experience than state-of-the-art baselines in offline experiments. Moreover, our model demonstrates a significant improvement in the online A/B test and has been fully deployed on Meituan feed to serve more than 300 millions of customers.",0,Human
"This study examines the connectivity characteristics of random annulus graphs and the geometric block model, both of which are frequently utilized in network science to represent real-world networks with inherent geometric features. Specifically, our research zeroes in on the critical point at which these networks transition from being disconnected to becoming interconnected. Initially, we delve into the random annulus graph, a type of graph formed by randomly linking points within an annulus based on their Euclidean distance. We prove that the critical point for connectivity in this graph depends on a straightforward function of the annulus's parameters. Subsequently, we look at the geometric block model, where a d-dimensional space is divided into regions (blocks), and points inside the same block are connected with a certain probability. We demonstrate that the critical point for connectivity in this model hinges on the product of block probabilities and the size of the blocks. Lastly, we compare the critical points for connectivity in both models, finding that the geometric block model consistently yields a higher critical point than the random annulus graph. This discovery holds significant implications for designing and analyzing networked systems with a geometric foundation, such as wireless sensor networks and social networks. Altogether, this work offers valuable insights into the connectivity properties of random graphs that possess a geometric basis.",1,AI
"Recent advances in deep convolutional neural networks (DCNNs) have shown impressive performance improvements on thermal to visible face synthesis and matching problems. However, current DCNN-based synthesis models do not perform well on thermal faces with large pose variations. In order to deal with this problem, heterogeneous face frontalization methods are needed in which a model takes a thermal profile face image and generates a frontal visible face. This is an extremely difficult problem due to the large domain as well as large pose discrepancies between the two modalities. Despite its applications in biometrics and surveillance, this problem is relatively unexplored in the literature. We propose a domain agnostic learning-based generative adversarial network (DAL-GAN) which can synthesize frontal views in the visible domain from thermal faces with pose variations. DAL-GAN consists of a generator with an auxiliary classifier and two discriminators which capture both local and global texture discriminations for better synthesis. A contrastive constraint is enforced in the latent space of the generator with the help of a dual-path training strategy, which improves the feature vector discrimination. Finally, a multi-purpose loss function is utilized to guide the network in synthesizing identity preserving cross-domain frontalization. Extensive experimental results demonstrate that DAL-GAN can generate better quality frontal views compared to the other baseline methods.",0,Human
"Recent results by Alagic and Russell have given some evidence that the Even-Mansour cipher may be secure against quantum adversaries with quantum queries, if considered over other groups than $(\mathbb{Z}/2)^n$. This prompts the question as to whether or not other classical schemes may be generalized to arbitrary groups and whether classical results still apply to those generalized schemes. In this thesis, we generalize the Even-Mansour cipher and the Feistel cipher. We show that Even and Mansour's original notions of secrecy are obtained on a one-key, group variant of the Even-Mansour cipher. We generalize the result by Kilian and Rogaway, that the Even-Mansour cipher is pseudorandom, to super pseudorandomness, also in the one-key, group case. Using a Slide Attack we match the bound found above. After generalizing the Feistel cipher to arbitrary groups we resolve an open problem of Patel, Ramzan, and Sundaram by showing that the 3-round Feistel cipher over an arbitrary group is not super pseudorandom. We generalize a result by Gentry and Ramzan showing that the Even-Mansour cipher can be implemented using the Feistel cipher as the public permutation. In this result, we also consider the one-key case over a group and generalize their bound. Finally, we consider Zhandry's result on quantum pseudorandom permutations, showing that his result may be generalized to hold for arbitrary groups. In this regard, we consider whether certain card shuffles may be generalized as well.",0,Human
"Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.",0,Human
"This study introduces a scalable shared-memory parallel algorithm for tackling large-scale linear programming tasks. The proposed technique leverages the inherent parallel processing capabilities of modern multi-core systems, utilizing optimized data structures and algorithms to enhance efficiency. It adopts a task-based parallelism approach, where tasks are allocated dynamically and executed across available processors. By decomposing the problem into manageable subtasks, each handled independently using the simplex method, the algorithm ensures effective utilization of computational resources. Load balancing strategies are employed to distribute workloads evenly among multiple processors, thereby maximizing performance. To assess its efficacy, extensive experimentation was conducted using diverse benchmark LP problems of varying sizes and complexities. Experimental outcomes indicate that the parallelized simplex algorithm demonstrates impressive scalability and outperforms its sequential counterpart significantly. Performance enhancements were further realized through sophisticated optimization methods like lazy updates and column-wise data organization. The proposed algorithm is anticipated to significantly influence the linear programming domain, particularly for large-scale optimization scenarios. Its versatility allows for easy adaptation to various types of LP problems, including mixed-integer programs, network flow challenges, and others. In summary, this research underscores the potential of leveraging shared-memory parallelism to address large-scale linear programming challenges, setting the stage for future advancements in this field.",1,AI
"This research introduces a new way to figure out theorems in implicational linear logic by using a declarative approach. Linear logic is a type of math logic that's particularly handy for simulating different computing systems because it can easily manage things like resource usage and management. However, coming up with theorems in linear logic can be tricky, especially for newcomers who aren't familiar with its rules and techniques. To tackle this issue, the researchers have come up with a declarative strategy for theorem derivation. Their method revolves around representing linear logic proofs at a high level and using algebraic structures. The core idea behind their technique is to employ a set of rewrite rules that capture the main features of linear logic proofs, then apply these rules methodically and automatically until the right theorem pops out. The study demonstrates that this method works well and efficiently for deriving theorems in implicational linear logic, and it suggests that it could be used in many different contexts and types of logic problems.",1,AI
"This study introduces an innovative method for face recognition utilizing transformed shape features. The proposed technique involves converting facial attributes into a novel shape representation, which serves as input for a recognition model. This conversion considers the typical variations found in facial characteristics, including alterations due to light, position, and expression, to generate a reliable depiction of the face. Various public datasets were tested, and the outcomes indicate that this proposed methodology surpasses current leading face recognition techniques in both accuracy and resilience. These findings suggest that transforming shape features holds potential for advancing future face recognition advancements.",1,AI
"This study introduces a new strategy for identifying anomalies through multi-criteria similarity measures and Pareto depth analysis. It tackles the shortcomings of conventional anomaly detection methods by evaluating data based on several criteria and examining Pareto depth scores. Pareto depth gauges how far a given data point is from the set of optimal alternatives, which represent the best compromises among various factors. By applying Pareto depth analysis, this method pinpoints anomalies that sharply diverge from these ideal balances and offers a clearer view of the trade-offs involved. The efficacy of the suggested technique has been tested using various real-life datasets, showing its outperformance compared to leading anomaly detection approaches currently available. This approach has broad applicability across different sectors, including finance, healthcare, and cybersecurity, where pinpointing anomalies is vital for safeguarding systems and maintaining security. Overall, the research enhances our understanding of anomaly detection and offers practical solutions applicable in numerous real-world scenarios.",1,AI
"This research introduces a new method for detecting cell-free massive multiple-input multiple-output (MIMO) systems utilizing distributed expectation propagation (DEP). Cell-free massive MIMO holds great promise for enhancing wireless communication network efficiency, but traditional detection techniques are often resource-intensive and complex. DEP addresses this issue by dividing the detection process into manageable chunks that can be solved concurrently by various processing nodes. The proposed technique is assessed via simulations and contrasted against existing best practices. Simulation outcomes indicate that DEP performs either equally well or surpasses other approaches in terms of both performance and efficiency—specifically in reducing computational demands and minimizing memory usage. Additionally, the distributed architecture allows the approach to expand seamlessly to larger networks without significant overhead. The study also explores how DEP might be applied beyond cell-free massive MIMO, including in distributed optimization and inference tasks. In summary, these findings suggest that DEP could significantly enhance the effectiveness and efficiency of cell-free massive MIMO detection and open up possibilities for its use in diverse distributed computing scenarios.",1,AI
"The security evaluation for Mail Distribution Systems focuses on certification and reliability of sensitive data between mail servers. The need to certify the information conveyed is a result of known weaknesses in the simple mail transfer protocol (SMTP). The most important consequence of these weaknesses is the possibility to mislead the recipient, which is achieved via spam (especially email spoofing). Email spoofing refers to alterations in the headers and/or the content of the message. Therefore, the authenticity of the message is compromised. Unfortunately, the broken link between certification and reliability of the information is unsolicited email (spam).  Unlike the current practice of estimating the cost of spam, which prompts organizations to purchase and maintain appropriate anti-spam software, our approach offers an alternative perspective of the economic and moral consequences of unsolicited mail. The financial data provided in this paper show that spam is a major contributor to the financial and production cost of an organization, necessitating further attention. Additionally, this paper highlights the importance and severity of the weaknesses of the SMTP protocol, which can be exploited even with the use of simple applications incorporated within most commonly used Operating Systems (e.g. Telnet).  As a consequence of these drawbacks Mail Distribution Systems need to be appropriate configured so as to provide the necessary security services to the users.",0,Human
"In recent years, deep neural network is widely used in machine learning. The multi-class classification problem is a class of important problem in machine learning. However, in order to solve those types of multi-class classification problems effectively, the required network size should have hyper-linear growth with respect to the number of classes. Therefore, it is infeasible to solve the multi-class classification problem using deep neural network when the number of classes are huge. This paper presents a method, so called Label Mapping (LM), to solve this problem by decomposing the original classification problem to several smaller sub-problems which are solvable theoretically. Our method is an ensemble method like error-correcting output codes (ECOC), but it allows base learners to be multi-class classifiers with different number of class labels. We propose two design principles for LM, one is to maximize the number of base classifier which can separate two different classes, and the other is to keep all base learners to be independent as possible in order to reduce the redundant information. Based on these principles, two different LM algorithms are derived using number theory and information theory. Since each base learner can be trained independently, it is easy to scale our method into a large scale training system. Experiments show that our proposed method outperforms the standard one-hot encoding and ECOC significantly in terms of accuracy and model complexity.",0,Human
"This research looks into how groups change over time in running races. By analyzing data from numerous races, we explore how group sizes and compositions shift throughout the event, along with the factors that lead to the creation and dissolution of these groups. We discover that groups usually form early on in the race, and their characteristics are shaped by elements like pacing, social interactions, and weather conditions. Moreover, we see that these groups tend to disintegrate as the race continues, with each runner settling on their own pace and strategy. These insights provide valuable understanding of group behaviors in endurance events and can inform the planning of race layouts and training regimens.",1,AI
"This paper investigates how to realize better and more efficient embedding learning to tackle the semi-supervised video object segmentation under challenging multi-object scenarios. The state-of-the-art methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times computing resources. To solve the problem, we propose an Associating Objects with Transformers (AOT) approach to match and decode multiple objects uniformly. In detail, AOT employs an identification mechanism to associate multiple targets into the same high-dimensional embedding space. Thus, we can simultaneously process multiple objects' matching and segmentation decoding as efficiently as processing a single object. For sufficiently modeling multi-object association, a Long Short-Term Transformer is designed for constructing hierarchical matching and propagation. We conduct extensive experiments on both multi-object and single-object benchmarks to examine AOT variant networks with different complexities. Particularly, our R50-AOT-L outperforms all the state-of-the-art competitors on three popular benchmarks, i.e., YouTube-VOS (84.1% J&F), DAVIS 2017 (84.9%), and DAVIS 2016 (91.1%), while keeping more than $3\times$ faster multi-object run-time. Meanwhile, our AOT-T can maintain real-time multi-object speed on the above benchmarks. Based on AOT, we ranked 1st in the 3rd Large-scale VOS Challenge.",0,Human
"This study offers a geometric viewpoint on sensitivity analysis for monomial models—a type of model frequently utilized across many scientific and engineering disciplines. Our technique reinterprets the sensitivity matrix as subsets of linear subspaces within the model parameter space. We reveal that the dimensions and orientations of these subspaces offer critical information about how sensitive the model output is to alterations in the input parameters. In essence, the sensitivity of the model is closely linked to the geometry of these subspaces and their interrelations. The findings present a fresh angle on sensitivity analysis for monomial models and establish a practical method for evaluating and visually assessing model sensitivity in complex, high-dimensional parameter spaces. To showcase our approach, we provide several numerical examples and contrast it with conventional methodologies.",1,AI
"This research examines how well wireless-powered cognitive radio networks (WPCRNs) perform in terms of data transfer speed, employing compressive sensing (CS) and matrix completion (MC) methods. It explores how surplus energy generated by primary users can be utilized to support secondary users in WPCRNs, and investigates the effect of various system variables like the quantity of both types of users and the compression ratios in CS and MC on the network's overall efficiency. Through simulations, the researchers assess the effectiveness of their proposed solution and contrast it against conventional approaches. The outcomes indicate that integrating CS and MC can substantially boost the data transfer rate in WPCRNs while also improving energy efficiency over conventional techniques. These insights could prove valuable for ongoing studies in WPCRNs and inform the development of advanced communication technologies.",1,AI
"This study explores how machine learning can enhance the efficiency of solving Boolean Satisfiability (SAT) problems. SAT is an extensively studied NP-complete issue applied in numerous areas such as hardware/software validation, scheduling, and cryptography. The paper introduces a new method that uses machine learning algorithms to assist SAT solvers during their search process. The findings indicate that the proposed technique markedly reduces the time needed to solve SAT instances when compared to current leading tools. Additionally, the method performs favorably against established benchmarks, proving its practical utility. This research offers valuable perspectives on integrating machine learning for tackling difficult computational challenges and may lead to future developments in this area.",1,AI
"This study explores how ""different strokes for different folks"" applies to pre-training strategies for varied dialogue tasks. Researchers looked at several pre-training techniques and measured their performance across different kinds of dialogue challenges. They found that there isn't a one-size-fits-all method, suggesting that different pre-training techniques work better depending on the type of dialogue task at hand. Ultimately, they recommend that when selecting a pre-training strategy, it's crucial to take into account the unique attributes of each particular task to get the best outcomes.",1,AI
"Several recent studies in privacy-preserving learning have considered the trade-off between utility or risk and the level of differential privacy guaranteed by mechanisms for statistical query processing. In this paper we study this trade-off in private Support Vector Machine (SVM) learning. We present two efficient mechanisms, one for the case of finite-dimensional feature mappings and one for potentially infinite-dimensional feature mappings with translation-invariant kernels. For the case of translation-invariant kernels, the proposed mechanism minimizes regularized empirical risk in a random Reproducing Kernel Hilbert Space whose kernel uniformly approximates the desired kernel with high probability. This technique, borrowed from large-scale learning, allows the mechanism to respond with a finite encoding of the classifier, even when the function class is of infinite VC dimension. Differential privacy is established using a proof technique from algorithmic stability. Utility--the mechanism's response function is pointwise epsilon-close to non-private SVM with probability 1-delta--is proven by appealing to the smoothness of regularized empirical risk minimization with respect to small perturbations to the feature mapping. We conclude with a lower bound on the optimal differential privacy of the SVM. This negative result states that for any delta, no mechanism can be simultaneously (epsilon,delta)-useful and beta-differentially private for small epsilon and small beta.",0,Human
"Background: The learning of genotype-phenotype associations and history of human disease by doing detailed and precise analysis of phenotypic abnormalities can be defined as deep phenotyping. To understand and detect this interaction between phenotype and genotype is a fundamental step when translating precision medicine to clinical practice. The recent advances in the field of machine learning is efficient to predict these interactions between abnormal human phenotypes and genes.  Methods: In this study, we developed a framework to predict links between human phenotype ontology (HPO) and genes. The annotation data from the heterogeneous knowledge resources i.e., orphanet, is used to parse human phenotype-gene associations. To generate the embeddings for the nodes (HPO & genes), an algorithm called node2vec was used. It performs node sampling on this graph based on random walks, then learns features over these sampled nodes to generate embeddings. These embeddings were used to perform the downstream task to predict the presence of the link between these nodes using 5 different supervised machine learning algorithms.  Results: The downstream link prediction task shows that the Gradient Boosting Decision Tree based model (LightGBM) achieved an optimal AUROC 0.904 and AUCPR 0.784. In addition, LightGBM achieved an optimal weighted F1 score of 0.87. Compared to the other 4 methods LightGBM is able to find more accurate interaction/link between human phenotype & gene pairs.",0,Human
"Adaptive motion planning is crucial for autonomous systems, especially in robotics. This study introduces an innovative motion planning technique employing artificial potential fields along with a pre-determined path. By integrating a potential field algorithm with a known trajectory, this approach addresses the shortcomings of conventional potential field methods, including issues like local minima and sluggish convergence rates. The proposed strategy has been validated through both simulation studies and practical testing on a real robotic setup, showing enhanced effectiveness over standard potential field techniques. These findings indicate that incorporating a prior path can significantly improve the efficiency of potential field-based motion planning algorithms, thereby making them more practical for real-world scenarios.",1,AI
