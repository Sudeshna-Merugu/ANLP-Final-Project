Original Text,Paraphrased Text
"This paper proposes a novel method for representative selection in big data using a combination of sparse graph and geodesic Grassmann manifold distance. The goal of representative selection is to identify a small subset of data points that capture the essential characteristics of the larger dataset, making it possible to analyze and draw conclusions from the data more efficiently. The proposed method first constructs a sparse graph from the data, where each data point is a node in the graph and the edges represent the pairwise similarity between data points. Then, it uses a graph clustering algorithm to identify clusters of highly similar data points. Next, it selects a representative from each cluster by computing the geodesic Grassmann manifold distance between each pair of data points and selecting the point that is closest to the mean of the cluster in the Grassmann manifold. The effectiveness of the proposed method is demonstrated through experiments on several real-world datasets. The results show that the method outperforms several existing representative selection methods in terms of both representativeness and efficiency. The proposed method can be used in a variety of applications, such as data summarization, visualization, and classification, where representative selection is a critical step in the analysis of large datasets.","This paper presents a new method for selecting representatives in large data sets that combines sparse graphs and geodesic distances on Grassmann manifolds. The aim is to pick a small set of representative samples that embody key features of a large dataset so that we can study and draw inferences more efficiently. First, this method creates a sparse graph based on data; nodes are data points and edges indicate similarities between pairs of points. After clustering high similarity points using graph clustering, representatives are chosen by calculating geodesic distances between pairs of data points and picking the one nearest to the cluster mean on the Grassmann manifold. Results on various real data sets show that this method outperforms other representative selection methods regarding both representativeness and efficiency. The method has broad applicability including summarizing data, visualizing data and classification tasks which all require careful selection of representative samples from large datasets."
"Thermodynamic-RAM (T-RAM) is a novel computational framework that leverages thermodynamic principles to perform calculations, and has shown promise in its ability to mimic the behavior of biological neurons. In this paper, we investigate the potential of T-RAM for simulating cortical processing in the brain. We first provide an overview of the architecture and operating principles of T-RAM, and compare it to other neuromorphic computing systems. We then describe our implementation of T-RAM for simulating cortical processing, using a simple model of the visual cortex as a test case. We demonstrate that T-RAM is able to replicate several key features of cortical processing, including selective attention, receptive fields, and ocular dominance. We also investigate the influence of various parameters on T-RAM performance, and discuss the implications of our findings for the development of more sophisticated T-RAM-based models of cortical processing. Our results suggest that T-RAM has the potential to be a useful tool for investigating the mechanisms of cortical processing, and may ultimately lead to the development of more efficient and biologically-inspired computational systems.","This paper investigates Thermodynamic RAM (T-RAM), a new computational framework that uses thermodynamics principles to perform calculations. It has been promising for modeling biological neurons. We here look at the potential of T-RAM to simulate cortical processing. First we introduce the architecture and working principles of T-RAM and compare them to other neuromorphic computing systems. Then we implement T-RAM for cortical processing simulation with a simple model of visual cortex as test subject. Results show that T RAM replicates important features of cortical processing such as selective attention, receptive fields and occlusion. We also study how different parameters affect performance and consider implications for developing more advanced models of cortical processing. Results suggest that T RAM could be a useful tool to study mechanisms of cortical processing and potentially lead to development of more efficient and inspired computational systems."
"This paper explores the non-deterministic self-assembly of two types of tiles on a lattice, which can lead to the creation of complex structures with emergent properties. The tiles are square-shaped and have four different types of edges, each of which can either be sticky or non-sticky. By placing the tiles on a lattice in a non-deterministic manner, meaning that the placement is not predetermined, the researchers observe the emergence of various structures, including aperiodic tilings and quasi-crystalline patterns. The study employs computer simulations and mathematical modeling to analyze the behavior of the tile assembly process and investigate the impact of different factors, such as tile concentration and temperature, on the resulting structures. The findings of this research demonstrate the potential of non-deterministic self-assembly for creating novel materials with unique properties, and contribute to the growing field of self-assembling systems.","This paper investigates how two kinds of tiles arranged randomly on a lattice create complex structures with emergent features. Tiles are squares with four different edge types which can be either adhesive or not. When tiles are placed randomly, they form different structures including aperiodic tilings and quasi crystals. Using both computer simulation and mathematical modeling, researchers study how parameters like concentration of tiles and temperature affect resultant structures. Results show promise for creating novel materials with special features through this kind of random assembly; contributions are made to the developing field of self assembling systems."
"The increasing demand for energy-efficient computing has led to the development of various techniques for virtual machine (VM) allocation in cloud computing environments. In this paper, we propose a new approach to minimize the total busy time of VMs while considering energy consumption as a crucial factor. We formulate the problem as an optimization problem and develop a heuristic algorithm that considers both the workload of the VMs and the energy consumption of the servers. The algorithm is designed to allocate VMs in a way that balances the energy consumption across the servers, while also minimizing the total busy time of VMs. We evaluate the performance of our approach using both real-world and synthetic datasets. Our results show that the proposed algorithm is highly effective in minimizing the total busy time of VMs while also significantly reducing energy consumption. The findings of this research will be of great significance in the design of energy-efficient cloud computing systems.","With the growing need for energy efficient computing, there have been many approaches developed for allocating virtual machines (VMs) in cloud computing environments. This paper introduces a new method to reduce overall busy time for VMs and considers energy use very important. We model this as an optimization problem and design a heuristic algorithm that takes into account the load of VMs and server energy use. This algorithm allocates VMs so that energy consumption is balanced among servers while also minimizing overall busy time. Performance of the proposed algorithm is assessed using both real data and synthetic data. Results demonstrate that the proposed algorithm is very effective at reducing busy time while also substantially lowering energy use. Findings from this work are important for designing energy efficient cloud systems."
"This paper explores the use of stacked multilayer perceptron (MLP) networks for learning dynamic classes of events. Dynamic classes of events refer to events that can change over time, such as changes in traffic patterns or weather conditions. The proposed approach involves training a series of MLPs to learn different stages of event dynamics, with the outputs of each MLP feeding into the next. This stacked approach allows for the system to capture the temporal dependencies of the events and improve prediction accuracy. The proposed method is evaluated on a dataset of traffic flow data, where the goal is to predict the traffic flow for the next hour. The results show that the stacked MLPs outperform traditional methods such as linear regression and support vector regression. Additionally, experiments are conducted to evaluate the sensitivity of the approach to different hyperparameters and architectural choices. Overall, this research provides a promising approach for learning dynamic classes of events, and the proposed stacked MLPs can be applied to a wide range of applications where the events are subject to changes over time.","This study looks at using stacked multilayer perceptrons (MLPs) to learn dynamic classes of events. Events here are those that vary over time such as traffic patterns or changes in weather. Proposed is an approach which trains a sequence of MLPs for learning different stages of event dynamics; the output of each layer feeds into the next. This stacking allows capturing dependency in time for events and thus improves accuracy of predictions. Performance is evaluated on a traffic flow dataset with goal to predict traffic flow for the following hour. Results indicate stacked MLPs perform better compared to traditional approaches like linear regression and support vector regression. Sensitivity to hyper parameters and architectural decisions is also tested experimentally. In sum, this work suggests a promising way to learn from changing events and stacked MLPs are applicable across many domains where events fluctuate over time."
"In this paper, the authors propose and evaluate a knowledge graph approach to investigating software usage in the social sciences. They begin by highlighting the importance of understanding the tools and methods used in social science research, and how a knowledge graph can provide a comprehensive and integrated view of software usage in this field. The authors then present the results of their investigation, which involves collecting and analyzing data from various sources, including academic publications, software repositories, and online forums. They demonstrate how the knowledge graph approach can reveal patterns and relationships among software, researchers, and research topics, and can help to identify opportunities for improvement and innovation in software usage in the social sciences. The authors conclude that a knowledge graph approach is a valuable tool for understanding software usage in this field and can provide a foundation for future research and development in this area.","This paper proposes and evaluates an approach to study software use using knowledge graphs and focuses on the social sciences. The authors emphasize that understanding tools and methods important for research and that such knowledge graphs offer a way to see the whole picture of software use. They show results from a study which collects and analyzes information from different sources such as academic papers, software repositories and discussion forums. They illustrate how this approach reveals connections between software, researchers and research topics and shows ways to improve software use. The conclusion is that using knowledge graphs is valuable for studying software use and they suggest that this approach can serve as a basis for future work and development."
"This paper presents a deep reinforcement learning (DRL) based approach for content caching in vehicular edge computing and networks (VECN) with permissioned blockchain technology. The DRL algorithm is used to determine the optimal caching strategy for each vehicle in the VECN by considering various factors such as available storage space, popularity of content, and network conditions. The permissioned blockchain is utilized to manage the distribution and sharing of cached content among vehicles and to ensure secure transactions among the participants in the VECN. The results of the simulation show that the proposed DRL-based caching approach outperforms traditional caching strategies in terms of hit rate and energy efficiency. The combination of DRL and permissioned blockchain technology provides a promising solution for content caching in VECN, which can significantly improve the overall performance of VECN systems.",This paper introduces an approach using deep reinforcement learning (DRL) for caching content in vehicular edge computing networks (VECNs) that use permissioned blockchains. The authors use DRL to select the best caching strategy for each vehicle by taking into account relevant factors like storage capacity and popularity of content along with network conditions. Permissioned blockchains are employed to handle distribution and sharing of cached content among vehicles and also to secure transactions among participants in VECNs. Results from simulations indicate that this DRL based caching approach performs better compared to traditional approaches in terms of hit rates and energy efficiency. Combining DRL with permissioned blockchain promises a promising solution for caching content in VECNs and substantially improves system performance.
"This paper provides a comprehensive survey of recent advancements in subpath queries on compressed graphs. It begins with a brief overview of the importance of graph compression and its applications in various domains. The paper then delves into the various methods of graph compression, including adjacency lists, adjacency matrices, and multilevel graph decomposition. The focus of the paper is on subpath query algorithms that operate on compressed graphs and their respective trade-offs in terms of time and space complexity. The survey covers both exact and approximate algorithms, as well as their performance evaluations on real-world and synthetic datasets. Finally, the paper concludes by discussing open challenges and future directions for research in this field.","This paper presents an extensive review of recent progress regarding subpath queries on compressed graphs. It starts with a short introduction about significance of graph compression and its use cases across different areas. The paper then discusses different methods of compressing graphs such as adjacency lists, matrices, and multilevel decompositions. The main focus here is on subquery algorithms that run on compressed graphs along with consideration of tradeoffs between time and space efficiency. Both exact and approximate algorithms are considered and performance evaluation results on both real world and synthetic data sets are also discussed. Finally, the paper ends by outlining current open questions and avenues for future research in this domain."
"This paper presents an empirical analysis of production machine learning pipelines and provides insights into optimization opportunities. The study evaluates the current state of production pipelines in industry and identifies common challenges and bottlenecks. Through a series of experiments and case studies, the authors demonstrate the impact of various optimization techniques on pipeline performance and efficiency. The results show that implementing a combination of hardware and software optimizations, such as parallel processing and model pruning, can significantly improve the speed and accuracy of machine learning pipelines. The paper concludes with practical recommendations for optimizing production pipelines and highlights the need for continued research in this area.","This paper examines empirical performance of machine learning pipelines used in industry and discusses ways to optimize them. It analyzes current status of pipelines and points out recurring difficulties and performance limitations. By performing experiments and case studies, authors illustrate how different optimization strategies affect pipeline performance and efficiency. Results show that combining hardware and software optimizations such as parallel processing and model pruning significantly enhance speed and quality of pipelines. Conclusions include specific recommendations for optimization and emphasize ongoing research necessary in this field."
This paper presents a comprehensive study of benchmarking feature-based algorithm selection systems for black-box numerical optimization. The study focuses on evaluating the performance of different algorithm selection methods based on their ability to identify the best algorithm for a given optimization problem. A set of benchmark problems and a suite of algorithms were used to evaluate the performance of the algorithm selection methods. The results show that feature-based algorithm selection systems can significantly improve the performance of optimization algorithms compared to using a single algorithm. The study provides insights into the strengths and weaknesses of different algorithm selection methods and highlights the importance of feature engineering in the algorithm selection process. The findings of this research can guide the development of future algorithm selection systems for black-box numerical optimization.,This work performs an overall evaluation of feature selection systems for selection of algorithms for black box numerical optimization problems. It considers performance comparison of different algorithm selection methods by their ability to select the best algorithm for each particular problem. Evaluation is performed using a set of benchmark problems and suites of algorithms. Results show that systems based on features do much better at improving performance compared to relying solely on one algorithm. Insights are also gained regarding comparative advantages and disadvantages of various selection methods and the significance of feature engineering. Findings from this research help direct further system development for optimization using black boxes.
"This paper presents FILIP, a novel fine-grained interactive language-image pre-training model for improving the performance of computer vision and natural language processing tasks. FILIP integrates a language model with an image recognition model and is trained on a large corpus of text and image data. The model is evaluated on several benchmark datasets and shows significant improvements over existing pre-training models, demonstrating the effectiveness of FILIP's fine-grained approach to language-image pre-training. The results suggest that FILIP has the potential to be a valuable tool for a wide range of applications in the field of AI.",This paper introduces FILIP as a new model that fine tunes interaction between language and images for better performance at tasks involving computer vision and natural language processing. FILIP combines a language model with an image recognition model and trains on a large corpus of both textual and visual data. Results show that when compared to previous pre training models FILIP performs much better. This study shows effectiveness of FILIP's approach to jointly training languages and images. Results also suggest that FILIP could prove to be a useful resource for many areas of AI research.
"In this paper, the authors aim to present a comprehensive study on modeling belief in dynamic systems. The paper is divided into two parts, with the first part focusing on the foundations of the topic. The authors begin by defining the concept of belief and its significance in the context of dynamic systems. They then provide a comprehensive review of the existing literature on belief modeling in dynamic systems and discuss the various approaches used in the field. The authors next present a framework for belief modeling in dynamic systems, which includes several key components such as belief representation, belief update, and belief evaluation. The framework also considers the role of uncertainty and the methods for its quantification in belief modeling. The authors then discuss the various challenges associated with belief modeling in dynamic systems and outline the future directions for research in this area. In conclusion, the first part of this paper provides a solid foundation for understanding belief modeling in dynamic systems. The authors have provided a comprehensive review of the existing literature and a framework for belief modeling that is useful for both researchers and practitioners in the field. This paper is an essential resource for anyone interested in the study of belief modeling in dynamic systems.","This paper aims to present a comprehensive study on modeling belief in dynamic systems. It consists of two parts; the first part lays out foundational elements of the subject matter. The authors start by defining the concept of belief and emphasize its significance within dynamic systems. They review existing literature comprehensively and examine different approaches within the field. Next they introduce a framework for modeling beliefs in dynamic systems which includes key elements such as representation of beliefs, updating beliefs, and evaluation of beliefs. This framework also addresses roles of uncertainty along with methods for measuring it. Authors then address challenges in modeling beliefs in dynamic systems and suggest future research directions. Concluding the first part, this paper provides a solid base for understanding belief modeling in systems that change over time. Authors provide a broad literature review and a useful framework for researchers and practitioners. This paper is a key resource for those who are interested in studying beliefs in dynamic systems."
"This paper investigates the optimality of universal Bayesian sequence prediction for a general loss function and alphabet. The study aims to determine the conditions under which the Bayesian prediction strategy is optimal and to provide a theoretical framework for the optimality proof. The paper first introduces the concept of Bayesian prediction and the principles behind it. Next, the authors define the general loss function and alphabet, and show how they can be used in the context of Bayesian prediction. The main results of the paper are then presented, including a theorem that provides sufficient conditions for the optimality of the Bayesian prediction strategy. The theorem is proven using a combination of mathematical analysis and simulation results. The paper concludes by discussing the implications of the results and pointing out avenues for future research.",This paper studies when Bayesian prediction strategies work well for any loss function and for any given set of symbols. It aims to find criteria for when these strategies perform optimally and to develop a theoretical basis for proving optimality. First the authors introduce Bayesian prediction and describe its basic ideas. Then they specify what kind of loss function and set of symbols to use and explain how to apply them in prediction. Results are then stated including a theorem specifying sufficient conditions for optimality of Bayesian prediction strategy. Proof uses both math analysis and simulation results. At last conclusions are drawn and directions for further research are highlighted.
"The paper presents a comprehensive and up-to-date overview of the state of social network research, with a specific focus on the issue of privacy in these sites. The paper analyzes the most recent trends and developments in this field, including the growing concern among users about the protection of their personal data and the increasing pressure on social network companies to address these concerns. The paper also highlights the key challenges and opportunities for future research in this area, including the need for a better understanding of how privacy is defined and managed in different social network contexts, and the potential for new technologies and approaches to improve privacy protection in these sites. The paper concludes with a call for further research and collaboration between academics, industry experts, and policymakers to address the complex and pressing issue of privacy in social network sites.","This paper gives a detailed review of recent work related to privacy issues in social networks; it highlights current trends and developments and focuses especially on user concerns about personal data security and high expectations from network operators regarding privacy protection. Key points for future research are also emphasized. These include clarifying what privacy means and how to manage it differently across various types of social networks, and exploring new technologies and methods to enhance privacy protection on such sites. The paper ends by calling for continued research and cooperation among scholars, practitioners, and policy makers to solve this complicated and important matter of privacy in social networking."
"This paper proposes a novel approach for indoor scene classification by combining a spatial layout representation and scale invariant feature representation. The spatial layout representation captures the global structure of an indoor scene, while the scale invariant feature representation extracts features that are robust to scale variations. The combination of these two representations results in a more robust and accurate scene classification system compared to existing methods. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed approach, achieving improved performance over state-of-the-art methods.",This paper introduces a new method for classifying scenes indoors using both spatial layout representation and scale invariant feature extraction. Spatial layout captures overall scene structure while scale invariant features extract features which resist variation in scale. By integrating both representations we obtain a more robust and accurate classifier. Experiments on various benchmark sets show effectiveness of this approach outperforming leading methods.
This paper presents a case study aimed at discovering co-location patterns between pollutants and child cancers. The study utilized a large dataset of pollutant levels and childhood cancer cases in a specific geographical area. The paper describes the methodology used to identify correlations between the two variables and the results of the analysis. The findings show that there is a significant co-location pattern between high levels of certain pollutants and a higher incidence of child cancers in the area studied. The results of this study contribute to the ongoing debate about the impact of environmental pollutants on human health and provide valuable information for policymakers and public health officials to consider when making decisions about environmental regulations.,This paper investigates patterns of co-location among pollutants and cancers in children through a case study. Data from specific geographic region includes measurements of pollution levels and incidence of childhood cancer. Results describe how to identify correlations between these two things. Results show that there is strong clustering of high levels of certain pollutants alongside high rates of cancer incidence. Results of this work contribute to the ongoing discussion of the relationship between environmental pollution and health and serve as important input for regulators and public health officials when deciding on regulatory actions regarding environment.
"Cyclus archetypes are a fundamental concept in the field of nuclear fuel cycle analysis. This paper provides a comprehensive review of the various cyclus archetypes that have been proposed and implemented in various studies. The paper begins by defining cyclus archetypes and explaining their importance in nuclear fuel cycle analysis. It then provides a comprehensive overview of the different archetypes that have been proposed and implemented, including the once-through, closed, and open archetypes. The paper also discusses the strengths and weaknesses of each archetype, as well as the limitations and assumptions that must be considered when using them. The paper concludes by discussing the implications of these findings for the future of nuclear fuel cycle analysis and the development of sustainable nuclear energy systems.","Cycling archetypes are essential concepts in analysis of nuclear fuel cycles. This study reviews all proposed and used archetypes comprehensively. The piece starts by defining what these archetypes are and explaining why they are important for analysis. Then an overview is given of the different archetypes that have been proposed and used including the once through, closed and open archetypes. Strengths and weaknesses of each archetype along with limitations and assumptions to consider are discussed as well. Finally, the results and implications for future research and development of sustainable nuclear energy systems are summarized."
"This paper presents a novel approach for identifying release sources in advection-diffusion systems by combining machine learning techniques with the Green Function Inverse Method. The proposed method utilizes supervised learning algorithms to estimate the unknown release sources from observed concentration data in the advection-diffusion system. The Green Function Inverse Method is then applied to validate the accuracy of the estimated sources and to reconstruct the unknown parameters in the system. The results show that the proposed approach outperforms traditional methods in terms of accuracy and computational efficiency, providing a promising tool for source identification in real-world advection-diffusion systems.",This paper introduces a new approach for identifying emission sources in advection diffusion systems by integrating learning algorithms with the Green Function Inverse Method. Using supervised learning we estimate unknown sources using measured concentrations. We apply this inverse method to verify accuracy and to reconstruct unknown parameters. Results indicate that this new method works better than previous methods regarding both accuracy and computational speed; this makes it an attractive new tool for identifying sources in practical advection diffusion systems.
"This paper investigates the extraction of belief dynamics, which refers to changes in the subjective probability of an event or a proposition over time. The study focuses on exploring methods for tracking and modeling belief dynamics in individual or group contexts. The methods are based on Bayesian belief networks and probabilistic graphical models, which are well-established frameworks for modeling uncertainty and belief update. The study evaluates the performance of these methods through simulations and empirical experiments, and provides insights into the key factors that affect the accuracy of belief dynamics extraction. The results of this study can contribute to the development of more sophisticated and effective models for monitoring and understanding belief dynamics in real-world applications, such as decision making, risk assessment, and information dissemination.","This paper studies changes in subjective probabilities of events or propositions over time; we call this belief dynamics. We focus on studying methods for tracking and modeling this change whether at the level of individuals or groups. Methods rely on Bayesian belief networks and probabilistic graphical models. These are established frameworks for modeling uncertainties and updating beliefs. Performance is evaluated by simulation and empirical testing and we also explore key factors influencing accuracy. Results from this work will improve models for real world tasks like decision making, risk assessment, and communication of information."
"This paper presents an analysis of spectral efficiency in the presence of correlated gamma-lognormal desired and interfering signals. The study focuses on evaluating the impact of signal correlation on the spectral efficiency of wireless communication systems. The results show that the spectral efficiency is significantly influenced by the correlation between the desired and interfering signals, which can lead to significant reductions in system performance. The paper develops a theoretical framework for characterizing the spectral efficiency under these conditions and provides insights into the trade-off between signal correlation and spectral efficiency. The findings of this study have important implications for the design and optimization of wireless communication systems and can be used to guide the development of more efficient and robust communication systems.",This paper looks at spectral efficiency when there are correlated desired and interfering signals that follow gamma log normal distribution. It focuses on understanding how correlation affects performance of wireless systems. Results show that performance declines sharply because of high correlation between desired and interfering signals. A theoretical basis is developed for characterizing efficiency under such conditions and insight is gained into balancing correlation against efficiency. This research has big importance for designing and optimizing communication systems and can serve as guidance for better design and robustness.
"This paper presents a geometric characterization of sensitivity analysis for monomial models, which are a class of mathematical models widely used in various fields of science and engineering. Our approach is based on a novel interpretation of the sensitivity matrix as a set of linear subspaces in the space of model parameters. We show that the dimension and orientation of these subspaces provide valuable insights into the sensitivity of the model output to changes in the input parameters. Specifically, we demonstrate that the sensitivity of the model is determined by the geometry of the subspaces and the degree of overlap between them. Our results provide a new perspective on sensitivity analysis for monomial models and offer a practical framework for analyzing and visualizing the sensitivity of these models in high-dimensional parameter spaces. We illustrate our approach with numerical examples and compare it with existing methods.","This paper characterizes sensitivity analysis for monomials through a geometric lens. Monomials are common in many disciplines such as science and engineering. Our approach interprets sensitivity matrix as sets of linear subspaces within the space of model parameters; these subspaces are shown to carry important information about how sensitive model outputs are to changes in input parameters. Specifically, dimensions and orientations of these subspaces are significant. Results indicate sensitivity depends on geometry of subspaces and their intersections. New results provide new ways to analyze and visualize sensitivities for this type of model in high dimensional spaces. Numerical examples illustrate the approach and compare favorably against previous methods."
"This paper investigates the problem of interference mitigation in Cloud Radio Access Networks (C-RANs) by using the rate-splitting technique combined with common message decoding. The rate-splitting technique is a promising approach to reduce interference and improve spectral efficiency in wireless networks, but its application in C-RANs has been limited due to the complexity of coordinating the transmission between the Baseband Unit (BBU) and Remote Radio Heads (RRHs). In this paper, we propose a joint design of rate-splitting and common message decoding at the BBU to mitigate interference in C-RANs. We develop a novel algorithm to optimize the rate-splitting parameters and the common message decoding coefficients to minimize the total transmission power while maintaining the desired Quality of Service (QoS) for all users. Simulation results show that our proposed method achieves significant performance gains compared to existing interference mitigation techniques, especially in dense and interference-limited scenarios. Our findings provide insights into the potential of rate-splitting and common message decoding for interference management in C-RANs and can inform the design of future wireless networks.",This paper studies interference reduction methods in Cloud Radio Access Networks (C RANs) combining Rate Splitting (RS) with Common Message Decoding (CMD). RS is an attractive way to reduce interference and increase spectral efficiency but applying it to C RANs has been challenging because coordination among Baseband Units (BBUs) and Remote Radio Heads (RRHs) is difficult. This paper proposes a joint design of RS and CMD at the BBU to mitigate interference in C RANs. We develop a new algorithm to optimize splitting parameters and CMD coefficients to minimize total transmitted power and ensure service quality for all users. Results from simulation show that this method performs significantly better than previous interference mitigation approaches especially under dense and interference prone conditions. Insights are gained about potential use of RS and CMD for interference management in C RANs and these findings could guide design of future wireless systems.
"This paper presents a new approach to compressing categorical features in machine learning models using submodular optimization. Categorical features are a common type of data in many real-world applications, but their high dimensionality can lead to significant computational overhead and storage requirements in machine learning models. The proposed method uses submodular optimization to select a subset of representative categories that capture the most important information in the original data. Specifically, we formulate the problem as a submodular maximization problem and propose a greedy algorithm to efficiently find a near-optimal solution. We also provide theoretical guarantees on the quality of the solution produced by our algorithm. Experiments on several benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of compression rate and accuracy compared to existing methods. The proposed method can be easily integrated into existing machine learning pipelines and has the potential to significantly improve the efficiency and scalability of machine learning models with categorical features.","This paper introduces a new way of compressing categorical data in machine learning models using submodular optimization. Many real world applications use this kind of data but their high dimensionality causes great computational burden and storage issues. We use optimization of submodular sets to choose a subset of representative categories which contain most important information from raw data. Specifically, we formulate this as a maximization problem using submodular functions and develop a greedy algorithm for efficient solution selection. We also establish theoretical guarantees for the quality of results generated by our algorithm. Empirical results show that this method outperforms previous methods both in terms of compression rate and accuracy on multiple benchmark datasets. Our approach is easy to incorporate into existing pipelines and thus promises to greatly enhance efficiency and scalability of models dealing with categorical data."
"This paper explores the concept of structured mappings for conferencing common information in multiple-access channels. Specifically, the paper investigates the potential benefits of using structured mappings to efficiently convey shared information among multiple users, while mitigating interference and increasing channel capacity. The study presents a theoretical framework for designing structured mappings that can optimally balance the trade-off between the amount of common information conveyed and the channel resources required. The research also proposes a practical algorithm for constructing structured mappings based on sparse-graph codes. Simulation results demonstrate that the proposed approach outperforms existing methods in terms of achievable rates and decoding complexity. The paper concludes that structured mappings can provide a promising solution for improving the efficiency and capacity of multiple-access channels, with potential applications in wireless networks, multimedia communications, and other domains.",This paper studies structured mapping as a way to efficiently share information among many users through shared communication channels. We look at how structured mappings reduce interference and increase channel capacity. We present a theoretical framework for designing structured mappings that optimize conveying common information against resource requirements. Proposed algorithm uses sparse graph codes for practical implementation. Results from simulations show this method performs better in terms of achievable rates and decoding complexity. Conclusions are that structured mappings offer a promising solution for improving efficiency and channel capacity and have potential application areas including wireless networks and multimedia communications.
"This paper presents a novel approach for designing a power-efficient artificial neuron using superconducting nanowires. The proposed neuron architecture consists of a superconducting loop with a nanowire junction that can produce a tunable response to input signals. The nanowire junction serves as the activation function for the neuron, and its low power consumption is achieved by using superconductivity to eliminate resistance in the loop. We have developed a detailed model of the proposed neuron and performed extensive simulations to demonstrate its performance characteristics. Our results show that the proposed neuron can achieve a high level of accuracy while consuming significantly less power than existing designs. We also present a design methodology for integrating multiple neurons into a larger network, and show that the proposed neuron architecture can be scaled up to create a high-performance, power-efficient neural network. Overall, this research presents a promising avenue for developing energy-efficient artificial neural networks, which can have a wide range of applications in areas such as machine learning, data analysis, and cognitive computing.","This paper introduces a new way of designing an artificial neuron that uses superconducting nanowires and reduces power consumption. Proposed neurons consist of loops with junctions made of nanowires that can react differently to different inputs. Using superconductivity, these junctions serve as activation functions and reduce resistance. We have developed a detailed model and performed many simulations to prove their performance features. Results indicate that proposed neurons perform very well and consume much less power compared to previous designs. We also detail a method for integration of many neurons into larger networks and show that the design can scale up to build powerful yet efficient networks. Overall this research opens an exciting path towards creating energy efficient artificial neural networks that are useful for tasks like machine learning and data analysis."
"In this paper, the focus is on the security of Internet of Things (IoT) edge nodes in the context of machine learning. The paper specifically explores the potential security attacks that can occur in IoT edge nodes, which are devices that are positioned at the edge of a network to collect and process data from IoT devices. With the increasing use of machine learning in IoT, there is a growing concern about the security of these systems, as machine learning algorithms are vulnerable to various types of attacks. The paper analyzes the various types of security attacks that can be launched against IoT edge nodes, including inference attacks, evasion attacks, and poisoning attacks. The authors also discuss the impact of these attacks on the performance of machine learning algorithms, and how they can be used to compromise the security of IoT networks. To address these security concerns, the paper proposes a number of countermeasures that can be used to defend against these attacks. These countermeasures include the use of secure communication protocols, the implementation of security mechanisms in machine learning algorithms, and the use of secure hardware and software systems. The authors also discuss the challenges that need to be overcome to effectively implement these countermeasures, and provide recommendations for future research in this area. Overall, this paper provides a comprehensive analysis of the security risks associated with IoT edge nodes in the context of machine learning, and provides practical solutions for defending against these risks. The findings and recommendations presented in this paper can be used by practitioners and researchers to improve the security of IoT networks and to ensure the safe and secure deployment of machine learning algorithms in IoT systems.","This paper focuses on security for edge nodes in IoT systems when considering machine learning. It specifically looks into security threats that can occur at edge nodes; these are devices placed close to a network to collect and process data from other IoT devices. With greater use of machine learning in IoT, security has become an increasing concern because machine learning algorithms are susceptible to different kinds of attacks. In this paper we analyze various kinds of attack vectors targeting these edge nodes like inference attacks, evasion attacks and poisoning attacks. We also consider what impact these attacks have on performance of ML algorithms and how they can undermine security of IoT networks. We propose several measures for defense against these attacks such as using secure communication protocols, implementing security features within ML algorithms and using secure hardware and software systems. Authors also highlight challenges for effective implementation of these countermeasures and give recommendations for future research in this field. Overall this paper offers a detailed analysis of security issues related to ML and IoT edge nodes and suggests practical means of protection. Results and suggestions in this work can help practitioners and researchers improve security of IoT networks and safe deployment of ML algorithms in IoT systems."
"This paper explores the potential impact of autonomous systems and artificial intelligence on the stability of the nuclear industry. It analyzes the role that these technologies could play in enhancing safety and security, as well as reducing the risk of accidents and incidents. The paper also evaluates the potential challenges associated with the integration of these technologies, including ethical considerations and cybersecurity risks. The findings suggest that the use of autonomous systems and artificial intelligence has the potential to revolutionize the nuclear industry and contribute to a stable nuclear future. However, careful planning and implementation strategies must be put in place to ensure that these technologies are integrated in a responsible and safe manner. This paper highlights the importance of continued collaboration between nuclear experts and technology experts to realize the full potential of these cutting-edge technologies in the pursuit of a stable nuclear future.","This paper looks at how autonomous systems and AI can affect stability in the nuclear sector. It investigates what role such tech might play in making things safer and securing facilities better by reducing risk of accidents and events. Also, this study considers challenges related to integrating this technology; this includes ethical issues and cyber security risks. Results indicate that using autonomous systems and AI could fundamentally change the nuclear sector and contribute to a secure future. Yet careful planning and strategy implementation is essential to ensure these technologies are used responsibly and safely. Collaboration among specialists in nuclear matters and technology is key to realizing these high tech innovations for a secure future."
"This paper presents a method for efficient sampling in polynomial chaos-based uncertainty quantification and sensitivity analysis. The proposed method uses weighted approximate Fekete points, which are known for their ability to provide a well-distributed set of points for polynomial interpolation. By utilizing these points, the authors show that the proposed method can achieve high accuracy with a significantly reduced number of samples compared to traditional Monte Carlo methods. The performance of the proposed method is evaluated through several numerical experiments, and the results demonstrate that it is a promising approach for efficient sampling in polynomial chaos-based uncertainty quantification and sensitivity analysis.",This paper introduces an approach for efficient sampling using polynomial chaos for quantifying uncertainties and analyzing sensitivities. Authors use weighted approximate Fekete points which are known for their strong performance at producing good distributions of sampling points for interpolation of polynomials. They show that this new method achieves high accuracy using much fewer samples than traditional Monte Carlo methods. Performance is tested by means of several numerical experiments; results indicate this new method performs well as an efficient sampling strategy for quantifying uncertainties and sensitivity analysis using polynomial chaos.
"This paper presents a novel approach to enhance the security and robustness of unmanned aerial vehicle (UAV) communication systems by utilizing reconfigurable intelligent surfaces (RISs). The paper first provides an overview of current challenges in UAV communication, including signal interference, jamming attacks, and weak wireless signal coverage. It then introduces the concept of RISs as a promising solution to these challenges. The authors propose a secure and robust UAV communication system that employs RISs to dynamically adjust the wireless channel conditions in real-time. The performance of the proposed system is evaluated through simulations and experiments, which demonstrate significant improvements in terms of signal quality, transmission reliability, and security compared to traditional UAV communication systems. The results of this study provide new insights into the potential of RISs as a key technology for secure and reliable UAV communication.","This paper introduces a new method to improve security and robustness of communication systems for unmanned aerial vehicles (UAVs) by using reconfigurable intelligent surfaces (RISs). The introduction begins by highlighting current issues such as interference, jamming attacks, and poor reception of wireless signals. Then, it describes how RISs are promising solutions to those problems. Authors suggest a secure and robust system for UAV communications that uses RISs to modify wireless channel conditions in real time. Performance evaluation of the proposed system uses both simulation and experiment data; this shows notable improvements such as better signal quality and higher reliability along with enhanced security compared to typical UAV communication systems. Results of this research offer fresh perspectives on the significance of RISs as a core technology for secure and reliable UAV communication."
"This paper explores the hardness and approximation of the probabilistic p-center problem under pressure. The p-center problem is a well-known problem in operations research and computer science, which involves finding the optimal location for p facilities in a network such that the maximum distance from any point to its closest facility is minimized. The probabilistic p-center problem extends this concept by incorporating a probability distribution on the demand of each point in the network. The pressure aspect of the problem refers to the presence of constraints or external factors that affect the decision-making process. The paper starts by introducing the probabilistic p-center problem and its variants, including the classical and the pressure-sensitive versions. It then discusses the computational complexity of the problem, proving its NP-hardness through reduction from well-known NP-hard problems. The paper also presents approximation algorithms for the probabilistic p-center problem under pressure, analyzing their performance and providing numerical results to illustrate their effectiveness. Finally, the paper concludes by summarizing the contributions of the research and discussing possible directions for future work. The results of this research provide valuable insights into the hardness and approximation of the probabilistic p-center problem under pressure, and can serve as a useful reference for researchers and practitioners in the field of operations research and computer science.","This paper investigates hardness and approximation of probabilistic p center problem under stress. Probabilistic p center is a familiar problem in OR and CS which seeks to find the best locations for p facilities in a network so that maximum distance to nearest facility is reduced. Stress here refers to conditions or external forces affecting decisions. This study first introduces probabilistic p center problem and some related variants like classic and stress sensitive ones. Next, it analyzes computational complexity and proves NP hardness via reduction from well known NP hard problems. Algorithms are presented for approximation of this problem under stress along with performance analysis and numerical results showing effectiveness. Results conclude the paper and suggest further research directions. Overall findings shed light on hardness and approximations of this problem under stress and serve as valuable references for researchers and practitioners in OR and CS."
"This paper presents a study on the joint constellation design for Non-Coherent Multiple-Input Multiple-Output (MIMO) Multiple-Access (MA) channels. The objective of the study is to optimize the design of the constellations used for transmitting data in Non-Coherent MIMO MA channels, with the aim of improving the overall system performance. The study proposes a novel joint constellation design approach that takes into account both the channel state information and the user pairing information. The performance of the proposed approach is evaluated through simulations and compared with existing methods. The results show that the proposed approach leads to significant improvement in terms of Bit Error Rate (BER) performance, especially in the high Signal-to-Noise Ratio (SNR) regime. The study concludes that the proposed joint constellation design is a promising solution for Non-Coherent MIMO MA channels and can be applied in various practical communication systems.",This paper investigates constellation design optimization for non coherent multiple input multiple output channels and multiple access channels. Objective is to improve performance of data transmission by optimizing constellation designs for use on such channels. A new approach for joint constellation design is introduced which considers both channel state information and pairing information of users. Performance evaluation is carried out using simulation studies and compared against other approaches. Results indicate that this new method significantly improves Bit Error Rate performance especially at high signal to noise ratio levels. The conclusion is that this new design for joint constellation is promising and useful for practical communications systems as well.
"The symbolic aggregate approximation (SAX) method is widely used in time series analysis due to its efficiency and ability to capture patterns in the data. However, its limitations in capturing segment trend information have been widely recognized. In this paper, we propose a modified SAX method that incorporates segment trend information in order to improve its performance in capturing patterns in time series data. Through extensive experimentation and evaluation, we demonstrate that the proposed method outperforms the traditional SAX method in capturing both global and local trends in time series data. The results of this research contribute to the development of more effective time series analysis techniques and have important implications for various real-world applications.","Symbolic Aggregate Approximation (SAX) has been popular for time series analysis because of its effectiveness and pattern recognition capabilities. However, there are well known limitations when it comes to recognizing segments of trend. This paper proposes modifications to SAX to incorporate such segment trend information to improve performance. Through rigorous testing and evaluation, we show that this modification outperforms traditional SAX at recognizing both large and small trends in time series data. Results from this work contribute to further refinement of high performing time series analysis methods and are significant for practical applications in reality."
"This paper proposes a novel approach to crowd counting using multi-resolution fusion and multi-scale input priors. The proposed method leverages the strengths of both multi-resolution fusion and multi-scale input priors to accurately estimate the crowd count in a given scene. The approach involves capturing the scene at different resolutions and fusing the information to generate a high-resolution image. Then, multi-scale input priors are used to refine the crowd count estimation. The proposed method is evaluated on several publicly available datasets, and the results demonstrate that it outperforms state-of-the-art crowd counting methods. This approach has the potential to significantly improve crowd counting in real-world applications, such as crowd management and safety monitoring.",This paper introduces a new way of counting crowds by combining multi resolution fusion and priors at multiple scales. The new method makes use of both benefits of these approaches to accurately estimate the number of people present in an environment. The method works by taking scenes at various resolutions and fusing data together to produce high resolution images. Then priors at different scales refine the estimation of crowd counts. Performance of this new method has been tested on some public datasets and results show that it surpasses leading crowd counting methods. This new approach could greatly improve performance for real world tasks like crowd management and safety supervision.
"This paper proposes a deep graph metric learning approach for improving the efficiency and effectiveness of proxy-based object recognition systems. Traditional proxy-based approaches rely on a large number of proxy examples to accurately represent object classes, which can lead to high computational costs and low scalability. In contrast, our proposed approach, ""Fewer is More,"" learns a compact representation of proxy examples using a deep graph metric learning model. By leveraging the relationships among proxies, our approach can effectively learn a low-dimensional feature space that captures the discriminative information of each object class using fewer proxies. Our experiments on several benchmark datasets demonstrate that our approach outperforms traditional proxy-based approaches in terms of accuracy and efficiency, while using significantly fewer proxies. Our work presents a promising direction for developing more efficient and scalable object recognition systems.","This paper introduces a method of deep learning for better performance of systems that use proxy examples for object recognition. Usually such systems require a lot of proxy samples; this leads to high costs in computation and low scalability. However, we propose an approach called ""Less but Better"" which learns compact representations of proxy samples using deep models of graphs. Using relationships among proxies, this approach learns a lower dimensional feature space capturing discriminative information for each object category using fewer proxies. Results on different benchmarks show that this new approach excels at both accuracy and efficiency compared to previous methods that use many proxies. Overall, this work offers promising directions towards more efficient and scalable systems for object recognition."
"This paper explores the problem of unsupervised domain adaptation in the context of bi-classifier determinacy maximization. The goal of unsupervised domain adaptation is to train a model on a source domain, where labeled data is available, and generalize it to a target domain, where no labeled data is available. Bi-classifier determinacy maximization is a method that leverages the decision boundaries of two classifiers trained on the source and target domains, respectively, to improve the generalization performance of the model. The proposed approach aims to learn a decision boundary that maximizes the agreement between the two classifiers, which effectively reduces the discrepancy between the source and target domains. The paper presents experimental results on various datasets to demonstrate the effectiveness of the proposed approach in improving the generalization performance of the model, compared to other state-of-the-art methods. The paper concludes with a discussion of the potential applications of bi-classifier determinacy maximization in various domains, such as computer vision and natural language processing.",This paper focuses on exploring the unsupervised domain adaptation problem with respect to maximizing the determinacy of binary classifiers. The aim of unsupervised domain adaptation is to learn a model using labeled data from a source domain and apply this model to a target domain where no labels are available. Determinacy maximization using binary classifiers is a method that uses decision boundaries learned from training classifiers separately on source and target domains to improve performance. An approach is introduced here that learns a boundary that maximizes agreement between classifiers. This reduces discrepancy between source and target domains. Results are presented for different datasets to show effectiveness of the proposed approach relative to leading methods. Concluding remarks discuss possible applications including areas like computer vision and NLP.
"This paper presents a novel motion planning algorithm called e$ \mathbf{^3} $mop, which combines heuristic-guided motion primitives pruning and path optimization with a sparse-banded structure. The algorithm is designed to efficiently solve high-dimensional motion planning problems with complex constraints, such as those encountered in robotics and autonomous systems. The e$ \mathbf{^3} $mop algorithm first generates a set of motion primitives using heuristic guidance to reduce the search space. The set of primitives is then pruned based on a novel cost function that takes into account both the cost of the motion primitive and the expected cost of the remaining path. This pruning step significantly reduces the number of primitives that need to be considered, leading to a significant reduction in computation time. The remaining primitives are then used to construct a sparse-banded graph that captures the connectivity between them. This graph is then optimized using a novel path optimization technique that takes advantage of the sparse-banded structure to efficiently compute the optimal path. Experimental results show that e$ \mathbf{^3} $mop outperforms state-of-the-art motion planning algorithms in terms of both efficiency and solution quality. The algorithm is also shown to be highly scalable, making it well-suited for use in large-scale applications. Overall, the e$ \mathbf{^3} $mop algorithm represents a significant advance in the field of motion planning, with the potential to have a major impact on a wide range of applications in robotics, autonomous systems, and beyond.","This paper introduces a new motion planning algorithm called e$ \mathbf{^3} $mop. This algorithm integrates heuristically guided motion primitives pruning and path optimization and has sparse band structure. This algorithm tackles high dimensional planning problems with complex constraints such as those found in robotics and autonomous systems. e$ \mathbf{^3} $mop generates a set of primitives using heuristic guidance to reduce search space; this set of primitives is then pruned by means of a new cost function that considers both costs associated with the primitive and expected future costs. This pruning step greatly reduces the number of primitives to consider, thus substantially reducing computational time. Remaining primitives are then used to build a sparse band graph capturing connectivity among them. Using a new path optimization technique that exploits the sparse band structure, this graph is optimized. Results from experiments show that e$ \mathbf{^3} $mop excels in efficiency and quality compared to current best algorithms. Scalability is also demonstrated, rendering it suitable for large scale applications. In summary, e$ \mathbf{^3} $mop marks an important advancement in motion planning, with great potential impact across robotics and other domains."
"This paper proposes a hybrid inference system for improved curvature estimation in the level-set method using machine learning. The level-set method is a widely used technique for shape modeling and image segmentation in various fields such as computer vision, medical imaging, and graphics. The method relies on the accurate estimation of the curvature of the evolving interface, which is essential for the accurate propagation of the level-set function. However, traditional methods for curvature estimation suffer from limitations such as sensitivity to noise and uneven sampling. To overcome these limitations, we propose a hybrid inference system that combines the traditional curvature estimation method with a machine learning-based approach. The machine learning model is trained on a large dataset of synthetic and real-world images, and it learns to predict the curvature at each point on the evolving interface based on the local image features. The hybrid system combines the predictions from the machine learning model with the results from the traditional curvature estimation method, resulting in more accurate and robust curvature estimates. We evaluate the proposed hybrid inference system on various datasets and compare its performance with traditional curvature estimation methods. The results show that the hybrid system outperforms the traditional methods in terms of accuracy and robustness. Moreover, the proposed system is computationally efficient and can be easily integrated into existing level-set algorithms. Overall, our research provides a promising direction for improving the accuracy and robustness of the level-set method through the use of machine learning-based approaches. The proposed hybrid inference system can also be extended to other shape modeling and segmentation techniques that rely on curvature estimation.","This paper suggests an integration of hybrid inference systems for better estimation of curvature using machine learning. Level set method is very common for modeling shapes and segmentation in computer vision, medicine and graphics. This method relies heavily on accurate estimation of curvature of evolving interfaces and this is critical for proper propagation of level set functions. Traditional methods for curvature estimation have some drawbacks like sensitivity to noise and uneven sampling. Therefore we propose a system that combines traditional methods with a learning model based on machine learning. This model has been trained using large datasets of both synthetically generated and real images. It uses local image features to predict curvature at every point on the evolving boundary. This hybrid system integrates predictions from the machine learning model along with results from traditional curvature estimation and produces more precise and reliable estimates of curvature. Results are evaluated on different data sets and compared against traditional methods. Results show that this hybrid system performs better than traditional methods regarding precision and reliability. Also this proposed system runs efficiently and is easy to integrate into current level set algorithms. Our research introduces a promising path towards improving both accuracy and reliability of level set methods using machine learning based approaches. System integration into other shape modeling and segmentation tasks that depend on curvature estimation could also be possible."
"This paper proposes a novel approach to unsupervised stereo depth estimation using H-Net, an attention-based deep neural network, and epipolar geometry. Depth estimation from stereo images is an important task in computer vision, with applications in robotics, autonomous driving, and 3D reconstruction. The proposed method employs an attention mechanism that learns to focus on informative image regions for depth estimation. In addition, epipolar geometry is utilized to enforce geometric constraints on the predicted depths. The experimental results demonstrate that the proposed approach outperforms state-of-the-art unsupervised methods on benchmark datasets. The proposed approach can also be easily adapted to handle other related tasks, such as optical flow estimation and image super-resolution. The results suggest that H-Net with epipolar geometry is a promising approach for unsupervised depth estimation, with potential for a wide range of applications in computer vision.","This study introduces a new way to estimate depth without supervision using H-Net, which is an attention based deep neural network and uses epipolar geometry. Depth estimation from pairs of stereo images is very important in computer vision and has applications in robotics and driving autonomously and for 3D reconstruction. This work uses an attention mechanism to learn to focus on important image areas for depth estimation. Epipolar geometry is used to impose geometric constraints on estimated depth. Experimental results show that this method performs better than leading methods on benchmark datasets. Moreover, this method can be straightforwardly adapted to tasks like optical flow estimation and super resolution too. Results indicate that H Net using epipolar geometry promises good performance for unsupervised depth estimation and could have broad application in computer vision."
"This paper proposes SCSGuard, a novel deep learning-based approach for detecting scams in Ethereum smart contracts. Scams are a significant problem in the Ethereum ecosystem, as they can cause significant financial losses for users. While existing approaches for detecting scams in smart contracts rely on rule-based methods or static analysis, SCSGuard uses a deep learning model to detect anomalous patterns in the contract's bytecode. To evaluate the effectiveness of SCSGuard, we collect a large dataset of known scams and benign contracts and train the model using a supervised learning approach. We then compare the performance of SCSGuard against several state-of-the-art methods for detecting scams in smart contracts. Our results show that SCSGuard achieves higher accuracy and recall than existing approaches, indicating that it can effectively detect scams in real-world smart contracts. We also analyze the factors that contribute to the success of SCSGuard, including the choice of neural network architecture, the size and quality of the training dataset, and the use of transfer learning to adapt the model to new types of scams. Additionally, we discuss the limitations of SCSGuard and identify directions for future research, such as improving the interpretability of the model and exploring the use of adversarial examples to evaluate its robustness. Overall, our research demonstrates the potential of deep learning approaches for detecting scams in Ethereum smart contracts and provides insights into the design and evaluation of such systems. We believe that SCSGuard has significant implications for improving the security and trustworthiness of the Ethereum ecosystem, and we encourage further research in this area.","This paper introduces SCSGuard as a novel deep learning approach for identifying frauds in Ethereum smart contracts. Fraud is an important issue because it can result in huge financial loss for users. Previous methods for fraud detection rely either on rule sets or static analysis. In contrast, SCSGuard employs deep learning to identify unusual features in contract bytecode. For evaluation, we collect a large dataset of known frauds and normal contracts and train the model via supervised learning. Performance comparison against leading methods shows that SCSGuard outperforms them both in accuracy and recall. Factors contributing to its success include selection of neural network architecture, quality and size of the training data, and adaptation through transfer learning to different types of frauds. We also point out limitations and suggest future work such as enhancing model interpretability and studying adversarial examples to test robustness. Overall results indicate great promise for using deep learning for contract fraud detection and offer insights into system design and evaluation. Results also have implications for strengthening security and trustworthiness in the Ethereum ecosystem and we hope that further research will follow along these lines."
"This paper presents an empirical comparison of off-policy prediction learning algorithms for the collision task in a simulated environment. The collision task involves predicting the likelihood of collision between two agents in a two-dimensional grid world. We compare four different off-policy prediction learning algorithms: Q-learning, SARSA, Expected SARSA, and Double Q-learning. We conduct experiments on a range of hyperparameters and evaluate the performance of each algorithm using metrics such as mean squared error and average reward. Our results show that Expected SARSA outperforms the other algorithms in terms of mean squared error and average reward, while Double Q-learning performs the worst. We also observe that the performance of each algorithm is highly dependent on the hyperparameters used, and that some algorithms are more sensitive to hyperparameter settings than others. Overall, this study provides a valuable empirical comparison of off-policy prediction learning algorithms for the collision task, and provides insights into the strengths and weaknesses of each algorithm. These insights can be useful for researchers and practitioners working on similar prediction tasks, and can help inform the selection of appropriate algorithms for different contexts.","This paper compares empirically the performance of four learning algorithms for collision prediction in a simulation environment: Q learning, SARSA, expected SARSA, and double Q learning. Collision prediction entails predicting the probability of collision between two agents in a two dimensional grid world. Experiments are carried out across a range of hyperparameters and performance metrics like mean squared error and average reward are evaluated for each algorithm. Results indicate that expected SARSA outperforms other methods both in terms of mean squared error and average reward whereas double Q learning performs worst. We also notice that performance of these algorithms varies strongly with hyperparameters and sensitivity to hyperparameters differs among them. Overall this study makes valuable comparisons of learning algorithms for collision prediction tasks; it reveals relative strengths and weaknesses of different algorithms which are useful for both researchers and practitioners working on related prediction tasks and helps select suitable algorithms according to different contexts."
"In recent years, self-supervised learning has gained significant attention as a powerful method to tackle computer vision problems in an unsupervised manner. This paper presents a novel approach to self-supervised tracking via target-aware data synthesis. The proposed method involves synthesizing realistic training data by combining images of the target object with diverse backgrounds. The synthesized data is used to train a deep neural network that can accurately track the target object in real-time videos. The network learns to distinguish the target object from its surroundings based on its appearance, motion, and context information. The effectiveness of the proposed method is evaluated on several benchmark datasets and compared with state-of-the-art self-supervised tracking methods. The results show that the proposed method achieves competitive performance and outperforms existing methods on several challenging scenarios. This work demonstrates the potential of self-supervised tracking via target-aware data synthesis as a promising direction for unsupervised visual object tracking.","Recently self supervised learning has gained strong attention as a powerful approach to deal with computer vision tasks without supervision. In this paper we introduce a new way to do target tracking using synthesis of data aware of targets. We combine target object images with various backgrounds to synthesize realistic training data and use this data to train deep networks to track targets in real time video. The trained network learns to discriminate target from other elements by using appearance, motion and contextual information. Results are evaluated against different benchmark datasets and compared to current top methods. Results show that our method performs competitively and excels on difficult situations compared to previous ones. This work suggests that synthesizing data aware of targets is a promising path towards unsupervised visual tracking."
"This paper explores the use of the Video Multimethod Assessment Fusion (VMAF) and Entropic Differences (ED) metrics for high frame rate (HFR) video quality assessment. With the increasing popularity of HFR video, it is essential to have reliable methods for evaluating the visual quality of these videos. The VMAF and ED metrics have been extensively used in video quality assessment for traditional frame rates, but their performance for HFR video has not been thoroughly studied. To investigate the suitability of these metrics for HFR video, the researchers conducted subjective tests with 30 participants who evaluated HFR video sequences using the Absolute Category Rating (ACR) methodology. They then compared the subjective scores to the objective scores obtained from the VMAF and ED metrics. The study used a diverse set of HFR videos with different genres and motion characteristics to ensure the reliability of the findings. The results of the study demonstrate that both the VMAF and ED metrics are reliable for HFR video quality assessment, as they show a high correlation with the subjective scores obtained from the ACR methodology. The study also found that the VMAF metric performed slightly better than the ED metric, but the difference was not significant. This research contributes to the development of objective methods for evaluating the visual quality of HFR videos, which can be useful for various applications such as video compression and streaming. The findings of this study can also guide the development of new quality assessment algorithms that are specifically designed for HFR video.","This paper investigates use of Video Multimethod Assessment Fusion (VMAF) and Entropy Differences (ED) metrics for quality assessment of high frame rate (HFR) videos. As HFR videos become increasingly popular, it becomes important to develop reliable metrics to assess visual quality of such videos. VMAF and ED have been widely used in assessing quality at regular frame rates, but their suitability for HFR videos has not been well studied yet. Researchers conduct subjective tests using 30 participants with ACR method to evaluate HFR video sequences and compare subjective scores to objective scores obtained from VMAF and ED metrics. Results of diverse HFR videos with different genres and motions are analyzed to assure reliability of findings. Results show that both VMAF and ED metrics correlate well with subjective scores obtained through ACR. VMAF performs slightly better than ED but significance difference is not notable. Results contribute to development of metrics for objectively assessing visual quality of HFR videos and suggest promising applications like video compression and streaming. Results also provide guidance for development of new quality assessment algorithms specifically designed for HFR videos."
"This paper presents a novel approach to enhance the security of Industry 4.0 systems using Quorum-Based Cloud-Edge Distributed Deep Neural Networks (QUDOS). In recent years, the increasing popularity of cloud-based applications and services in Industry 4.0 has led to a growing demand for secure and efficient data processing. However, existing security solutions for these systems are often inadequate in addressing the complex threat landscape of today's industry. To address these challenges, the authors propose QUDOS, a quorum-based framework for securely executing deep neural networks (DNNs) in a cloud-edge computing environment. QUDOS leverages the strengths of both cloud and edge computing to provide a secure and efficient solution for data processing in Industry 4.0. It employs a quorum-based consensus mechanism to ensure the secure and decentralized execution of DNNs across multiple edge devices and the cloud. The proposed framework enables secure data processing by distributing the execution of DNNs across multiple devices, reducing the risk of single-point failures and data breaches. Additionally, QUDOS uses secure multi-party computation techniques to ensure the privacy and confidentiality of sensitive data during the DNN execution process. The authors evaluate the performance and security of QUDOS through a series of experiments and simulations, demonstrating its ability to achieve high accuracy and low latency in DNN execution. They also show that QUDOS outperforms state-of-the-art security solutions in terms of both accuracy and latency, making it an attractive solution for secure and efficient data processing in Industry 4.0 systems. In conclusion, the authors present QUDOS as a promising solution for enhancing the security of Industry 4.0 systems, providing a secure and efficient framework for executing DNNs in a cloud-edge computing environment.","This paper introduces a new way to improve security for Industry 4.0 systems using QUDOS, a system based on cloud and edge computing. With the growing use of cloud applications and services, there is now increased demand for secure and fast processing of data. Yet current security solutions have not sufficiently addressed the complicated nature of threats faced today by industry. Therefore, the authors introduce QUDOS—a framework that executes deep neural networks securely. QUDOS leverages benefits from both cloud and edge computing to offer a strong solution for data processing in Industry 4.0. Using a consensus method based on quorums ensures secure and decentralized execution of DNNs across various edge devices and cloud servers. Secure multiparty computation methods are used to protect sensitive data during processing. Performance and security are evaluated through experiments and simulations; results demonstrate high accuracy and low latency. Results also show QUDOS excels compared to leading security solutions. In summary, QUDOS is presented as a promising enhancement for security in Industry 4.0 and offers a strong framework for executing DNNs in cloud and edge computing environments."
"This paper proposes a self-supervised autoregressive domain adaptation approach for time series data, aimed at addressing the challenges of domain shift and limited labeled data. The proposed method leverages the inherent temporal dependencies within time series data and adapts to new domains through self-supervised learning without requiring labeled data from the target domain. The approach involves learning a self-supervised autoregressive model on the source domain, and using it to generate synthetic samples for the target domain. The synthetic samples are used to train a domain adaptation model, which learns to minimize the distance between the source and target domains. The proposed method was evaluated on two real-world datasets and compared with state-of-the-art methods. The results showed that the proposed method outperformed existing approaches in terms of accuracy and robustness to domain shift. The study suggests that self-supervised autoregressive domain adaptation can effectively leverage temporal dependencies within time series data, and provide a promising approach for domain adaptation in scenarios where labeled data is limited or unavailable.",This paper introduces an approach based on supervised learning that aims to address domain shift and lack of labeled data specifically for time series data. The method uses learned temporal dependencies within time series and adapts to new domains via unsupervised learning and uses few if any labels from target domain. Learning begins by training an autoregressive model on the source domain and generating synthetic samples using this model for target domain. Using these synthetic samples the authors train a domain adaptation model to reduce the gap between source and target domains. Results on two real datasets show that this method performs better than current best practices in both accuracy and resistance to shift among domains. Results suggest that leveraging temporal dependencies in time series data using unsupervised learning is promising for adapting to domains where labeled data is scarce or absent.
"This paper presents an analysis of consensus-based distributed filtering with a fusion step in a network of interconnected agents. The filtering problem is to estimate a common signal from noisy measurements distributed over the network. We propose a distributed filtering algorithm based on consensus theory, where each agent updates its estimate by combining its own measurement with the estimates of its neighboring agents. The algorithm includes a fusion step, where the estimates are combined to obtain a global estimate of the signal. We analyze the convergence properties of the algorithm and derive conditions for the convergence to a consensus estimate. We also investigate the effect of the fusion step on the accuracy of the final estimate and derive bounds on the estimation error. Finally, we illustrate the performance of the algorithm through simulations, demonstrating its effectiveness in accurately estimating the signal in noisy and dynamic environments. The proposed algorithm has applications in sensor networks, distributed control, and multi-agent systems, where distributed estimation of a common signal is required.","This paper investigates filtering with consensus for a network of agents; they aim to estimate a common signal from noisy data distributed among them. An algorithm is proposed that each agent uses to update its estimate by averaging local data with estimates from neighbors. There is a fusion step as well where results combine to get an overall estimate of the signal. Performance analysis is done including conditions for convergence to consensus estimates. Bounds are also derived for error in this estimation. Simulations show effectiveness under noise and dynamics and applications include sensor networks, distributed control and multiagent systems where distributed estimation of common signals is needed."
"This paper proposes a new approach for label-guided graph exploration, which allows the user to adjust the ratio of labels used in the exploration process. Label-guided exploration is a common technique used to navigate through large and complex graphs, where labels serve as a guide to direct the exploration towards nodes of interest. However, traditional label-guided exploration methods typically use a fixed ratio of labels, which may not be optimal for all scenarios. Our approach enables the user to adjust the ratio of labels based on their specific needs and goals. We introduce a new algorithm that dynamically calculates the optimal ratio of labels to use based on the current exploration progress. This algorithm takes into account various factors, such as the density of the graph, the distribution of labels, and the user's feedback, to determine the most effective label ratio. To evaluate the effectiveness of our approach, we conducted a series of experiments on various real-world and synthetic datasets. Our results show that our approach outperforms traditional label-guided exploration methods in terms of efficiency and accuracy, particularly in scenarios where the optimal label ratio is unknown or changes over time. We also conducted a user study to assess the usability of our approach, which received positive feedback from the participants. Overall, our proposed label-guided graph exploration approach with adjustable ratio of labels provides a flexible and effective solution for navigating through large and complex graphs, which can be beneficial for various applications in fields such as social network analysis, recommendation systems, and bioinformatics.","This paper introduces a new approach to guided exploration of graphs using labels; users can now fine tune the proportion of labels used during this process. Guided exploration is frequently employed to navigate through big and complicated graphs and labels act as guides directing us towards interesting nodes. But traditional methods usually use a fixed proportion of labels which may not suit every situation. This new method lets users customize the proportion according to their individual preferences and aims. We present an algorithm that computes dynamically the best proportion of labels based on current progress of exploration. The algorithm considers several factors including graph density, distribution of labels and user feedback to select an optimal proportion. In order to test effectiveness we ran experiments using both real data and synthetic data. Results show that this new method excels traditional ones regarding efficiency and accuracy especially when the optimal proportion varies or is not known. We also carried out a usability study which users rated highly. In summary, we propose a flexible and effective way to navigate through big and complicated graphs and this new approach would be valuable for diverse areas such as social network analysis, recommendation systems and bioinformatics."
"This paper presents a recurrence relation approach for analyzing the effective capacity of retransmission schemes in wireless communication systems. The effective capacity is a measurement of the maximum sustainable rate of transmission under strict delay constraints. The proposed recurrence relation method provides a computational tool for finding the effective capacity for different retransmission schemes, including ARQ and HARQ protocols. The effectiveness of the proposed approach is evaluated through numerical examples and simulations, which demonstrate its accuracy and efficiency. The results of this study provide valuable insights into the design of efficient retransmission schemes and can be used as a reference for future research in the field of wireless communications.",This paper proposes an approach using recurrence relations to analyze effective capacity for retransmission schemes in wireless communication systems. Effective capacity measures maximum achievable data rates with strict delay requirements. Proposed recurrence relation methodology supplies a tool for calculating effective capacity for various types of retransmission protocols such as ARQ and HARP protocols. Results from evaluation via numerical examples and simulation show effectiveness and efficiency of the proposed approach. Results of this work offer important guidance on design of efficient retransmission schemes and serve as a reference for future research related to wireless communications.
"This paper investigates the maximal minimum distance of linear locally repairable codes, with the goal of finding tight bounds on this distance. The concept of locally repairable codes has gained attention in recent years due to its application in fault-tolerant storage systems. The maximal minimum distance of a code represents the largest minimum distance possible among all subcodes of the code, and is an important parameter for the error-correction capability of a code. The paper presents a detailed analysis of this distance, including its relationship to other code parameters and the effect of code design on its value. The authors use various mathematical tools to derive new bounds on the maximal minimum distance, and evaluate the tightness of these bounds through numerical simulations. The results of this study provide useful insights into the design of linear locally repairable codes with improved error-correction performance.","This paper studies the maximum minimal distance of linear codes that allow local repair and aims to find tight bounds on this distance. Recent interest has focused on such codes because they are important for designing fault tolerant storage systems. Maximum minimal distance for a code refers to the largest minimum distance that any subcode of that code can have; this is an important parameter related to the correction of errors by codes. The paper carefully examines this distance along with relationships to other code parameters and effects of code design on its value. Using various mathematical tools, the authors derive new bounds on this maximum minimal distance and assess their tightness using simulation evaluations. Results of this research offer valuable insights into improving code performance for linear codes that allow local repairs."
"The off-switch game is a simple two-player game that has been studied in game theory and decision making. In this game, one player chooses to either ""turn off"" the game or to continue playing, while the other player is given the option to accept or reject this choice. This paper provides a comprehensive overview of the existing research on the off-switch game, including its mathematical formulation, its equilibria, and its applications in various fields. The authors also propose new approaches to analyze the off-switch game and its variations, and demonstrate their usefulness through case studies and simulations. The results of this research contribute to our understanding of decision making in situations where there is an option to prematurely end the interaction, and have implications for areas such as bargaining, negotiations, and social dynamics.","This paper offers a broad review of the literature on the Off Switch Game which has been studied within the context of game theory and decision theory. The game features two players; one player can turn off play while the other can accept or reject this choice. Results include a detailed description of the game's mathematical formulation and equilibrium states, along with discussion of application areas such as bargaining and negotiation. We also present new analytical methods for studying the game and its variants. Case studies and simulation results show their practical utility. Results from this research improve understanding of how people make decisions when they have the option to terminate interaction early and are relevant to many fields."
"This paper investigates the use of adaptive pilot patterns for carrier-aggregated orthogonal frequency division multiplexing (CA-OFDM) systems in nonstationary wireless channels. Nonstationary wireless channels pose a challenge for CA-OFDM systems due to the time-varying channel conditions, which can cause severe performance degradation. To address this challenge, adaptive pilot patterns are proposed as a means of improving the system's ability to track the changing channel conditions. The effectiveness of the proposed adaptive pilot patterns is evaluated through simulation experiments in nonstationary wireless channels. The results show that the adaptive pilot patterns improve the tracking performance of the CA-OFDM system in nonstationary wireless channels, leading to better system throughput and error rates. This paper contributes to the development of adaptive techniques for wireless communication systems, which are essential for meeting the growing demand for high-speed wireless communication in nonstationary environments.",This paper looks at using adaptive pilot patterns for CA OFDM systems operating in channels that vary over time. Time varying channel conditions are problematic for CA OFDM because they result in poor performance. Adaptive pilot patterns are proposed as means to improve tracking of these time variations. Effectiveness is measured by simulations on channels that also vary. Results show improved tracking performance and better throughput and bit error rate. Contribute to developing adaptive techniques important for high speed wireless communications under dynamic conditions.
"Contextual bandits, a popular approach for online decision making, have gained significant attention in recent years due to their ability to balance exploration and exploitation in sequential decision problems. In this paper, we focus on the estimation considerations in contextual bandits, examining the impact of various factors such as the choice of reward function, the number of arms, the choice of context representation, and the sample size on the performance of the algorithms. We provide a comprehensive review of the existing literature and propose several directions for future research. Our results demonstrate that the performance of contextual bandits is sensitive to the estimation considerations, and highlight the importance of careful design and evaluation of these algorithms. The insights gained from this study can be used to guide practitioners in the design and deployment of contextual bandits, and provide a foundation for further research in this area.","Contextual bandits are an approach that has been receiving increased attention recently because they excel at balancing exploration and exploitation when dealing sequentially with decision problems. This paper focuses on considering estimation issues in contextual bandits; we look at how different factors such as reward selection, number of arms, context representation choices, and sample size affect algorithm performance. We review current literature comprehensively and suggest some future research directions. Results show that performance of contextual bandits varies depending on estimation issues and emphasize the importance of careful design and evaluation of these algorithms. Insights learned here can help practitioners to design and deploy contextual bandits effectively and serve as a basis for further research."
"This paper explores the effectiveness of hexaconv, a type of convolutional neural network architecture that uses hexagonal filters instead of traditional square filters. The study compares the performance of hexaconv with that of traditional square convolutional neural networks on several image classification tasks. The results show that hexaconv is able to achieve comparable or even better accuracy than square convolutional neural networks, while requiring fewer parameters and computation. The paper concludes that hexaconv is a promising alternative to traditional square convolutional neural networks, and could be particularly useful for image classification tasks involving hexagonally structured data, such as satellite imagery or cellular networks.","This paper investigates the effectiveness of hexaconv, a new kind of neural network using hexagonal filters rather than square ones. Results show that this architecture performs similarly or better compared to regular convolutional networks when classifying images. Compared to regular networks, hexaconv also uses fewer parameters and computing resources. The conclusion drawn from these results is that hexaconv is promising and potentially useful especially for classification tasks that use hexagonal data such as satellite imagery or cell phone networks."
"This paper presents a model-free adaptive optimal control approach for manufacturing processes with a fixed time horizon using reinforcement learning. The method is based on the use of reinforcement learning algorithms to optimize the control policy for the manufacturing process, without relying on an explicit model of the system dynamics. The approach is evaluated on a set of episodic manufacturing processes, where the goal is to maximize the total reward obtained over a fixed time horizon. Results show that the proposed approach is able to effectively adapt to the changing system dynamics, resulting in improved control performance compared to traditional model-based methods. The results demonstrate the potential of reinforcement learning for real-time control of manufacturing processes and provide insights for future work in this area.",This paper introduces a model free approach for adaptive optimal control for manufacturing processes using reinforcement learning. We use reinforcement learning algorithms to optimize control policies directly and without assuming any specific model for system dynamics. We evaluate performance on tasks with a fixed time horizon; we aim to maximize total rewards. Results show that the approach is successful at adapting to changing dynamics and results in better control performance compared to traditional methods which rely on models. Results also indicate potential for real time control of manufacturing and suggest directions for further research.
"This paper presents the results of a comprehensive survey of Delay Tolerant Networks (DTN) routing protocols. DTN is a type of communication network that is designed to operate in challenging and dynamic environments, such as those encountered in disaster relief operations, space missions, or rural areas with limited connectivity. The survey focuses on the most recent advances in DTN routing, covering both traditional and novel approaches. The paper provides a systematic overview of the main features and performance metrics of the various DTN routing protocols, including their strengths and weaknesses. The authors also identify the current challenges and open research questions in the field of DTN routing, and suggest directions for future work. The survey results will be valuable for researchers, practitioners, and decision-makers who are interested in developing, deploying, or using DTN networks.","This paper surveys recent advances in routing protocols for Delay Tolerant Networks (DTN). DTNs are used in situations where communications are difficult and conditions change rapidly, like during emergencies, space missions, or in remote places lacking reliable connectivity. The survey covers both old and new methods of routing. Results provide a clear picture of main features and performance metrics of different routing protocols; these include advantages and disadvantages. Authors also pinpoint present challenges and open questions about DTN routing and propose ways forward. Results from this study will prove useful for researchers, developers and others who wish to design, deploy or utilize DTN networks."
"This paper presents a general approach for enhancing slope limiters on non-uniform rectilinear grids. Slope limiters are mathematical methods used to enforce stability and prevent unphysical solutions in numerical simulations, such as computational fluid dynamics. The study focuses on the use of slope limiters on non-uniform rectilinear grids, which are widely used in engineering and scientific applications but pose challenges for slope limiting algorithms. The proposed approach uses a combination of grid adaptation techniques and novel slope limiters to improve the accuracy and robustness of numerical simulations on non-uniform grids. The results show that the proposed approach can effectively resolve the limitations of existing slope limiters and provide reliable solutions for a range of test problems. The findings of this study are expected to have important implications for the development of numerical methods for simulations on non-uniform grids.","This paper proposes a new way to enhance slope limiters for use on grids that vary in size and direction. Slope limiters help stop unphysical results from occurring during computer simulations like those of fluid dynamics. Previous work has mostly concentrated on using these limiters on regular grids; this study looks at them on irregular grids too. Using a mix of adaptation techniques and new limiters, the new approach improves simulation accuracy and reliability. Results show that this new approach overcomes shortcomings of previous limiters and works well on a variety of tests. Study outcomes should be important for further development of simulation methods for irregular grids."
"This paper examines the current state of electronic health record (EHR) interoperability in Tanzania's hospitals, and identifies the key issues, challenges, and opportunities that need to be addressed in order to improve the interoperability of EHR systems in the country. Through a comprehensive literature review and interviews with healthcare professionals and stakeholders, the authors highlight the major barriers to EHR interoperability in Tanzania, including a lack of standardization, inadequate infrastructure and technology, and a lack of trained personnel. The authors also explore the potential benefits of improving EHR interoperability, including improved patient care, reduced medical errors, and enhanced public health surveillance. Finally, the authors provide recommendations for policymakers, healthcare organizations, and technology vendors to help improve EHR interoperability in Tanzania, including the development of standards and guidelines, investment in technology and infrastructure, and the implementation of training programs for healthcare professionals. This research provides important insights into the state of EHR interoperability in Tanzania, and will be of interest to healthcare professionals, policymakers, and technology vendors working in the field of health information technology.","This paper looks at current status of interoperability of electronic health records (EHR) in hospitals in Tanzania and identifies important issues, challenges, and opportunities that should be addressed to improve interoperability of these systems. Using a thorough review of literature and interviews with healthcare workers and other stakeholders, researchers pinpoint major barriers to interoperability such as lack of standardization, insufficient infrastructure and technology and lack of trained personnel. Researchers also look at potential benefits like better patient care, fewer medical errors and enhanced surveillance of public health. Finally, they give recommendations to policymakers, health care organizations and technology providers on how to improve interoperability through developing standards and guidelines, investing in technology and infrastructure, and implementing professional training programs. Research results offer valuable insights into interoperability of EHRs in Tanzania and are of interest to professionals in health IT."
"This paper presents a novel method for efficient blind compressed sensing in magnetic resonance imaging (MRI). The proposed method leverages sparsifying transforms to achieve compression while preserving the important features of the data. The use of sparsifying transforms allows for a reduced number of measurements to be taken, leading to faster acquisition times and reduced costs. Additionally, the method includes convergence guarantees, ensuring that the reconstructed image accurately represents the original data. The effectiveness of the proposed method was evaluated through experiments on MRI data, demonstrating improved performance compared to existing methods. This work advances the state of the art in blind compressed sensing and has potential applications in a variety of imaging modalities.",This paper introduces new methodology for efficient blind compressed sensing used in MRI. Using transform sparsification we can reduce the amount of data to acquire while retaining key features. Use of transform sparsification also reduces acquisition time and lowers cost. Method also comes with convergence guarantees that ensure reconstructed images match the original. Performance improvements are demonstrated by experiments using MRI data. Results show this new approach advances current standards in blind compressed sensing and opens up possibilities for different imaging technologies as well.
"This paper explores the impact of catastrophic collisions and collision avoidance on swarming behavior. The paper first provides an overview of swarming behavior in animal and artificial systems, and then discusses the various factors that can impact swarming, including catastrophic collisions and collision avoidance. The paper then presents a series of simulations and experiments that demonstrate the impact of catastrophic collisions and collision avoidance on swarming behavior, and analyzes the results to draw conclusions about the relationship between these factors and the overall effectiveness of swarming. The results suggest that catastrophic collisions and effective collision avoidance are critical components of successful swarming behavior, and that these factors can have a significant impact on the overall performance of a swarm.","This paper looks at how important collisions and collision avoidance are for collective behavior. First, we look at collective behavior among animals and artificial swarms and then discuss various things that influence this behavior such as catastrophic collisions and collision avoidance. We also simulate and conduct experiments to show how collisions and avoidance affect collective behavior. We analyze the results to determine if there is a connection between those factors and the performance of a swarm. Results indicate that avoiding collisions catastrophically and effectively is essential for successful collective behavior and impacts swarm performance significantly."
"This paper presents a comprehensive study on the prediction reliability of graph neural networks (GNNs) for virtual screening. The study was conducted to evaluate the performance of GNNs in predicting the binding affinity of small molecules with protein targets. The authors tested several state-of-the-art GNN models on a diverse set of protein-ligand complexes and compared their performance to other commonly used methods in virtual screening. The results showed that GNNs achieved comparable or superior performance compared to traditional methods, with high predictive accuracy and robustness. The authors also identified important factors that can affect the reliability of GNN predictions, such as the choice of hyperparameters and the quality of training data. The findings of this study provide valuable insights into the potential of GNNs for virtual screening and highlight their utility as a reliable tool for drug discovery.",This research evaluates performance of Graph Neural Networks (GNNs) in virtual screening by evaluating binding affinity predictions of small molecules against protein targets. Authors test multiple top GNNs on diverse complexes and compare performances with other standard methods. Results show that GNNs perform comparably or better than usual methods with high accuracy and stability. Authors also identify important aspects affecting prediction reliability like selection of hyper parameters and quality of training data. Findings from this research point out potential usefulness of GNNs for screening and highlight them as valuable tools for drug discovery.
"This paper presents a novel approach for anti-isomorphism mapping of unordered labeled trees, called ""Anti-Tai Mapping"". The proposed method addresses the challenge of finding a one-to-one correspondence between nodes in two unordered labeled trees, while preserving the structural and label similarities between the nodes. The approach is based on the concept of ""tree tai-mapping"", which is modified to handle the unordered nature of the trees and to enforce the anti-isomorphism constraint. The performance of the Anti-Tai Mapping method was evaluated on several datasets and compared with existing methods, demonstrating its superiority in terms of accuracy and computational efficiency. The results of this study have potential applications in various fields, such as pattern recognition, machine learning, and computational biology.","This paper introduces a new method to map unordered labeled trees with opposite isomorphisms, called Anti Tai Mapping. It tackles the problem of establishing one to one relations among nodes from different unordered labeled trees while preserving similarity in both structure and labels. Based on tree tai mapping concept, we modify this idea to deal with unordered trees and to ensure anti isomorphism. Performance evaluation of this method has been conducted on different data sets and compared with previous methods; results show high performance levels in terms of accuracy and speed. Results of this research will be useful for a variety of domains including pattern recognition, machine learning and computational biology."
This paper presents an investigation into the automated identification of security discussions in microservices systems. The study includes both industrial surveys and experimental results to provide a comprehensive analysis of the current state of the field. The surveys were conducted with industry professionals to gather their perspectives on the importance and challenges of security in microservices systems. The experimental results focus on the development and evaluation of a machine learning-based approach for automatically identifying security discussions in microservices systems. The results of the study provide insights into the current state of security in microservices systems and suggest that automated identification of security discussions is a promising area for future research and development.,This paper investigates the automatic recognition of security discussion topics in microservice systems. It combines both survey results from industry professionals and experimental results to conduct a thorough analysis of this field. Surveys are conducted to understand how important security is and what difficulties exist for systems based on microservices. Experiments focus on developing and evaluating a machine learning approach to automatically recognize security discussions. Results show insights into current security levels in microservice systems and indicate that automated identification of security discussions is promising for future research and development.
This paper proposes a new framework for counting and localization in crowds that utilizes a purely point-based approach. The current methods for counting and localizing individuals in crowds rely on region-based techniques which are limited by their assumptions about crowd behavior and can be prone to errors. The proposed point-based framework aims to overcome these limitations by treating each person in a crowd as a discrete point and using this information to make more accurate counts and localizations. The framework has been tested and validated using real-world data and has shown promising results in improving the accuracy of crowd counting and localization. This research provides a new perspective on the current limitations of region-based techniques and offers a potential solution to these limitations through a point-based approach.,"This paper presents a new framework that counts and locates people in crowds using points directly rather than regions. Current approaches assume fixed behaviors for crowds but they are often inaccurate; the new framework avoids such assumptions and treats each person as an individual point. Results from real data have demonstrated improved accuracy. By taking a point approach instead of relying on regions, this work opens a new line of thought on how to better count and locate people in crowds."
"In this paper, the authors present a novel method for interpolation called Neurint which uses neural ordinary differential equations (ODEs). The method utilizes a parametric approach to approximate the solution of a system of ODEs and learns the parameters through backpropagation, enabling the model to interpolate between any two points in the input space. The authors conduct experiments to compare Neurint with traditional interpolation methods and show that Neurint outperforms these methods in terms of accuracy and generalizability. The results of this study demonstrate the potential of Neurint as a promising tool for various interpolation tasks in machine learning and beyond.","This paper introduces a new method called Neurint that interpolates using neural ordinary differential equations (ODEs). Using a parametric approach, Neurint approximates solutions to systems of ODEs and learns parameters via backpropagation. Thus allowing the model to interpolate between any points within the input space. Experiments comparing Neurint against other traditional methods show higher performance and better generalization. Results suggest Neurint is a promising tool for interpolation tasks in ML and other areas."
"This paper presents an investigation of efficient labeling algorithms for solving the adjacent quadratic shortest path problem. The authors analyze the performance of various labeling algorithms, including the A* and Dijkstra algorithms, and propose a novel hybrid approach combining elements from both methods. The experimental results show that the proposed hybrid algorithm outperforms existing methods in terms of computation time and solution quality. The findings of this study contribute to the development of more efficient algorithms for solving the adjacent quadratic shortest path problem and have implications for applications in fields such as transportation, logistics, and network design.","This paper investigates efficient algorithms for solving adjacent quadratic shortest paths and compares different labeling algorithms including A* and Dijkstra. They introduce a new hybrid approach that combines features from both algorithms and experimentally find their proposed hybrid to perform better than other methods in terms of computation speed and solution quality. Results of this research contribute to further development of more efficient algorithms and have practical implications in areas like transportation, logistics, and network design."
"Scene parsing is a crucial task in computer vision, where the goal is to segment and categorize objects in an image or video. In this paper, we propose a novel approach to scene parsing using multiscale feature learning, purity trees, and optimal covers. Our approach first uses multiscale feature learning to extract features from an image at multiple scales, capturing both local and global context. Then, we employ purity trees to hierarchically divide the image into regions, grouping similar pixels and objects. Finally, we use optimal covers to assign each region to a semantic class, resulting in a comprehensive scene parse. Our approach is tested on benchmark datasets and demonstrates state-of-the-art performance in scene parsing accuracy.","Scene parsing is an important task in computer vision. The aim is to segment and classify objects within images or videos. This paper introduces a new method for scene parsing that leverages multiscale feature learning, purity trees, and optimal coverings. First, we extract features of an image at different levels of scale to capture both local and global context. We then use purity trees to hierarchically partition the image into regions by clustering similar pixels and objects together. Finally, we assign these regions to semantic classes via optimal covering; this results in comprehensive scene segmentation. Experiments on benchmark datasets show that our approach excels at performance for scene parsing."
"The lattice Boltzmann method (LBM) is a powerful computational fluid dynamics (CFD) tool that simulates fluid flow by solving the Boltzmann equation. In this paper, we present a new approach to massively parallelize the LBM on non-uniform grids. Non-uniform grids are more flexible than uniform grids and can better resolve complex geometries, but they have been challenging to parallelize. Our approach takes advantage of the data-parallel nature of the LBM to parallelize the simulation across multiple compute nodes. We implement the LBM on non-uniform grids using a hybrid MPI/OpenMP approach, which balances computational load and communication overhead. The performance of our implementation is evaluated on a range of test cases and compared to existing parallel implementations. Our results show that our parallel algorithm can scale efficiently up to a large number of compute nodes and achieve significant speedup over existing methods, demonstrating the viability of our approach for large-scale CFD simulations.","Using the lattice Boltzmann method (LBM), a computational tool for fluid dynamics simulations, this paper presents an innovative way of massively parallelizing this method using uneven grids. While these irregular grids can flexibly handle complex shapes better than regular ones, they have proven difficult to parallelize efficiently. By leveraging the inherent parallelism in LBM we develop an approach that distributes simulations among different processors. Implementation uses hybrid MPI and OpenMP to balance workload and communication overhead. Performance evaluation shows strong scaling across various test cases and compares favorably against other parallel implementations; results indicate that the developed algorithm scales well and achieves high speedup. This work demonstrates feasibility for large scale CFD simulations."
"This paper presents a modified scheme for a quantum sealed-bid auction using a multiparty circular quantum key agreement. The proposed scheme is designed to address the limitations of traditional sealed-bid auctions, which are vulnerable to security threats such as bid manipulation and information leakage. By incorporating principles of quantum cryptography, the proposed scheme provides increased security for the auction process. The multiparty circular quantum key agreement enhances the security of the auction by ensuring that all parties involved in the process have secure communication channels and that the bids are kept confidential. Results from simulations and experiments indicate that the proposed scheme is efficient and secure, offering a promising solution for conducting secure sealed-bid auctions. The findings of this research have important implications for the design and implementation of secure electronic auction systems.",This study introduces an improved method for conducting sealed bidding auctions via multiparty circular quantum key agreement. This proposal aims to solve weaknesses in traditional auctions which are susceptible to threats like bid manipulation and information leakage. By integrating principles of quantum cryptography this proposal offers higher security assurance during auction processing. Multiparty circular key agreement improves security through guaranteed secure channels among all participants along with keeping bids secret. Results from simulation and experiment show that this new protocol performs well and securely; it represents a promising approach towards running auctions securely. Findings from this work have great significance for designing and implementing secure electronic auction systems.
"In this paper, we introduce a novel approach for designing faster sublinear algorithms using conditional sampling. Sublinear algorithms are a class of algorithms that can process large datasets efficiently without having to read the entire dataset. Our approach is based on conditional sampling, which allows us to obtain unbiased estimates of specific functions with high probability using only a small subset of the data. This leads to significant speedup in the computation time while maintaining the accuracy of the results. Our proposed method has been tested on several real-world datasets and showed improvement over existing sublinear algorithms. This work opens up new avenues for research in the area of sublinear algorithms and has practical applications in various domains including machine learning, data mining, and big data analytics.",We propose a new approach to design faster algorithms that use conditional sampling. Sampling is used here to process large datasets quickly without reading all the data. Using our approach we sample only a small fraction of data to get accurate estimates with high probability of certain functions. This leads to considerable reduction of computation time and preserves accuracy. Results from testing on real data sets have shown that this new method performs better compared to previous methods. This work initiates new research paths into sub linear algorithms and has practical uses in areas like machine learning and data mining.
"This paper proposes a joint optimization approach for computing resource allocation in three-tier IoT fog networks that combines Stackelberg game and matching. The three-tier architecture of IoT fog networks consists of cloud, fog, and edge layers, each with their own computing resources. The Stackelberg game models the interactions between the layers as a leader-follower relationship, while the matching algorithm finds the optimal allocation of resources based on the network's constraints and objectives. Simulation results demonstrate the effectiveness of the proposed approach in achieving efficient resource allocation, reduced latency, and improved network performance compared to traditional methods. This work sheds light on the potential of combining game theory and optimization techniques for addressing resource allocation challenges in multi-tier IoT networks.","This research introduces an integrated approach for computing resource allocation in three layered IoT fog networks that merges Stackelberg games with matching algorithms. The structure includes cloud, fog and edge layers with distinct computing resources. A leader follower relationship is modelled using Stackelberg game interaction among the layers, while matching algorithms optimize resource allocation based on network boundaries and goals. Results from simulation show effectiveness of the suggested approach: high efficiency of resource allocation, reduced latency and better network performance relative to conventional methods. Results also point toward potential for combining game theory with optimization to deal with allocation issues in higher tier IoT networks."
"In this paper, the authors propose a new beamforming technique called ""Layered Group Sparse Beamforming (LGSB)"" for cache-enabled green wireless networks. LGSB is designed to address the challenge of balancing energy efficiency and data rate in wireless networks with caching capabilities. The technique is based on grouping users into different layers and allocating sparse beamforming resources to each layer. The results of simulations show that LGSB provides significant improvements in energy efficiency while maintaining high data rates compared to traditional beamforming techniques. The findings of this study can contribute to the development of more sustainable and energy-efficient wireless networks.",Authors introduce a new beamforming method called Layered Group Sparse Beamforming (LGSB) for green wireless networks which have caching capabilities. This new method addresses the dual challenge of balancing energy efficiency and data rate. LGSB works by classifying users into different levels and assigning sparse beamforming resources to each level. Simulation results indicate LGSB performs much better than traditional methods regarding both energy efficiency and high data rates. Results from this research may help develop more sustainable and efficient wireless networks.
"This paper proposes a novel approach for clustering stream data based on the concept of density mountain evolution. The approach utilizes the notion of density peaks to track the evolution of clusters over time and identify the evolution patterns of these density peaks. This method allows for a more dynamic and adaptive clustering of stream data, compared to traditional methods that often assume a static cluster structure. The performance of the proposed method was evaluated through experiments on synthetic and real-world data sets, and the results showed that the approach outperforms existing methods in terms of accuracy, efficiency, and scalability. The findings of this study contribute to the advancement of clustering techniques for stream data and have potential applications in various domains, such as online market segmentation and fraud detection.","This paper introduces a new approach to clustering streaming data using the idea of evolving density mountains. It uses peak density notion to track changes in clusters over time and identify patterns of those peaks. This method is superior because it clusters in a more responsive way compared to typical methods which usually presuppose static cluster structures. Performance was tested using both artificial and real data sets; results indicate this approach excels at accuracy, efficiency and scalability. Results from this research advance clustering techniques for streams and suggest applicability to diverse fields like online market segmentation and fraud detection."
"This paper investigates the potential of using machine learning techniques to improve the efficiency of boolean satisfiability problem (SAT) solving. SAT is a well-known NP-complete problem and is widely used in various fields, including hardware and software verification, scheduling, and cryptography. The paper proposes a novel approach that leverages machine learning models to guide the search process of traditional SAT solvers. The results show that the proposed approach significantly improves the solving time of SAT instances compared to state-of-the-art solvers. The proposed approach also achieves competitive results on a range of standard benchmark datasets, demonstrating the effectiveness of the approach in practical scenarios. This research provides insights into the application of machine learning in solving computationally hard problems and has potential for further advancements in this field.","This study looks at the potential of applying machine learning techniques to enhance efficiency of solving Boolean Satisfiability Problems (SAT). SAT is a common problem known to be NP complete and is frequently used in diverse areas like hardware and software verification, scheduling, and cryptography. Authors propose a new method that uses machine learning models to direct the search process of current SAT solvers. Results indicate that proposed method greatly reduces solving time compared to leading solvers. Performance on different benchmark data sets shows comparable results as well; thus showing effectiveness under practical conditions. Research findings also shed light on application of machine learning to solving difficult computational tasks and suggest potential advancement possibilities."
"The IEEE Computational Intelligence and Games (CIG) 2017 Game Data Mining Competition aimed to find the best solution for predicting player behavior in a video game. In this paper, the winning solution is presented and analyzed. The approach used in the winning solution was based on machine learning techniques, specifically gradient boosting and deep neural networks. The results showed that the winning solution outperformed other competitors by a significant margin in terms of prediction accuracy. The findings from this study demonstrate the potential of combining machine learning techniques for predicting player behavior in video games and highlights the importance of careful feature selection and model architecture design. The authors also provide insights into the limitations and challenges of the approach and suggest areas for future research.",The IEEE CIG 2017 competition on Game Data Mining sought the best way to predict how players behave in video games. This paper describes winning solutions and analyzes them. Winning method used machine learning including boosting gradients and deep neural nets. Results show that winning methods significantly outperform other competitors regarding accuracy of prediction. Results of this study point to potential use of combining machine learning to predict player behavior and emphasize the significance of careful choice of features and design of model structure. Authors also offer suggestions as to what might be improved and future directions for research.
"This paper proposes a meta-transfer objective for learning to disentangle causal mechanisms. The objective is based on the idea of transfer learning, where knowledge learned from one task is used to improve performance on a related but different task. The proposed objective aims to transfer the ability to disentangle causal mechanisms from one problem to another. The objective is evaluated on a series of experiments and compared to existing methods for disentangling causal mechanisms. The results show that the proposed meta-transfer objective outperforms existing methods in terms of accuracy and generalization performance, demonstrating its effectiveness in improving the ability to disentangle causal mechanisms.",This paper introduces a meta transfer objective aimed at learning to separate causal mechanisms. It uses ideas from transfer learning wherein learning from one task improves performance on related tasks that differ. This new objective aims to transfer the skill of separating mechanisms from one problem to another. Results from a set of experiments are reported and compared against existing methods for separating causal mechanisms. Results show the proposed objective performs better in terms of accuracy and general performance; thus it demonstrates effectiveness at improving separation of mechanisms.
"This paper presents a novel approach for decoding in communication systems, referred to as SRGRAND (Symbol Reliability Guided Random Additive Noise Decoding). The conventional decoding methods rely solely on channel channel state information (CSI) to perform the decoding. However, in practical communication systems, channel CSI is often unavailable or outdated, which results in performance degradation. The proposed SRGRAND approach leverages symbol reliability information in addition to channel CSI to enhance the decoding performance. The symbol reliability information is calculated based on the statistical properties of the transmitted symbols and the channel noise, which provides additional information about the quality of each symbol. This information is then used to guide the decoding process and improve the error rate. Simulation results show that the SRGRAND approach outperforms the conventional decoding methods and provides a more robust performance in practical communication systems.","This paper introduces an innovative approach to decoding in communications systems called SRGRAND - Symbol Reliability Guided Random Additive Noise Decoding. Conventional methods for decoding rely entirely on channel state information (CSI) to operate. However, in real communication systems, CSI is usually unreliable or stale leading to degraded performance. Proposed SRGRAND uses not just CSI but also information about the reliability of symbols. Calculated using statistical properties of transmitted symbols and channel noise, this information gives insight into the quality of each individual symbol. This extra information is used to direct decoding and reduces errors. Simulation results show that SRGRAND performs better than traditional decoding methods and provides stronger performance in practical systems."
"Semi-supervised Generative Adversarial Networks (ssGANs) are a popular approach for unsupervised learning tasks in computer vision and natural language processing. However, their application to regression problems has been limited due to the difficulty in defining a suitable loss function. In this paper, we propose a new approach to generalize ssGANs to regression problems by using a feature contrasting loss function. The proposed method effectively leverages both labeled and unlabeled data to learn a mapping from the input space to the output space. We evaluate the performance of the proposed method on various regression benchmarks and compare it with existing methods. The results show that the proposed method outperforms the state-of-the-art methods in terms of both accuracy and stability. This work opens up new possibilities for ssGANs in regression problems and demonstrates their potential for solving real-world problems.",Semi supervised GANs have been very popular for unsupervised learning tasks in vision and NLP but they have not been applied well to regression because it is hard to define a good loss function. This paper introduces a novel approach to extend these GANs to solve regression problems through use of feature contrast loss. This new method effectively uses both labeled and unlabeled data to learn a mapping between input and output spaces. Results on diverse benchmarks demonstrate that this approach outperforms current best methods regarding both accuracy and reliability. This research indicates that these kinds of GANs can perform much better when solving practical problems.
"The paper presents a novel approach to semantic segmentation called Criss-Cross Attention Network (CCNet). The method is based on the idea of criss-cross attention mechanism, which has shown promising results in various computer vision tasks. In CCNet, the criss-cross attention mechanism is utilized to capture long-range dependencies between pixels, which are critical for accurate semantic segmentation. The network architecture consists of a feature extraction backbone, a criss-cross attention module, and a segmentation head. The criss-cross attention module utilizes both spatial and channel-wise attention to model the inter-dependencies between pixels. The experiments conducted on several benchmark datasets demonstrate the effectiveness of CCNet, achieving state-of-the-art performance with a relatively lightweight network architecture. The results suggest that the proposed criss-cross attention mechanism is a promising approach for semantic segmentation and can be further extended to other computer vision tasks.","A new way of doing semantic segmentation called CCNet is presented in this paper. This method uses an attention mechanism that works well on other computer vision tasks. The core of CCNet involves using cross attention to get information from distant pixels. These distant pixels are important for good segmentation. The network structure includes a backbone for feature extraction, an attention module, and a segmentation head. This module focuses on both spatial and channel level to model pixel relationships. Results show that CCNet performs very well on benchmarks and does so with a light network design. Results indicate that proposed cross attention is promising and could work well on other tasks too."
"This paper presents a study on the development of an efficient object detection model for real-time unmanned aerial vehicle (UAV) applications. With the growing demand for UAVs in various industries, there is a need for efficient and reliable object detection algorithms that can be used in real-time scenarios. The proposed model is based on a combination of deep learning and computer vision techniques, which are optimized to provide accurate and fast object detection in real-time UAV applications. The study includes a comprehensive evaluation of the proposed model on a benchmark dataset, and the results show that it outperforms existing state-of-the-art object detection models in terms of both accuracy and processing speed. The model is also tested in various real-world UAV applications, including surveillance and inspection tasks, and the results demonstrate its effectiveness and practicality. This research contributes to the field of UAV object detection by providing a highly efficient and accurate model for real-time applications.","This paper reports on a study aimed at developing an efficient model for real time use of UAVs. As demand increases for UAVs across different sectors, we need high performance and reliable algorithms for real time detection of objects. Proposed method uses a blend of deep learning and computer vision techniques that are fine tuned to perform precise and quick object detection in real time. Comprehensive testing on a standard dataset shows this new model performs better than current leading methods regarding both precision and speed. Results are also verified through varied real world tests such as surveillance and inspection tasks and show efficacy and practical utility. Contribution of this research is significant because it provides an extremely efficient and accurate model for real time UAV work."
"This paper presents a quantitative analysis of cryptocurrency egalitarianism, exploring how the distribution of cryptocurrency holdings among users reflects or diverges from principles of egalitarianism. To do so, we analyzed transaction data from a sample of the most widely used cryptocurrencies, including Bitcoin, Ethereum, and Litecoin. Our approach involves constructing a Gini coefficient for each cryptocurrency, which measures the level of inequality in its distribution of wealth. We also examined the concentration of wealth among the top percentile of cryptocurrency holders and assessed the impact of various factors, such as mining, trading, and adoption rates, on the level of egalitarianism in each cryptocurrency's ecosystem. Our findings suggest that, while cryptocurrency is often touted as a decentralized and democratized form of currency, the reality is that the distribution of wealth in many cryptocurrencies is highly unequal. However, we also identify several factors that could help promote greater egalitarianism in the cryptocurrency space, including measures to address concentration of wealth and the promotion of fair mining practices. Overall, our research sheds light on the extent to which cryptocurrency adheres to principles of egalitarianism and provides insights into potential strategies for promoting a more equitable distribution of wealth in this emerging financial ecosystem.","This paper conducts a quantitative study on cryptocurrency egalitarianism and analyzes how wealth distribution among users relates to egalitarian principles. We look at Bitcoin, Ethereum, and Litecoin, three of the most used crypto currencies. We compute Gini coefficients for each to measure wealth inequality. We also look at wealth concentration among top percentiles and consider different factors such as mining and trading rates. We find that in many cryptos wealth distribution is quite unequal despite frequent claims of decentralization and democracy. We also note however that there are some ways to enhance equity in this emerging financial ecosystem such as addressing wealth concentration and promoting fair mining practices. In sum we illuminate how much crypto aligns with egalitarian ideals and offer insight into strategies for greater wealth equity."
"This paper investigates the problem of simple and approximately optimal pricing for proportional complementarities. Proportional complementarities arise in various economic settings, where the value of a good to a consumer increases as the adoption of the good by others increases. The paper focuses on finding pricing strategies that maximize the total surplus generated by the market while being simple and easy to implement. The paper presents a novel approach that combines the principles of optimal pricing with approximation methods to derive pricing formulas that are both optimal and practical. The approach is tested through simulation and the results show that the proposed pricing formulas lead to near-optimal results, while being much simpler than traditional optimal pricing methods. The findings of this paper contribute to a better understanding of how to design pricing strategies for proportional complementarities and provide valuable insights for practitioners and policymakers.","This paper studies pricing problems that are simple yet nearly optimal when dealing with proportional complementarity issues. These complementarities occur frequently in economics where value rises with increased adoption among others. The research concentrates on developing pricing schemes that produce maximum total surplus from trading and that are straightforward and easy to put into practice. It introduces a new approach that integrates ideas of optimal pricing with approximation techniques to derive pricing formulas that are both high quality and practical. Results from simulations indicate that proposed pricing formulas work well, producing near optimum performance while being much easier to use than traditional methods of optimal pricing. Findings also offer significant insights for people who make pricing decisions for goods with proportional complementarity and policy makers."
"This paper investigates the design flow of digital microfluidic biochips (DMFBs) with the aim of improving their fault-tolerance. DMFBs have the potential to revolutionize the field of bioanalysis by enabling miniaturization and automation of complex laboratory protocols. However, the high complexity and large-scale integration of DMFBs can lead to various faults and errors, which can compromise their functionality and reliability. To improve fault-tolerance in DMFBs, the proposed design flow comprises four main steps: fault modeling, fault analysis, fault-tolerant synthesis, and fault-tolerant testing. The fault modeling step involves identifying possible faults and errors in the DMFB design and their root causes. The fault analysis step assesses the impact of faults on the DMFB and its overall performance. The fault-tolerant synthesis step involves generating a fault-tolerant design that can mitigate the identified faults. Finally, the fault-tolerant testing step involves validating the performance and reliability of the fault-tolerant DMFB design. The proposed design flow is demonstrated through the design of a droplet-based DMFB for protein crystallization, which involves a complex sequence of biochemical reactions. Fault injection experiments are conducted to validate the fault-tolerant design, and the results show that the proposed approach can effectively improve the fault-tolerance of DMFBs. The contributions of this paper include a comprehensive design flow for fault-tolerant DMFBs, a case study demonstrating the effectiveness of the proposed approach, and a discussion on the potential limitations and future directions of the proposed design flow. The proposed design flow can be used as a framework for the development of reliable and robust DMFBs for a range of bioanalysis applications.","This paper looks at how to design digital microfluidic biochips (DMFBs), aiming to improve their resistance to faults. DMFBs promise major advances in bioanalysis by allowing for automation and miniaturization of complex lab procedures. Yet DMFB designs are complex and integrate many components which can result in many different types of faults and errors. This can reduce their reliability. Therefore we propose a design flow that includes four main stages: fault modeling, fault analysis, synthesis of fault tolerance and fault tolerance testing. In fault modeling we identify likely faults and determine their root causes. Fault analysis measures the effects of these faults on performance. During synthesis of fault tolerance we create designs that reduce known faults. Finally during fault tolerance testing we evaluate if the resulting design functions reliably. We demonstrate this design flow using an example of designing a chip for crystallization of proteins, involving complex biochemical reactions. Fault injection tests confirm the new design is robust and effective. Contributions of this work include a comprehensive design flow for DMFBs that is illustrated through a case study, along with consideration of potential limits and future directions. This design flow serves as a foundation for developing high reliability and robustness into diverse bioanalysis applications."
"The purpose of this paper is to present a new approach for behavior planning in connected autonomous vehicles using feedback deep reinforcement learning. The study focuses on developing a system that can learn from past experiences and make informed decisions in real-time to ensure safe and efficient navigation on the roads. The proposed approach utilizes a combination of deep reinforcement learning and feedback control, where the feedback control component provides additional stability and robustness to the system. The effectiveness of the proposed approach is evaluated through a series of simulations and experiments, which show significant improvement in terms of safety and efficiency compared to traditional behavior planning methods. The results of the study demonstrate the potential of the proposed approach in providing a practical solution for behavior planning in connected autonomous vehicles.",This paper introduces a new method of behavior planning for connected autonomous vehicles by using feedback based deep reinforcement learning. Research goals focus on developing a system capable of learning from prior experience and making timely decisions to ensure road safety and efficiency. Proposed approach uses a combination of deep reinforcement learning and feedback control. Stability and robustness are enhanced by the use of control feedback components. Results show this approach performs much better than traditional planning methods in terms of both safety and efficiency. Results indicate the feasibility of this approach as a practical solution for vehicle behavior planning.
"This paper presents a novel approach to fast Bayesian calibration of brain tumor models using a geometry-aware neural solver. The proposed method leverages the power of deep learning to efficiently solve the inverse problem of mapping MRI scans to physical parameters of the tumor, while accounting for the underlying geometrical constraints. The proposed solver is trained using a combination of simulated and real data, and the results demonstrate significant improvements in accuracy and computation time compared to traditional optimization-based methods. This work provides a promising solution to the challenging problem of estimating the physical properties of brain tumors from medical images, and has potential applications in personalized medicine and radiation therapy planning.",This paper introduces a new approach to fast Bayesian calibration of brain tumor models by means of a geometry aware neural solver. Proposed method uses deep learning to solve efficiently the inverse problem that maps MR scans to tumor physical parameters while respecting underlying geometric constraints. The solver is trained using a mix of simulated and real data; results show marked improvement in both accuracy and computational time over traditional optimization methods. Results of this study offer promising solutions for estimating physical tumor properties based on medical images and have potential for use in personalized medicine and radiation therapy planning.
"This paper explores the use of self-supervised learning techniques to improve the representation learning of tabular data. Traditional methods for learning representations from tables rely on handcrafted features or pre-defined embeddings, which may not capture the underlying structure of the data. To overcome these limitations, we propose a self-supervised learning approach that leverages the inherent structure and relationships within the table to learn more informative representations. Specifically, we introduce two self-supervised tasks that focus on predicting missing values and reconstructing corrupted tables. We demonstrate the effectiveness of our approach on several real-world datasets, showing that it outperforms existing methods for tabular data representation learning. Our findings suggest that self-supervised learning can provide a promising avenue for enhancing the representation of tabular data and improving downstream applications such as classification and regression.","This paper investigates use of self supervised learning approaches to enhance representation learning from tabular data. Typical methods for learning features from tables require predefined features or fixed embeddings which often do not reflect underlying structure. We introduce an alternative approach that makes use of intrinsic structure and relationships within the table to learn more useful feature representations. Specifically, we define two supervised learning tasks targeting prediction of missing values and reconstruction of corrupted tables. Results show that this method performs better compared to current methods on real world data sets. Results suggest that supervised learning self supervised offers promise for improvement in representation learning of tabular data and also other downstream tasks such as classification and regression."
"This paper presents a study of few-shot audio classification, which aims to classify audio signals into a limited number of categories with only a small amount of labeled data. The study focuses on exploring the effectiveness of various few-shot learning techniques, such as transfer learning and meta-learning, in addressing the challenge of limited labeled data. The research includes an extensive evaluation of different models and techniques on several benchmark datasets, using various metrics such as accuracy, precision, recall, and F1-score. The results demonstrate that transfer learning and meta-learning can significantly improve the classification performance in few-shot scenarios, compared to conventional approaches. The study also investigates the impact of different factors such as the size of labeled data, the number of categories, and the complexity of audio signals on the classification performance. The findings of this study provide valuable insights into the design of effective few-shot audio classification models, which have important applications in speech recognition, music analysis, and audio-based surveillance systems.","This paper studies few shot audio classification, where one tries to categorize sounds into a limited number of classes based on very little labeled data. Focus is on evaluating various methods for dealing with scarcity of labeled data like transfer learning and meta learning. Results show these methods greatly improve performance compared to standard approaches. Performance is also analyzed depending on factors including size of labeled data used, number of classes, and signal complexity. Insights from this study contribute to designing better models for classification tasks which are useful for speech recognition, music analysis and other audio surveillance systems."
"Ride-hailing services have become a popular mode of transportation in many urban cities around the world. In Nigeria, ORide is one of the prominent ride-hailing services that provide affordable transportation to commuters. However, the anonymity of ORide drivers has been a long-standing issue that raises concerns about user safety and trust. In this paper, we present an empirical study that explores the perceptions of ORide users towards driver anonymity and its implications for user safety. We conducted a survey of 500 ORide users in Lagos, Nigeria, to gather their opinions on driver anonymity and the factors that influence their decision to use the service. Our findings show that a significant number of ORide users are concerned about driver anonymity and would prefer to have more information about their drivers, such as their names and photos. We also found that users' perception of driver anonymity is influenced by their prior experience with the service, gender, and age. Additionally, our study reveals the potential safety implications of driver anonymity, such as difficulty in identifying drivers in case of accidents or security breaches. Based on our findings, we provide recommendations for ORide and other ride-hailing services on how to address the issue of driver anonymity while maintaining user safety and privacy. Our study contributes to the growing body of literature on ride-hailing services in developing countries and sheds light on the importance of balancing user privacy and safety in the design of such services.","Ride sharing services have become popular in most big cities all over the world. In Nigeria, ORide is one of the main ride sharing services that offer convenient transportation to people commuting. Driver anonymity is a longstanding problem which worries people's safety and trust. This paper presents an empirical study into user perceptions regarding anonymity of drivers and related safety implications. We surveyed 500 users in Lagos, Nigeria and asked them what they think about anonymity and what influences their choice to use this service. Results showed that many users are worried about anonymity and want more information like driver names and pictures. Factors influencing this concern include previous experiences with the service, gender and age. Also, our research indicates risks of anonymity including trouble recognizing drivers during accidents or security incidents. From our results we recommend ways for ORide and other companies to reduce anonymity risks while still protecting users' safety and privacy. Our study adds to the growing literature on ride sharing services in developing countries and highlights the importance of finding balance between privacy and safety for such services."
"This paper explores the concept of intermittent status updating in wireless sensor networks. In particular, it focuses on the joint scheduling of sensing and retransmission operations to optimize the efficiency of status updates. The authors propose a new scheduling algorithm that balances the trade-off between sensing accuracy and energy consumption by dynamically adjusting the sensing and retransmission intervals based on network conditions. Simulation results demonstrate the effectiveness of the proposed algorithm in terms of reduced energy consumption and improved status update accuracy compared to traditional scheduling approaches. The findings of this study have important implications for the design and deployment of wireless sensor networks in various applications, including environmental monitoring, industrial automation, and healthcare.","This paper looks at status reporting in wireless sensor networks which is not continuous but intermittent. It focuses specifically on joint scheduling of sensing and retransmission operations for high efficiency. Authors suggest a new scheduling algorithm that strikes a balance between sensing quality and energy usage by dynamically changing sensing and retransmission intervals according to network conditions. Results from simulations show effectiveness of this new algorithm compared to conventional scheduling methods. Findings are significant for design and deployment of such networks for diverse applications such as environmental monitoring, industrial automation and healthcare."
"This paper presents a novel approach to cell-free massive multiple-input multiple-output (MIMO) detection using distributed expectation propagation (DEP). Cell-free massive MIMO is a promising technology for achieving high spectral efficiency in wireless communication networks, but its detection algorithms can be computationally expensive and difficult to implement. DEP offers a distributed and computationally efficient solution to this problem by breaking down the detection problem into smaller subproblems that can be solved in parallel by multiple processing units. The proposed approach is evaluated through simulations and compared with other state-of-the-art detection methods. Results show that the DEP approach achieves comparable or better performance than other methods, while also being more efficient in terms of computational complexity and memory usage. Furthermore, the distributed nature of the approach enables it to scale to larger networks with minimal overhead. The paper also discusses potential applications of the DEP approach beyond cell-free massive MIMO, such as in distributed optimization and inference problems. Overall, the results suggest that the DEP approach is a promising method for improving the performance and efficiency of cell-free massive MIMO detection and has potential for broader applications in distributed computation.","This paper introduces a new approach to detect cells in large MIMO systems using distributed expectation propagation (DEP). Large MIMO systems promise high spectral efficiency in wireless networks but detection algorithms are often complex and hard to implement. DEP solves this issue by decomposing detection into smaller tasks which multiple processing elements can work on simultaneously. Performance evaluation through simulations compares this approach favorably to current best methods. Results indicate DEP performs comparably or better while being less computationally intensive and consuming less memory. In addition, because it distributes processing load, this approach scales easily to large networks with low overhead. Discussion also considers potential use beyond cell detection including distributed optimization and inference. Results show DEP is promising both for improving performance and efficiency in cell detection and potentially in other distributed computing contexts."
"This paper proposes a novel approach to address the challenge of object detection with rare classes and slices in the context of targeted active learning using submodular mutual information. The proposed method, called Talisman, is designed to iteratively select informative samples to label, with the aim of improving the performance of object detectors on rare classes and slices. The Talisman framework employs a submodular function to measure the mutual information between the labeled and unlabeled samples, and utilizes this measure to select the most informative samples for labeling. Additionally, the proposed approach incorporates the notion of target classes and slices, which enables the system to prioritize the selection of samples from the rare classes and slices of interest. The proposed method was evaluated on several benchmark datasets, and the results demonstrate the effectiveness of the Talisman framework in improving the performance of object detectors on rare classes and slices. The experimental results also show that the proposed approach outperforms several state-of-the-art active learning methods, especially in the scenarios with rare classes and slices. Overall, the proposed Talisman framework provides a promising approach for targeted active learning in the context of object detection, especially for the challenging scenario of rare classes and slices.","This paper introduces a new way to solve the problem of detecting objects when some types are rare and slices appear rarely. We use targeted active learning and use submodular mutual information. Our solution is called Talisman. It selects important samples iteratively to label aiming at improving detector performance on rare types and slices. Talisman uses a sub modular function to measure mutual information among labeled and unlabeled samples; it uses this measure to choose samples that are most informative. Also this method takes into account target classes and slices, so that the system can give priority to selecting samples from those interesting rare classes and slices. Results show that Talisman improves detector performance on rare types and slices compared to other benchmark datasets. Results also show that our approach performs better than many leading active learning methods especially in scenarios where such rare types and slices appear. Overall, Talisman framework shows promise as a promising approach to targeted active learning for object detection, especially dealing with rare types and slices."
"This paper proposes a novel approach for attribute artifacts removal in geometry-based point cloud compression. The proposed approach utilizes a two-step process, first performing point cloud downsampling using a quadtree-based method, followed by an attribute artifact removal step. The artifact removal step employs a local regression method that uses the spatial and attribute information of neighboring points to estimate and remove the artifact. The effectiveness of the proposed approach is evaluated on several benchmark datasets, and the results show that it outperforms state-of-the-art methods in terms of both objective metrics and visual quality. The proposed approach is computationally efficient and can be easily integrated into existing point cloud compression frameworks. The results of this research have significant implications for the development of more efficient and effective compression techniques for point cloud data.",This paper introduces a new method for removing artifacts from attributes of geometry point clouds through compression. The method uses a two step process: downsampling of point cloud data using a quadtree method and then removal of artifacts. The artifact removal uses local regression and takes advantage of spatial and attribute information of nearby points to estimate and remove artifacts. Results are compared to benchmarks and show improved performance based on both quantitative and qualitative metrics; the processing is efficient and easy to integrate into existing frameworks. Results of this research suggest that new high performance compression techniques are developed for point cloud data.
"This paper presents tight bounds for randomized load balancing on arbitrary network topologies. Load balancing is a fundamental problem in distributed computing, and randomized algorithms are widely used to solve it. However, previous work has only provided loose bounds for the performance of these algorithms on arbitrary network topologies. To address this gap, we propose a new analysis framework that enables us to derive tight bounds for the expected maximum load in a randomized load balancing algorithm. Our framework is based on a novel application of the classic balls-and-bins problem, which allows us to bound the probability of any bin receiving too many balls. We then use this result to derive tight bounds for the expected maximum load, which hold for any network topology. We also present experimental results that demonstrate the effectiveness of our analysis framework. Specifically, we show that our framework can accurately predict the performance of randomized load balancing algorithms on a variety of network topologies, including real-world internet topologies. Our results provide important insights into the performance of randomized load balancing on arbitrary network topologies, and can inform the design and analysis of distributed systems that rely on load balancing.",This paper develops tight bounds for randomized load balancing under arbitrary network topologies. Load balancing is a basic issue in distributed computing and randomized algorithms have been widely applied to resolve it. Previous work has given loose bounds for performance of such algorithms on arbitrary networks. To close this gap we introduce a new analysis framework that lets us derive tight bounds for expected maximum load of randomized load balancers. We use this framework by means of an innovative application of classic balls and bins problem and bound probability that any bin gets too many balls. Using this result we obtain tight bounds on expected maximum load that apply for any topology. Experimental results show effectiveness of our framework. In particular we show that our framework can accurately predict performance of load balancing algorithms on diverse topologies including real internet topologies. Results from this study are very valuable because they shed light on performance of load balancing on arbitrary topologies and can guide system design and analysis relying on load balancing.
"This paper investigates medium chemical reaction in diffusion-based molecular communication in a two-way relaying scenario. Diffusion-based molecular communication is a promising paradigm for communication in nanoscale devices and systems, where the use of electromagnetic waves is not feasible. In this study, a two-way relaying scenario is considered, where two nanomachines exchange information through the diffusion of molecules in a medium. The medium is modeled as a compartmentalized system, where each compartment represents a different environment with different reaction rates. The paper proposes a mathematical framework to model the reaction rates of the medium and the molecular signals, and derives the analytical expressions for the bit error rate (BER) and the capacity of the communication system. The proposed framework is used to analyze the effect of medium reactions on the performance of the communication system. The simulation results demonstrate that the medium reaction significantly affects the BER and the capacity of the system, and the proposed framework can be used to optimize the communication system by adjusting the reaction rates of the medium. The findings of this study can be useful for the design and optimization of diffusion-based molecular communication systems, especially in scenarios where medium chemical reactions are significant.",This paper studies diffusion based molecular communication using relaying between two nanomachines. Diffusion is considered a promising way for communications at very small scales where electromagnetic waves don't work well. In this study we consider relaying in which two nanomachines exchange information via diffusion of molecules in a medium. We model the medium as a compartmental system where each compartment represents distinct environments with different reaction rates. We develop a mathematical framework to model reaction rates within the medium and signal molecules and derive analytical expressions for bit error rate (BER) and capacity of the communication system. Using this proposed framework we investigate how reaction of medium affects system performance. Simulation results show that medium reaction has a big impact on BER and capacity and the framework allows us to optimize performance by tuning reaction rates. Results from this study may be helpful for designing and optimizing systems using diffusion communication especially when chemical reactions of medium matter.
"In this paper, we propose a novel approach to correlated signal inference using free energy exploration. The method involves the estimation of the free energy landscape of a set of correlated signals and the exploration of this landscape to identify the most likely configuration of the signals. The approach is based on the assumption that the free energy is a good proxy for the entropy of the system and that the exploration of the free energy landscape provides a way to infer the underlying structure of the correlated signals. To validate the proposed approach, we apply it to a variety of simulated and real-world data sets and compare its performance to existing methods. The results demonstrate that the free energy exploration approach is effective in inferring the structure of correlated signals and outperforms existing methods in a number of cases. The approach has the potential to be applied to a wide range of problems in signal processing and machine learning, including the analysis of complex systems, the inference of hidden variables, and the discovery of patterns in high-dimensional data.","We present here a new approach to infer correlated signals through free energy exploration. This method estimates the free energy landscape for a set of correlated signals and explores that landscape to find the most probable configuration of those signals. We base our method on assuming that free energy is an adequate approximation for system entropy and that exploration of this landscape leads to inferring underlying structures among correlated signals. To validate our approach we use different data sets both synthetic and real and we compare performances against other methods. Results show that free energy exploration outperforms previous approaches in many cases and infers structure effectively. Our approach has broad application in signal processing and machine learning including analysis of complex systems, inference of hidden variables and pattern discovery in high dimensions."
"As software systems become more complex, ensuring their deployability and reliability has become a critical concern. Record and replay (R&R) techniques have emerged as a promising approach for improving software system deployability by capturing and reproducing execution traces of the system in a controlled environment. However, existing R&R techniques have limitations in terms of scalability, precision, and overhead. In this extended technical report, we present an engineering approach for R&R that addresses these limitations. We first identify the key requirements for engineering R&R systems, including support for multi-tier applications, efficient data collection and storage, and low overhead during recording and replaying. We then present our R&R system architecture, which leverages virtualization and distributed tracing to achieve high scalability and precision. Our system also employs novel techniques to reduce overhead during recording and replaying, including adaptive sampling and selective replay. We evaluate the effectiveness of our R&R system through a series of experiments on a set of real-world applications. Our results demonstrate that our system can capture and reproduce complex execution traces with high accuracy and low overhead, while also providing insights into the behavior of the system under different conditions. We also show that our system can be used to identify and diagnose performance and reliability issues in the system. In conclusion, our engineering approach to R&R provides a practical solution for improving software system deployability and reliability, especially for large-scale, complex systems. Our system can be used by software engineers and developers to diagnose and debug issues in the system, and to ensure that the system behaves as expected in different environments. Future work will focus on integrating our R&R system with existing software development and testing workflows, and exploring new applications for R&R in areas such as security and compliance.","With increasing complexity of software systems, reliability and deployability are becoming critical issues. There has been emerging promise in using record and replay (R&R) approaches to improve system reliability by recording and reproducing execution traces in a controlled environment. However, existing methods face challenges related to scalability, precision and overhead. This technical report introduces an engineering approach for R&R that overcomes these limitations. First, it identifies critical requirements for engineering R&R systems such as support for multi tier systems, efficient collection and storage of data, and low overhead for recording and replay. Then we present our system architecture that uses virtualization and distributed tracing to achieve high scalability and precision. We also introduce new techniques to minimize overhead such as adaptive sampling and selective replay. Experiments conducted on real world applications have demonstrated effectiveness of our approach: our system captures and reproduces complex execution traces accurately and efficiently along with giving insight into system behavior under various conditions. Results also show that this system is useful for identifying and diagnosing performance and reliability problems. In summary, our engineering approach for R&R provides a practical way to improve deployability and reliability of software systems especially for large and complex ones. The system facilitates debugging and testing tasks by software engineers and developers; future research will integrate this R&R system into typical workflow and explore further applications in fields like security and compliance."
"This paper examines the effectiveness of two common heuristics for solving vehicle routing problems (VRPs): sequence optimization and set optimization. Sequence optimization involves optimizing the order in which customers are visited by each vehicle, while set optimization involves partitioning the set of customers into clusters that are visited by different vehicles. To compare the two approaches, we conducted a series of computational experiments on a set of benchmark instances of VRPs. We tested both heuristics using a variety of problem sizes and configurations, and evaluated their performance based on solution quality, computational time, and scalability. Our results show that the choice of heuristic depends on the specific characteristics of the problem. For small to medium-sized instances with relatively homogeneous customer demands, sequence optimization tends to be more effective, while for larger and more complex instances with heterogeneous demands and multiple depots, set optimization outperforms sequence optimization. However, we also found that a hybrid approach combining both heuristics can be effective for certain types of VRPs. Overall, this study provides insights into the strengths and weaknesses of different heuristics for VRPs, and offers guidance for practitioners and researchers in selecting the most appropriate approach for a given problem.","This paper studies effectiveness of two main methods to solve Vehicle Routing Problems: sequence optimization and set optimization. Sequence optimization means optimizing the order in which stops are made at various customers by each vehicle; set optimization means grouping customers into clusters that different vehicles will visit. Comparisons between these methods are made through conducting a series of computational tests using benchmark instances of VRP. Different problem sizes and configurations were tested using different combinations of methods, and performance is judged by quality of solutions, computation times and scalability. Results indicate that effectiveness of one method over another depends largely on specific features of the problem. For smaller and medium sized instances where demands among customers are similar, sequence optimization performs better; for larger and more complex ones with varied customer demands and multiple depot locations, set optimization excels. However, mixed methods combining both approaches are also effective for some cases. Overall findings provide insight into relative advantages and disadvantages of different methods for VRP and suggest useful guidelines for professionals and researchers in VRP selection."
"This paper presents a novel approach to the task of interpretable visual question answering (VQA) by leveraging dependency trees to encode visual and textual information. The proposed method uses a graph-based representation of the dependencies between words in the question and image features to perform reasoning. The experiments demonstrate that the dependency trees effectively capture the relationships between the visual and textual inputs, leading to improved performance on VQA benchmarks compared to state-of-the-art methods. Furthermore, the interpretability of the approach is demonstrated by visualizing the reasoning process in the form of the dependency trees. The results of this study provide a new direction for developing interpretable VQA systems that can explain their predictions to human users.","This paper introduces a new way to answer questions about images by using dependency trees to combine information from both visuals and language. This new method uses graphs based on dependencies among words in the question and also incorporates features of the image. Through reasoning using these elements, the proposed technique performs better than top methods on benchmark tasks for visual question answering. Results show that this system also explains its results clearly by using visualizations of dependency trees. Overall this research points toward new ways to develop systems that are transparent and can communicate their decisions to humans."
"This paper proposes a low-complexity full-diversity detection scheme for multi-user multiple-input multiple-output (MIMO) multiple-access channels. The proposed scheme is designed to achieve full-diversity gain without sacrificing the low-complexity advantage of conventional detection schemes. Specifically, the scheme exploits the inherent structure of the channel matrix and performs iterative detection to achieve full-diversity gain with low complexity. The performance of the proposed scheme is evaluated through simulations and compared with conventional detection schemes in terms of bit error rate (BER) and computational complexity. The simulation results show that the proposed scheme achieves full-diversity gain and outperforms conventional detection schemes in terms of BER, while maintaining low computational complexity. The proposed scheme can be a promising solution for practical MIMO communication systems that require high data rate and low computational complexity.","This paper introduces a new detection method for MIMO multiple access channels that combines low complexity with full diversity gain. It exploits structural features of the channel matrix and uses iterative detection to achieve this. Performance is assessed via simulations comparing this proposed method against traditional detection methods using metrics like Bit Error Rate (BER) and computational load. Results show this new scheme achieves full diversity benefits along with better BER performance compared to current methods; meanwhile, its computational load remains very low. Proposed method has promise as an important solution for high speed communication systems which need low computational cost."
"This paper investigates the phenomenon of asymmetric evaluations of erasure and undetected error probabilities in digital communication systems. Specifically, the study focuses on the ways in which users tend to perceive and evaluate these two types of errors differently, and the factors that contribute to such differences. The paper begins by introducing the concept of erasure and undetected errors, and explaining the significance of these errors in digital communication systems. It then reviews existing literature on user perceptions of these two types of errors, and highlights the lack of consensus in the field. The research methodology involves conducting a series of experiments that assess participants' evaluations of erasure and undetected error probabilities in a simulated digital communication environment. The study collects both quantitative and qualitative data, including measures of perceived severity, subjective probability estimates, and verbal explanations of participants' evaluations. The results of the experiments indicate that users tend to evaluate erasure and undetected errors differently, with undetected errors generally being considered more severe and having a greater impact on user trust. The study also identifies several factors that contribute to these differences, including the role of system design, user experience, and individual differences in risk perception. Overall, this paper contributes to our understanding of how users evaluate errors in digital communication systems, and provides insights for designers and engineers to create more effective and trustworthy systems.","This paper looks at why people judge differently when it comes to errors called erasure and undetected errors in digital communication systems. It focuses specifically on why people perceive and appraise these different kinds of errors differently and what makes them do so. The paper starts by defining erasure and undetected errors and explains their importance in digital communication systems. Next, it reviews previous work about how people judge these errors and highlights that there is no consensus among researchers. Methodology includes experiments where people rate the severity of probabilities of erasures and undetected errors in a simulation of digital communication. Both quantitative measures (like severity ratings) and qualitative information (such as estimated probabilities and reasons for judgment) are collected from participants. Results show that people usually judge undetected errors more harshly and they think that they affect trust more strongly. Factors influencing this distinction include system design, users' experience, and personal differences regarding risk perception. Overall this paper deepens our understanding of how we judge errors in digital communication and offers guidance for designers and engineers to build better systems."
"The abandoned side of the internet refers to domain names that have expired and are no longer in use. This paper investigates the phenomenon of hijacking these expired domain names and the associated internet resources. The research focuses on the motivations and methods used by hijackers to seize control of these abandoned assets, as well as the potential consequences for the previous owners and the internet as a whole. The results of the study suggest that the hijacking of expired domain names is a growing problem, with a significant impact on the stability and security of the internet. The paper concludes by offering recommendations for policy-makers and domain registrars to address this issue and protect the rights of domain name owners.",Expired domain names are referred to as the neglected side of the internet. This paper looks into the practice of taking over these domain names along with the related online resources. We investigate reasons and tactics used by those who take over these abandoned assets and the possible consequences for previous owners and the internet at large. Results from our research show that hijacking of such domains is becoming a growing concern; it poses a significant risk to internet stability and security. We end with suggestions for policymakers and registrars to deal with this issue and safeguard domain owner rights.
"This paper proposes a novel approach for anomaly detection using multi-criteria similarity-based techniques and Pareto depth analysis. The proposed method addresses the limitations of existing anomaly detection techniques by considering multiple similarity criteria and analyzing data points' Pareto depth scores. Pareto depth is a measure of how far a data point is from the set of non-dominated points, representing the best trade-offs between different criteria. By using Pareto depth analysis, the proposed method identifies anomalies that deviate significantly from the best trade-offs and provides a better understanding of the trade-offs among different criteria. The effectiveness of the proposed method is evaluated using several real-world datasets, and the results demonstrate its superiority over existing state-of-the-art anomaly detection techniques. The proposed approach can be used in various domains, such as finance, healthcare, and cybersecurity, where detecting anomalies is crucial for ensuring system integrity and security. The research provides a valuable contribution to the field of anomaly detection, and the proposed method has the potential to be integrated into various real-world applications.","This paper introduces a new method for anomaly detection using multiple similarity criteria and Pareto depth analysis. It tackles shortcomings in current methods by looking at many criteria and examining scores of Pareto depth. Pareto depth measures how far away a data point is from other points that represent good tradeoffs among different attributes. Through Pareto depth analysis this new method finds anomalies that stand out significantly from those good tradeoffs and clarifies the tradeoffs among different attributes. Performance of this new method is evaluated on various real data sets and results show higher performance compared to leading methods. This approach works in diverse fields such as finance, healthcare, and cybersecurity and thus detection of anomalies is critical to system integrity and security. Results of this study contribute to anomaly detection research and this new method could be incorporated into practical applications."
This paper proposes a deep reinforcement learning approach to the joint optimization of spectrum and energy efficiency in C-V2X communication networks while considering the security aspect. The study takes a novel approach by utilizing a deep reinforcement learning algorithm to solve the complex optimization problem and provides an optimal solution that balances the trade-off between spectrum and energy efficiency while enhancing the security of the network. The results of the simulation experiments demonstrate the effectiveness and superiority of the proposed approach compared to the traditional optimization methods. The findings of this study provide valuable insights into the joint optimization of spectrum and energy efficiency with security considerations in C-V2X communication networks.,This paper introduces a deep reinforcement learning approach for joint optimization of spectrum and energy efficiency alongside security in C-V2X communication networks. Novel use of deep reinforcement learning is employed here to address complex optimization problems; resulting in high quality solutions which balance spectrum and energy efficiency and strengthen network security. Results from simulation experiments show the proposed method performs better than previous optimization approaches. Insights regarding joint optimization with security are also provided by this research.
"This paper presents a multi-class classification model for vulnerabilities in smart contracts using an Attention-based Bi-directional Long Short-Term Memory (AWD-LSTM) network with a pre-trained encoder inspired by natural language processing. The goal of this study is to address the challenge of accurately identifying different types of vulnerabilities in smart contract code, which is crucial for ensuring the security and reliability of decentralized applications. The model uses a pre-trained encoder from natural language processing to extract features from smart contract code and an AWD-LSTM network for classification. The results of the experiments show that the proposed approach outperforms existing methods and provides a promising solution for multi-class classification of vulnerabilities in smart contracts.",This paper introduces a model for classifying different kinds of vulnerabilities in smart contracts using an Attention based Bidirectional LSTM (AWDLSTM) network with an encoder based on natural language processing that has been pre trained. The aim here is to effectively identify various types of vulnerabilities so that decentralized applications can be secure and dependable. Using an encoder from natural language processing to extract features from smart contract code and using the AWDLSTM network for classification are employed by the model. Results of testing demonstrate that the proposed method surpasses previous approaches and offers a promising solution for classification of vulnerabilities in smart contracts.
"Adaptive motion planning is a critical aspect of autonomous systems, particularly in robotics. This paper presents a new approach to motion planning using artificial potential fields and a prior path. The method utilizes a combination of a potential field-based algorithm and a prior path to overcome the limitations of traditional potential field methods, such as local minima and slow convergence. The proposed approach is evaluated using simulations and experiments with a real-world robotic system, demonstrating improved performance compared to traditional potential field methods. The results suggest that the use of a prior path can enhance the performance of potential field-based motion planning algorithms, making them more suitable for real-world applications.",Adaptive motion planning is important for autonomous systems especially robots. We present a new approach to motion planning using artificial potentials and prior paths. This method combines an algorithm based on potentials with a previous path to overcome limitations of traditional potential fields such as local minima and slow convergence. Performance is compared using simulations and real robot experiments and this new approach shows better performance compared to traditional potential field approaches. Results show that using prior paths can improve performance of planning algorithms based on potentials and make them more fit for practical use.
"The paper provides a review of Ultra-Reliable Low Latency Communications (URLLC) for Massive Machine Type Communications (MMTC) with the aim of enabling critical MMTC applications. The paper first provides an overview of MMTC and its requirements, including low latency, high reliability, and massive scalability. The paper then focuses on URLLC as a solution to meet the stringent requirements of critical MMTC applications, such as autonomous vehicles and industrial automation. The paper presents the key concepts and design principles of URLLC, including reliability and latency enhancement techniques, channel coding and modulation schemes, and network architecture. The paper also discusses the challenges and limitations of implementing URLLC in MMTC systems and provides insights into the future direction of research and development in this area. The paper concludes by summarizing the main findings and providing recommendations for further research.","This paper reviews Ultra Reliable Low Latency Communications (URLLC) targeting critical applications of Massive Machine Type Communications (MMTC). It starts by introducing the basics of MMTC along with important demands like low latency and high reliability. Then it concentrates on how URLLC serves to satisfy very stringent demands of important MMTC tasks like autonomous driving and industrial automation. The authors discuss key features and design principles of URLLC including reliability and latency improvements, coding and modulation techniques, and network architecture. Challenges in deploying URLLC in MMTC systems are discussed and there is insight into future research directions in this field. Finally the authors summarize results and suggest future lines of research."
"This paper presents a new approach to cooperative verification by using collective invariant generation. The method is based on the idea that multiple agents can work together to generate invariants, or mathematical representations of the desired properties of a system, which can then be used to verify that the system behaves as expected. The authors show that this approach is more effective than traditional verification methods, as it allows for a more thorough exploration of the system's behavior and can find errors that would be missed by other methods. The results of the study demonstrate the effectiveness of the approach in a variety of real-world cases and highlight its potential for widespread adoption in the field of verification.",This paper introduces a new approach to cooperative verification using collective generation of invariants. The method relies on the idea that many agents working together are able to produce mathematical descriptions of what the system should do. These invariants can be used to verify that the system functions correctly. Results show that this method performs better than previous approaches because it thoroughly explores system behavior and finds errors overlooked by other methods. Results from experiments show effectiveness across diverse practical examples and emphasize that this approach has broad applicability within verification domain.
"This paper presents a novel stereo camera visual simultaneous localization and mapping (SLAM) approach for outdoor construction sites containing large dynamic objects. The proposed method employs hierarchical masking to exclude the dynamic objects from the SLAM estimation process and improve accuracy. The masking process is performed in multiple stages, starting with a coarse mask and refining it through subsequent stages to obtain a more precise mask. Additionally, the paper introduces a motion-state classification module to distinguish between static and dynamic objects and to improve the robustness of the masking process. The effectiveness of the proposed method is demonstrated through extensive experimentation in real-world outdoor construction scenarios. The results show that the proposed method outperforms the state-of-the-art visual SLAM approaches in terms of accuracy and robustness in the presence of large dynamic objects.",This study introduces a new approach to simultaneous localization and mapping using stereo cameras for outdoor construction sites where there are large moving things. Proposed here is an approach which uses hierarchical masking to ignore dynamic objects and improve precision. Masking occurs through different stages starting with a coarse mask and refining it further to get a better mask. Also introduced is a module that classifies object states to distinguish static objects and enhance robustness of masking. Effectiveness of this method is shown by extensive testing on real outdoor construction site scenarios; results show performance advantage over current top methods in terms of accuracy and robustness regarding big moving stuff.
"This paper investigates the concept of ""different strokes for different folks"" in the context of pre-training approaches for diverse dialogue tasks. The authors examine various pre-training methods and evaluate their effectiveness on different types of dialogue tasks. The results show that there is no one-size-fits-all solution, and that different pre-training approaches are better suited for different types of dialogue tasks. The authors conclude that it is important to carefully consider the specific characteristics of each task when choosing a pre-training approach, in order to achieve optimal results.","This paper looks at how ""different things work for different people"" applies to different kinds of dialogue tasks by investigating different pre training methods. They test effectiveness of these methods on various dialogue tasks and find that there is no best fit method that works equally well for everything. Different methods work better depending on what kind of task you have. Researchers conclude that careful consideration of specific task characteristics is necessary when selecting a method of pre training to get top performance."
This paper presents a newton-type method for the optimal control of switched systems that leverages the structure of the systems to improve computation efficiency. The proposed method is compared with existing methods and shown to produce solutions with higher accuracy and faster convergence. The results demonstrate the effectiveness of the structure-exploiting newton-type method in addressing the challenges associated with the optimal control of switched systems and offer promising directions for future research in this area.,This paper introduces a Newton type method for switching system optimal control that exploits system structure to increase computational efficiency. Results compare favorably against other methods and show greater accuracy and faster convergence. Results indicate effectiveness of exploiting structure using Newton type method and suggest promising directions for future research into this problem.
"In this paper, we explore the relationships between Artin automorphisms, cyclotomic function fields, and folded list-decodable codes. Artin automorphisms, also known as Artin maps, are certain algebraic maps that play a crucial role in number theory and algebraic geometry. On the other hand, cyclotomic function fields are algebraic extensions of the field of rational numbers, obtained by adding the roots of unity to the field. Finally, folded list-decodable codes are a class of error-correcting codes with specific properties, widely used in the field of communication engineering. We demonstrate that Artin automorphisms can be applied to cyclotomic function fields to obtain folded list-decodable codes with improved performance compared to previously known methods. Our results suggest that a deeper understanding of Artin automorphisms and their applications can lead to new advances in the design of efficient error-correcting codes. Our findings have potential applications in various fields, such as cryptography, communication engineering, and computer science. The paper is accompanied by numerical simulations that validate our results and highlight the potential of our approach. Our study opens up new avenues for further research and provides a valuable contribution to the fields of number theory, algebraic geometry, and coding theory.","This paper explores connections among Artin automorphisms, cyclotomic function fields and folded list decodable codes. Artin automorphisms are important algebraic functions central to number theory and algebraic geometry; cyclotomic function fields are algebraic extensions of rationals obtained by adding roots of unity. Also, folded list decodable codes are special kinds of error correcting codes with desirable properties and they are extensively used in communication engineering. We show how one can use Artin automorphisms on cyclotomic function fields to produce codes that outperform previous approaches. Results indicate that better understanding of these automorphisms could lead to new advances for code design. Our work has practical implications in cryptography, communication engineering and computer science. Numerical simulations support results and point to advantages of the approach. This study paves new ways for future research and makes an important contribution to number theory, algebraic geometry and coding theory."
"This paper explores the need for traceability in neuroimaging analyses and proposes a solution for ensuring the robustness and reproducibility of results. The increasing complexity and heterogeneity of neuroimaging data pose significant challenges for researchers, including the potential for errors, inconsistencies, and incomplete documentation of data processing and analysis steps. The paper argues that a traceable and transparent workflow is crucial for maintaining the integrity and reliability of neuroimaging results, and outlines a comprehensive approach to achieve this goal. The proposed solution involves the integration of existing tools and best practices from the fields of software engineering and data management, with a focus on automation, version control, and metadata tracking. The paper provides a detailed evaluation of the proposed solution and its potential benefits, including increased efficiency, reduced risk of errors, and improved collaboration and sharing of results. The findings of this research have important implications for the neuroimaging community and contribute to the broader efforts to improve the reliability and transparency of scientific research.","This paper looks into how important traceability is for neuroimaging analysis and offers a way to ensure high quality results. With increasing complexity and diversity in imaging data, there are many difficulties for researchers like potential mistakes and inconsistency in data handling and analysis. The study asserts that having a traceable and transparent process is key to maintain integrity and reliability of results and describes an integrated approach towards achieving that. This approach integrates tools and best practices from software engineering and data management with emphasis on automation, version control and metadata tracking. Detailed evaluation is also given of this solution along with advantages such as higher efficiency, lower error risk and better collaboration sharing results. Results of this work have significant implications for neuroimaging community and support broader efforts to enhance reliability and transparency of research."
"This paper presents a study on resolving multi-party privacy conflicts in social media. Social media platforms, with their vast user base and rich personal data, have become a prime target for privacy violations. As such, there is a growing need for effective mechanisms to resolve multi-party privacy conflicts in these platforms. The paper proposes a privacy conflict resolution framework that considers the privacy preferences of multiple parties involved in a conflict. The framework combines game theory, computational social choice, and privacy-enhancing technologies to reach a mutually acceptable solution. The paper also provides an experimental evaluation of the proposed framework and demonstrates its efficacy in resolving multi-party privacy conflicts in a realistic social media setting. The findings of the study contribute to the development of privacy-preserving social media platforms and offer practical insights for practitioners and policymakers in the field.","This paper studies ways to resolve privacy conflicts among different users in social media. Because these platforms collect large amounts of personal information from millions of users and are therefore targets for privacy violations, there is now a strong need for good methods to handle conflicts between different parties. The research introduces a framework for handling privacy conflicts which considers privacy preferences of people involved. It integrates ideas from game theory, computational social choice and privacy enhancing technology to find a common solution. Experimental evaluations show that this proposed framework works well and can resolve conflicts realistically within a typical social media environment. Results from this research will help build privacy preserving social media sites and give useful guidance to practitioners and policymakers."
"The study of learning and inferring relations in cortical networks is a crucial aspect of understanding the workings of the brain. In this paper, we present a new computational framework that models the process of learning and inference of relations in cortical networks. Our approach is based on the theory of deep learning, which uses artificial neural networks to analyze large amounts of data. The model is trained on a dataset of sensory inputs and corresponding target outputs to learn the relationship between these inputs and outputs. Our results demonstrate that the proposed framework is capable of accurately inferring the underlying relationships in cortical networks, providing a new avenue for investigating the functioning of the brain. The findings of this study have important implications for the development of new artificial intelligence systems and the advancement of our understanding of brain function.",Learning and inference of relations within cortical networks is critical for understanding brain functioning. This paper presents a novel computational framework that models this process using deep learning theory. The framework uses artificial neural networks to study large datasets of sensory input and associated output targets. Through training on this dataset the framework learns the relationship between inputs and outputs. Results show that this new framework can reliably infer underlying relationships within cortical networks and opens new avenues for further investigation into brain function. Results also have significant implications for developing new AI systems and advancing knowledge about brain functions.
"This paper examines the use of neologisms (newly created words or phrases) on Facebook. The study analyzed a large corpus of data collected from Facebook posts and comments to identify the most common neologisms and their usage patterns. The results showed that neologisms on Facebook are primarily used to express emotions, create humor, and emphasize a sense of belonging among users. The study also found that neologisms are more commonly used by younger users and that they spread quickly through the platform due to its high level of connectivity. This research sheds light on the role of social media in shaping language and highlights the importance of monitoring the development of neologisms in this context.",This paper looks into the use of new words and phrases on Facebook. It collected lots of data from posts and comments for analysis and identified the most frequent ones along with their usage patterns. Results show that on Facebook such terms are mainly used to convey emotion and to be funny and to express feeling of belonging. Findings also indicate that young people use these new terms more frequently and that they spread quickly because of high connectivity. This study brings light to the role of social media in shaping language and underscores importance of tracking development of new terms in this context.
"This paper presents a parallel algorithm for counting triangles in networks with large degrees. The proposed algorithm leverages the parallel processing capabilities of modern computing systems to efficiently handle the high degree nodes in the network, which are a major bottleneck in traditional triangle counting algorithms. The algorithm is designed to be scalable, allowing for the processing of large-scale networks in a reasonable amount of time. The experimental results show that the proposed algorithm significantly outperforms existing triangle counting methods in terms of speed and accuracy. The results demonstrate the effectiveness of the parallel approach for tackling the problem of triangle counting in networks with large degrees and provide new insights into the design of efficient algorithms for graph analysis.","This paper introduces an algorithm that performs well on large networks by exploiting parallel processing capabilities of modern computers. It targets nodes with very high degree, which are problematic when using traditional methods to count triangles. The authors designed their method so that it scales up and processes big data quickly. Experimental results show the proposed method runs faster and better than other methods at triangle counting. Results show effectiveness of parallel processing and open new avenues for designing efficient algorithms for graph analysis."
"This paper presents a novel approach to understanding the causal structure of a system through a categorical semantic framework. The framework utilizes category theory, a branch of mathematics, to define the relationships between variables in a system and the effects of changing one variable on another. The central contribution of the paper is the introduction of a categorical causal model that provides a way to reason about the causal relationships between variables in a compact and intuitive way. The paper also presents several examples to demonstrate the usefulness of the categorical causal model in different domains, including physics, biology, and social sciences. The results show that the categorical semantic framework provides a powerful tool for formalizing and reasoning about causal relationships in a clear and consistent manner.","This paper introduces a new method for understanding causal structures via a semantic framework based on category theory. Category theory is a mathematical discipline used here to define relationships among variables and effects resulting from changes in these variables. The core contribution of this paper is the introduction of a categorical causal model that enables reasoning about causal relations among variables concisely and intuitively. The authors provide various case studies to show how useful this categorical causal model is across diverse fields such as physics, biology and social sciences. Results show that using the semantic framework based on category theory is an effective way to formalize and reason clearly and consistently about causal relationships."
"This paper explores the process of scaling out acid applications with operation partitioning. Acid (Atomicity, Consistency, Isolation, Durability) is a crucial characteristic for database transactions, ensuring that operations are performed in a secure and reliable manner. However, as databases grow in size and complexity, it becomes increasingly difficult to maintain Acid properties. This paper proposes the use of operation partitioning as a means of scaling out acid applications, allowing for improved performance and increased reliability. The methodology of this research involves the implementation of operation partitioning in a real-world acid application, evaluating its impact on various performance metrics. The results demonstrate that operation partitioning is a promising approach for scaling out acid applications, offering improved performance and increased reliability without sacrificing the essential Acid properties.","This paper looks at scaling up acid transactions by partitioning operations. Acid refers to atomicity, consistency, isolation and durability which are essential for transaction security and reliability. As databases grow larger and more complex, it becomes hard to maintain these qualities. This paper suggests using operation partitioning as a method to scale up acid transactions and thus improving performance and reliability. Methodology involves implementation of partitioning in a real application and evaluation of results for different metrics. Results show that partitioning operations is promising; it increases performance and reliability while maintaining fundamental Acid features."
"This paper presents a novel approach for efficiently handling congestion in RDMA-based datacenter networks. The approach, called Dart, divides the network into multiple subnets and specializes each subnet to handle specific types of traffic. By doing so, Dart can quickly respond to congestion and reduce the latency of critical applications. The evaluation of Dart shows that it outperforms state-of-the-art congestion control methods in terms of both throughput and fairness, while maintaining low latency for real-time applications. The results of this research demonstrate that Dart is a promising solution for addressing congestion in RDMA-based datacenter networks.","This paper introduces a new way to manage congestion in networks using Remote Direct Memory Access (RDMA). Called Dart, this method splits the network into smaller subnets and assigns each subnet to deal with different kinds of traffic. Dart reacts quickly to congestion and reduces latency for important applications. Results show Dart performs better than leading methods at both throughput and fairness; it also maintains low latency for real time apps. Results indicate Dart is promising as a solution for managing congestion in RDMA data center networks."
"This paper presents a novel approach for image classification and semantic segmentation using structured binary neural networks. The proposed method uses binary weights and activations, reducing the memory and computational requirements while maintaining high accuracy. The paper demonstrates that this structure enhances the accuracy of image classification compared to traditional neural networks with floating-point weights and activations. Additionally, the method is extended to semantic segmentation tasks and showed significant improvement over existing methods. The results of extensive experiments on benchmark datasets confirm the efficacy of the proposed method. The structured binary neural network approach provides a promising solution for accurate image classification and semantic segmentation with efficient computational and memory requirements.","This paper introduces an innovative approach for both classification and semantic segmentation using structured networks which use only binary weights and activations. Compared to networks with floating point weights and activations, this new method reduces memory and computational load and maintains high accuracy. Results show that this specific structure improves classification performance. This work also extends this approach to segmentation tasks and demonstrates great improvements over current methods. Experiments conducted on benchmark datasets validate effectiveness of this new method. Overall, this structured binary neural network offers a promising way to achieve accurate classification and segmentation at low computational cost."
"In this paper, we propose a novel method for combinatorial Bayesian optimization that leverages the graph cartesian product to address the challenge of optimizing high-dimensional, discrete, and non-separable functions. We demonstrate that our method can efficiently navigate the search space and achieve superior performance compared to existing algorithms on a range of benchmark problems. Additionally, our method is shown to be scalable to large datasets, making it suitable for real-world applications in a variety of domains. The results of this research contribute to the growing field of Bayesian optimization and have potential implications for a wide range of optimization problems in machine learning and other areas.","This paper introduces a new approach to Bayesian optimization for combinatorics which uses the Cartesian product of graphs to solve challenges related to high dimensional spaces, discrete elements and lack of separability. We show that our method excels at navigating through the search space and outperforms other methods on a broad set of benchmark problems. In addition, scalability to big data sets is demonstrated and thus the method is applicable to many different real world domains. Results of this work are important contributions to the growing community of Bayesian optimization and have broader relevance to diverse optimization tasks in machine learning and elsewhere."
"This paper focuses on the problem of energy plan selection in energy markets with retail choice. The authors introduce two competitive online algorithms for energy consumers to make decisions about whether to stay with their current energy plan or switch to a different one. The first algorithm, called ""Stay-or-Switch"", takes into account the historical energy consumption and costs of the consumer to determine the optimal strategy. The second algorithm, called ""Adaptive Stay-or-Switch"", adapts to changing market conditions and adjusts the energy plan selection accordingly. The performance of these algorithms is evaluated using simulations based on real-world energy market data. The results show that both algorithms outperform existing approaches and can lead to significant cost savings for energy consumers.","This paper looks at how to select an energy plan in markets where consumers have retail choice. Authors introduce two competitive algorithms for consumers to decide whether to continue with their current plan or to switch to a different one. One algorithm called ""Stay or Switch"" uses history of energy use and costs to find best strategy. The other called ""Adaptive Stay or Switch"" learns from changes in the market and adjusts plan selection. Performance of these algorithms is evaluated via simulations using real market data; results show both perform better than others and save consumers money."
"Iterative optimization heuristics (IOHs) are a class of optimization algorithms that are widely used to solve complex problems in various domains. The performance and scalability of these algorithms are critical to their success, and benchmarking and profiling are essential tools for evaluating and improving IOHs. This paper presents iohprofiler, a new benchmarking and profiling tool for IOHs that is designed to meet the specific requirements of this class of algorithms. iohprofiler provides a comprehensive set of benchmark problems, performance metrics, and profiling tools to enable researchers and practitioners to evaluate and improve the performance and scalability of IOHs. The paper provides an overview of the design and implementation of iohprofiler, and presents experimental results that demonstrate its effectiveness in benchmarking and profiling a variety of IOHs. These results highlight the benefits of using iohprofiler for evaluating and improving the performance of IOHs and make it a valuable tool for researchers and practitioners in the field.","Heuristics for iterative optimization (IOHs) are a class of algorithms widely employed to deal with complex issues across different domains. Performance and scalability are key to IOH success; therefore benchmarking and profiling are important means for assessing and enhancing performance. We introduce here iohprofiler as a new benchmark and profiling tool specifically designed to meet demands of this algorithm class. iohprofiler supplies a broad range of benchmark problems, performance measures and profiling tools to facilitate researchers and practitioners to assess and enhance performance and scalability of IOHs. The paper introduces the design and implementation of iohprofiler and reports experimental results showing effectiveness at benchmarking and profiling different IOHs. Results show advantages of using iohprofiler for performance evaluation and improvement and thus it becomes very useful for researchers and practitioners working on this field."
"This paper examines the evolution patterns of groups in running races. Using data collected from various races, we analyze the changes in group size and composition over time, as well as the factors that contribute to the formation and dissolution of these groups. We find that groups tend to form naturally during the early stages of a race, and that their size and composition are influenced by factors such as pacing strategies, social dynamics, and environmental conditions. Furthermore, we observe that groups tend to break apart as the race progresses, with individual runners assuming their own pace and strategy. Our findings shed light on the complex dynamics of group behavior in endurance events, and have implications for the design of race courses and training programs.","This paper studies how groups evolve during running races. We use data from different races to look at changes in group size and composition through time and at what causes groups to be formed and dissolved. We find that groups typically form quickly at the start of a race; they are shaped by pacing strategies, social interactions and environmental conditions. We also see that groups disintegrate as a race continues and runners adopt independent paces. Results indicate important dynamics among group behaviors in long events and suggest implications for designing course layouts and training plans."
"This paper proposes a novel approach for music artist classification using Convolutional Recurrent Neural Networks (CRNNs). The model is designed to extract both temporal and spectral features from raw audio signals, allowing it to capture both short-term and long-term patterns in the audio data. We evaluate our model on a large-scale music dataset and compare it to state-of-the-art methods. Our results show that the proposed CRNN-based approach achieves superior performance in terms of accuracy and efficiency, demonstrating the potential of deep learning techniques for music classification tasks.",This study introduces a new method for classifying artists based on music using Convolutional Recurrent Neural Networks (CRNNs). The system extracts features both temporally and spectrally directly from raw audio signals. This allows capturing fine short and long term patterns in the audio data. Performance is evaluated against large datasets and compared to leading methods; results indicate that the proposed model using CRNNs performs very well in terms of accuracy and speed. Results also show great promise for applying deep learning for artist classification tasks.
"This paper explores the prevalence and functional roles of heterogeneous network motifs in complex systems. Network motifs are recurring patterns of interconnections between nodes in a network, and they have been extensively studied in the context of homogeneous motifs where all nodes in the motif have the same functional role. However, many real-world networks are heterogeneous, with nodes having different functional roles and properties. In this paper, we introduce a method for identifying heterogeneous motifs in networks and analyze their prevalence across a range of real-world systems, including biological, social, and technological networks. Our analysis reveals that heterogeneous motifs are significantly more prevalent than homogeneous motifs in many real-world systems, and they play important functional roles in information processing and regulation. We find that many heterogeneous motifs are involved in modular organization, signal propagation, and regulatory feedback loops. Moreover, we demonstrate that the same heterogeneous motifs can be found in multiple systems, suggesting that they represent fundamental building blocks of complex networks. Our findings have important implications for the design and understanding of complex systems. By considering the functional roles of heterogeneous motifs, we can gain insights into the mechanisms that underlie the behavior of real-world systems, and develop more effective strategies for controlling and manipulating these systems. Furthermore, our method for identifying heterogeneous motifs can be applied to a wide range of networks, enabling a deeper understanding of the complex interplay between structure and function in complex systems.","This paper investigates how common and what roles heterogeneous motifs play within complex systems. Motifs are specific connections among system components, and they have been widely studied in homogeneous motifs in which all components have the same function. But most real world networks are heterogeneous; nodes have diverse functions and characteristics. Here we introduce a method for finding heterogeneous motifs and study their prevalence in diverse real systems such as biology, society and technology. Analysis shows that heterogeneous motifs appear much more frequently than homogeneous ones in many systems. They are crucial for processing and regulating information. We also find that many motifs are related to modular organization, propagation of signals and feedback loops. We also show that the same motifs appear in many systems suggesting that they are fundamental building blocks of complex networks. Results have major implications for designing and understanding complex systems. Considering the function of heterogeneous motifs leads to insight into mechanisms that underlie system behavior and developing better strategies for manipulation and control of systems. Moreover this method for finding motifs works on a broad range of networks thereby enhancing understanding of the intricate connection between structure and function in complex systems."
"This paper proposes a novel approach for bidirectional updates on fine-grained medical data using blockchain technology. Traditional methods of updating medical records are often time-consuming, error-prone, and lack transparency. The proposed system aims to address these issues by leveraging the security and immutability properties of blockchain to enable efficient and transparent bidirectional updates on medical data. The system architecture consists of a blockchain network, a smart contract, and a user interface. The smart contract defines the rules for updating medical data, including the conditions for granting access and the types of updates allowed. The user interface provides an intuitive and user-friendly way for patients and healthcare providers to interact with the system and update the medical records. To ensure privacy, the system implements a fine-grained access control mechanism that allows patients to grant different levels of access to different healthcare providers based on their roles and responsibilities. Additionally, the system employs data encryption and decryption techniques to protect sensitive medical data. We evaluated the proposed system by implementing a proof-of-concept prototype and conducting a series of experiments to measure its performance and security. The experimental results demonstrate that the system is highly efficient and can handle a large number of transactions per second. Moreover, the system is highly secure, and its immutability properties ensure that the medical data is tamper-proof. In conclusion, our proposed blockchain-based bidirectional updates system provides an innovative solution for updating medical records efficiently and transparently. The system's security and privacy features make it suitable for deployment in real-world healthcare settings, and its scalability and performance characteristics ensure that it can handle a large volume of transactions.","This paper introduces a new approach to bidirectional updates on detailed medical records through blockchain technology. Typical ways of updating medical information are slow, prone to errors and not transparent. This new method aims to overcome these shortcomings by making use of security and immutability attributes of blockchain to enable fast and transparent updates of medical data. The system architecture includes a blockchain network, smart contract and user interface. Smart contracts define rules for updating medical information including conditions for granting access and types of updates permitted. User interface makes interaction between patients and providers easy and comfortable. Privacy is ensured through fine granular access control allowing patients to grant varying levels of access depending on provider roles and responsibilities. Encryption and decryption are also used to protect sensitive data. Performance and security were evaluated by implementing a prototype and conducting experiments. Results show high efficiency and scalability with ability to process many transactions per second. Security is strong and immutability prevents tampering. Concluding remarks state this blockchain system offers a new solution for efficient and transparent updates of medical records and is suitable for practical use due to its security and performance characteristics."
"This paper explores the use of machine learning algorithms to predict routing congestion in FPGA high-level synthesis. Routing congestion is a significant bottleneck in the design process of FPGAs and can cause delays and increased costs. Therefore, accurately predicting routing congestion is crucial to optimize design parameters and ensure the timely delivery of FPGA-based products. The proposed methodology combines machine learning models, including decision trees and random forests, with traditional congestion prediction metrics to predict routing congestion during the high-level synthesis process. The research also investigates the impact of using different feature extraction techniques and machine learning algorithms on the accuracy of congestion prediction. Experimental results show that the proposed approach achieves a high prediction accuracy, outperforming traditional congestion prediction methods. Additionally, the results show that feature extraction techniques, such as principal component analysis (PCA) and independent component analysis (ICA), can significantly improve prediction accuracy. This research provides an efficient and effective method for predicting routing congestion in FPGA high-level synthesis. The proposed approach can be used to optimize design parameters and reduce design iterations, ultimately reducing development costs and accelerating product delivery. The approach also has the potential to be applied to other FPGA design optimization problems, demonstrating the value of integrating machine learning in FPGA design.","This paper investigates how to use machine learning algorithms to forecast congestion related to routing in High Level Synthesis of FPGAs. Congestion issues are major impediments in FPGA design and can result in delays and higher costs. Accurately predicting congestion is important for optimizing design parameters and ensuring timely product delivery. We propose a methodology that integrates machine learning models like decision trees and random forests with traditional congestion prediction metrics. We also study how different feature extraction techniques and learning algorithms affect the accuracy of congestion prediction. Experimental results indicate that this approach performs well and surpasses traditional congestion forecasting methods. Results show that techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) significantly enhance performance. Results show this research offers an efficient way to forecast congestion during synthesis; this approach optimizes design parameters and reduces iteration counts, which ultimately reduces development costs and speeds up delivery time. This approach also opens the door for further application in optimization problems specific to FPGA design. This research shows the importance of incorporating machine learning into FPGA design."
"Farsighted collusion is a phenomenon in game theory where two or more players, in the pursuit of a common goal, make decisions that may not be in their immediate best interest but lead to a better outcome in the long run. This concept has been widely studied in various fields, including economics, psychology, and computer science. In this paper, we apply the concept of farsighted collusion to the stable marriage problem, a classical problem in the field of algorithms and discrete mathematics. The stable marriage problem involves finding a stable matching between two sets of elements, such that no two elements would both prefer each other over their assigned partners. We examine the conditions under which farsighted collusion can occur in stable marriage problems and demonstrate how it can lead to more stable outcomes. Our results provide new insights into the role of farsighted behavior in matching problems and have important implications for the design of algorithms and mechanisms for solving stable marriage problems in practical settings.",A concept from game theory called far sighted collusion occurs when multiple players act together to achieve a common goal and sometimes sacrifice short term benefits for long term gains. This idea has been explored in different fields such as economics and psychology and recently also in computer science. In this paper we look at the stable marriage problem from an algorithmic and discrete math point of view. This problem deals with pairing two sets of elements so there are no pairs that would prefer each other over what they are matched with. We study when collusion among players of this problem can be beneficial and show that such collusion can yield more stable solutions. Results from our work shed new light on how far sighted behavior plays a role in matching problems and have significant implications for designing algorithms and mechanisms for practical solutions to the stable marriage problem.
"This paper aims to examine the changing landscape of political discussions on Twitter between 2014 and 2019. The study utilizes Twitter data to analyze the trend of political discussions and identify the significant shifts that have occurred. The research findings show that Twitter has become an increasingly important platform for political discussions, with a growing number of users participating in these conversations. Additionally, the study reveals that the nature of political discussions on Twitter has changed, with users becoming more polarized and the tone of discussions becoming more confrontational. The paper concludes that the changes in political discussions on Twitter reflect broader shifts in the political landscape, with an increasing number of individuals using social media to voice their opinions and engage in political discussions. The research highlights the importance of monitoring the changing landscape of political discussions on social media and the need for continued research to understand the impact of these changes on political discourse and democracy.",This paper looks at how political discussions on Twitter have changed from 2014 to 2019. Using data from Twitter we look at trends in political discussions and identify key shifts. Results show that Twitter is becoming more important for political talk as more people are joining these conversations. We also find that political discussions on Twitter have become more divided among participants and have grown more confrontational. Results suggest this reflects larger changes in politics where more people use social media to express views and get involved politically. Research shows we need to keep an eye on changes in political discussions online and continue studying what these changes mean for discourse and democracy.
"This paper presents a decomposition-based multi-objective evolutionary algorithm (MOEA) design under two algorithm frameworks. The first framework focuses on designing a MOEA based on decomposition techniques, which are used to transform a multi-objective optimization problem into a set of sub-problems. The second framework focuses on incorporating decomposition techniques into existing MOEAs to improve their performance. The proposed algorithms are tested on a set of benchmark multi-objective optimization problems, and their performance is compared to that of state-of-the-art MOEAs. The results show that the proposed algorithms have better performance in terms of convergence and spread, and are able to find a set of well-distributed solutions for multi-objective optimization problems. The findings of this study provide useful insights for the design of MOEAs, and demonstrate the effectiveness of decomposition-based MOEAs in solving multi-objective optimization problems.",This research develops new algorithms for solving multi objective optimization problems using decomposition methods. It introduces two different designs of algorithms using different frameworks. One focuses on developing an MOEA based on decomposition techniques; these techniques convert a complex problem into simpler ones. The other framework integrates decomposition methods into existing MOEAs to enhance them. Algorithms are tested against a set of benchmark problems and compared with top performing MOEAs. Results indicate they converge and spread better and produce good sets of diverse solutions. This work gives important guidance on MOEA design and shows effectiveness of MOEAs using decomposition methods.
"This paper presents a novel heterogeneous graph embedding framework, called MTHetGNN, for multivariate time series forecasting. MTHetGNN combines the strengths of graph neural networks and attention mechanisms to model the complex relationships among multiple time series and their dependencies. The framework employs a heterogeneous graph to represent the different types of relationships between the time series and applies graph convolutional neural networks to learn the representations of the nodes in the graph. The attention mechanism is used to capture the importance of each node in the forecasting process. The proposed framework is evaluated on several multivariate time series forecasting tasks and demonstrates significant improvement over the existing state-of-the-art methods in terms of accuracy and computational efficiency. This study provides a new approach to address the challenge of multivariate time series forecasting and has potential applications in various fields, such as finance, energy, and transportation.","This paper introduces a new heterogeneous graph embedding framework named MTHetGNN for forecasting multivariate time series. By integrating strengths of Graph Neural Networks (GNN) and Attention Mechanisms (AM), this framework models intricate interrelationships among multiple time series along with their dependencies. This framework uses a heterogeneous graph to represent relationships between different types of time series and applies graph convolutional neural networks (GCN) to learn node representations within the graph. Attention mechanism is used to capture significance of each node during forecasting. Performance of this framework is evaluated on different tasks and compared favorably against leading methods; results indicate superior performance in both accuracy and computational efficiency. Results show that this research offers new approaches to tackle challenges in forecasting multivariate time series and suggests promising application domains including finance, energy and transportation."
"This paper presents a novel approach to deriving theorems in implicational linear logic, using a declarative method. Linear logic is a type of mathematical logic that is particularly well-suited for modeling various computational systems, due to its ability to naturally handle the concepts of resource consumption and resource management. However, the task of deriving theorems in linear logic can be challenging, especially for those who are not familiar with its proof rules and strategies. To address this challenge, the authors propose a declarative method for theorem derivation, which is based on a high-level, algebraic representation of linear logic proofs. The key idea of this method is to use a set of rewrite rules that capture the essential structure of linear logic proofs, and to apply these rules in a systematic, automated manner, until the desired theorem is derived. The results of the study show that this method is effective and efficient for deriving theorems in implicational linear logic, and that it has the potential to be applied to a wide range of other logics and theorem proving tasks.","This paper introduces a new way to derive theorems in linear logic that uses a declarative approach. Linear logic is a kind of logic very useful for studying computation because it handles resource usage naturally. However, deriving theorems from linear logic poses a challenge to those unfamiliar with its rules and strategies. In order to address this difficulty, authors suggest an approach to deriving theorems that relies on a high level algebraic representation of proofs. The main idea behind their method is to use rewrite rules capturing core structural elements of proofs and systematically applying them in an automated fashion until a theorem is obtained. Results indicate that this method works well and efficiently for deriving theorems in implication logic and also suggests its applicability to other logics and related tasks."
"Temporal pattern mining is a crucial task in many fields such as finance, health, and engineering, where large amounts of time series data are generated regularly. Traditional methods of temporal pattern mining are inefficient and may lead to high computational costs when dealing with big time series data. Mutual information has been widely used in various fields to measure the dependence between two variables, and it has been shown to be effective in reducing the dimensionality of data. In this paper, we propose an efficient approach to temporal pattern mining in big time series data using mutual information. The proposed approach consists of three stages: data pre-processing, mutual information-based feature selection, and temporal pattern mining. In the pre-processing stage, we filter the data to remove irrelevant data points and perform data normalization. In the feature selection stage, we use mutual information to select the most relevant features for temporal pattern mining. Finally, in the temporal pattern mining stage, we apply a modified version of the PrefixSpan algorithm to extract frequent temporal patterns from the selected features. We demonstrate the effectiveness of our approach on several real-world datasets and compare our results with existing state-of-the-art methods. Our experiments show that our approach outperforms the existing methods in terms of efficiency and accuracy, and can handle big time series data with high-dimensional features. The proposed approach can be applied in various domains, including finance, health, and engineering, to extract valuable temporal patterns from large amounts of time series data efficiently.","Temporal pattern mining is important across finance, health care, and engineering because they generate lots of time series data regularly. Conventional methods tend to be slow and expensive for dealing with big data. Mutual information has been widely used to measure dependency among variables and has been successful at reducing data dimensionality. This paper introduces an efficient way to mine temporal patterns from large data sets by utilizing mutual information. The method involves three steps: preprocessing data, feature selection based on mutual information and mining temporal patterns. Preprocessing removes irrelevant data and normalizes the data. Feature selection selects important features by using mutual information. Finally, prefix span algorithm is modified to extract frequent temporal patterns from selected features. Results show effectiveness on different real datasets and compare favorably against leading methods. Compared to other methods, our method performs better in terms of both speed and accuracy and deals well with high dimensional data. This new method can be used broadly for diverse applications such as finance, health and engineering to quickly extract valuable patterns from large time series data."
"This paper focuses on fair division problems under heterogeneous matroid constraints, which arise in various allocation scenarios where resources have different properties and priorities. We propose a new algorithmic framework that guarantees both fairness and efficiency by exploiting the structure of matroid constraints and the concept of envy-freeness up to one good (EF1). Our approach extends existing results on matroid intersection and partitioning, and adapts well to settings with multiple agents and indivisible items. We also provide a theoretical analysis of the algorithm, proving that it achieves a constant approximation ratio in terms of social welfare and EF1 envy-freeness. Our experiments on synthetic and real-world datasets demonstrate the practical effectiveness of the proposed method, outperforming state-of-the-art algorithms in terms of both objective metrics and subjective evaluations by human participants. Overall, our work contributes to the growing literature on fair allocation mechanisms and provides a useful tool for solving real-world problems with complex constraints.",This paper is about fair division issues under heterogenous matroids that appear in diverse allocation scenarios where different resources have different properties and priority levels. We develop a new algorithmic framework that ensures both fairness and efficiency by making use of structural features of matroids and the idea of envy freedom up to one item (EF1). Our approach builds upon existing research on matroid intersections and partitions and is suitable for many agents and indivisible goods. We also conduct a theoretical analysis of the algorithm showing that it attains a constant approximation ratio for social welfare and EF1 envy freedom. Empirical results using synthetic and real data show practical effectiveness of this proposal surpassing current leading algorithms on both objective and evaluation by human subjects. Overall we contribute to literature on fair mechanisms allocation and provide an important tool for dealing with complicated constraints in practice.
"This paper proposes a deep learning-based approach for detecting Distributed Denial of Service (DDoS) and Denial of Service (DoS) attacks in Internet of Things (IoT) networks. The proposed approach utilizes a deep convolutional neural network, specifically the Residual Network (ResNet), to learn features and classify network traffic patterns associated with normal and attack behaviors. The ResNet model is trained on a large-scale dataset of network traffic samples, which include various types of attacks and normal network traffic. The proposed approach is evaluated on a real-world IoT network dataset, and the results show that the ResNet-based model achieves high detection accuracy and outperforms several baseline models. The proposed approach is also shown to be effective in detecting zero-day attacks, which are new and previously unknown types of attacks that can evade traditional signature-based detection methods. Furthermore, the proposed approach is shown to have a low false positive rate, which reduces the likelihood of triggering false alarms and the subsequent disruption of legitimate network traffic. Overall, the results demonstrate the effectiveness of deep learning-based approaches for detecting DDoS and DoS attacks in IoT networks and highlight the potential of using ResNet for this purpose.","This paper introduces an approach based on deep learning for detecting attacks like DDoS and DoS in IoT networks. Using residual networks (ResNet) as a core component, the method learns features from traffic data and distinguishes between normal behavior and attacks. A large dataset containing various attack types and normal traffic is used to train the ResNet model. Results show that using this model leads to high accuracy for detection and outperforms baselines. Results also show the approach is effective against zero days; these are new types of attacks that avoid detection via signatures. Additionally, the false positive rate is very low which prevents unnecessary disruptions due to false alarms. Results show overall effectiveness of deep learning approaches for detecting attacks in IoT networks and emphasize usefulness of ResNets."
"With the increasing demand for high-speed connectivity and mobile network coverage, drone-mounted base stations have emerged as a promising solution to enhance network coverage in remote and disaster-stricken areas. However, handover management, which refers to the process of transferring a user's connection from one base station to another, remains a critical issue for drone-mounted base stations due to their high mobility and limited coverage. In this paper, we propose a group handover strategy for drone-mounted base stations in mobile networks. The proposed group handover strategy aims to minimize the handover latency and ensure seamless connectivity for multiple users when the drone-mounted base station is in motion. The strategy is based on a proactive handover approach, where the base station initiates the handover process before losing the connection with the user. The proposed strategy also considers the location and velocity of the drone-mounted base station and the quality of the communication link between the user and the base station. To evaluate the performance of the proposed strategy, we conducted a simulation study using a realistic urban scenario. The results show that the proposed strategy can reduce the handover latency by up to 50% compared to the traditional reactive handover approach. Moreover, the proposed strategy can improve the overall network throughput and reduce the number of handover failures. In conclusion, our study demonstrates the effectiveness of the proposed group handover strategy for drone-mounted base stations in mobile networks. The proposed strategy can enhance the quality of service and provide seamless connectivity for users in areas with limited network coverage. Further research can explore the integration of the proposed strategy with other handover management techniques and evaluate its performance in different deployment scenarios.","As there is growing need for fast internet access and robust mobile network coverage, drones with built-in cell stations are emerging as promising solutions to boost network coverage in remote and disaster zones. But managing handovers, i.e., transferring user connections to new stations, is difficult because these drones move quickly and have small coverage. This paper introduces a strategy for managing handovers for stations mounted on drones. Our strategy minimizes latency and ensures smooth connections for multiple users when such station moves. It uses an active rather than passive approach where the station initiates the handover before losing user contact. We also consider the position and speed of the station and quality of communication links between user and station. Performance of the proposed method was evaluated through simulations of a realistic urban environment. Results showed that latency was reduced by up to 50% compared to traditional reactive approaches. Throughput also improved and failure rates decreased. Overall, our research shows that this strategy works well and enhances service quality and connectivity for users in low network coverage regions. Future work might integrate this strategy with other methods and assess its efficacy under various deployment conditions."
"DeepCert is a new method for verifying the robustness of neural network image classifiers in contextually relevant scenarios. It addresses the challenge of ensuring that these classifiers can accurately identify objects in real-world situations, where images may be degraded, occluded, or otherwise altered from their training data. DeepCert uses a combination of formal verification techniques and machine learning to provide a probabilistic guarantee of robustness for a given classifier, with respect to a specified set of transformations. The method is evaluated on a range of benchmark datasets, and results demonstrate significant improvements over existing methods for robustness verification. This research demonstrates the feasibility of DeepCert for improving the confidence and trust in neural network image classifiers for practical applications.","DeepCert introduces a new way of checking how well neural networks recognize images under realistic conditions. The goal is to ensure that these networks perform reliably in practice, handling images that might have been corrupted, partially covered or changed from what they were trained on. Using a blend of formal verification and learning, DeepCert provides probabilistic guarantees regarding robustness for specific classifiers with respect to certain transformation sets. Results show this approach performs much better compared to previous work when verifying robustness. Results also suggest DeepCert is viable for increasing confidence and trust in classification systems used practically."
"This paper proposes a novel approach to scalable and efficient training of multitask multilingual models. Multitask multilingual models are becoming increasingly popular in natural language processing due to their ability to handle multiple languages and tasks simultaneously. However, training such models is often time-consuming and computationally expensive, limiting their practicality. The proposed approach leverages the power of multi-node distributed training and adaptive optimization techniques to accelerate training time and reduce memory usage. Specifically, we introduce a new technique called task-aware gradient accumulation, which effectively balances the computational resources allocated to each task during training. Additionally, we employ an adaptive optimizer that dynamically adjusts learning rates and momentum values based on the characteristics of each task. Experimental results on a large-scale dataset demonstrate that our approach achieves state-of-the-art performance on several tasks, including language modeling, machine translation, and part-of-speech tagging, while significantly reducing training time and memory usage. Our approach provides a promising direction for the scalable and efficient training of multitask multilingual models, which could have significant implications for real-world natural language processing applications.","This paper introduces a new method for efficient and scalable training of models that handle multiple languages and tasks together. Recent research has shown these models to be very useful in NLP because they can process different languages simultaneously. However, training them can take a long time and consume a lot of computing resources, thus making them impractical. To counter this limitation, this new method makes use of distributed training across multiple nodes along with adaptive optimization techniques to decrease time needed for training and lower memory use. Task awareness through gradient accumulation is introduced as a new technique where resources are better balanced among different tasks during training. Furthermore, an adaptive optimizer is used that modifies learning rate and momentum values according to task characteristics dynamically. Results using a large dataset show high performance on various tasks including language modeling, translation, and POS tagging. Our method reduces training times and memory use substantially and promises a way forward for scaling up and improving efficiency of multitask models that handle multiple languages. Implications for real world NLP work are very promising."
"This paper proposes a novel deep learning model for traffic forecasting, called CDGNet. CDGNet is a cross-time dynamic graph-based model that incorporates both spatial and temporal dependencies in traffic data. Specifically, CDGNet employs a dynamic graph convolutional neural network to capture spatial correlations among different regions in traffic networks, and a recurrent neural network with a time attention mechanism to capture temporal patterns in traffic data. To evaluate the performance of CDGNet, experiments are conducted on real-world traffic datasets, and compared with state-of-the-art traffic forecasting models. The experimental results demonstrate that CDGNet outperforms other models in terms of both short-term and long-term traffic forecasting accuracy. CDGNet can effectively capture complex spatiotemporal correlations in traffic data, and produce accurate traffic forecasts with high efficiency. Overall, this research presents a promising approach for traffic forecasting, which has important applications in traffic management, urban planning, and intelligent transportation systems. CDGNet can contribute to the development of smarter and more efficient transportation systems, and help to reduce traffic congestion, improve safety, and enhance the quality of life in urban areas.","This paper introduces a new deep learning model for traffic forecasting called CDGNet. CDGNet is based on dynamic graphs and incorporates both spatial and temporal relationships into traffic data. Specifically, CDGNet uses a dynamic graph convolutional neural network to recognize correlations among different regions within traffic networks and uses a recurrent neural network with an attention mechanism for time pattern recognition. Performance of CDGNet was evaluated using real data and compared against leading models. Results show CDGNet excels in both short term and long term accuracy of traffic forecasting. CDGNet successfully captures complex spatio temporal relationships and generates high performance forecasts efficiently. Overall, this study offers promising approaches to traffic forecasting with broad implications such as improved management of traffic, urban planning and intelligent transportation systems. Contributions include the advancement of smart and efficient transportation systems which mitigate congestion, enhance safety and raise quality of life in cities."
"This paper investigates the problem of constructing a critical geometric graph (CGG) in dense wireless sensor networks (WSNs) using a distributed approach. CGG is a fundamental structure in WSNs that captures the geometric relationships between nodes and is widely used in various applications. However, constructing CGG in dense WSNs is a challenging task due to the large number of nodes and the limited communication range. To address this problem, we propose a novel distributed algorithm that leverages the properties of Delaunay triangulation and Gabriel graph to construct the CGG in a distributed manner. The proposed algorithm works in an iterative fashion and consists of three main phases: node selection, local construction, and global synchronization. In the node selection phase, each node selects a set of candidate neighbors based on the Delaunay triangulation. In the local construction phase, each node constructs a local subgraph using the selected neighbors and Gabriel graph. Finally, in the global synchronization phase, the local subgraphs are merged to construct the final CGG. We evaluate the proposed algorithm through extensive simulations and compare it with existing approaches. The simulation results demonstrate that the proposed algorithm can construct the CGG with high accuracy and efficiency, even in dense WSNs. Furthermore, the proposed algorithm outperforms the existing approaches in terms of construction time and energy consumption. Overall, the proposed distributed algorithm provides a promising solution for constructing the CGG in dense WSNs, which can be used in various WSN applications such as localization, routing, and coverage control.","This paper looks at how to create a Critical Geometric Graph (CGG) via a distributed method in dense Wireless Sensor Networks (WSNs). CGG is important in WSNs and useful for many applications because it describes relationships among sensors. But making CGG in dense networks is difficult because there are lots of sensors and they can communicate only short distances. Therefore, we propose a new distributed algorithm that uses properties of Delaunay triangulation and Gabriel graph. This algorithm iterates and has three major phases: selecting nodes, local construction and global synchronization. Each node selects candidates neighbors based on triangulation in selecting phase. Local construction phase lets each node use selected neighbors and build local subgraphs with Gabriel graph. Finally in synchronization phase, local graphs are merged into final CGG. Results show that the proposed algorithm makes accurate and efficient CGG even in dense networks. Performance comparison also shows it is faster and uses less power. Overall, this distributed algorithm provides a good way to make CGG in dense WSNs, which is important for tasks such as localization, routing and coverage control."
"In today's world, communication is more pervasive than ever before. Large-scale communication networks like social media, instant messaging platforms, and online forums have become ubiquitous, generating vast amounts of conversational data every day. Overhearing, the act of observing and analyzing conversations between individuals, has emerged as a valuable tool for understanding and predicting behavior in these networks. However, as the scale of these networks continues to grow, there is a need for more efficient methods for analyzing conversational data. In this paper, we present a framework for representing conversations that is designed to facilitate scalable overhearing. Our framework leverages the graph structure of communication networks to create a compact representation of conversational data that can be efficiently analyzed. We use a combination of graph clustering and sequence modeling techniques to identify conversation topics and represent conversations as sequences of topic vectors. By representing conversations in this way, we are able to reduce the dimensionality of the data and improve the efficiency of overhearing algorithms. To evaluate our framework, we conduct experiments on a large-scale dataset of online forum conversations. We compare the performance of our framework to baseline approaches and show that our framework is able to achieve better accuracy and efficiency in overhearing tasks. We also demonstrate the scalability of our framework by analyzing datasets of varying sizes, showing that our approach can handle datasets with millions of messages. Overall, our framework provides a novel approach to representing conversations for scalable overhearing. By leveraging the structure of communication networks and reducing the dimensionality of conversational data, our approach enables more efficient analysis of large-scale communication networks.","Today communications are everywhere. Social media, instant messaging apps, and online forums have become widespread; they produce huge amounts of conversational data daily. Observing and analyzing conversations among people has become a useful tool to understand and forecast behavior in these networks. But with larger and larger networks, new methods are needed to analyze conversation data. This paper introduces a framework for representing conversations designed for scalability. It uses graph structure of communication networks to make a compact representation of data that can be quickly processed. We use clustering and sequence modeling to discover topics in conversations and represent them as sequences of topic vectors. This representation reduces data dimensionality and makes overhear algorithms more efficient. We test our framework on a large dataset of forum conversations and compare its performance against baselines. Results show better accuracy and efficiency. Scalability is demonstrated through various size datasets containing millions of messages. Overall, our framework offers a new way to represent conversations for scalability. By exploiting network structure and lowering data dimensionality, our approach allows for more efficient large network analysis."
"This paper explores the concept of random caching based cooperative transmission in heterogeneous wireless networks. The authors propose a novel method for increasing the efficiency of wireless data transmission by utilizing random caching techniques at the intermediate nodes in the network. This approach combines the benefits of cooperative transmission and caching, resulting in improved network performance and reduced latency. The authors conduct extensive simulations and evaluations to demonstrate the effectiveness of the proposed method. The results show that the proposed random caching based cooperative transmission scheme significantly improves the network performance in terms of network throughput and energy efficiency compared to traditional cooperative transmission methods. The study also provides insights into the impact of various network parameters, such as the number of caches and the size of the cache, on the performance of the proposed method. These results provide valuable information for the design and optimization of future wireless networks. In conclusion, this paper provides a promising solution for improving the performance of wireless networks through the integration of random caching and cooperative transmission techniques. The results of this study have important implications for the design of future wireless networks and can be used to guide the development of new technologies in this area.","This paper studies random caching and cooperative transmission together in networks with different characteristics. Authors suggest a new method for better wireless data transmission using caching at intermediary nodes. This approach merges advantages of cooperation and caching and results in better performance and lower latency. Authors perform detailed simulation and evaluation to show effectiveness of their suggestion. Results indicate significant improvement in network performance, specifically in terms of throughput and energy efficiency over traditional methods. Results also show how various factors like number of caches and cache size impact performance. Results are valuable for designing future networks. In summary this paper offers promising solutions for enhancing network performance through integration of caching and cooperative transmission. Results of this research are important for network design and can serve as a basis for developing new technology."
"This paper proposes an efficient simulation method for predicting the temperature evolution of overhead transmission lines. The method combines an analytical solution for heat transfer with numerical weather prediction (NWP) data to accurately simulate the temperature distribution along the line. The proposed method is computationally efficient, making it suitable for real-time monitoring and control of transmission lines. The effectiveness of the proposed method is demonstrated through a case study, where it is shown to accurately predict the temperature of a transmission line under varying weather conditions. The results of this study highlight the potential of the proposed simulation method to improve the reliability and efficiency of overhead transmission line operations.",This paper introduces a fast simulation technique that predicts temperature changes along overhead power lines. This method uses analytical solutions for heat transfer coupled with numerical weather predictions (NWP) to simulate temperature distributions along these lines very accurately. Proposed simulation method is efficient enough to work well in real time for monitoring and control of lines. Results of a demonstration case show that method works effectively at different weather conditions and clearly demonstrates that this new simulation method has great promise for enhancing reliability and efficiency of operation for overhead power lines.
"This paper proposes a Bayesian optimization method that incorporates domain knowledge for optimizing the performance of the ATRIAS biped, a two-legged robot. Bayesian optimization is a popular technique for global optimization of black-box functions, but its performance can be improved by leveraging relevant domain knowledge. The proposed method uses a Bayesian optimization framework that incorporates domain knowledge in the form of prior beliefs about the parameters that affect the performance of the ATRIAS biped. These prior beliefs are incorporated into the Bayesian optimization algorithm through the choice of the prior distribution. The method is evaluated on a set of simulation experiments, where it is compared with standard Bayesian optimization and other popular optimization algorithms. The experimental results demonstrate that the proposed method outperforms other methods in terms of the number of function evaluations required to obtain an optimal solution. The method also provides a better understanding of the ATRIAS biped's behavior by identifying the most important parameters and their interaction effects. The results suggest that incorporating domain knowledge into Bayesian optimization can lead to more efficient optimization and a better understanding of the system under consideration. Overall, this research contributes to the growing body of literature on Bayesian optimization and demonstrates its potential for optimizing complex systems such as bipedal robots. It also highlights the importance of incorporating domain knowledge to improve the performance of optimization algorithms.","This paper introduces a new method of Bayesian optimization which takes advantage of specific domain knowledge for optimizing performance of the ATRIAS biped, a bipedal robot. Bayesian optimization is an effective tool for finding solutions for difficult problems, but it can perform even better when we take into account relevant background information. In this study, we use Bayesian optimization using prior beliefs about parameters affecting performance of ATRIAS. These prior beliefs are used as priors in the optimization algorithm. Results of simulation experiments show that this new method requires fewer function evaluations than others to find optimal solutions. This method also reveals important aspects of how the ATRIAS biped behaves by identifying key parameters and their interactions. Results indicate that integrating domain knowledge improves optimization efficiency and deeper understanding of considered systems. This research advances the literature on Bayesian optimization and shows promise for optimizing complex systems such as bipeds. It also emphasizes importance of incorporating domain knowledge to enhance performance of optimization algorithms."
"This paper explores the learning of coordination policies for robotic swarms. Robotic swarms consist of a large number of relatively simple robots that can work together to accomplish complex tasks. However, achieving coordination in such swarms is challenging due to the inherent complexity of the system and the lack of a centralized control mechanism. In this paper, we propose a reinforcement learning approach to learn coordination policies for robotic swarms. Our approach involves using a centralized critic and decentralized actors to learn the coordination policies. The centralized critic observes the global state of the swarm and provides feedback to the decentralized actors, which take local actions based on their observations of the local environment. The feedback from the centralized critic is used to update the policy parameters for the decentralized actors, which gradually learn to coordinate their actions with those of the other robots in the swarm. We evaluate our approach on a set of benchmark tasks for robotic swarms, including formation control, obstacle avoidance, and cooperative transport. Our results show that our approach is effective at learning coordination policies for robotic swarms, achieving high performance on these tasks while also being able to generalize to new environments. Overall, our work demonstrates the potential of reinforcement learning for learning coordination policies in robotic swarms, and provides a foundation for further research in this area. The ability to learn coordination policies in this way could enable the development of more sophisticated and efficient robotic swarms that can accomplish complex tasks in a variety of environments.",This paper investigates coordination policies for robotic swarms. These swarms consist of a large number of simple robots that can cooperate to do complicated things. But it's hard to coordinate them because they're complicated systems and there's no central controller. We suggest using reinforcement learning here. Using this approach we use a central critic and decentralized agents to learn coordination policies. The critic looks at overall swarm state and gives feedback to the agents who then act locally based on observation of local environment. Critic feedback updates actor policies to gradually learn coordination among swarming robots. Performance on benchmarks such as formation control and avoidance of obstacles and cooperation transport is evaluated. Results show that this method learns coordination well; performing high performance and generalizing to new environments too. Overall this work shows promise for reinforcement learning of coordination policies for swarms and establishes a basis for further research. Coordination through this method could result in advanced and efficient swarms that can handle various environments effectively.
"The paper presents Serket, a novel architecture for connecting stochastic models in order to realize a large-scale cognitive model. The architecture is based on a modular design that allows for the integration of multiple models into a single system. Serket takes advantage of the strengths of stochastic models, such as their ability to handle uncertainty and make probabilistic predictions, while also addressing their limitations, such as the difficulty of integrating multiple models. The architecture is evaluated on a range of tasks, including language understanding and generation, decision making, and perception. The results show that Serket outperforms existing approaches in terms of accuracy, scalability, and interpretability. The paper concludes by discussing the implications of the Serket architecture for the development of more advanced cognitive models and its potential applications in areas such as artificial intelligence and robotics.","This paper introduces Serket which is a new architecture for linking stochastic models into large cognitive systems. The architecture relies on modularity and enables integration of different models into one system. Using strength of stochastic models such as handling uncertainty and making probabilistic predictions while also dealing with their drawbacks like difficulty of integrating diverse models, Serket is evaluated on various tasks including language understanding and generation, decision making and perception. Results show that performance of Serket surpasses previous work concerning accuracy, scalability and interpretability. At the conclusion, this paper discusses implications for further development of advanced cognitive systems and potential applications such as AI and robotics."
"This paper examines the ethical debate surrounding actuarial risk assessment and the use of interventions to modify predicted outcomes. The traditional view of actuarial risk assessment has been to use predictive algorithms to identify individuals at high risk for negative outcomes and allocate resources accordingly. However, this approach raises ethical concerns about the potential for discrimination and unequal treatment. The paper argues that reframing the debate to focus on interventions rather than predictions can shift the conversation towards a more ethical and equitable approach to actuarial risk assessment. The authors propose that interventions should be designed to address the root causes of risk and promote positive outcomes, rather than simply mitigating predicted negative outcomes. The paper concludes by highlighting the importance of considering the ethical implications of actuarial risk assessment and the need for ongoing dialogue about the appropriate use of interventions in this field.","This paper investigates ethical issues concerning actuarial risk assessment and intervention methods to reduce forecasted negative results. Traditionally, actuaries assess risks using predictive algorithms to determine which people are at high risk for bad results and then distribute resources accordingly. But there are ethical concerns over discrimination and disparate treatment through this method. The paper contends that reframing this debate to intervene rather than predict results leads to more ethical and fair practice for risk assessment. Interventions should aim to fix underlying causes of risk and increase good results instead of just reducing foreseen bad results. In conclusion, consideration of ethics is important for actuarial risk assessment and dialogue is necessary about proper use of interventions."
"This paper presents a framework for enriching lexical semantic resources with distributional semantics. The framework aims to address the limitations of traditional lexical resources, such as wordnets and ontologies, by incorporating distributional information from large text corpora. The proposed framework consists of three main components: (1) data collection, (2) feature extraction, and (3) semantic enrichment. The data collection component involves the retrieval of large amounts of text data from various sources, including online forums, news articles, and social media platforms. The feature extraction component involves the use of algorithms such as word2vec and GloVe to compute vector representations of words in the text data. Finally, the semantic enrichment component involves the integration of these vector representations into existing lexical semantic resources, such as wordnets and ontologies, to enhance their semantic coverage and accuracy. The framework is evaluated on a set of benchmark tasks, including word similarity and analogy tasks, and the results show that the proposed framework significantly improves the performance of existing lexical semantic resources. The framework provides a practical solution for enhancing lexical semantic resources and has the potential to impact a wide range of natural language processing tasks.","This study introduces a framework for enhancing lexical semantic resources using distributional semantics. We aim to overcome the shortcomings of traditional lexical resources like WordNet and Ontology by incorporating statistical information extracted from large corpora of text. This framework consists of three main parts: data acquisition, feature extraction, and semantic enhancement. Data acquisition retrieves a vast amount of text from diverse sources such as forums, news, and social media. Feature extraction uses algorithms such as word2vec and GloVe to extract word vector representations from acquired text. Finally, we integrate these vectors into existing resources like WordNet and Ontology to increase semantic scope and quality. Performance evaluation on benchmark tasks such as word similarity and analogy shows that this proposed framework greatly enhances performance of current lexical semantic resources. This framework offers practical means for improving lexical semantic resources and could affect many NLP tasks."
"This paper proposes a novel approach to sequence-to-sequence learning using a recurrent neural network-based semantic variational autoencoder (RNN-SVAE). The model leverages the encoding-decoding architecture of traditional autoencoders while also incorporating variational inference and a recurrent neural network for improved sequential modeling. The RNN-SVAE is capable of learning meaningful and latent representations of input sequences, allowing for effective generation of output sequences with a high degree of semantic coherence. Experimental results show that the proposed model outperforms traditional sequence-to-sequence models on various text generation tasks, demonstrating its effectiveness in capturing the underlying structure and meaning of sequences.",This paper introduces a new method for sequence learning using Recurrent Neural Networks along with Semantic Variational Autoencoders (RNN-SVAE). This system combines an encoding and decoding architecture from typical autoencoders together with variational inference and recurrence to enhance learning of sequential data. Learning meaningful latent representations is at core of this approach and allows for high coherence semantic generation of outputs. Results indicate performance improvements over previous sequence models on many text generation tasks and shows capability to capture deep structure and meaning in sequences.
This paper proposes a novel approach for detecting deception attacks in networked control systems using a sequential detection framework with watermarking. The proposed method involves embedding a unique watermark into the control system's input signal and then monitoring the output signal for any deviations from the expected behavior. A sequential hypothesis testing algorithm is used to detect the presence of a deception attack based on the watermark's correlation with the output signal. Simulation results demonstrate the effectiveness of the proposed method in detecting various types of deception attacks with high accuracy while maintaining low false alarm rates. This research provides a promising solution for enhancing the security and reliability of networked control systems against malicious attacks.,This paper introduces a new way of detecting deception attacks in systems that use networks. It uses a sequential detection framework along with watermark embedding. The authors embed a unique watermark into the system inputs and monitor the outputs for any unusual behaviors. Using sequential hypothesis testing they can detect if there are attacks by looking at how well the watermark correlates with the output. Results show this method works very well at finding different kinds of deception attacks with high accuracy but low false alarms. Research here shows promise for making these systems more secure and reliable against harmful attacks.
"This paper presents a scalable shared-memory parallel simplex algorithm for solving large-scale linear programming problems. The proposed algorithm is designed to exploit the parallelism inherent in modern multi-core processors and uses efficient data structures and algorithms to optimize performance. The algorithm employs a task-based parallelism model, in which tasks are dynamically scheduled and executed on available processors. The parallelism is achieved by dividing the problem into smaller subproblems, which are then solved independently using the simplex method. The proposed algorithm also uses load balancing techniques to ensure that the workload is distributed evenly among all the available processors. To evaluate the proposed algorithm, we conducted a series of experiments using several benchmark linear programming problems with different sizes and characteristics. The experimental results show that the parallel simplex algorithm is highly scalable and achieves significant speedups over the sequential simplex algorithm. The performance of the algorithm is further improved by using advanced optimization techniques, such as lazy updates and column-wise data layout. The proposed algorithm is expected to have a significant impact on the field of linear programming, especially in large-scale optimization problems. The algorithm is highly flexible and can be easily extended to handle various types of linear programming problems, including mixed-integer programming, network flow problems, and more. Overall, the research presented in this paper demonstrates the feasibility and effectiveness of using shared-memory parallelism to solve large-scale linear programming problems, paving the way for future research in this area.",This paper introduces an algorithm for solving large linear programs efficiently through use of shared memory parallel processing. The new algorithm makes use of inherent parallelism in modern processors and uses efficient data structures and methods to improve performance. It utilizes dynamic scheduling of tasks based on a task parallelism model. Problems are divided into smaller tasks and solved separately using the simplex method. Load balancing is used to distribute work evenly among processors. Performance was tested on diverse benchmark problems of differing size and nature. Results show that the algorithm scales well and outperforms sequential simplex. Further performance enhancement is made through advanced optimization techniques such as lazy updates and column layout. Results indicate that this approach will have a strong impact on LP and particularly on large scale optimization problems. The algorithm is very flexible and easily extensible to other types of LP like mixed integer programming and network flow. Overall results show feasibility and efficacy of shared memory parallelism for large scale LP problems opening up avenues for future research.
"This paper proposes a novel method called ""Zoom-SVD"" for extracting key patterns in an arbitrary time range from large data sets. The method is designed to be both fast and memory-efficient, making it suitable for use in analyzing time-series data sets with a large number of observations. The Zoom-SVD approach is based on an extension of the singular value decomposition (SVD) algorithm, which enables efficient computation of the most important modes of variation in the data. The method also incorporates a zooming mechanism, which allows the user to focus on specific time intervals of interest, and only compute the SVD on those portions of the data. The performance of the Zoom-SVD method is evaluated on a range of real-world data sets, demonstrating its effectiveness in identifying key patterns in various time-series applications, including weather forecasting, financial analysis, and speech recognition. The results show that the Zoom-SVD method is significantly faster and more memory-efficient than existing SVD-based methods, while also achieving comparable or superior accuracy in detecting patterns of interest. Overall, this research presents a promising approach for efficient and effective analysis of time-series data in a variety of fields.","This paper introduces a new method called Zoom SVD for extracting important features at specified time intervals from very large data sets. This method is intended to be both quick and low memory usage, making it useful for studying high numbers of observations in series of time. Zoom SVD relies on extending the Singular Value Decomposition (SVD) algorithm so as to efficiently compute major modes of variation within the data. It also uses a zooming mechanism which lets users focus on particular intervals of interest and computes SVD just on those parts of data. Performance is tested using diverse real datasets and results show that this method outperforms other SVD approaches by being much quicker and saving more memory while still performing equally or better at finding relevant features. Overall, this study presents promising means for processing time series data broadly across disciplines."
"This paper presents a novel approach to face recognition using transformed shape features. The proposed algorithm is based on the transformation of facial features into a new shape representation, which is then used as input to a recognition model. This transformation process takes into account the inherent variability of facial features, such as changes in lighting, pose, and expression, to produce a robust representation of the face. Experiments were conducted on several publicly available datasets and the results show that the proposed approach outperforms existing state-of-the-art face recognition algorithms in terms of accuracy and robustness. The results demonstrate that the transformed shape features approach is a promising direction for future face recognition research.","This paper introduces a new method for face recognition using transformed shape features. Proposed method transforms facial features into a new shape representation which serves as input for a recognition model. Transformation considers variability inherent in features like illumination, pose and expression to produce robust face representation. Results obtained from experiments on various public datasets show this new approach performs better compared to leading methods in accuracy and robustness. Results indicate that transformation of shape features holds promise for future research into face recognition."
"This paper proposes the use of neural networks as a tool to analyze attack patterns and generate effective solutions to potential threats. Attack patterns are a critical aspect of cybersecurity, and identifying and addressing these patterns is a key challenge faced by security professionals. In this study, we develop a neural network-based approach that leverages machine learning techniques to analyze and understand patterns of cyber-attacks. The proposed model utilizes both supervised and unsupervised learning techniques to extract meaningful features from large-scale datasets of attack patterns. The neural network then employs these features to identify and categorize potential threats, and generate effective solutions for mitigating the impact of these threats. Our experiments demonstrate that the proposed approach can effectively analyze attack patterns and generate actionable solutions, with high accuracy and efficiency. The proposed model has the potential to improve cybersecurity practices by providing a comprehensive and automated solution for detecting and addressing security threats.",This paper puts forward using neural nets as an instrument for analysis of attack patterns and generation of solutions for anticipated threats. Identification of attack patterns is crucial for cybersecurity; recognizing and dealing with such patterns is a significant task for cybersecurity experts. We develop a new method based on neural nets which uses machine learning techniques to detect and understand attack pattern features. Supervised and unsupervised learning is used to extract important features from large datasets of attacks. Neural nets use these features to recognize and classify threats and generate practical solutions for reducing threat impact. Results show the approach successfully analyzes attack patterns and generates actionable results with high precision and efficiency. Proposed model could enhance cybersecurity by offering a broad and automated way to detect and handle security risks.
"The Internet of Things (IoT) is rapidly expanding, with billions of devices connected to the internet. This expansion brings numerous security challenges, as many IoT devices are resource-constrained and lack robust security mechanisms. The OSCAR (Object Security Architecture for the Internet of Things) architecture is proposed to address these security challenges. OSCAR is a lightweight security architecture that uses object-oriented security mechanisms to protect IoT devices from various types of attacks. The proposed OSCAR architecture consists of three main components: object modeling, object security, and object communication. Object modeling is used to describe the structure and behavior of IoT objects, including their security attributes. Object security is responsible for protecting the objects and their attributes from various security threats. Object communication defines the protocols and procedures used for secure communication between IoT objects. The proposed OSCAR architecture is evaluated using several performance metrics, including memory and processing overhead, communication latency, and energy consumption. The experimental results show that OSCAR provides robust security while incurring minimal overhead. OSCAR also supports various security services, including confidentiality, integrity, authenticity, and access control. The OSCAR architecture is compared to several existing IoT security architectures, and the results show that OSCAR provides better security and performance compared to these architectures. Finally, the proposed OSCAR architecture is implemented on several IoT devices, including sensors, actuators, and gateways, demonstrating its practicality and effectiveness. In conclusion, the proposed OSCAR architecture provides a lightweight and effective security solution for the IoT, which can be applied to various IoT devices and networks. The use of object-oriented security mechanisms makes OSCAR adaptable and scalable, addressing the security challenges posed by the IoT.","The Internet of Things (IoT) is growing very fast; billions of devices are now connected to the Internet. As such growth has come, there are many security problems because most IoT devices have limited resources and don't have strong security measures. To combat this situation, a new architecture called OSCAR (Object Security Architecture for IoT) is introduced. OSCAR is a lightweight security design that employs object oriented security methods to defend against different kinds of attacks. This proposed architecture includes three major parts: object modeling, security for objects and communication among objects. Object modeling describes the structure and behavior of IoT objects and their security attributes. Security for objects is designed to protect them against various security threats. Communication specifies protocols and procedures for secure interactions among IoT objects. Performance metrics like memory and processing overheads, latency, and energy consumption are used to evaluate the proposed architecture. Results indicate that OSCAR offers good security performance and low overhead. It also supports services like confidentiality, integrity, authentication and access control. Compared to other existing architectures, OSCAR performs better on security and performance. Finally, OSCAR is implemented on diverse IoT devices including sensors, actuators and gateways showing practicality and effectiveness. In summary, this proposed architecture OSCAR offers lightweight and effective security solutions that can be widely adopted to IoT systems. Using object oriented security methods makes OSCAR adaptable and scalable, dealing with security issues posed by IoT."
"In recent years, 5G technology has become a crucial aspect of wireless communication systems, providing high-speed data transfer and reduced latency. The increasing demand for 5G services in dense urban areas has created a need for effective approaches to selecting radio access technologies (RATs) for 5G networks. This paper presents an in-depth analysis of the key factors that influence RAT selection for 5G dense wireless networks, such as network architecture, network density, available spectrum, and user requirements. The study proposes a novel RAT selection approach that leverages machine learning algorithms to predict the most suitable RAT for a given scenario based on real-time network conditions. The approach considers various metrics, such as network traffic, interference, and network coverage, to make informed RAT selection decisions. The proposed approach is evaluated through extensive simulations and experiments, and the results show that it outperforms conventional RAT selection methods in terms of network performance and user satisfaction. This research provides valuable insights into the challenges of RAT selection for 5G dense wireless networks and offers a promising solution for operators and network designers to optimize their networks and improve the overall user experience. The proposed approach is expected to have a significant impact on the development of 5G networks and contribute to the advancement of the field of wireless communication systems.","In recent years, 5G technology has become important for wireless communications systems and delivers high speed data transfers along with low latency. With growing demand for 5G services in high population areas, there is a need for effective ways to select radio access technologies (RATs) for these networks. This paper examines key factors that affect RAT selection for dense 5G wireless networks including network architecture, density, available spectrum and user needs. A new approach using machine learning algorithms is proposed which selects the best RAT for a given situation based on current network conditions. Metrics such as network traffic, interference and coverage are used to make rational decisions about RAT selection. Results of extensive simulations and experiments show this approach performs better than traditional methods in terms of network performance and user satisfaction. Results of this research shed light on challenges of selection of RAT for dense 5G networks and suggest useful solutions for operators and network designers to optimize networks and enhance user experience. Expected impact of this approach on network development and advancement of wireless communication systems is large."
"This paper presents Lamassu, a storage-efficient host-side encryption technique that allows sensitive data to be stored on a client device without compromising its security. Lamassu is designed to provide confidentiality, integrity, and authenticity of the data, while minimizing the storage space required for the encrypted data. To achieve its goal, Lamassu employs a novel approach of combining authenticated encryption with a technique called inline encryption. This allows Lamassu to encrypt data inline with its creation, without requiring additional storage space for the encrypted data. The technique also ensures that the data is not tampered with during transmission and storage. The paper presents a thorough analysis of the security properties of Lamassu, including a formal security proof, and a performance evaluation that demonstrates its effectiveness in terms of storage efficiency, speed, and security. The authors also discuss the practical applications of Lamassu in various settings, including cloud storage, mobile devices, and network communication. Overall, the results of this research demonstrate the potential of Lamassu as an effective technique for storage-efficient host-side encryption, with promising practical applications in various settings.","This paper introduces Lamassu which is a new technique for secure storage at the host side that enables storage of sensitive data directly on client devices without compromising security. Lamassu secures data through confidentiality, integrity and authenticity while using minimal storage space for encrypted data. It achieves this by using an innovative approach combining authenticated encryption with inline encryption techniques. Thus Lamassu encrypts data as it is created and does not require extra storage space for the encrypted data. Lamassu also ensures that data is secure throughout transmission and storage. Results of this work show security properties such as formal proofs and performance evaluations demonstrating effectiveness in terms of efficiency in storage, speed and security. Authors also detail practical uses of Lamassu in diverse contexts like cloud storage and networking. In summary results of this research indicate that Lamassu performs well as an effective technique for efficient host side storage and has important practical uses across many scenarios."
"This paper presents an overview of tensor decomposition techniques and their applications in signal processing and machine learning. Tensor decomposition is a powerful tool for analyzing high-dimensional data, which arises in many real-world problems. The paper first provides a brief introduction to tensors and tensor decomposition, including various tensor formats and the mathematical foundations of tensor decomposition. Then, it discusses the main tensor decomposition methods, including Tucker decomposition, canonical polyadic decomposition (CPD), and PARAFAC2. For each method, the paper describes its algorithm, advantages, and limitations. In addition, the paper discusses several applications of tensor decomposition in signal processing and machine learning, such as tensor completion, tensor regression, and tensor clustering. Finally, the paper concludes with some future directions and challenges in the field of tensor decomposition. Overall, this paper provides a comprehensive overview of tensor decomposition and its potential applications in signal processing and machine learning.","This paper surveys the decomposition of tensors and its application in signal processing and machine learning. Decomposition of tensors is a strong method to analyze high dimensional data that frequently occurs in practical situations. Initially, this paper introduces tensors and tensor decomposition briefly, discussing different formats and the mathematical basis of decomposition. Following this, the paper details key decomposition methods like Tucker decomposition, CPD, and PARAFAC2; describing algorithms, pros and cons for each one. Also, the paper reviews various applications, including tensor completion, regression and clustering. Finally, concluding remarks and open questions are presented. Overall, this paper provides a broad overview of decomposition of tensors and related use cases."
"The theory of matchings in set families has been an active area of research since the seminal work of Erdős and Kleitman in 1971. In this paper, we investigate two fundamental problems in this area: the maximum matching problem and the maximal matching problem. The maximum matching problem seeks to find a matching of maximum cardinality in a given set family. We provide a new algorithm that achieves an improved upper bound on the number of edges in the matching, which improves upon the previously best-known bounds. Our algorithm also has the advantage of being faster than existing algorithms, making it practical for larger set families. The maximal matching problem seeks to find a matching that cannot be extended to a larger matching. We show that this problem is closely related to the combinatorial structure of set families and introduce a new measure called the maximal matching degree. Using this measure, we prove a new upper bound on the maximal matching degree, which has implications for the existence of maximal matchings in certain classes of set families. Our results extend the work of Erdős and Kleitman and shed new light on the structure of matchings in set families. They have implications for a variety of applications in computer science, operations research, and other fields.","Since Erdős and Kleitman's seminal work from 1971, there has been ongoing research into matching theory within families of sets. This paper focuses on two key questions: finding maximum matchings and maximal matchings. Maximum matching concerns finding largest possible matchings within given families of sets. We propose a new algorithm that obtains better upper bounds on the number of edges in matchings compared to previous bounds. It is also faster and thus practical for large families of sets. Maximal matching aims to find matchings that cannot be further enlarged. We show this is strongly connected to combinatorial structure of these families and introduce a new measure called maximal matching degree. Using this measure we prove new upper bounds on maximal matching degree and this leads to results about existence of maximal matchings in some classes of set families. Results improve those of Erdős and Kleitman and open up new ways of thinking about matching structures. Applications range broadly across computer science, operations research and other disciplines."
"This paper proposes Augury, a time-series based application for the analysis and forecasting of system and network performance metrics. Augury employs advanced machine learning techniques such as neural networks and time-series decomposition to identify trends, patterns, and anomalies in performance metrics. Augury also provides a comprehensive set of forecasting models that enable system administrators to predict future performance metrics accurately. The effectiveness of Augury was evaluated using real-world data collected from multiple systems and networks. The results indicate that Augury outperforms existing solutions in terms of accuracy, scalability, and speed. Furthermore, Augury provides valuable insights into the underlying causes of performance issues, which can be used to optimize system and network performance. Augury has the potential to improve the efficiency and reliability of complex systems and networks, and to reduce the cost and effort associated with performance monitoring and management.","This paper introduces Augury, an application focused on time series analysis and forecasting of performance metrics of systems and networks. Augury uses advanced machine learning methods like neural nets and time series decomposition to discern trends and anomalies in performance metrics. Augury also supplies a broad range of forecast models that enable system admins to accurately predict future performance metrics. Effectiveness of Augury was validated using real data from different systems and networks. Results show that Augury excels at accuracy, scalability and speed compared to previous solutions. Moreover, Augury offers important clues about root causes of performance problems which may help optimize system and network performance. Augury promises to improve efficiency and reliability of complicated systems and networks and lower costs and effort related to performance monitoring and management."
"This paper examines the packing and covering properties of subspace codes for error control in random linear network coding. We analyze the trade-off between the minimum distance and the maximum rate of subspace codes and provide constructions of subspace codes with optimal packing and covering properties. Specifically, we propose a method for constructing subspace codes with high minimum distance that achieves the optimal trade-off between the minimum distance and the maximum rate. We also investigate the covering radius of subspace codes and show that our constructions achieve the optimal covering radius in certain regimes. Our results have implications for the design of practical subspace codes for error control in random linear network coding.","This paper studies packing and covering properties of subspace codes for error correction using random linear network coding. We study the relationship between minimum distance and maximal rate of such codes and construct new codes with optimal packing and covering properties. Specifically, we suggest a method to construct codes having large minimum distance which optimize the balance between minimum distance and maximal rate. We also look into the covering radius of these codes and show that this optimization is achieved in certain ranges. Results are important for designing practical codes that work well for error correction through random linear network coding."
"This paper presents an analysis of the two most promising mobile broadband solutions for South Asia: WiMAX and LTE. The paper aims to determine which technology would be the better solution for providing reliable and high-speed mobile broadband connectivity to the region. The study examines the current state of mobile broadband infrastructure in South Asia, and the unique challenges that the region presents, such as population density and geography. The paper then provides an in-depth analysis of WiMAX and LTE, including their technical specifications, deployment requirements, and performance characteristics. The study also includes a comparison of the two technologies based on factors such as coverage, capacity, latency, and cost. The paper concludes with a recommendation for the best mobile broadband solution for South Asia, taking into account the specific needs of the region. The findings of this paper will be useful for policymakers, mobile network operators, and other stakeholders in the telecommunications industry who are seeking to improve mobile broadband connectivity in South Asia.","This paper analyzes two leading solutions for mobile broadband in South Asia: WiMAX and LTE. It seeks to discern which technology offers reliable and high speeds. We look at the current infrastructure for mobile broadband in South Asia and specific challenges like population density and geography. An in depth look at these technologies is given: including technical specs, deployment needs and performance features. Results of this study compare both technologies in terms of factors such as coverage, capacity, latency and cost. Conclusions are drawn about best technology to meet South Asian needs. Findings from this paper will be helpful to policy makers, mobile operators and others in telecom industry aiming to enhance broadband connectivity in South Asia."
"This paper presents a comprehensive study of bloated dependencies in the Maven ecosystem. The study aims to understand the extent and impact of bloated dependencies on the build process and overall performance of Maven projects. The study employs a combination of static code analysis and dynamic profiling to identify and quantify the impact of bloated dependencies. The results of the study show that bloated dependencies are a common problem in Maven projects and can significantly slow down the build process and increase the memory footprint of the build system. The paper also provides recommendations for mitigating the impact of bloated dependencies, including the use of dependency management techniques and the adoption of lightweight alternatives to bloated dependencies. The study provides valuable insights for developers, build engineers, and managers who are looking to optimize their Maven build processes and improve the performance of their projects.","This paper reports a comprehensive study on bloated dependencies within the Maven ecosystem. Aimed at understanding the degree and effect of bloated dependencies on Maven project builds and performance, this research uses both static code analysis and dynamic profiling to identify and measure these dependencies. Results indicate that bloated dependencies are frequent among Maven projects and have significant effects on slowing down build times and increasing build system memory usage. Recommendations for reducing these dependencies' influence are given such as using dependency management methods and adopting lighter alternatives. The findings contribute greatly to developers, build engineers, and managers aiming to optimize Maven build processes and enhance project performance."
"This paper investigates the factors that make a good summary in an educational context and the focus of automatic summarization technology. The study aims to determine the importance of key elements such as conciseness, relevance, and coherence in summarization and how they affect the effectiveness of summarization in an educational setting. The paper also explores the current state of automatic summarization technology, including its limitations and potential for improvement. The results of the study provide insights into the effective use of summarization in education and the future development of automatic summarization technology.",This paper looks at what makes a good summary relevant to education and considers what aspects of this are important for automatic summarization technology. Its goal is to find out which key features like conciseness and coherence matter most and how those features influence effectiveness when summaries are used in education. Results will illuminate how well summaries work in education and suggest ways to improve automatic summarization technology.
"This paper explores the utilization of heterogeneity in robust federated best-arm identification. The authors aim to address the challenge of selecting the best arm in a multi-armed bandit problem in a federated setting where the arms are distributed across multiple agents and may have different distributions. The proposed solution leverages the heterogeneity of the agents to design a robust and efficient algorithm for best-arm identification. The algorithm is tested on various simulation scenarios and the results demonstrate its effectiveness in comparison to other existing methods. The findings of this study contribute to the development of robust algorithms for federated multi-armed bandit problems, which have applications in various fields including personalized recommendation systems and online advertising.",This paper investigates using heterogeneity for robust best arm selection in federated settings. Authors consider the challenge of choosing the best option from multiple arms distributed among many agents which may have different distributions. Proposed solution uses heterogeneity among agents to develop an efficient and robust algorithm for identifying the best arm. Performance of this algorithm is assessed on diverse simulated scenarios; results show it outperforms other methods. Results of this work advance robust algorithms for federated bandit selection which has wide application such as personalized recommendation systems and online advertising.
"Frequency estimation is a critical problem in various signal processing applications. In this paper, we propose a novel approach to frequency estimation with one-sided error, which is a type of estimation problem where the estimated value is required to be within a specified range, either above or below the true value. The proposed method is based on the use of a modified version of the periodogram, combined with a statistical hypothesis test. The proposed approach is compared with existing frequency estimation methods in terms of accuracy, robustness, and computational complexity, and the results demonstrate that the proposed method outperforms existing methods in a variety of scenarios. The proposed method is a valuable tool for researchers and practitioners who are working on signal processing applications where frequency estimation with one-sided error is a critical requirement.","Frequency estimation is crucial for many applications in signal processing. This paper introduces a new approach to estimating frequencies with one sided errors, a type of estimation where the estimated value must fall within an acceptable upper or lower limit of the actual value. Our proposed method uses a modified version of periodogram together with a statistical hypothesis test. Results compare our proposed approach favorably against previous methods regarding accuracy, robustness, and computational costs. We show that our method performs better under different scenarios. Proposed method is useful for researchers and practitioners in signal processing where high quality frequency estimation with one-sided limits is essential."
"Fishing is a popular leisure activity, but anglers often face challenges in finding and catching fish. This study aimed to understand the anglers' fishing problem and identify potential solutions. A survey was conducted with a sample of experienced anglers to gather information about their fishing experiences, the challenges they face, and their preferred methods for finding and catching fish. The results showed that the most common challenges faced by anglers include limited access to fishing locations, poor water quality, and declining fish populations. To address these challenges, the study recommends improving access to fishing locations, implementing conservation efforts to protect fish populations and their habitats, and providing education and resources for anglers on sustainable fishing practices. The findings of this study can inform management and policy decisions to support the sustainable use of fishing resources and improve the fishing experience for anglers.","Fishing is a very popular pastime but anglers frequently encounter difficulties related to finding and catching fish. This research set out to investigate these difficulties and propose ways to solve them. We interviewed some experienced anglers to find out about their experiences, challenges, and preferred means of locating and catching fish. Results indicate that angling challenges are mainly about restricted access to spots, poor water quality and dwindling fish populations. To deal with those issues, the study suggests making access to fishing sites better, taking conservation steps to safeguard both fish and their habitat and educating anglers on best practices for sustainable fishing. Findings from this research could guide management and policy to support sustainable use of fishing resources and enhance the overall enjoyment of fishing among anglers."
"This paper explores the concept of processes, roles, and their interactions within organizational settings. The study aims to gain a deeper understanding of the dynamics of these elements and how they impact organizational performance. Through an extensive review of relevant literature, the paper identifies the various types of processes and roles and the functions they perform. It also examines the different interactions between processes and roles, including their mutual dependencies, conflicts, and collaborations. The research methodology includes qualitative and quantitative approaches, such as case studies, surveys, and statistical analysis. The findings highlight the crucial role of processes and roles in driving organizational success and the importance of effective interactions between them. The paper concludes with practical recommendations for organizations to optimize their processes, roles, and interactions for improved performance.","This paper looks into processes and roles and how they interact within organizational contexts. Its goal is to understand better how these things work together and how that impacts overall performance. By reviewing relevant literature carefully, this paper identifies different kinds of processes and roles and the tasks they do. Also it looks at different interactions between processes and roles like dependencies, conflicts, and cooperation. Methods used include both qualitative and quantitative approaches such as case studies and surveys along with statistical analysis. Results show that processes and roles are key drivers of success and that good interaction is important. In conclusion the paper gives some practical advice for organizations to improve process, roles and interactions for better performance."
"This paper presents a new approach for optimizing spread dynamics on graphs by utilizing message passing. The method is based on a combination of graph theory and probability theory to accurately model the spread of information or influence through a network. The authors demonstrate the efficacy of their method through extensive simulations and real-world experiments on various graph structures, showing significant improvements in spread optimization compared to existing methods. The results of this work have important implications for various fields, including social network analysis, marketing, and network design.","This paper introduces a new method for spreading dynamics optimization via message passing. Based on a blend of graph theory and probability theory, this method accurately models how information or influence spreads through networks. Authors show effectiveness of this approach using simulations and experiments on diverse network structures; they show substantial performance gains over other approaches. Results from this study have wide relevance including for such areas as social network analysis, marketing and network design."
"This paper presents a novel algorithm for determining optimal electricity pricing in a smart microgrid network. The algorithm is based on a multi-objective optimization framework that considers both economic and technical factors. The economic objective seeks to maximize the revenue of the microgrid operator, while the technical objective minimizes the total energy cost to the consumers. The proposed algorithm incorporates real-time demand and supply information to dynamically adjust electricity prices, taking into account factors such as renewable energy generation, energy storage capacity, and load management strategies. The performance of the algorithm was evaluated through simulation studies and compared to traditional pricing methods. The results showed that the proposed algorithm was able to significantly improve the overall efficiency of the microgrid network and provide substantial economic benefits to both the operator and consumers. This study contributes to the development of smart grid technologies and provides a practical tool for optimizing electricity pricing in microgrid networks.","This paper introduces a new algorithm to determine optimal pricing for an electricity system in smart microgrid networks. The algorithm uses a framework for solving multiple objectives that takes into consideration economic and technical factors. Economic goals aim to maximize profits of the microgrid operator; technical goals minimize total costs to consumers. The new method uses real time information about demand and supply and adjusts electricity prices accordingly by considering factors like renewable generation capacity, storage capacity and load management strategies. Performance of this algorithm has been assessed through simulation studies and compared to other pricing methods; results show significant improvement in overall network efficiency along with substantial economic benefits for both operator and consumers. This work also advances smart grid technology and supplies a useful means to optimize pricing in microgrid systems."
"In recent years, there has been growing interest in the study of computational complexity of various variants of combinatorial voter control problems in elections. Combinatorial voter control refers to the problem of manipulating the outcome of an election by making strategic changes to the votes of a subset of voters. This paper provides a comprehensive analysis of the computational complexity of several variants of this problem. First, we consider the classic voter control problem, where the goal is to make a minimum number of vote changes to ensure a desired candidate wins the election. We show that this problem is NP-hard, meaning that finding the optimal solution is computationally infeasible for large instances of the problem. Next, we explore the complexity of variants of this problem that take into account constraints such as budget limitations or restrictions on the number of vote changes that can be made to a single voter. We show that these variants are also NP-hard, and provide algorithms for finding approximate solutions to these problems. Finally, we consider the computational complexity of the voter control problem under different election systems, including single-winner elections, multi-winner elections, and proportional representation elections. We show that the complexity of the problem varies depending on the election system and provide algorithms for solving the problem under each system. This paper provides a comprehensive analysis of the computational complexity of various variants of combinatorial voter control problems in elections, and provides insights into the design of algorithms for solving these problems.","In recent years there has been increasing interest in studying computational complexity of different versions of combinatorial voter control problems in elections. Combinatorial voter control refers to the issue of strategically changing votes of some subset of voters to influence the result of an election. This paper analyzes the computational complexity of many variants of this problem comprehensively. Initially we look at the standard voter control problem, where the aim is to minimize the number of vote changes needed to secure a desired winner. We show this problem is NP hard, meaning it is computationally impractical for large instances of the problem. Then we investigate variants which consider limits like budget and restriction on number of changes per voter. We find that these variants are also hard and provide approximation algorithms for solving them. Finally we look at the problem under different election systems such as single winner, multi winner and proportional representation. We show the problem's hardness depends on the system and develop algorithms for each. Overall this paper provides an analysis of the computational complexity of combinatorial voting control problems and gives insight into designing algorithms for these problems."
"This paper presents a method for estimating subgraph frequencies from egocentrically sampled data, with or without attributes. The method is based on the observation that subgraph frequencies can be estimated using the product of the subgraph frequencies in the sampled data and the ratio of the number of subgraphs in the full graph to the number of subgraphs in the sampled data. The proposed method is evaluated on synthetic and real-world graphs and is shown to be effective in estimating subgraph frequencies with high accuracy. The results demonstrate the utility of the proposed method in various applications such as graph pattern mining and network analysis.","This paper introduces a method for estimating subgraph frequency from data sampled from ego-centric views, both with attributes and without. The approach uses the fact that subgraph frequencies can be estimated by multiplying the frequencies of subgraphs found in sample data by a ratio between total subgraphs in the full graph and those in the sample. Results show that this method performs well in terms of accuracy when compared to synthetic and real data. Results also suggest that this approach has utility in diverse applications including mining graph patterns and conducting network analysis."
"The paper presents Visual Genome, a new platform that connects language and vision using dense image annotations obtained through crowdsourcing. The paper describes the process of annotating and creating a large-scale dataset of images with detailed scene descriptions, object and relationship annotations, and question-answer pairs. The authors evaluate the quality of the data by comparing it to existing datasets and demonstrate its potential for various computer vision and natural language processing tasks. They also discuss the challenges and future directions for the platform, including improving data quality and expanding the scope of annotations. The paper concludes that Visual Genome provides a valuable resource for research in both computer vision and natural language processing and has the potential to significantly advance the field.",A new platform called Visual Genome is introduced in this paper which couples language and vision by using dense annotations from crowd workers. This paper describes how they annotate and create a very large set of images with fine scene descriptions and object and relationship annotations along with question answer pairs. Quality of the data is evaluated by comparison against other datasets and demonstrated usefulness for different vision and NLP tasks. Challenges and future work are discussed as well including quality improvements and expansion of annotation scope. Concluding remarks state that this platform provides great benefit for research in both vision and NLP and has strong potential to move the field forward.
"This paper explores the use of distributed scheduling algorithms for event analytics in a hybrid edge-cloud environment. The objective is to balance the computation load between edge devices and the cloud while ensuring timely and accurate event analytics. The proposed approach utilizes a combination of task-splitting and load balancing algorithms to dynamically distribute the event analytics tasks based on the available resources at the edge and in the cloud. The experimental results demonstrate that the proposed approach effectively balances the computation load, reduces the latency, and improves the overall efficiency of the event analytics process. This research provides a practical solution for event analytics in edge-cloud environments, which is critical for many emerging Internet of Things (IoT) and cyber-physical systems applications.","This paper investigates scheduling algorithms for distributed event analysis in hybrid environments involving both edges and clouds. The goal is to distribute workload evenly between these components to ensure prompt and reliable analysis of events. A hybrid approach of splitting tasks and balancing loads is used dynamically according to resource availability on edges and clouds. Results show that this method distributes processing work effectively, reduces latency and enhances overall performance for event analysis. Results also indicate this research offers practical solutions for analytics in edge cloud environments that are important for applications like IoT and CPS systems."
"This paper presents a novel deep bilateral learning approach for real-time image enhancement. The proposed method leverages the power of deep learning to enhance the visual quality of an image while preserving important features such as edges and textures. The method uses a bilateral filter to enforce smoothness and preserve edges, and a deep neural network to learn a mapping from low-quality to high-quality images. Experiments on a benchmark dataset demonstrate that the proposed approach outperforms state-of-the-art image enhancement methods in terms of both visual quality and computational efficiency. The results of this research could have significant impact in a variety of applications, including photography, computer vision, and image processing.","This study introduces a new deep bilateral learning approach for enhancing images in real time. The approach makes use of deep learning to improve visual quality while preserving essential features like edges and textures. Bilateral filtering is used to enforce smoothness and retain edges, and a deep neural network learns a mapping from low quality to high quality images. Results on a benchmark dataset show that performance of this approach surpasses leading methods regarding both visual quality and computational efficiency. Results from this work will have substantial impact across many different application domains including photography, computer vision and image processing."
"This paper presents a framework for entity-specific ranking of knowledge base properties. The proposed method is evaluated on two case studies: one using a doctoral advisor dataset and the other using a medical condition dataset. The results demonstrate that the entity-specific ranking framework outperforms traditional methods in terms of accuracy and relevance. The findings have significant implications for improving the quality and performance of knowledge-based systems, particularly in domains such as academic advising and medical diagnosis. The extended version of the paper provides a more in-depth analysis of the proposed method and its implementation, along with a thorough evaluation of its effectiveness on the two case studies.","This paper introduces a framework for specific ranking of knowledge base attributes. The method was tested using two case studies: one based on data about advisors for doctoral students and another based on medical conditions. Results show this specific ranking framework performs better than previous methods in terms of accuracy and relevance. Results suggest benefits for enhancing quality and performance of systems that rely on knowledge bases, especially those related to academic advising and diagnosis of diseases. Full version of the paper includes further analysis of the proposed approach including detailed description of implementation and thorough evaluation against these two case studies."
"Color constancy is the ability of the human visual system to perceive the color of an object accurately under different lighting conditions. In computer vision, this is a challenging problem that has been addressed using various algorithms such as gray-world, white-patch, and shade-of-gray. However, these algorithms do not perform well in all scenarios and require significant computational resources. In this paper, we formulate camera-adaptive color constancy as a few-shot meta-learning problem. The proposed approach leverages a meta-learning framework to enable the model to quickly adapt to new cameras and lighting conditions using only a small number of examples. Experiments on a variety of datasets demonstrate that the proposed approach outperforms traditional color constancy algorithms in terms of accuracy and computational efficiency. This work provides a promising solution for camera-adaptive color constancy and has the potential to impact a wide range of applications in computer vision and image processing.","Color constancy refers to the human visual system's ability to judge accurately the colors of objects under varying illumination conditions. For this reason, this has been difficult for computer vision systems which have attempted solutions such as using gray world, white patch and shades of gray. Yet those methods are not effective under all circumstances and they demand high computational power. This paper formulates adaptive color constancy as a few shot learning problem. Proposed approach uses meta learning framework to allow the model to quickly adapt to new cameras and light conditions using very little data. Results show the approach excels in both performance and efficiency compared to previous algorithms using diverse datasets. This work introduces a promising avenue for handling adaptive color constancy and could have broad impacts in vision computing and image processing."
"This paper explores the concept of liveness in broadcast networks. Liveness refers to the real-time transmission of content, as opposed to pre-recorded or on-demand material. The paper examines how the notion of liveness has evolved over time, particularly in relation to the rise of digital media and the increasing use of social media platforms. The paper provides a comprehensive review of the literature on liveness, drawing on both theoretical and empirical studies from a range of disciplines. It also presents original research on the perception and interpretation of liveness among different audiences, using both qualitative and quantitative methods. The findings suggest that the definition of liveness is not fixed, but rather varies depending on cultural, technological, and social factors. Moreover, audiences may have different expectations and interpretations of liveness depending on the type of content being broadcast and the context in which it is consumed. The paper concludes by arguing that liveness remains a crucial aspect of broadcast media, particularly in the context of live events and breaking news. However, as digital media continues to evolve, the concept of liveness may need to be re-examined in order to account for new forms of engagement and interactivity.","This paper looks at the concept of liveness within broadcasting networks. Liveness means real time transmission as opposed to prerecorded or on demand materials. We study how this idea has developed along with digital media and growing usage of social media sites. We look at existing literature that covers both theoretical and empirical studies across diverse disciplines. We report our own research on audience perceptions and understanding of liveness through qualitative and quantitative methods. We find that definition of liveness is fluid and depends on various cultural, technical and social factors. People's expectations and interpretations of liveness vary based on content type and context. In conclusion, liveness remains an important feature of broadcasting especially for live events and breaking news. But with ongoing development of digital media, we should reconsider liveness to accommodate newer engagement and interactivity."
"This paper presents a generative learning approach for spatio-temporal modeling in connected vehicular networks. The proposed approach combines deep learning techniques with generative models to effectively capture the complex spatial and temporal dependencies present in connected vehicular network data. The method is evaluated on a real-world dataset and demonstrates superior performance compared to traditional machine learning methods. The results of this study provide a promising direction for future research on spatio-temporal modeling in connected vehicular networks and have potential applications in traffic prediction and control, as well as in the design of intelligent transportation systems.",This paper introduces a new method for spatiotemporal modeling using connected vehicles. It integrates deep learning with generative models and performs effective modeling of complex spatial and temporal relationships among vehicle data. Results show high performance relative to traditional machine learning methods. Results from this study open promising avenues of future research into modeling spatiotemporal data; it also has practical implications for prediction and control of traffic and intelligent system design.
"Adversarial attacks on deep neural networks have become a significant security concern in recent years. In this paper, we propose a new approach for defending against such attacks by utilizing the shape information of the input data. Our method consists of pre-processing the input data to preserve its original shape and then feeding it into a deep neural network that has been trained to recognize the shape. Our experiments show that this approach is effective in defending against various types of adversarial attacks, including white-box and black-box attacks, with a significant improvement in overall accuracy compared to traditional defense methods. Our results demonstrate the effectiveness of incorporating shape information in defending against adversarial attacks, and suggest that this approach has the potential to be a promising direction for future research in this field.","Recent years have seen growing concerns about security threats posed by adversarial attacks targeting deep neural networks. This paper introduces a new strategy for defense that leverages information about the shapes of inputs. Our solution involves preprocessing input data to retain its shape and then feeding it into a trained deep network that can recognize those shapes. Results from our experiments show that this method works well against different types of attacks, both white box and black box, and significantly outperforms traditional defenses in terms of overall accuracy. Results indicate that including information about shape is effective for defense against such attacks; this research suggests that this approach merits further study as a promising path forward."
"Explanatory machine learning refers to the use of algorithms to understand and interpret the predictions made by machine learning models. While these algorithms can provide valuable insights into how models make decisions, they can also have harmful consequences if used improperly. This paper provides an overview of the beneficial and harmful aspects of explanatory machine learning, with a focus on the ethical implications of its use. On the beneficial side, explanatory machine learning can improve transparency and accountability in decision-making processes by providing clear and concise explanations of how models arrived at their predictions. This can help stakeholders understand and trust the decisions made by these models, leading to more effective and efficient outcomes. However, there are also harmful aspects to consider. Explanatory machine learning can perpetuate biases and reinforce discriminatory practices if it is trained on biased data. Additionally, the explanations generated by these algorithms can be misleading, leading to incorrect decisions based on incomplete or inaccurate information. The ethical implications of explanatory machine learning are significant, and it is important for practitioners to consider these implications when designing and deploying these algorithms. This paper concludes with a call for further research and discussion to ensure that the benefits of explanatory machine learning are realized while minimizing its potential harm.","Explanation of machine learning means using algorithms to explain why a model makes predictions. These algorithms offer great benefits but can cause harm if misused. This paper looks at both sides of this issue, focusing on ethical considerations. On the positive side, explanation through algorithmic means increases transparency and responsibility regarding decision making by offering clear reasons behind model predictions. This helps stakeholders to understand and trust decisions made by those models which improves effectiveness and efficiency. Yet there are also negative aspects to consider. If training data has bias, explanations produced from algorithms can perpetuate such bias and promote discrimination. Moreover, explanations generated by algorithms can be misleading so that decisions are wrong because of missing or inaccurate information. Ethical concerns are very high and practitioners must think about these when designing and deploying these algorithms. In conclusion, more research and discussion are needed to realize benefits while reducing harms of explanation via machine learning."
"This paper investigates the use of Local Interpretable Model-Agnostic Explanations with Outlier Detection (LIMEOUT) to improve the fairness of machine learning (ML) models. LIMEOUT is a recently developed approach that combines LIME, which explains the predictions of ML models at the local level, with outlier detection, which identifies instances that are far from the training data distribution. The paper demonstrates the effectiveness of LIMEOUT in improving the fairness of ML models by analyzing its performance on several benchmark datasets that exhibit different types of fairness-related challenges. The results show that LIMEOUT can significantly improve the fairness of ML models by identifying and mitigating bias in the predictions. Furthermore, the paper shows that LIMEOUT can be used to provide transparent and interpretable explanations for the decision-making process of ML models, which can enhance their trustworthiness and accountability. Overall, the paper suggests that LIMEOUT is a promising approach for making ML models fairer and more transparent, which can help mitigate the ethical concerns associated with their use in various domains.","This study looks into using Local Interpretability Measure of Explanation with Outlier Detection (LIMEOUT) to improve fairness of machine learning (ML) models. LIMEOUT is a new approach that integrates LIME for explanation of model predictions locally with outlier detection for finding examples far from the training data distribution. Results show that LIMEOUT effectively enhances fairness of ML models by performing well on diverse fairness challenges benchmark datasets. Results indicate that LIMEOUT significantly reduces bias in prediction outcomes. Moreover, this paper shows that LIMEOUT can also serve to provide clear and understandable explanation for decisions made by ML models thereby enhancing credibility and accountability. Overall, the study indicates that LIMEOUT is an important advance towards making ML models both fairer and more transparent and hence mitigates ethical issues that arise with their usage across different sectors."
"Permutations are widely used in various fields, such as cryptography, computer science, and mathematics. Understanding the relations between permutations is important for many applications, but determining these relations can be a challenging task. In this paper, we investigate the testability of relations between permutations. Specifically, we focus on the problem of testing whether two permutations are equal, inverse, or conjugate to each other, given access to their image and inverse images under some fixed set of points. We provide efficient algorithms for these testing problems and show that they have low query complexity, i.e., the number of queries to the input permutations needed by the algorithm is relatively small. Our algorithms are based on a combination of group theory and graph theory techniques, and their analysis involves tools from probabilistic combinatorics and algebraic geometry. We also provide lower bounds for the query complexity of these testing problems, showing that our algorithms are optimal up to constant factors. Our results have implications for the study of permutation groups and their applications in computer science and cryptography.","Permutations play a role in many different fields such as cryptography and computer science as well as pure mathematics. Relations among permutations are important for practical application; however, discerning those relations is difficult. This paper investigates the testability of such relation among permutations specifically. We concentrate on whether two permutations are identical, inverses, or conjugates to each other by probing them through some fixed set of points. We give efficient algorithms for testing these properties and prove they use only a low number of queries. The algorithms rely on methods from both group theory and graph theory, and their proofs use tools from probabilistic combinatorics and algebraic geometry. We also establish lower bounds on the number of queries needed which shows that our algorithms are nearly optimal. Results of this work impact studies into permutation groups and their uses in computing and cryptography."
"This paper presents a new approach to object re-identification (re-ID) using transformer-based models, referred to as TransReID. The proposed method leverages the self-attention mechanism of transformers to capture the long-range dependencies between feature maps and global contextual information. The experimental results demonstrate the superiority of TransReID over state-of-the-art re-ID methods on several benchmark datasets. The paper also analyzes the impact of different components of the TransReID framework, including the number of layers, the size of the model, and the use of additional data augmentation techniques. The findings provide insights into the effectiveness of the proposed method and its potential for improving the performance of re-ID tasks in practical applications.","This paper introduces a new way of doing something called Re-ID by using transformer models. We call this method TransReID. Using attention mechanisms from transformers, we capture important relationships between feature maps and global context. Results show that TransReID outperforms other top methods on some important datasets. We also study the effect of different parts of TransReID such as number of layers and model size and also augmentations. Results help us understand how good this new method is and how it can work well in real life."
"This paper conducts a bibliometric analysis literature review to examine the research trends and patterns related to the digital agricultural revolution. The review covers studies published from 2011 to 2021 in several databases, including Scopus, Web of Science, and Google Scholar. The analysis focuses on the growth of research in this field, the most cited authors and publications, and the main research themes and trends. The findings suggest that research on the digital agricultural revolution has been growing steadily over the past decade, with a particular emphasis on precision agriculture, sensor technologies, and big data analytics. The most cited authors and publications are mainly from developed countries, indicating a significant gap in research from developing countries. The study highlights the need for further research to address the challenges and opportunities of the digital agricultural revolution, particularly in developing countries.","This paper does a review of bibliometrics to study research trends and patterns of the digital revolution in agriculture. It looks at studies published between 2011 and 2021 using databases such as Scopus, Web of Science and Google Scholar. Focus of the analysis is growth of research in this area, top cited researchers and papers, and major themes and trends. Results show steady growth of research over recent decade with special focus on precision agriculture and sensor technologies along with big data analytics. Most cited researchers and papers tend to be from developed countries showing a large gap in research from developing countries. Results point to further work needed especially in developing countries to deal with challenges and opportunities of digital agricultural revolution."
"The Ethics and Society Review of Artificial Intelligence Research (ESR) is a comprehensive examination of the moral and ethical implications of artificial intelligence (AI) research and development. This paper provides an in-depth analysis of the current state of AI ethics, including discussions of the various ethical frameworks and principles that have been proposed to guide AI development, as well as the challenges and limitations of these frameworks. The paper also explores the role of social and political institutions in shaping AI ethics, including the role of governments, corporations, and non-governmental organizations. The paper concludes by presenting recommendations for future AI research, including the need for greater collaboration between AI researchers, ethicists, and society, and the importance of a proactive approach to AI ethics that takes into account the social, political, and economic impacts of AI on society.","The Ethics and Society Review of Artificial Intelligence (ESR) is a detailed review of the moral and ethical implications of AI research. It looks closely at current issues in AI ethics and analyzes different ethical frameworks and principles that have been suggested for AI development. Challenges and limits of those frameworks are also discussed. The ESR also considers how different kinds of social and political institutions shape AI ethics; this includes government agencies, businesses, and nongovernmental groups. Finally the paper suggests some recommendations for future research such as increased cooperation among AI researchers, ethicists and society and proactivity towards ethical issues that take into account societal impacts of AI."
"The increasing need for continuous environmental monitoring has led to the development of energy-efficient sensor nodes that can harvest energy from the environment. In this paper, we present eico, an energy-harvesting long-range environmental sensor node that co-optimizes energy and information. Eico incorporates a number of novel features including an adaptive power management system that optimizes the energy consumption of the node, a dynamic sensing module that adjusts the sensing frequency based on the level of activity in the environment, and a machine learning algorithm that predicts energy availability and adjusts the sensing frequency accordingly. We evaluate the performance of eico through a series of experiments, and demonstrate that eico achieves a longer lifetime and higher sensing accuracy than existing energy-harvesting sensor nodes. Our results suggest that eico has the potential to be a game-changer in the field of environmental monitoring, enabling continuous and accurate data collection with minimal energy consumption.","As demand grows for continuous monitoring of the environment, there is development of energy efficient sensor nodes which can collect energy directly from their surroundings. This paper introduces EICO; an energy harvesting sensor node that optimizes both energy and information. It includes several new features such as an adaptive power management system that minimizes energy consumption, a sensing module that dynamically adjusts sensing frequency based on environmental activity levels and a machine learning algorithm that forecasts energy availability and adjusts sensing frequency. Results show that EICO has much longer lifetimes and better sensing accuracy compared to other nodes that also harvest energy. Results suggest EICO could revolutionize environmental monitoring by collecting continuous and precise data with very low energy use."
"This paper presents a new approach to modelling epistemic uncertainty using Bayesian confidence calibration. Epistemic uncertainty arises from a lack of knowledge or understanding about a system, which can result in significant errors when making predictions. Bayesian methods have been used extensively in the literature to model epistemic uncertainty, but they can suffer from overconfidence when the model is misspecified or when the data is limited. To address this issue, we propose a new method based on Bayesian confidence calibration, which aims to better calibrate the uncertainty estimates produced by Bayesian models. Specifically, we introduce a calibration function that adjusts the uncertainty estimates produced by the Bayesian model based on the reliability of the data and the degree of model misspecification. We demonstrate the effectiveness of our approach on synthetic and real-world datasets and compare it to other popular methods for modelling epistemic uncertainty. Our results show that our approach produces more accurate and reliable uncertainty estimates, which can be valuable for decision-making under uncertainty. We conclude by discussing the potential applications of our method in various domains, including finance, engineering, and healthcare.","This paper introduces a new way to model epistemic uncertainty via Bayesian confidence calibration. Uncertainty due to lack of knowledge about a system often leads to large prediction errors. Bayesian methods are frequently used to model such uncertainty, but these methods tend to overestimate uncertainty when the model is misspecified or when data is scarce. To solve this problem, we present a new approach based on calibrated confidence estimation. Specifically, we introduce a calibration function that adjusts uncertainty estimates made by Bayesian models based on reliability of data and level of specification mismatch. We show effectiveness using both synthetic and real data and compare against popular methods for uncertainty modeling. Results indicate that our method produces better and more reliable uncertainty estimates that are useful for decisions under uncertainty. Finally we discuss possible application areas for this method including finance, engineering and healthcare."
"This paper investigates the improvement of loadability in distribution systems using genetic algorithm optimization techniques. The aim of the research is to optimize the load transfer between distribution feeders and improve the overall system loadability. The study uses a realistic distribution system model and proposes a genetic algorithm-based approach to solve the optimization problem. The proposed algorithm is applied to a range of load profiles, and its effectiveness is evaluated based on the improvement in loadability achieved. The results show that the genetic algorithm approach can effectively improve load transfer and increase the loadability of the distribution system. The study also compares the performance of the proposed approach with other existing methods and shows that the genetic algorithm-based approach outperforms them in terms of loadability improvement. The findings of this study can be used by distribution system operators to optimize their load transfer strategies and improve system performance.",This paper investigates how genetic algorithms can be used to enhance loadability in distribution systems. The purpose is to maximize transfer of load among feeders and thereby overall system performance. Using a realistic system model the authors propose an approach using genetic algorithms to solve the optimization problem. Performance of this method is tested on different load profiles and results indicate that genetic algorithm methods work well for load transfer and increase loadability of the system. Comparisons with other approaches also show that genetic algorithm methods outperform others in terms of improved loadability. Results of this research can help distribution system operators to develop better transfer strategies and enhance performance.
"This paper presents an analysis of the throughput performance of wireless powered cognitive radio networks (WPCRNs) using compressive sensing (CS) and matrix completion (MC) techniques. The study focuses on the utilization of surplus energy from primary users to power secondary users in WPCRNs and the impact of different system parameters, such as the number of primary and secondary users, the compression ratio of CS and the MC rank, on the overall throughput performance of the system. The authors use simulations to evaluate the proposed approach and compare its performance with existing methods. The results show that the combination of CS and MC techniques can significantly enhance the throughput performance of WPCRNs and provide better energy efficiency compared to traditional methods. The findings of this study can be useful for future research in the field of WPCRNs and provide insights into the design of efficient and effective communication systems.","This paper studies performance metrics for wireless powered cognitive radio networks (WPCRNs) using CS and MC methods. It focuses on using extra energy from main users to energize secondary users. It also looks at performance changes due to different system parameters such as the number of primary and secondary users, compression ratios for CS and MC ranks. Simulations are used to test proposed approaches and compare them against other methods. Results indicate that combining CS and MC methods substantially improves system performance and increases efficiency compared to usual means. Results from this study could be very useful for further research on WPCRNs and contribute to designing good communication systems."
"This paper investigates the Conditional Lucas & Kanade (CLK) algorithm, a popular computer vision technique for estimating the optical flow between two frames in a video sequence. Specifically, we propose a novel extension to the original Lucas & Kanade (LK) algorithm by incorporating a spatially varying weighting matrix that conditions the computation of the optical flow on local image features. This extension improves the accuracy and robustness of the original LK algorithm by accounting for variations in the image data, such as illumination changes and occlusions. To evaluate the performance of the proposed CLK algorithm, we conducted experiments on several standard benchmark datasets and compared its results to those of other state-of-the-art optical flow methods. Our experiments demonstrate that the CLK algorithm achieves superior accuracy and robustness compared to the original LK algorithm and other competing methods, particularly in challenging scenarios with large displacements and occlusions. Furthermore, we demonstrate the applicability of the CLK algorithm in real-world applications such as visual odometry, where accurate and robust estimation of the motion of a moving camera is essential. Our results show that the CLK algorithm outperforms other optical flow methods in terms of both accuracy and computational efficiency. Overall, our research highlights the effectiveness of the proposed CLK algorithm as an improved version of the original LK algorithm, and its potential to advance the field of computer vision in various real-world applications.",This paper studies the Conditional Lucas & Kanade (CLK) algorithm which is a common technique in computer vision for computing optical flow from one frame to another in a video sequence. We introduce a new enhancement of the original Lucas and Kanade (LK) algorithm by including a spatially varying weight matrix. This modification allows us to compute optical flow conditioned on local features and thus enhance both accuracy and robustness to changes in lighting and occlusion. Performance evaluation was done using standard benchmark datasets and comparisons were made against top algorithms for optical flow. Results show that this enhanced CLK algorithm performs better than the original LK algorithm and competing methods especially in hard cases with large displacements and occlusions. We also show practical benefits of the CLK algorithm in real life such as visual odometry where reliable estimation of camera motion is important. Results indicate that this algorithm excels both in accuracy and computational efficiency. Overall we conclude that the enhanced CLK algorithm is an effective improvement over the original LK algorithm and has promise to advance computer vision in diverse practical tasks.
"This paper presents a new dataset called ""Amigos"" that is designed for affect, personality, and mood research on individuals and groups. The dataset consists of audio recordings, video recordings, and self-reported questionnaires collected from a diverse sample of participants in various social contexts. The paper provides a detailed description of the dataset, including the data collection procedures, the measures of affect, personality, and mood, and the ethical considerations involved in the study. The paper also presents several examples of how the dataset can be used for research in different areas such as social psychology, affective computing, and human-computer interaction. Overall, the Amigos dataset provides a valuable resource for researchers interested in studying affect, personality, and mood in both individual and social contexts.","This paper introduces a new dataset called 'Amigos' which focuses on research into emotions, personalities and moods at both individual and group level. Data includes audio and video recordings alongside self reports. Procedures for collecting data are described along with measures of affect, personality and mood. Ethical concerns are also addressed. Results show how this data set is useful for research in psychology of sociality, affective computing and HCI among others. In summary, Amigos dataset is very important for those who want to study emotions, personality and mood across both individual and social levels."
"This paper investigates the connectivity properties of random annulus graphs and the geometric block model. These models are commonly used in network science to model real-world networks with underlying geometric structure. In particular, we focus on the critical threshold for connectivity, which is the probability at which the network transitions from being disconnected to being connected. We first introduce the random annulus graph, which is a random graph generated by connecting points in an annulus with probability determined by the Euclidean distance between the points. We prove that the critical threshold for connectivity in the random annulus graph is determined by a simple function of the parameters of the annulus. Next, we consider the geometric block model, which is a random graph generated by partitioning a d-dimensional space into blocks and connecting points within the same block with some probability. We show that the critical threshold for connectivity in the geometric block model is determined by the product of the block probabilities and the volume of the blocks. Finally, we compare the critical thresholds for connectivity in the random annulus graph and the geometric block model. We demonstrate that the critical threshold in the geometric block model is always greater than or equal to that in the random annulus graph. This result has important implications for the design and analysis of networked systems with underlying geometric structure, such as wireless sensor networks and social networks. Overall, this paper provides important insights into the connectivity properties of random graphs with underlying geometric structure.","This study looks at connectivity properties of random annulus graphs and geometric block models. These models are frequently employed in network science to represent networks with inherent geometric structures. Specifically, we focus on critical threshold for connectivity: where networks go from disconnected to connected. Initially we define random annulus graphs, which are random graphs formed by linking points within an annulus with connection probability depending on Euclidean distances between points. We prove that critical threshold for connectivity depends on simple functions of annulus parameters. Next we study geometric block model which generates random graphs by dividing d dimensional space into blocks and linking points within the same block with some probability. Results show that critical threshold of connectivity depends on product of block probabilities and volume of blocks. Finally we compare thresholds for connectivity in random annulus graph versus geometric block model. Results show that connectivity threshold in block model is larger than or equal to that of annulus model. This result matters a lot to design and analysis of systems like wireless sensor networks and social networks. In sum, this paper offers key insight into connectivity properties of random graphs with inherent geometric features."
"The Fast Fourier Transform (FFT) is a crucial tool in a wide range of scientific and engineering applications, including signal processing, image processing, and scientific simulations. As the size of the data increases, the computational cost of computing the FFT also increases, making it necessary to develop efficient parallel algorithms for the FFT. In this paper, we present a new parallel algorithm for the multidimensional FFT using Advanced Message Passing Interface (MPI) on high-performance computing systems. We propose a two-stage approach that combines a distributed 1D FFT algorithm with a block-cyclic data distribution scheme. Our algorithm exploits the properties of the block-cyclic distribution to minimize the communication overhead and balance the workload among the processors. We also use the latest features of MPI to optimize the communication and reduce the synchronization overhead. We demonstrate the performance and scalability of our algorithm on a range of hardware platforms, including large-scale clusters and supercomputers. Our experiments show that our algorithm achieves near-linear speedup up to thousands of processors, and outperforms existing parallel FFT libraries for large-scale data. Our approach is highly efficient and can significantly reduce the computation time for the multidimensional FFT, enabling faster and more accurate scientific simulations and data analysis in various fields.","The fast Fourier transform (FFT) is a key tool in diverse areas like science and engineering; examples include processing signals and images and doing simulations. As data volume grows, so does the computational cost for computing FFTs. Thus, development of efficient parallel algorithms for FFTs is important. This paper introduces a new parallel algorithm for multidimensional FFT using MPI on high performance computers. A two stage approach is proposed that uses distributed one dimensional FFT alongside block cyclic distribution. We exploit properties of cyclic distribution to minimize communication burden and balance load among processors. We also employ recent features of MPI to optimize communication and lower synchronization overhead. Performance and scalability of this algorithm are demonstrated on different hardware setups such as large cluster and supercomputers. Results show that the algorithm attains nearly linear speedup up to thousands of processors and outperforms existing parallel FFT libraries for big data. Our method is very effective and significantly reduces computation time for multidimensional FFT; this will improve simulation speed and data analysis accuracy across many disciplines."
"This paper presents an approximate dynamic programming (ADP) approach to community recovery management following a natural disaster. Traditional recovery management approaches have focused on pre-defined and static recovery plans, which may not be applicable or effective in dynamic and complex post-disaster scenarios. The proposed ADP approach is designed to learn from past disaster events and dynamically adapt recovery strategies based on the evolving state of the affected community. The ADP approach combines data-driven machine learning techniques with a dynamic optimization framework to provide adaptive decision-making capabilities for recovery management. The framework considers multiple objectives, such as minimizing recovery time, maximizing resource utilization, and minimizing costs, while accounting for the uncertainties and constraints associated with the recovery process. The ADP approach is also able to incorporate feedback from the community and recovery stakeholders, enabling the system to learn from its own performance and improve over time. To evaluate the effectiveness of the ADP approach, a case study is conducted on a hypothetical community impacted by a major earthquake. The results demonstrate that the proposed approach is able to provide effective recovery strategies that outperform traditional static recovery plans. Moreover, the ADP approach is able to quickly adapt to changes in the community and recovery environment, enabling efficient and effective recovery management. Overall, this research highlights the potential benefits of applying ADP to community recovery management, providing decision-makers with a powerful tool to better prepare for and respond to natural disasters. The ADP approach is able to learn from past experiences, dynamically adapt to changes in the recovery environment, and effectively balance multiple recovery objectives, ultimately resulting in more resilient and sustainable communities.","This paper introduces an Approximate Dynamic Programming (ADP) method for disaster recovery management. Current methods tend to rely on fixed and rigid plans that fail to suit the unpredictable nature of post disaster scenarios. Proposed ADP uses past disaster experience to adaptively adjust strategies according to evolving conditions of the affected community. This approach blends data driven machine learning techniques with a framework for dynamic optimization to achieve flexible decision making for recovery management. It addresses diverse goals including reducing recovery time, maximizing resource use, and lowering costs while considering uncertainties and constraints of the recovery process. Feedback from community and recovery stakeholders can also enhance learning and ongoing improvement. A hypothetical community hit by a major earthquake serves as a case study. Results show that this new approach produces high quality recovery plans surpassing previous static plans. Furthermore, ADP swiftly adjusts to environmental changes allowing effective and efficient management. Overall, this research shows potential benefits of ADP for disaster recovery management offering decision makers a strong tool for disaster preparation and response. ADP learns from past events, flexibly adapts to changing environments and efficiently balances diverse recovery goals thus creating more resilient and sustainable communities."
"This paper proposes a novel approach for optimizing the trajectories of cooperative dual-band UAV swarms in order to improve their overall performance. The proposed approach leverages the unique capabilities of dual-band UAVs, which can operate in two different frequency bands simultaneously, to enhance the swarm's communication and sensing capabilities. The trajectory optimization problem is formulated as a multi-objective optimization problem that considers several objectives, including minimizing the total time of the mission, minimizing the energy consumption of the UAVs, and maximizing the quality of the communication and sensing links between the UAVs. The proposed approach utilizes a genetic algorithm to search for the optimal trajectories that satisfy the specified objectives. The paper presents simulation results that demonstrate the effectiveness of the proposed approach in improving the performance of the cooperative dual-band UAV swarm. The simulation results show that the proposed approach can significantly reduce the mission time and energy consumption while also improving the quality of the communication and sensing links. The paper also discusses the practical implications of the proposed approach and identifies several areas for future research. Overall, this research contributes to the growing body of literature on cooperative UAV swarms and provides a promising approach for optimizing the trajectories of dual-band UAVs in real-world applications.","This paper introduces a new method for optimizing trajectories of cooperative swarms of UAVs operating at dual frequencies so that performance improves. This new method makes use of the specific advantages of dual frequency UAVs which can work concurrently in different frequency bands to boost both communication and sensing abilities. This paper formulates the trajectory optimization as a multi objective optimization problem considering various goals like reducing total time spent on mission, reducing energy consumption by UAVs and increasing quality of communication and sensing links among UAVs. Genetic algorithms are used to search for trajectories that optimize these goals. Simulation results presented show effectiveness of this method to significantly reduce mission times and energy consumption while also improving quality of communication and sensing links among UAVs. Practical significance and future research directions are also discussed in this paper. Overall this research advances the body of knowledge about coordinated UAV swarms and offers a promising approach for trajectory optimization for dual frequency UAVs in practice."
"Reinforcement learning (RL) is a popular machine learning approach for decision making problems, where an agent learns from the consequences of its actions to maximize a reward signal. However, in many real-world applications, the reward signal is subject to noise or perturbations, which can lead to suboptimal or undesirable behavior. This paper presents a comprehensive study of RL with perturbed rewards, focusing on two main aspects: (1) the impact of reward perturbations on the learning process and the performance of RL algorithms, and (2) the design of robust RL algorithms that are able to handle reward perturbations effectively. The study explores different forms of reward perturbations, such as Gaussian noise, stochastic reward functions, and reward tampering attacks, and evaluates various RL algorithms, including value-based, policy-based, and actor-critic methods, under different perturbation scenarios. The results show that reward perturbations can have a significant impact on the learning process and the performance of RL algorithms, and highlight the importance of designing robust RL algorithms that can effectively handle reward perturbations. The paper provides insights and guidelines for future research in this area and has implications for a wide range of applications, including robotics, gaming, autonomous systems, and more.","Reinforcement learning (RL) is one of the most important methods for solving decision making problems; agents learn by acting and receiving a reward. In practice however, there is often noise or variation in rewards that can result in poor decisions. This paper looks at reinforcement learning with noisy rewards through two major themes: (1) how reward noise impacts learning and algorithm performance and (2) development of robust algorithms capable of handling noise well. The study considers different types of noise like Gaussian noise and attacks and compares different RL algorithms such as value based and actor critic methods. Results show that noise has a strong effect on learning and algorithm performance and suggests that we need better algorithms to deal with these issues. The paper offers guidance and ideas for further research and applications ranging from robotics to games and autonomous systems."
"This paper presents a lower bound for the Boolean Satisfiability (SAT) problem on deterministic Turing machines. The SAT problem is a fundamental problem in computer science, which asks whether a given boolean formula can be satisfied by some assignment of truth values to its variables. The lower bound established in this paper states that any deterministic Turing machine algorithm that solves the SAT problem requires at least 2^n / poly(n) time, where n is the number of variables in the input formula and poly(n) is some polynomial function of n. To prove this lower bound, the paper introduces a novel technique for constructing hard instances of the SAT problem that require a large number of steps to solve on a Turing machine. The technique is based on the notion of ""expanding"" formulas, which involves adding redundant clauses to the formula in such a way that any satisfying assignment must satisfy a large subset of the added clauses. The paper then shows that any deterministic Turing machine algorithm for the SAT problem must spend a significant amount of time checking these redundant clauses, which leads to the lower bound. The significance of this result is that it provides a strong lower bound for the SAT problem on deterministic Turing machines, which is a widely studied problem in theoretical computer science. The lower bound implies that there is a fundamental limit to the efficiency of any deterministic algorithm for solving the SAT problem, which has important implications for the design and analysis of algorithms in general. The paper concludes by discussing some potential avenues for future research based on the techniques developed in this work.","This paper establishes a lower bound for solving Boolean satisfiability problems using deterministic Turing machines. SAT is an important question in computer science asking whether we can assign true/false values to variables so that a given Boolean formula evaluates to true. Here we show that any algorithm running on such machines will take at least 2 to the power of n divided by a polynomial function of n in time; here n refers to the number of variables in the input formula and polynomial functions are related to n. To prove this result, this paper develops a new method of creating hard SAT instances. This method involves adding redundant clauses in a way such that satisfying assignments have to satisfy a large fraction of them. We show that checking these clauses takes a long time and this results in our lower bound. The significance of this work is that it proves a strong lower bound for SAT on deterministic machines which is a central topic in theoretical computer science. Results imply that efficient deterministic algorithms do not exist for solving SAT and have broader implications for algorithm design and analysis. Concluding remarks mention some promising lines for further research using ideas from this paper."
"In this paper, we present a parallel implementation of the Ensemble Kalman Filter (EnKF) based on a modified Cholesky decomposition. The EnKF is a popular data assimilation method that uses a Monte Carlo approach to estimate the state of a complex dynamic system. However, its implementation can be computationally expensive, especially for large-scale problems. To address this issue, we propose a parallel implementation of the EnKF that leverages the modified Cholesky decomposition to distribute the computation across multiple processors. The modified Cholesky decomposition is a matrix factorization method that provides an efficient way to compute the square root of a positive-definite matrix. By using this technique, our parallel implementation of the EnKF is able to significantly reduce the computation time without sacrificing accuracy. The performance of the parallel implementation is evaluated using numerical experiments, and the results show that it is able to achieve substantial speed-ups compared to a sequential implementation of the EnKF. This work has implications for a wide range of applications, including weather forecasting, climate modeling, and seismic imaging, where the EnKF is used to process large amounts of data in real-time.","We introduce a parallel implementation of Ensemble Kalman Filter (EnKF) that uses modifications of Cholesky decomposition. EnKF is a common method for data assimilation using Monte Carlo approaches to estimate states of complicated systems. But its usual implementation can be quite costly in terms of computing power, especially when dealing with large problems. We improve on this by proposing a parallel version of EnKF which exploits modification of Cholesky decomposition to distribute processing among many processors. Cholesky decomposition factors matrices efficiently and computes square roots of positive definite ones. Using this trick, our parallel EnKF reduces computation time substantially while maintaining high accuracy. Performance is assessed through numerical tests and results show substantial speed ups compared to sequential code. Results have wide impact such as in weather prediction, climate modeling and seismic imaging where EnKF processes very large datasets in near real time."
"The increasing popularity of YouTube as a platform for content sharing has raised concerns about the potential risks to children from exposure to harmful or inappropriate content. This paper focuses on the detection, characterization, and analysis of child unsafe content and promoters on YouTube. We conducted a large-scale analysis of the Kidstube dataset, consisting of over 100,000 videos, to identify content that poses a risk to children, including violent, sexual, and misleading content. We also analyzed the behavior of promoters who exploit children's vulnerabilities for their benefit, including through the use of deceptive advertising and manipulation of search algorithms. Our results provide insights into the prevalence of child unsafe content on YouTube, the tactics used by promoters to attract young audiences, and the potential impact on children's wellbeing. The findings highlight the need for improved detection and removal of harmful content, as well as better regulation of advertising practices on the platform to protect children from harm.","As YouTube becomes more popular as a place to share content, there are growing concerns regarding risks to children due to exposure to harmful or inappropriate material. This paper focuses on detection, characterization and analysis of unsafe content and people who promote such content on YouTube. We performed a large study of the Kidstube dataset containing more than 100,000 videos to find dangerous content that could harm children including violence, sex, and misleading information. We also looked at behavior of those who take advantage of children's vulnerabilities including deceptive ads and manipulation of search algorithms. Results reveal trends about prevalence of unsafe content on YouTube and tactics used by promoters to reach younger audiences. They also show possible impact on children's welfare. Results emphasize importance of better detection and removal of harmful content along with better regulation of ads to safeguard children against harm."
This paper focuses on the proof search specifications of bisimulation and modal logics for the π-calculus. The π-calculus is a formal system for modeling the dynamic behavior of distributed systems. The paper examines the use of bisimulation and modal logics as proof search specifications for verifying the correctness of π-calculus models. The authors present a detailed analysis of the proof search algorithms and their implementation in the context of the π-calculus. The results of the study demonstrate the effectiveness of using bisimulation and modal logics as proof search specifications for verifying the correctness of π-calculus models. The findings of the research have important implications for the design and implementation of verification tools for distributed systems.,This paper looks at specification methods for bisimulation and modal logic for the π calculus. π calculus is a formal system for modeling dynamic behavior of distributed systems. The paper studies use of bisimulation and modal logic for checking correctness of π calculus models. Authors detail an analysis of algorithm search proof and its implementation within π calculus. Results show effectiveness of using bisimulation and modal logic for checking correctness of π calculus models. Results of this research are significant for designing and implementing verification tools for distributed systems.
