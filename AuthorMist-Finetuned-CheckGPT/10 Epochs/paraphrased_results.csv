Original Text,Paraphrased Text
"This paper proposes a novel method for representative selection in big data using a combination of sparse graph and geodesic Grassmann manifold distance. The goal of representative selection is to identify a small subset of data points that capture the essential characteristics of the larger dataset, making it possible to analyze and draw conclusions from the data more efficiently. The proposed method first constructs a sparse graph from the data, where each data point is a node in the graph and the edges represent the pairwise similarity between data points. Then, it uses a graph clustering algorithm to identify clusters of highly similar data points. Next, it selects a representative from each cluster by computing the geodesic Grassmann manifold distance between each pair of data points and selecting the point that is closest to the mean of the cluster in the Grassmann manifold. The effectiveness of the proposed method is demonstrated through experiments on several real-world datasets. The results show that the method outperforms several existing representative selection methods in terms of both representativeness and efficiency. The proposed method can be used in a variety of applications, such as data summarization, visualization, and classification, where representative selection is a critical step in the analysis of large datasets.","This paper introduces a new way of choosing representatives from large amounts of data using a combination of sparse graphs and geodesic distances on Grassmann manifolds. The main aim is to select a small subset of representative samples that reflect the essence of the whole dataset. To do this, we start by constructing a sparse graph from the data where nodes are data points and edges denote similarity between pairs of points. We then use clustering algorithms to find groups of closely related data points. Finally, we choose a representative element from each group by computing geodesic distances on the Grassmann manifold and picking the one closest to the mean of the group. Results show that our method works better than others in terms of both representing and efficiency. We expect this method to be useful in many tasks like summarizing data, visualizing and classifying large data sets where choosing good representatives is important."
"Thermodynamic-RAM (T-RAM) is a novel computational framework that leverages thermodynamic principles to perform calculations, and has shown promise in its ability to mimic the behavior of biological neurons. In this paper, we investigate the potential of T-RAM for simulating cortical processing in the brain. We first provide an overview of the architecture and operating principles of T-RAM, and compare it to other neuromorphic computing systems. We then describe our implementation of T-RAM for simulating cortical processing, using a simple model of the visual cortex as a test case. We demonstrate that T-RAM is able to replicate several key features of cortical processing, including selective attention, receptive fields, and ocular dominance. We also investigate the influence of various parameters on T-RAM performance, and discuss the implications of our findings for the development of more sophisticated T-RAM-based models of cortical processing. Our results suggest that T-RAM has the potential to be a useful tool for investigating the mechanisms of cortical processing, and may ultimately lead to the development of more efficient and biologically-inspired computational systems.","Thermodynamic RAM (T RAM) is a new computational system which uses principles from thermodynamics to perform computations. This has shown great potential to simulate neuron behavior. In this paper we explore the possibility of using T RAM to simulate processing in the cerebral cortex. We start by outlining the architecture and operation rules of T RAM and comparing them to other neuromorphic computers. Next we report on our implementation of T RAM for processing simulation using a simple model of visual cortex as a case study. Results show that T RAM can replicate some key aspects of processing such as selective attention, receptive fields and binocular rivalry. We also look into different parameters' impact on performance and discuss implications for further development of more complex models of cortical processing. Results suggest that T RAM could prove to be useful in studying mechanisms of cortical processing and may eventually result in better designed systems inspired by biology."
"This paper explores the non-deterministic self-assembly of two types of tiles on a lattice, which can lead to the creation of complex structures with emergent properties. The tiles are square-shaped and have four different types of edges, each of which can either be sticky or non-sticky. By placing the tiles on a lattice in a non-deterministic manner, meaning that the placement is not predetermined, the researchers observe the emergence of various structures, including aperiodic tilings and quasi-crystalline patterns. The study employs computer simulations and mathematical modeling to analyze the behavior of the tile assembly process and investigate the impact of different factors, such as tile concentration and temperature, on the resulting structures. The findings of this research demonstrate the potential of non-deterministic self-assembly for creating novel materials with unique properties, and contribute to the growing field of self-assembling systems.","This paper investigates the assembly of two kinds of tiles onto a lattice by using random placement and observes the formation of complex structures with emergent characteristics. Tiles are square shaped and have edges that are either sticky or nonsticky; these edges allow for different arrangements. Through random placement, researchers observe formation of different structures such as aperiodic tilings and quasi crystals. Analysis uses computer simulation and modeling to study assembly processes and assess effects of different variables like concentration of tiles and temperature on resultant structures. Results show promise for producing new materials with special properties through random assembly and contribute to the growing field of self assembling systems."
"The increasing demand for energy-efficient computing has led to the development of various techniques for virtual machine (VM) allocation in cloud computing environments. In this paper, we propose a new approach to minimize the total busy time of VMs while considering energy consumption as a crucial factor. We formulate the problem as an optimization problem and develop a heuristic algorithm that considers both the workload of the VMs and the energy consumption of the servers. The algorithm is designed to allocate VMs in a way that balances the energy consumption across the servers, while also minimizing the total busy time of VMs. We evaluate the performance of our approach using both real-world and synthetic datasets. Our results show that the proposed algorithm is highly effective in minimizing the total busy time of VMs while also significantly reducing energy consumption. The findings of this research will be of great significance in the design of energy-efficient cloud computing systems.","Because there is growing need for efficient computing, different approaches have been developed for allocating virtual machines (VMs) in cloud computing environments. This paper presents a new method aiming at minimizing total busy times of VMs while energy consumption becomes a major consideration. We formulate this into an optimization problem and design an algorithm that considers workloads and server energy consumption. The algorithm allocates VMs so that energy usage is balanced among servers while also minimizing total busy times. Performance evaluation using both real data and synthesized data shows that this proposed method works very well at reducing busy times and energy consumption significantly. Results from this research are important for designing efficient cloud computing systems."
"This paper explores the use of stacked multilayer perceptron (MLP) networks for learning dynamic classes of events. Dynamic classes of events refer to events that can change over time, such as changes in traffic patterns or weather conditions. The proposed approach involves training a series of MLPs to learn different stages of event dynamics, with the outputs of each MLP feeding into the next. This stacked approach allows for the system to capture the temporal dependencies of the events and improve prediction accuracy. The proposed method is evaluated on a dataset of traffic flow data, where the goal is to predict the traffic flow for the next hour. The results show that the stacked MLPs outperform traditional methods such as linear regression and support vector regression. Additionally, experiments are conducted to evaluate the sensitivity of the approach to different hyperparameters and architectural choices. Overall, this research provides a promising approach for learning dynamic classes of events, and the proposed stacked MLPs can be applied to a wide range of applications where the events are subject to changes over time.",This paper investigates using stacked multilayer perceptrons (MLPs) for learning events that vary over time. Events which change over time include traffic patterns or weather changes. Proposed is an approach that trains a sequence of MLPs to learn different stages of dynamic event evolution. Each network's output feeds into the next so the system learns temporal dependency and improves prediction accuracy. Results from a dataset of traffic flow prediction indicate that stacked MLPs perform better than traditional methods like linear regression and support vector regression. Sensitivity tests also explore how performance varies depending on different settings for parameters and architecture. Overall findings suggest this approach works well for learning events that evolve dynamically; stacked MLPs have broad applicability to diverse tasks where events vary with time.
"In this paper, the authors propose and evaluate a knowledge graph approach to investigating software usage in the social sciences. They begin by highlighting the importance of understanding the tools and methods used in social science research, and how a knowledge graph can provide a comprehensive and integrated view of software usage in this field. The authors then present the results of their investigation, which involves collecting and analyzing data from various sources, including academic publications, software repositories, and online forums. They demonstrate how the knowledge graph approach can reveal patterns and relationships among software, researchers, and research topics, and can help to identify opportunities for improvement and innovation in software usage in the social sciences. The authors conclude that a knowledge graph approach is a valuable tool for understanding software usage in this field and can provide a foundation for future research and development in this area.","Authors propose and assess an approach to study use of software in the social sciences using a knowledge graph. They emphasize importance of understanding tools and methods used in research and show how this graph allows integration and overview of software use in this discipline. They report results of their study, collecting and analyzing data from different places such as academic journals, software repositories and online discussion boards. Results indicate patterns and connections among software users, research topics and they show benefits for improving and innovating use of software in social sciences. In conclusion, authors say that graph approach useful for understanding software use and provides basis for further research and development work in this domain."
"This paper presents a deep reinforcement learning (DRL) based approach for content caching in vehicular edge computing and networks (VECN) with permissioned blockchain technology. The DRL algorithm is used to determine the optimal caching strategy for each vehicle in the VECN by considering various factors such as available storage space, popularity of content, and network conditions. The permissioned blockchain is utilized to manage the distribution and sharing of cached content among vehicles and to ensure secure transactions among the participants in the VECN. The results of the simulation show that the proposed DRL-based caching approach outperforms traditional caching strategies in terms of hit rate and energy efficiency. The combination of DRL and permissioned blockchain technology provides a promising solution for content caching in VECN, which can significantly improve the overall performance of VECN systems.","This paper introduces an approach using deep reinforcement learning (DRL) for content caching in VECNs along with permissioned blockchains. This DRL algorithm optimizes caching strategies per vehicle by taking into account things like free space, content popularity and network conditions. Permissioned blockchains handle distribution and sharing of cached content and assure secure transactions among participants in this system. Simulation results indicate this proposed DRL caching method performs better than traditional methods in hit rates and energy efficiency. Combining DRL and permissioned blockchains thus suggests a promising solution for caching content within VECNs and greatly enhances system performance."
"This paper provides a comprehensive survey of recent advancements in subpath queries on compressed graphs. It begins with a brief overview of the importance of graph compression and its applications in various domains. The paper then delves into the various methods of graph compression, including adjacency lists, adjacency matrices, and multilevel graph decomposition. The focus of the paper is on subpath query algorithms that operate on compressed graphs and their respective trade-offs in terms of time and space complexity. The survey covers both exact and approximate algorithms, as well as their performance evaluations on real-world and synthetic datasets. Finally, the paper concludes by discussing open challenges and future directions for research in this field.",This paper gives an extensive review of recent advances in querying paths in compressed graphs. It starts with a short introduction on why graph compression matters and where it finds use. The paper goes into detail about different approaches for compressing graphs such as adjacency lists and matrices and levels. The focus is on algorithms that process compressed graphs specifically and on balancing costs of time and space. Both exact and approximate algorithms are considered and performance metrics are compared using real data and simulated data. In conclusion the paper addresses remaining questions and points towards possible future research directions.
"This paper presents an empirical analysis of production machine learning pipelines and provides insights into optimization opportunities. The study evaluates the current state of production pipelines in industry and identifies common challenges and bottlenecks. Through a series of experiments and case studies, the authors demonstrate the impact of various optimization techniques on pipeline performance and efficiency. The results show that implementing a combination of hardware and software optimizations, such as parallel processing and model pruning, can significantly improve the speed and accuracy of machine learning pipelines. The paper concludes with practical recommendations for optimizing production pipelines and highlights the need for continued research in this area.","This study performs empirical analysis on machine learning pipeline production and offers new insight into optimization opportunities. It assesses current condition of industrial pipelines and points out common issues and roadblocks. Through various experiments and case studies, researchers show impact of different optimization methods on performance and efficiency. Results show that using a combination of hardware and software optimization approaches like parallel processing and pruning improves both speed and accuracy of machine learning pipelines. Conclusions suggest practical recommendations for improving production pipelines and point to future research needs."
This paper presents a comprehensive study of benchmarking feature-based algorithm selection systems for black-box numerical optimization. The study focuses on evaluating the performance of different algorithm selection methods based on their ability to identify the best algorithm for a given optimization problem. A set of benchmark problems and a suite of algorithms were used to evaluate the performance of the algorithm selection methods. The results show that feature-based algorithm selection systems can significantly improve the performance of optimization algorithms compared to using a single algorithm. The study provides insights into the strengths and weaknesses of different algorithm selection methods and highlights the importance of feature engineering in the algorithm selection process. The findings of this research can guide the development of future algorithm selection systems for black-box numerical optimization.,"This paper looks at comprehensive studies comparing systems for selecting features for algorithm selection focusing on evaluating performance of different methods for choosing best algorithms for specific optimization problems. Using a set of benchmark problems and a suite of algorithms, the study measures performance of these selection methods. Results show that systems that use features perform much better than using one algorithm alone. Results also reveal strengths and weaknesses of different selection methods and stress the importance of feature engineering. Findings from this research will help guide development of future systems for black box numerical optimization."
"This paper presents FILIP, a novel fine-grained interactive language-image pre-training model for improving the performance of computer vision and natural language processing tasks. FILIP integrates a language model with an image recognition model and is trained on a large corpus of text and image data. The model is evaluated on several benchmark datasets and shows significant improvements over existing pre-training models, demonstrating the effectiveness of FILIP's fine-grained approach to language-image pre-training. The results suggest that FILIP has the potential to be a valuable tool for a wide range of applications in the field of AI.",This study introduces FILIP as a new model which uses fine integration of language and image models for better performance in tasks involving computer vision and NLP. FILIP combines both language and visual recognition models and is trained using a large corpus of text and images. Evaluation results on various benchmarks show marked improvement compared to current models; this indicates the effectiveness of the fine integration approach to training models combining language and vision. Results indicate that FILIP could prove useful across a broad spectrum of tasks within AI.
"In this paper, the authors aim to present a comprehensive study on modeling belief in dynamic systems. The paper is divided into two parts, with the first part focusing on the foundations of the topic. The authors begin by defining the concept of belief and its significance in the context of dynamic systems. They then provide a comprehensive review of the existing literature on belief modeling in dynamic systems and discuss the various approaches used in the field. The authors next present a framework for belief modeling in dynamic systems, which includes several key components such as belief representation, belief update, and belief evaluation. The framework also considers the role of uncertainty and the methods for its quantification in belief modeling. The authors then discuss the various challenges associated with belief modeling in dynamic systems and outline the future directions for research in this area. In conclusion, the first part of this paper provides a solid foundation for understanding belief modeling in dynamic systems. The authors have provided a comprehensive review of the existing literature and a framework for belief modeling that is useful for both researchers and practitioners in the field. This paper is an essential resource for anyone interested in the study of belief modeling in dynamic systems.","This paper aims to conduct a comprehensive study on modeling beliefs in systems that change over time. It is divided into two sections: the first focuses on basic concepts. The authors start by defining belief and its importance within dynamic systems. They also review existing literature on modeling beliefs in these systems and discuss different approaches used. Next they introduce a framework for modeling beliefs in dynamic systems; this framework contains key elements including belief representation, updating belief and evaluating belief. It also considers aspects of uncertainty and ways of measuring this uncertainty. They also consider the challenges of modeling beliefs in systems that vary over time and list directions for future research. In summary, this paper lays a strong foundation for understanding how to model beliefs in systems that change. Authors review relevant literature and introduce a useful framework for both researchers and practitioners in this field. This paper is essential reading for anyone studying beliefs in changing systems."
"This paper investigates the optimality of universal Bayesian sequence prediction for a general loss function and alphabet. The study aims to determine the conditions under which the Bayesian prediction strategy is optimal and to provide a theoretical framework for the optimality proof. The paper first introduces the concept of Bayesian prediction and the principles behind it. Next, the authors define the general loss function and alphabet, and show how they can be used in the context of Bayesian prediction. The main results of the paper are then presented, including a theorem that provides sufficient conditions for the optimality of the Bayesian prediction strategy. The theorem is proven using a combination of mathematical analysis and simulation results. The paper concludes by discussing the implications of the results and pointing out avenues for future research.","This paper looks at optimality of universal Bayesian sequence prediction for any given loss function and alphabet. It tries to establish conditions under which this Bayesian prediction method works well theoretically and gives a basis for proving optimality. First it explains what Bayesian prediction is and why it works. Then it defines the general loss function and alphabet and explains how they work in Bayesian prediction. Results are summarized and a theorem showing sufficient conditions for optimality is presented. Proving this theorem uses both math analysis and simulation results. Finally, conclusions are drawn about results and new lines of inquiry are pointed out."
"The paper presents a comprehensive and up-to-date overview of the state of social network research, with a specific focus on the issue of privacy in these sites. The paper analyzes the most recent trends and developments in this field, including the growing concern among users about the protection of their personal data and the increasing pressure on social network companies to address these concerns. The paper also highlights the key challenges and opportunities for future research in this area, including the need for a better understanding of how privacy is defined and managed in different social network contexts, and the potential for new technologies and approaches to improve privacy protection in these sites. The paper concludes with a call for further research and collaboration between academics, industry experts, and policymakers to address the complex and pressing issue of privacy in social network sites.","This paper reviews the latest research on social networks in an integrated way, focusing particularly on privacy issues. It looks at recent trends and development in this area and stresses that users increasingly worry about protecting personal information and that companies are under strong pressure to deal with these concerns. The paper also points out major challenges and opportunities for future work. These include the need to understand better what privacy means and how to manage it differently across various network contexts and potential new technology solutions to enhance privacy protection on those sites. Finally, the paper urges further research and cooperation among scholars, industry leaders, and policy makers regarding the important and urgent matter of privacy in social networking sites."
"This paper proposes a novel approach for indoor scene classification by combining a spatial layout representation and scale invariant feature representation. The spatial layout representation captures the global structure of an indoor scene, while the scale invariant feature representation extracts features that are robust to scale variations. The combination of these two representations results in a more robust and accurate scene classification system compared to existing methods. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed approach, achieving improved performance over state-of-the-art methods.","This paper introduces a new way of classifying indoor scenes by using both spatial layout and scale invariant features. Spatial layout captures overall structure of the scene, while scale invariant features extract features which are less sensitive to scale changes. Combining these two approaches leads to a better and more accurate system for classification compared to previous methods. Experiments on different benchmarks show this new method performs better than leading ones."
This paper presents a case study aimed at discovering co-location patterns between pollutants and child cancers. The study utilized a large dataset of pollutant levels and childhood cancer cases in a specific geographical area. The paper describes the methodology used to identify correlations between the two variables and the results of the analysis. The findings show that there is a significant co-location pattern between high levels of certain pollutants and a higher incidence of child cancers in the area studied. The results of this study contribute to the ongoing debate about the impact of environmental pollutants on human health and provide valuable information for policymakers and public health officials to consider when making decisions about environmental regulations.,This research report looks into correlation patterns between pollutants and child cancer incidence. It uses a large dataset of pollution levels and cancer cases in a particular region. It explains how they identified relationships among these variables and reports the outcomes of their analysis. Results show strong clustering of high levels of certain pollutants with increased cancer incidence in the study region. Results from this work add to the discussion about environmental pollution effects on health and offer important data to policy makers and public health authorities for regulatory decision making.
"Cyclus archetypes are a fundamental concept in the field of nuclear fuel cycle analysis. This paper provides a comprehensive review of the various cyclus archetypes that have been proposed and implemented in various studies. The paper begins by defining cyclus archetypes and explaining their importance in nuclear fuel cycle analysis. It then provides a comprehensive overview of the different archetypes that have been proposed and implemented, including the once-through, closed, and open archetypes. The paper also discusses the strengths and weaknesses of each archetype, as well as the limitations and assumptions that must be considered when using them. The paper concludes by discussing the implications of these findings for the future of nuclear fuel cycle analysis and the development of sustainable nuclear energy systems.","Cyclus archetypes are important in the study of nuclear fuel cycles. This paper reviews different ones that have been studied by other researches. We start by defining what they are and explain why they matter. Then we take a broad look at different types of archetypes such as once through, closed and open. We also look at benefits and drawbacks of each one and note any limitations or assumptions involved. Finally we discuss how this knowledge could affect future studies into nuclear fuel cycles and sustainable development of nuclear power systems."
"This paper presents a novel approach for identifying release sources in advection-diffusion systems by combining machine learning techniques with the Green Function Inverse Method. The proposed method utilizes supervised learning algorithms to estimate the unknown release sources from observed concentration data in the advection-diffusion system. The Green Function Inverse Method is then applied to validate the accuracy of the estimated sources and to reconstruct the unknown parameters in the system. The results show that the proposed approach outperforms traditional methods in terms of accuracy and computational efficiency, providing a promising tool for source identification in real-world advection-diffusion systems.",This paper introduces a new way to identify sources in systems where particles spread both by moving (advection) and diffusing randomly (diffusion). It combines machine learning with a method using Green functions and inverse problems. Supervised learning algorithms are used to estimate unknown release sources based on observed concentration data. Then Green function method is used to verify accuracy of estimated sources and reconstruct system parameters. Results show this new approach excels at both accuracy and speed compared to previous methods and thus promises to be a useful tool for finding sources in practical systems.
"This paper investigates the extraction of belief dynamics, which refers to changes in the subjective probability of an event or a proposition over time. The study focuses on exploring methods for tracking and modeling belief dynamics in individual or group contexts. The methods are based on Bayesian belief networks and probabilistic graphical models, which are well-established frameworks for modeling uncertainty and belief update. The study evaluates the performance of these methods through simulations and empirical experiments, and provides insights into the key factors that affect the accuracy of belief dynamics extraction. The results of this study can contribute to the development of more sophisticated and effective models for monitoring and understanding belief dynamics in real-world applications, such as decision making, risk assessment, and information dissemination.","This paper looks into dynamic beliefs; that is, how subjective probabilities of events or propositions change over time. It studies how to track and model such beliefs both individually and among groups. Methods rely on Bayesian belief networks and probabilistic graphs, which have been successful frameworks for dealing with uncertainty and updating beliefs. Performance of these methods is evaluated via simulations and empirical studies and important factors affecting accuracy of extracting belief dynamics are also explored. Results of this research can help to develop better models for monitoring and understanding beliefs in practical situations like decision making, risk assessment and information sharing."
"This paper presents an analysis of spectral efficiency in the presence of correlated gamma-lognormal desired and interfering signals. The study focuses on evaluating the impact of signal correlation on the spectral efficiency of wireless communication systems. The results show that the spectral efficiency is significantly influenced by the correlation between the desired and interfering signals, which can lead to significant reductions in system performance. The paper develops a theoretical framework for characterizing the spectral efficiency under these conditions and provides insights into the trade-off between signal correlation and spectral efficiency. The findings of this study have important implications for the design and optimization of wireless communication systems and can be used to guide the development of more efficient and robust communication systems.",This paper investigates how spectral efficiency is affected when there is correlation among desired and interfering signals that follow Gamma Lognormal distributions. It looks at the effect of this correlation on wireless communications systems. Results indicate that spectral efficiency drops sharply due to strong correlation between desired and interfering signals and leads to substantial performance degradation. A theoretical framework is developed to characterize efficiency under such conditions and provides insight into balance between correlation and efficiency. Results of this research are significant because they influence design and optimization of wireless communication systems and suggest guidelines for development of better performing and more robust systems.
"This paper presents a geometric characterization of sensitivity analysis for monomial models, which are a class of mathematical models widely used in various fields of science and engineering. Our approach is based on a novel interpretation of the sensitivity matrix as a set of linear subspaces in the space of model parameters. We show that the dimension and orientation of these subspaces provide valuable insights into the sensitivity of the model output to changes in the input parameters. Specifically, we demonstrate that the sensitivity of the model is determined by the geometry of the subspaces and the degree of overlap between them. Our results provide a new perspective on sensitivity analysis for monomial models and offer a practical framework for analyzing and visualizing the sensitivity of these models in high-dimensional parameter spaces. We illustrate our approach with numerical examples and compare it with existing methods.","This paper develops a new geometric understanding of sensitivity analysis for monomials which are commonly used across diverse scientific and engineering disciplines. Based on an innovative interpretation of sensitivity matrix as subspaces of parameter space this work shows that dimensions and orientations of these subspaces contain important information about how sensitive outputs of the model are to variations of input parameters. Specifically, sensitivity is shown to depend on geometric features of the subspaces and amount of overlap among them. Results provide a fresh outlook on sensitivity analysis for monomials and introduce a practical framework for analysis and visualization of sensitivities in high dimensional parameter spaces. Numerical examples are also given and comparison with other approaches is made."
"This paper investigates the problem of interference mitigation in Cloud Radio Access Networks (C-RANs) by using the rate-splitting technique combined with common message decoding. The rate-splitting technique is a promising approach to reduce interference and improve spectral efficiency in wireless networks, but its application in C-RANs has been limited due to the complexity of coordinating the transmission between the Baseband Unit (BBU) and Remote Radio Heads (RRHs). In this paper, we propose a joint design of rate-splitting and common message decoding at the BBU to mitigate interference in C-RANs. We develop a novel algorithm to optimize the rate-splitting parameters and the common message decoding coefficients to minimize the total transmission power while maintaining the desired Quality of Service (QoS) for all users. Simulation results show that our proposed method achieves significant performance gains compared to existing interference mitigation techniques, especially in dense and interference-limited scenarios. Our findings provide insights into the potential of rate-splitting and common message decoding for interference management in C-RANs and can inform the design of future wireless networks.","This paper looks at reducing interference in Cloud Radio Access Networks (C R ANs) using the splitting of rates combined with decoding of common messages. Splitting rates is a promising way to reduce interference and enhance spectral efficiency in wireless networks, but applying it to C R ANs has been difficult because coordination among Baseband Units (BBUs) and Remote Radio Heads (RR Hs) is complex. We propose joint design of splitting rates and common message decoding at BBU to reduce interference in these networks. We develop a new algorithm to optimize splitting rate parameters and coefficients for common message decoding to minimize total transmitted power and maintain QoS for all users. Results from simulations show significant performance improvement compared to other methods especially under dense and highly interfering conditions. Insights are gained regarding potential use of splitting rates and common message decoding for interference management and this knowledge informs design of future wireless networks."
"This paper presents a new approach to compressing categorical features in machine learning models using submodular optimization. Categorical features are a common type of data in many real-world applications, but their high dimensionality can lead to significant computational overhead and storage requirements in machine learning models. The proposed method uses submodular optimization to select a subset of representative categories that capture the most important information in the original data. Specifically, we formulate the problem as a submodular maximization problem and propose a greedy algorithm to efficiently find a near-optimal solution. We also provide theoretical guarantees on the quality of the solution produced by our algorithm. Experiments on several benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of compression rate and accuracy compared to existing methods. The proposed method can be easily integrated into existing machine learning pipelines and has the potential to significantly improve the efficiency and scalability of machine learning models with categorical features.",This paper introduces a new way of compressing categorical features using submodular optimization. Features of this kind are very common in practice but they often cause high computational cost and storage demands in machine learning. Our approach uses submodular optimization to select an important subset of representative categories which capture most of the information from raw data. We formulate the problem as a maximization task using submodularity and develop a greedy algorithm for efficient solution finding. We also give theoretical assurance about quality of results from our algorithm. Results show that this method compares favorably against others in terms of both compression ratio and accuracy on different benchmarks. Our method is easy to integrate into current pipelines and has promise to greatly enhance efficiency and scalability for models working with categorical features.
"This paper explores the concept of structured mappings for conferencing common information in multiple-access channels. Specifically, the paper investigates the potential benefits of using structured mappings to efficiently convey shared information among multiple users, while mitigating interference and increasing channel capacity. The study presents a theoretical framework for designing structured mappings that can optimally balance the trade-off between the amount of common information conveyed and the channel resources required. The research also proposes a practical algorithm for constructing structured mappings based on sparse-graph codes. Simulation results demonstrate that the proposed approach outperforms existing methods in terms of achievable rates and decoding complexity. The paper concludes that structured mappings can provide a promising solution for improving the efficiency and capacity of multiple-access channels, with potential applications in wireless networks, multimedia communications, and other domains.",This paper looks into structured mappings for sharing information among many users through multiple access channels. Specifically this work focuses on whether structured mappings offer advantages like efficient communication of shared information and reducing interference along with increasing channel capacity. A theoretical framework is presented for designing such mappings which balance well between conveying common information and channel resource requirements. Practical algorithms are also developed based on sparse graph codes for constructing such mappings. Results from simulations show that this method performs better in terms of rate achieved and decoding complexity compared to previous approaches. At the end the authors conclude that structured mappings are promising solutions to enhance efficiency and capacity of channels and they have potential uses in wireless networks and multimedia communications as well as others.
"This paper presents a novel approach for designing a power-efficient artificial neuron using superconducting nanowires. The proposed neuron architecture consists of a superconducting loop with a nanowire junction that can produce a tunable response to input signals. The nanowire junction serves as the activation function for the neuron, and its low power consumption is achieved by using superconductivity to eliminate resistance in the loop. We have developed a detailed model of the proposed neuron and performed extensive simulations to demonstrate its performance characteristics. Our results show that the proposed neuron can achieve a high level of accuracy while consuming significantly less power than existing designs. We also present a design methodology for integrating multiple neurons into a larger network, and show that the proposed neuron architecture can be scaled up to create a high-performance, power-efficient neural network. Overall, this research presents a promising avenue for developing energy-efficient artificial neural networks, which can have a wide range of applications in areas such as machine learning, data analysis, and cognitive computing.",This paper introduces a new way of designing efficient artificial neurons using superconducting wires. The architecture includes a superconducting loop with a junction for adjustable response to inputs. The junction performs as an activation function and low power usage is obtained by using superconductivity to eliminate resistance in the loop. We develop a detailed model for the proposed neuron and perform extensive simulations to validate its performance characteristics. Results indicate high accuracy and much lower power consumption compared to other designs. We also develop a method for integrating many neurons into a larger network and demonstrate that this neuron architecture scales well to create high performance networks that consume less power. Overall this work opens promising avenues for development of energy efficient neural networks applicable to diverse fields including machine learning and cognitive computing.
"In this paper, the focus is on the security of Internet of Things (IoT) edge nodes in the context of machine learning. The paper specifically explores the potential security attacks that can occur in IoT edge nodes, which are devices that are positioned at the edge of a network to collect and process data from IoT devices. With the increasing use of machine learning in IoT, there is a growing concern about the security of these systems, as machine learning algorithms are vulnerable to various types of attacks. The paper analyzes the various types of security attacks that can be launched against IoT edge nodes, including inference attacks, evasion attacks, and poisoning attacks. The authors also discuss the impact of these attacks on the performance of machine learning algorithms, and how they can be used to compromise the security of IoT networks. To address these security concerns, the paper proposes a number of countermeasures that can be used to defend against these attacks. These countermeasures include the use of secure communication protocols, the implementation of security mechanisms in machine learning algorithms, and the use of secure hardware and software systems. The authors also discuss the challenges that need to be overcome to effectively implement these countermeasures, and provide recommendations for future research in this area. Overall, this paper provides a comprehensive analysis of the security risks associated with IoT edge nodes in the context of machine learning, and provides practical solutions for defending against these risks. The findings and recommendations presented in this paper can be used by practitioners and researchers to improve the security of IoT networks and to ensure the safe and secure deployment of machine learning algorithms in IoT systems.","This paper focuses on security of edge nodes in the Internet of Things (IoT) context. We investigate possible security attacks that can happen at edge nodes, which are small devices located near network edges collecting and processing data from other IoT devices. As machine learning becomes increasingly common in IoT, there is growing concern about system security because learning algorithms are vulnerable to different types of attacks. This paper looks into various types of attacks such as inference attacks, evasion attacks and poisoning attacks. We analyze impact of these attacks on performance of learning algorithms and how they can undermine security of IoT networks. Countermeasures proposed here aim to defend against these attacks. Countermeasures include using secure communication protocols, implementing security measures within machine learning algorithms and using secure hardware and software systems. Authors also point out challenges to effective implementation of these countermeasures and offer suggestions for further research. Overall this paper presents an in depth look at security threats related to learning systems in IoT and offers practical ways to mitigate those risks. Findings and recommendations from this work should be useful for practitioners and researchers to enhance security of IoT networks and to deploy learning algorithms safely."
"This paper explores the potential impact of autonomous systems and artificial intelligence on the stability of the nuclear industry. It analyzes the role that these technologies could play in enhancing safety and security, as well as reducing the risk of accidents and incidents. The paper also evaluates the potential challenges associated with the integration of these technologies, including ethical considerations and cybersecurity risks. The findings suggest that the use of autonomous systems and artificial intelligence has the potential to revolutionize the nuclear industry and contribute to a stable nuclear future. However, careful planning and implementation strategies must be put in place to ensure that these technologies are integrated in a responsible and safe manner. This paper highlights the importance of continued collaboration between nuclear experts and technology experts to realize the full potential of these cutting-edge technologies in the pursuit of a stable nuclear future.",This paper investigates how autonomous systems and AI can affect stability in the nuclear sector. It looks at how such systems might enhance safety and security and reduce risks of accidents and incidents. Also considered are challenges involved with integrating these systems including ethical issues and cybersecurity risks. Results show that integration of these systems can transform the nuclear sector and contribute toward a stable future. Careful planning and implementation strategies are needed to integrate them responsibly and safely. Collaboration between experts in both nuclear and technology domains is crucial for realizing the full benefits of these advanced technologies towards attaining a stable future.
"This paper presents a method for efficient sampling in polynomial chaos-based uncertainty quantification and sensitivity analysis. The proposed method uses weighted approximate Fekete points, which are known for their ability to provide a well-distributed set of points for polynomial interpolation. By utilizing these points, the authors show that the proposed method can achieve high accuracy with a significantly reduced number of samples compared to traditional Monte Carlo methods. The performance of the proposed method is evaluated through several numerical experiments, and the results demonstrate that it is a promising approach for efficient sampling in polynomial chaos-based uncertainty quantification and sensitivity analysis.","This paper introduces an efficient sampling method based on polynomial chaos for quantifying uncertainties and analyzing sensitivities. Proposed method uses weighted approximations of Fekete points, known for their ability to distribute points evenly for interpolating polynomials. Using such points allows authors to show that proposed method achieves high accuracy with fewer samples than traditional Monte Carlo methods. Performance of this method is assessed by various numerical tests; results indicate that this approach is promising for efficient sampling in quantification of uncertainties and sensitivities using polynomial chaos."
"This paper presents a novel approach to enhance the security and robustness of unmanned aerial vehicle (UAV) communication systems by utilizing reconfigurable intelligent surfaces (RISs). The paper first provides an overview of current challenges in UAV communication, including signal interference, jamming attacks, and weak wireless signal coverage. It then introduces the concept of RISs as a promising solution to these challenges. The authors propose a secure and robust UAV communication system that employs RISs to dynamically adjust the wireless channel conditions in real-time. The performance of the proposed system is evaluated through simulations and experiments, which demonstrate significant improvements in terms of signal quality, transmission reliability, and security compared to traditional UAV communication systems. The results of this study provide new insights into the potential of RISs as a key technology for secure and reliable UAV communication.","This paper introduces a new way to improve security and robustness of communication for unmanned aerial vehicles (UAVs) using reconfigurable intelligent surfaces (RISs). The paper first reviews current challenges in UAV communications like interference and attack by jammers as well as weak signals. Next, the idea of RISs is introduced as a promising solution to these problems. Authors propose a secure and robust communication system for UAVs that uses RISs to adapt channel conditions on the fly. Performance of this proposed system is evaluated using simulations and experiments showing clear advantages such as improved signal quality, reliability and security compared to conventional UAV communication systems. Results from this study offer fresh insights into the value of RISs as critical technology for secure and reliable UAV communication."
"This paper explores the hardness and approximation of the probabilistic p-center problem under pressure. The p-center problem is a well-known problem in operations research and computer science, which involves finding the optimal location for p facilities in a network such that the maximum distance from any point to its closest facility is minimized. The probabilistic p-center problem extends this concept by incorporating a probability distribution on the demand of each point in the network. The pressure aspect of the problem refers to the presence of constraints or external factors that affect the decision-making process. The paper starts by introducing the probabilistic p-center problem and its variants, including the classical and the pressure-sensitive versions. It then discusses the computational complexity of the problem, proving its NP-hardness through reduction from well-known NP-hard problems. The paper also presents approximation algorithms for the probabilistic p-center problem under pressure, analyzing their performance and providing numerical results to illustrate their effectiveness. Finally, the paper concludes by summarizing the contributions of the research and discussing possible directions for future work. The results of this research provide valuable insights into the hardness and approximation of the probabilistic p-center problem under pressure, and can serve as a useful reference for researchers and practitioners in the field of operations research and computer science.",This paper looks at hardness and approximation for probabilistic p centers when there are pressures. The p center problem is one of the classic ones in OR and CS; it finds the best location for p facilities in a network so that maximum distance to nearest facility is minimal. Probabilistic p centers extend this idea by considering probability distributions over demands at points in the network. Pressure means there are limitations or external things affecting decision making. First the paper defines probabilistic centers and related variants like standard and sensitive to pressure. Then it discusses computational complexity proving NP hardness via reductions from known hard problems. It introduces approximation algorithms for probabilistic centers with pressure and analyzes performance along with some numbers to show effectiveness. At last the authors summarize results and suggest future work. Results of this work contribute important insights into hardness and approximations for probabilistic centers under pressure and serve as useful references for researchers and professionals in OR and CS.
"This paper presents a study on the joint constellation design for Non-Coherent Multiple-Input Multiple-Output (MIMO) Multiple-Access (MA) channels. The objective of the study is to optimize the design of the constellations used for transmitting data in Non-Coherent MIMO MA channels, with the aim of improving the overall system performance. The study proposes a novel joint constellation design approach that takes into account both the channel state information and the user pairing information. The performance of the proposed approach is evaluated through simulations and compared with existing methods. The results show that the proposed approach leads to significant improvement in terms of Bit Error Rate (BER) performance, especially in the high Signal-to-Noise Ratio (SNR) regime. The study concludes that the proposed joint constellation design is a promising solution for Non-Coherent MIMO MA channels and can be applied in various practical communication systems.",This paper investigates design of constellation for Non Coherent MIMO Multiple Access Channels. The main goal is to improve system performance by optimizing constellation design for data transmission. A new joint design method which considers both channel state information and user pairing is proposed. Performance of this method is assessed via simulations and compared against other approaches. Results indicate clear improvement of Bit Error Rate (BER) especially at high SNR levels. Concluding that this joint constellation design shows promise and applicability in practice.
"The symbolic aggregate approximation (SAX) method is widely used in time series analysis due to its efficiency and ability to capture patterns in the data. However, its limitations in capturing segment trend information have been widely recognized. In this paper, we propose a modified SAX method that incorporates segment trend information in order to improve its performance in capturing patterns in time series data. Through extensive experimentation and evaluation, we demonstrate that the proposed method outperforms the traditional SAX method in capturing both global and local trends in time series data. The results of this research contribute to the development of more effective time series analysis techniques and have important implications for various real-world applications.",Symbolic Aggregate Approximation (SAX) has become popular for analyzing time series because it excels at both processing efficiency and capturing patterns. But limitations exist when it comes to detecting trend information at segments. We propose an improved SAX which integrates trend information aiming to enhance performance at capturing patterns in time series. Extensive testing and evaluation show that our new method performs better compared to traditional SAX both at global and local trends. Results from this study advance effective time series analysis methods and have practical significance for many real world tasks.
"This paper proposes a novel approach to crowd counting using multi-resolution fusion and multi-scale input priors. The proposed method leverages the strengths of both multi-resolution fusion and multi-scale input priors to accurately estimate the crowd count in a given scene. The approach involves capturing the scene at different resolutions and fusing the information to generate a high-resolution image. Then, multi-scale input priors are used to refine the crowd count estimation. The proposed method is evaluated on several publicly available datasets, and the results demonstrate that it outperforms state-of-the-art crowd counting methods. This approach has the potential to significantly improve crowd counting in real-world applications, such as crowd management and safety monitoring.",This paper introduces a new method for estimating crowd numbers using fusion of different resolution levels and use of priors across multiple scales. Proposed method combines strengths from fusion of various resolutions and use of priors at different scales to accurately estimate crowd size in a scene. Method uses capture of scenes at different resolutions and fuses information together to generate high resolution images. Further refinement of crowd estimation is achieved by use of priors at multiple scales. Results show this method performs better than current leading methods on different public datasets. This approach promises significant improvement for real world tasks like crowd management and safety monitoring.
"This paper proposes a deep graph metric learning approach for improving the efficiency and effectiveness of proxy-based object recognition systems. Traditional proxy-based approaches rely on a large number of proxy examples to accurately represent object classes, which can lead to high computational costs and low scalability. In contrast, our proposed approach, ""Fewer is More,"" learns a compact representation of proxy examples using a deep graph metric learning model. By leveraging the relationships among proxies, our approach can effectively learn a low-dimensional feature space that captures the discriminative information of each object class using fewer proxies. Our experiments on several benchmark datasets demonstrate that our approach outperforms traditional proxy-based approaches in terms of accuracy and efficiency, while using significantly fewer proxies. Our work presents a promising direction for developing more efficient and scalable object recognition systems.","This paper introduces a new method of deep metric learning for proxies to improve performance and scalability of recognition systems based on proxies. Previous methods use many proxy samples but this can be costly computationally and impractical for scalability. We propose ""Less is More"" learning of proxy examples into a compact feature space using deep graph metric learning models. Leveraging relationships among proxies we learn a low dimensional feature space that captures discriminative information per class with much fewer samples. Results show this method performs better than previous ones in both accuracy and efficiency using significantly less proxies. Our work points toward development of more efficient and scalable systems for object recognition."
"This paper explores the problem of unsupervised domain adaptation in the context of bi-classifier determinacy maximization. The goal of unsupervised domain adaptation is to train a model on a source domain, where labeled data is available, and generalize it to a target domain, where no labeled data is available. Bi-classifier determinacy maximization is a method that leverages the decision boundaries of two classifiers trained on the source and target domains, respectively, to improve the generalization performance of the model. The proposed approach aims to learn a decision boundary that maximizes the agreement between the two classifiers, which effectively reduces the discrepancy between the source and target domains. The paper presents experimental results on various datasets to demonstrate the effectiveness of the proposed approach in improving the generalization performance of the model, compared to other state-of-the-art methods. The paper concludes with a discussion of the potential applications of bi-classifier determinacy maximization in various domains, such as computer vision and natural language processing.",This paper investigates unsupervised domain adaptation focusing on maximizing determinacy using classifiers. Goal is to train a model on a source domain where labeled data is available and then generalize performance to a target domain where there are no labels. A classifier that leverages decision boundaries learned by two classifiers trained separately on source and target domains is used to improve performance. We propose learning a decision boundary that maximizes consistency between these classifiers so that we reduce discrepancies between source and target domains. Results show this approach outperforms current best methods using experiments on different datasets. Conclusions highlight application areas including computer vision and NLP.
"This paper presents a novel motion planning algorithm called e$ \mathbf{^3} $mop, which combines heuristic-guided motion primitives pruning and path optimization with a sparse-banded structure. The algorithm is designed to efficiently solve high-dimensional motion planning problems with complex constraints, such as those encountered in robotics and autonomous systems. The e$ \mathbf{^3} $mop algorithm first generates a set of motion primitives using heuristic guidance to reduce the search space. The set of primitives is then pruned based on a novel cost function that takes into account both the cost of the motion primitive and the expected cost of the remaining path. This pruning step significantly reduces the number of primitives that need to be considered, leading to a significant reduction in computation time. The remaining primitives are then used to construct a sparse-banded graph that captures the connectivity between them. This graph is then optimized using a novel path optimization technique that takes advantage of the sparse-banded structure to efficiently compute the optimal path. Experimental results show that e$ \mathbf{^3} $mop outperforms state-of-the-art motion planning algorithms in terms of both efficiency and solution quality. The algorithm is also shown to be highly scalable, making it well-suited for use in large-scale applications. Overall, the e$ \mathbf{^3} $mop algorithm represents a significant advance in the field of motion planning, with the potential to have a major impact on a wide range of applications in robotics, autonomous systems, and beyond.",This paper introduces an innovative planning algorithm named e$ \mathbf{^3} $mop that integrates guided motion primitives pruning along with path optimization using a sparse banded structure. This algorithm is developed for high dimensional motion planning with complex constraints such as those relevant to robotics and autonomous systems. e$ \mathbf{^3} $mop begins by generating a set of primitives through heuristic guidance to reduce search space. Then this set of primitives is pruned using a new cost function that considers both the cost of the primitive and remaining path expected cost. This pruning greatly reduces consideration of primitives thus reducing computation time. Remaining primitives are used to construct a sparse banded graph capturing connectivity among them. Optimization of this graph uses a new method that exploits sparse banded structure to compute optimal path efficiently. Results show that e$ \mathbf{^3} $mop excels compared to leading algorithms regarding both performance and quality of solutions. Scalability results also show that the algorithm performs very well for large scale applications. Overall e$ \mathbf{^3} $mop represents a major advancement in motion planning and promises to have a broad impact across diverse application domains including robotics and autonomous systems.
"This paper proposes a hybrid inference system for improved curvature estimation in the level-set method using machine learning. The level-set method is a widely used technique for shape modeling and image segmentation in various fields such as computer vision, medical imaging, and graphics. The method relies on the accurate estimation of the curvature of the evolving interface, which is essential for the accurate propagation of the level-set function. However, traditional methods for curvature estimation suffer from limitations such as sensitivity to noise and uneven sampling. To overcome these limitations, we propose a hybrid inference system that combines the traditional curvature estimation method with a machine learning-based approach. The machine learning model is trained on a large dataset of synthetic and real-world images, and it learns to predict the curvature at each point on the evolving interface based on the local image features. The hybrid system combines the predictions from the machine learning model with the results from the traditional curvature estimation method, resulting in more accurate and robust curvature estimates. We evaluate the proposed hybrid inference system on various datasets and compare its performance with traditional curvature estimation methods. The results show that the hybrid system outperforms the traditional methods in terms of accuracy and robustness. Moreover, the proposed system is computationally efficient and can be easily integrated into existing level-set algorithms. Overall, our research provides a promising direction for improving the accuracy and robustness of the level-set method through the use of machine learning-based approaches. The proposed hybrid inference system can also be extended to other shape modeling and segmentation techniques that rely on curvature estimation.","This paper introduces an improved system for estimating curvature using level set method by combining traditional methods with machine learning. Level set method is commonly used in computer vision, medical imaging and graphics for modeling shapes and segmenting images. Curvature estimation is important here because it is necessary for proper propagation of level set functions. But traditional methods often have shortcomings such as being sensitive to noise and irregular sampling. To address this we propose a hybrid system that blends traditional curvature estimation with machine learning. Learning models are trained on a large set of both synthetic and real images and they learn to predict curvature at every point on evolving interface based on local features. Hybrid system mixes predictions from learning models with results from traditional methods and thus produces more accurate and robust results. Results on different data sets show that this hybrid system performs better compared to traditional ones regarding accuracy and robustness. It is also fast enough to be integrated into existing level set algorithms. In summary this work suggests promising avenues to improve level set method accuracy and robustness using learning models. This system could also be extended to other shape modeling and segmentation tasks relying on curvature estimation."
"This paper proposes a novel approach to unsupervised stereo depth estimation using H-Net, an attention-based deep neural network, and epipolar geometry. Depth estimation from stereo images is an important task in computer vision, with applications in robotics, autonomous driving, and 3D reconstruction. The proposed method employs an attention mechanism that learns to focus on informative image regions for depth estimation. In addition, epipolar geometry is utilized to enforce geometric constraints on the predicted depths. The experimental results demonstrate that the proposed approach outperforms state-of-the-art unsupervised methods on benchmark datasets. The proposed approach can also be easily adapted to handle other related tasks, such as optical flow estimation and image super-resolution. The results suggest that H-Net with epipolar geometry is a promising approach for unsupervised depth estimation, with potential for a wide range of applications in computer vision.","This paper introduces a new approach to estimating depth unsupervisedly by using H-Net, which is a deep neural network with an attention mechanism, and using epipolar geometry. Depth estimation from stereo images is an important task in computer vision and has applications such as robotics and autonomous driving and 3D reconstruction. The proposed approach uses attention to focus on key parts of the images for estimating depth. Additionally, using epipolar geometry enforces geometric consistency on the estimated depths. Experimental results show this approach performs better compared to leading unsupervised methods on benchmark data sets. Also this approach is easy to adapt for other closely related tasks like flow estimation and super resolution. Results suggest that using H Net along with epipolar geometry is promising for unsupervised depth estimation with wide applicability in computer vision."
"This paper proposes SCSGuard, a novel deep learning-based approach for detecting scams in Ethereum smart contracts. Scams are a significant problem in the Ethereum ecosystem, as they can cause significant financial losses for users. While existing approaches for detecting scams in smart contracts rely on rule-based methods or static analysis, SCSGuard uses a deep learning model to detect anomalous patterns in the contract's bytecode. To evaluate the effectiveness of SCSGuard, we collect a large dataset of known scams and benign contracts and train the model using a supervised learning approach. We then compare the performance of SCSGuard against several state-of-the-art methods for detecting scams in smart contracts. Our results show that SCSGuard achieves higher accuracy and recall than existing approaches, indicating that it can effectively detect scams in real-world smart contracts. We also analyze the factors that contribute to the success of SCSGuard, including the choice of neural network architecture, the size and quality of the training dataset, and the use of transfer learning to adapt the model to new types of scams. Additionally, we discuss the limitations of SCSGuard and identify directions for future research, such as improving the interpretability of the model and exploring the use of adversarial examples to evaluate its robustness. Overall, our research demonstrates the potential of deep learning approaches for detecting scams in Ethereum smart contracts and provides insights into the design and evaluation of such systems. We believe that SCSGuard has significant implications for improving the security and trustworthiness of the Ethereum ecosystem, and we encourage further research in this area.","This paper introduces SCSGuard, a new approach using deep learning for spotting scams in smart contracts on Ethereum. Scams are problematic because they cost users big money. Previous methods usually rely on rules or static code analysis but SCSGuard uses a deep learning model to spot unusual patterns in bytecode. Performance is evaluated using a large dataset of known scams and clean code and training is done with supervised learning. Results show high performance compared to current best methods. Factors contributing to success include the choice of neural architecture, size and quality of dataset, and transfer learning. Limitations are also discussed; future work includes enhancing interpretability and evaluating robustness with adversarial examples. Overall we demonstrate that deep learning approaches have potential for detecting scams and suggest future research. We also think this system has great importance for making Ethereum ecosystem safer and more trustworthy. We hope future research continues this line."
"This paper presents an empirical comparison of off-policy prediction learning algorithms for the collision task in a simulated environment. The collision task involves predicting the likelihood of collision between two agents in a two-dimensional grid world. We compare four different off-policy prediction learning algorithms: Q-learning, SARSA, Expected SARSA, and Double Q-learning. We conduct experiments on a range of hyperparameters and evaluate the performance of each algorithm using metrics such as mean squared error and average reward. Our results show that Expected SARSA outperforms the other algorithms in terms of mean squared error and average reward, while Double Q-learning performs the worst. We also observe that the performance of each algorithm is highly dependent on the hyperparameters used, and that some algorithms are more sensitive to hyperparameter settings than others. Overall, this study provides a valuable empirical comparison of off-policy prediction learning algorithms for the collision task, and provides insights into the strengths and weaknesses of each algorithm. These insights can be useful for researchers and practitioners working on similar prediction tasks, and can help inform the selection of appropriate algorithms for different contexts.","This paper compares empirically four learning algorithms for collision prediction in a simulated 2D grid world: Q learning, SARSA, Expected SARSA and Double Q learning. Comparisons are made on a range of hyperparameters and performance is measured using metrics such as mean squared error and average reward. Results show that Expected SARSA performs best with regards to mean squared error and average reward whereas Double Q learning does poorly. Performance variation among algorithms is also observed to depend strongly on hyperparameters and sensitivity to them varies between algorithms. Overall this work conducts valuable empirical comparisons among these learning algorithms for collision prediction and suggests important insights regarding their relative merits and demerits. Insights gained are valuable for research and practice aiming at similar prediction tasks and may guide choice of appropriate algorithms for specific contexts."
"In recent years, self-supervised learning has gained significant attention as a powerful method to tackle computer vision problems in an unsupervised manner. This paper presents a novel approach to self-supervised tracking via target-aware data synthesis. The proposed method involves synthesizing realistic training data by combining images of the target object with diverse backgrounds. The synthesized data is used to train a deep neural network that can accurately track the target object in real-time videos. The network learns to distinguish the target object from its surroundings based on its appearance, motion, and context information. The effectiveness of the proposed method is evaluated on several benchmark datasets and compared with state-of-the-art self-supervised tracking methods. The results show that the proposed method achieves competitive performance and outperforms existing methods on several challenging scenarios. This work demonstrates the potential of self-supervised tracking via target-aware data synthesis as a promising direction for unsupervised visual object tracking.","In recent years self supervised learning has received significant attention as an effective way to deal with computer vision tasks without supervision. This paper introduces a new approach to tracking using target aware data synthesis. Proposed method synthesizes realistic training data by blending target object images with different backgrounds. Training data synthesized this way is then used to train a deep neural network which tracks targets in real time video. Network learns to distinguish target from background based on appearance, motion and contextual information. Results are compared against top performing current methods on benchmark datasets and show performance comparable to current methods and superior performance on hard cases. This work highlights that target aware data synthesis for supervised tracking is promising."
"This paper explores the use of the Video Multimethod Assessment Fusion (VMAF) and Entropic Differences (ED) metrics for high frame rate (HFR) video quality assessment. With the increasing popularity of HFR video, it is essential to have reliable methods for evaluating the visual quality of these videos. The VMAF and ED metrics have been extensively used in video quality assessment for traditional frame rates, but their performance for HFR video has not been thoroughly studied. To investigate the suitability of these metrics for HFR video, the researchers conducted subjective tests with 30 participants who evaluated HFR video sequences using the Absolute Category Rating (ACR) methodology. They then compared the subjective scores to the objective scores obtained from the VMAF and ED metrics. The study used a diverse set of HFR videos with different genres and motion characteristics to ensure the reliability of the findings. The results of the study demonstrate that both the VMAF and ED metrics are reliable for HFR video quality assessment, as they show a high correlation with the subjective scores obtained from the ACR methodology. The study also found that the VMAF metric performed slightly better than the ED metric, but the difference was not significant. This research contributes to the development of objective methods for evaluating the visual quality of HFR videos, which can be useful for various applications such as video compression and streaming. The findings of this study can also guide the development of new quality assessment algorithms that are specifically designed for HFR video.","This paper investigates how to use two metrics, VMAF and ED, for assessing the quality of high frame rate videos. As high frame rate videos become more popular, it is important to have good ways to assess visual quality. These metrics have worked well for regular video before but there hasn't been much work done on their effectiveness with high frame rate. To test if they perform well, researchers asked 30 people to judge sequences of videos using the Absolute Category Rating method. They then compared their scores to those obtained by VMAF and ED metrics. Using videos of diverse genres and motions, the authors ensured reliability of results. Results show that both VMAF and ED perform reliably; they correlate very well with scores given by people using the AC rating method. VMAF performs slightly better but this difference is not statistically significant. This research improves objective methods for assessing quality of high frame rate videos, which will be helpful for compression and streaming. Findings of this study may also help design new algorithms especially tailored to high frame rate videos."
"This paper presents a novel approach to enhance the security of Industry 4.0 systems using Quorum-Based Cloud-Edge Distributed Deep Neural Networks (QUDOS). In recent years, the increasing popularity of cloud-based applications and services in Industry 4.0 has led to a growing demand for secure and efficient data processing. However, existing security solutions for these systems are often inadequate in addressing the complex threat landscape of today's industry. To address these challenges, the authors propose QUDOS, a quorum-based framework for securely executing deep neural networks (DNNs) in a cloud-edge computing environment. QUDOS leverages the strengths of both cloud and edge computing to provide a secure and efficient solution for data processing in Industry 4.0. It employs a quorum-based consensus mechanism to ensure the secure and decentralized execution of DNNs across multiple edge devices and the cloud. The proposed framework enables secure data processing by distributing the execution of DNNs across multiple devices, reducing the risk of single-point failures and data breaches. Additionally, QUDOS uses secure multi-party computation techniques to ensure the privacy and confidentiality of sensitive data during the DNN execution process. The authors evaluate the performance and security of QUDOS through a series of experiments and simulations, demonstrating its ability to achieve high accuracy and low latency in DNN execution. They also show that QUDOS outperforms state-of-the-art security solutions in terms of both accuracy and latency, making it an attractive solution for secure and efficient data processing in Industry 4.0 systems. In conclusion, the authors present QUDOS as a promising solution for enhancing the security of Industry 4.0 systems, providing a secure and efficient framework for executing DNNs in a cloud-edge computing environment.","This paper introduces a new method to improve security for systems of Industry 4.0 using Quorum Based Cloud Edge Distributed Deep Neural Network System (QUDOS). Recently, there is a growing demand for secure and efficient data processing because cloud services have become very popular in Industry 4.0. Existing solutions for security of such systems are inadequate to handle the complicated nature of threats. Authors propose QUDOS, a framework based on quorums for secure execution of deep neural networks (DNNs) in cloud and edge computing environments. QUDOS combines benefits from both clouds and edges to deliver secure and efficient processing of data in Industry 4.0. Using a consensus mechanism based on quorums ensures secure and decentralized execution of DNNs across different edge devices and cloud. This framework secures data processing by distributing execution of DNNs among different devices which reduces risks of single points of failure and data breaches. Secure multiparty computation techniques are used to ensure privacy and confidentiality of sensitive data during execution. Performance and security of QUDOS are evaluated experimentally and simulation results show high accuracy and low latency compared to current top solutions. Results also indicate that QUDOS outperforms previous methods in accuracy and latency; therefore it is appealing for secure and efficient data processing in Industry 4.0 systems. Conclusion: Authors present QUDOS as a promising way to enhance security of Industry 4.0 systems and introduce a secure and efficient framework for executing DNNs in cloud and edge computing environments."
"This paper proposes a self-supervised autoregressive domain adaptation approach for time series data, aimed at addressing the challenges of domain shift and limited labeled data. The proposed method leverages the inherent temporal dependencies within time series data and adapts to new domains through self-supervised learning without requiring labeled data from the target domain. The approach involves learning a self-supervised autoregressive model on the source domain, and using it to generate synthetic samples for the target domain. The synthetic samples are used to train a domain adaptation model, which learns to minimize the distance between the source and target domains. The proposed method was evaluated on two real-world datasets and compared with state-of-the-art methods. The results showed that the proposed method outperformed existing approaches in terms of accuracy and robustness to domain shift. The study suggests that self-supervised autoregressive domain adaptation can effectively leverage temporal dependencies within time series data, and provide a promising approach for domain adaptation in scenarios where labeled data is limited or unavailable.","This paper introduces an approach to domain adaptation using unsupervised learning on time series data. The goal is to address issues related to domain shift and shortage of labeled data. This method uses intrinsic temporal relationships in time series data and adapts to new domains using unsupervised learning. No labeled data from target domain is required. The approach trains an unsupervised autoregressive model on source data; then it synthesizes new samples for target domain using this model. These synthetic samples are then used to train a model for domain adaptation, which minimizes the discrepancy between source and target domains. Results show that this method performs better than other methods on real datasets in terms of performance and robustness to domain shift. Study indicates that unsupervised autoregressive domain adaptation makes good use of temporal relationships within time series and offers promising solutions especially when labeled data is scarce or unavailable."
"This paper presents an analysis of consensus-based distributed filtering with a fusion step in a network of interconnected agents. The filtering problem is to estimate a common signal from noisy measurements distributed over the network. We propose a distributed filtering algorithm based on consensus theory, where each agent updates its estimate by combining its own measurement with the estimates of its neighboring agents. The algorithm includes a fusion step, where the estimates are combined to obtain a global estimate of the signal. We analyze the convergence properties of the algorithm and derive conditions for the convergence to a consensus estimate. We also investigate the effect of the fusion step on the accuracy of the final estimate and derive bounds on the estimation error. Finally, we illustrate the performance of the algorithm through simulations, demonstrating its effectiveness in accurately estimating the signal in noisy and dynamic environments. The proposed algorithm has applications in sensor networks, distributed control, and multi-agent systems, where distributed estimation of a common signal is required.","This paper analyzes filtering using consensus theory for agents that are interconnected in a network. The goal is to estimate a common signal from noisy measurements distributed among them. We present an algorithm for distributed filtering; each node updates its own estimate by combining its measurement with those from neighbors. There is a fusion step as well where these estimates are combined to get a global estimate of the signal. We study convergence properties of this algorithm and derive conditions under which the consensus estimate converges. We also look into how fusion affects accuracy of final results and derive error bounds. Through simulations we show effectiveness of the algorithm in noisy and changing environments for accurate estimation of common signals. The method applies to sensor networks, distributed control and systems of agents where common signal estimation is important."
"This paper proposes a new approach for label-guided graph exploration, which allows the user to adjust the ratio of labels used in the exploration process. Label-guided exploration is a common technique used to navigate through large and complex graphs, where labels serve as a guide to direct the exploration towards nodes of interest. However, traditional label-guided exploration methods typically use a fixed ratio of labels, which may not be optimal for all scenarios. Our approach enables the user to adjust the ratio of labels based on their specific needs and goals. We introduce a new algorithm that dynamically calculates the optimal ratio of labels to use based on the current exploration progress. This algorithm takes into account various factors, such as the density of the graph, the distribution of labels, and the user's feedback, to determine the most effective label ratio. To evaluate the effectiveness of our approach, we conducted a series of experiments on various real-world and synthetic datasets. Our results show that our approach outperforms traditional label-guided exploration methods in terms of efficiency and accuracy, particularly in scenarios where the optimal label ratio is unknown or changes over time. We also conducted a user study to assess the usability of our approach, which received positive feedback from the participants. Overall, our proposed label-guided graph exploration approach with adjustable ratio of labels provides a flexible and effective solution for navigating through large and complex graphs, which can be beneficial for various applications in fields such as social network analysis, recommendation systems, and bioinformatics.","This paper introduces a new method for exploring labeled graphs guided by user labels. This differs from previous methods which use a constant proportion of labels; this new method lets users change this proportion according to their specific needs and goals. The authors developed an algorithm that computes the best proportion of labels dynamically depending on the exploration progress. They consider different factors like graph density and label distribution along with user feedback to compute this proportion. Experiments on real data and synthetic data showed this method outperforms previous methods in both efficiency and accuracy especially when the best proportion is uncertain or varies over time. An experiment with users also found good results. In summary, this new method allows flexible and efficient navigation of large and complicated graphs useful for diverse applications such as social network analysis and bioinformatics."
"This paper presents a recurrence relation approach for analyzing the effective capacity of retransmission schemes in wireless communication systems. The effective capacity is a measurement of the maximum sustainable rate of transmission under strict delay constraints. The proposed recurrence relation method provides a computational tool for finding the effective capacity for different retransmission schemes, including ARQ and HARQ protocols. The effectiveness of the proposed approach is evaluated through numerical examples and simulations, which demonstrate its accuracy and efficiency. The results of this study provide valuable insights into the design of efficient retransmission schemes and can be used as a reference for future research in the field of wireless communications.",This paper introduces an approach based on recurrence relations for analysis of effective capacity for schemes of retransmissions in wireless communication systems. Effective capacity measures the highest attainable rate of data transmission while adhering to strict limits on delays. We propose a computational tool that finds effective capacity for various schemes like ARQ and HARQ protocols. Effectiveness of our method has been validated by numerical examples and simulations; results show high accuracy and efficiency. Results from this research are very useful for designing efficient schemes and serve as reference material for future work in wireless communications.
"This paper investigates the maximal minimum distance of linear locally repairable codes, with the goal of finding tight bounds on this distance. The concept of locally repairable codes has gained attention in recent years due to its application in fault-tolerant storage systems. The maximal minimum distance of a code represents the largest minimum distance possible among all subcodes of the code, and is an important parameter for the error-correction capability of a code. The paper presents a detailed analysis of this distance, including its relationship to other code parameters and the effect of code design on its value. The authors use various mathematical tools to derive new bounds on the maximal minimum distance, and evaluate the tightness of these bounds through numerical simulations. The results of this study provide useful insights into the design of linear locally repairable codes with improved error-correction performance.","This research studies maximum minimum distance of linear codes that allow local repair, aiming to set tight upper limits on this distance. Recent years have seen increased interest in codes that are resilient to faults and storage system design. Maximum minimum distance of a code is an important measure related to correction of errors. This paper carefully examines this distance and relates it to other code parameters. Designing codes also affects their value. Authors use different math tools to derive new bounds on this maximum minimum distance and test tightness of these bounds via numerical experiments. Results from this work give valuable insight into designing codes with better performance for correcting errors."
"The off-switch game is a simple two-player game that has been studied in game theory and decision making. In this game, one player chooses to either ""turn off"" the game or to continue playing, while the other player is given the option to accept or reject this choice. This paper provides a comprehensive overview of the existing research on the off-switch game, including its mathematical formulation, its equilibria, and its applications in various fields. The authors also propose new approaches to analyze the off-switch game and its variations, and demonstrate their usefulness through case studies and simulations. The results of this research contribute to our understanding of decision making in situations where there is an option to prematurely end the interaction, and have implications for areas such as bargaining, negotiations, and social dynamics.","The off switch game is a straightforward two player game that has been researched in game theory and decision making. In this game, one player selects whether to turn off play or to keep playing while the other can choose to accept or reject that choice. Researchers discuss this game extensively including its mathematical formulation, equilibria and its use in different fields. Authors also introduce new ways to study the game and its variations and show their practical value via case studies and simulation. Results from these studies enhance our understanding of how people make decisions when they can opt out early and have relevance in bargaining and social dynamics among others."
"This paper investigates the use of adaptive pilot patterns for carrier-aggregated orthogonal frequency division multiplexing (CA-OFDM) systems in nonstationary wireless channels. Nonstationary wireless channels pose a challenge for CA-OFDM systems due to the time-varying channel conditions, which can cause severe performance degradation. To address this challenge, adaptive pilot patterns are proposed as a means of improving the system's ability to track the changing channel conditions. The effectiveness of the proposed adaptive pilot patterns is evaluated through simulation experiments in nonstationary wireless channels. The results show that the adaptive pilot patterns improve the tracking performance of the CA-OFDM system in nonstationary wireless channels, leading to better system throughput and error rates. This paper contributes to the development of adaptive techniques for wireless communication systems, which are essential for meeting the growing demand for high-speed wireless communication in nonstationary environments.","This paper evaluates adaptive pilot pattern usage for CA OFDM systems operating under nonstationary wireless channels. Such channels bring challenges because they vary over time, which adversely impacts performance. To combat this issue, adaptive pilot patterns are suggested as a means to improve tracking of such varying channel conditions. Performance improvement using these adaptive pilot patterns is measured via simulations on channels that vary over time. Results show that using adaptive pilot patterns improves tracking performance of CA OFDM systems, thereby achieving higher throughput and lower error rates. Contributions here include further development of adaptive techniques important for meeting high speed demands in variable environments."
"Contextual bandits, a popular approach for online decision making, have gained significant attention in recent years due to their ability to balance exploration and exploitation in sequential decision problems. In this paper, we focus on the estimation considerations in contextual bandits, examining the impact of various factors such as the choice of reward function, the number of arms, the choice of context representation, and the sample size on the performance of the algorithms. We provide a comprehensive review of the existing literature and propose several directions for future research. Our results demonstrate that the performance of contextual bandits is sensitive to the estimation considerations, and highlight the importance of careful design and evaluation of these algorithms. The insights gained from this study can be used to guide practitioners in the design and deployment of contextual bandits, and provide a foundation for further research in this area.","Contextual bandit algorithms are very important now because they excel at balancing exploration and exploitation for sequential decisions. This paper looks into estimation issues related to contextual bandits. We look at different things like choice of reward function, number of arms, context representation choice, and sample size and see how they affect performance. We review existing literature carefully and suggest some future research topics. Results show that performance depends a lot on how well you estimate things; careful design and evaluation of algorithms matters a lot. Insights from this study help practitioners design and deploy better contextual bandit algorithms and serve as a basis for further work in this field."
"This paper explores the effectiveness of hexaconv, a type of convolutional neural network architecture that uses hexagonal filters instead of traditional square filters. The study compares the performance of hexaconv with that of traditional square convolutional neural networks on several image classification tasks. The results show that hexaconv is able to achieve comparable or even better accuracy than square convolutional neural networks, while requiring fewer parameters and computation. The paper concludes that hexaconv is a promising alternative to traditional square convolutional neural networks, and could be particularly useful for image classification tasks involving hexagonally structured data, such as satellite imagery or cellular networks.","This paper looks at effectiveness of hexaconv, a neural network architecture using hexagonal filters rather than usual square ones. The study compares performance of hexaconv against traditional square conv nets on different image classification problems. Results indicate hexaconv performs comparably or even excels in accuracy but requires fewer parameters and computational resources. The paper ends up suggesting hexaconv is an attractive alternative to square convs and especially valuable for classification involving regular hexagonal data like satellite photos or cellular networks."
"This paper presents a model-free adaptive optimal control approach for manufacturing processes with a fixed time horizon using reinforcement learning. The method is based on the use of reinforcement learning algorithms to optimize the control policy for the manufacturing process, without relying on an explicit model of the system dynamics. The approach is evaluated on a set of episodic manufacturing processes, where the goal is to maximize the total reward obtained over a fixed time horizon. Results show that the proposed approach is able to effectively adapt to the changing system dynamics, resulting in improved control performance compared to traditional model-based methods. The results demonstrate the potential of reinforcement learning for real-time control of manufacturing processes and provide insights for future work in this area.",This paper introduces an approach for adaptive control of manufacturing processes through reinforcement learning that uses learning algorithms directly rather than relying on explicit models of dynamics. It focuses on problems where the time horizon is fixed and evaluates performance by maximizing total rewards across that time frame. Results show that this new approach successfully adapts to changes in dynamics and results in better control performance compared to conventional methods. Results highlight reinforcement learning's promise for real time control of manufacturing processes and suggest avenues for further research.
"This paper presents the results of a comprehensive survey of Delay Tolerant Networks (DTN) routing protocols. DTN is a type of communication network that is designed to operate in challenging and dynamic environments, such as those encountered in disaster relief operations, space missions, or rural areas with limited connectivity. The survey focuses on the most recent advances in DTN routing, covering both traditional and novel approaches. The paper provides a systematic overview of the main features and performance metrics of the various DTN routing protocols, including their strengths and weaknesses. The authors also identify the current challenges and open research questions in the field of DTN routing, and suggest directions for future work. The survey results will be valuable for researchers, practitioners, and decision-makers who are interested in developing, deploying, or using DTN networks.","This paper reports results from a thorough review of routing protocols for Delay Tolerant Networks (DTNs). DTNs are designed to communicate reliably in hard environments such as those found in rescue missions, space exploration, and remote regions where connectivity is poor. The report concentrates on recent advancements in routing; both traditional and newer methods are covered. Features and performance metrics of diverse DTN routing protocols are presented systematically along with advantages and disadvantages. Challenges and open research issues currently facing this domain of routing are identified and recommendations for future work are suggested. Results of this survey are important for researchers, practitioners, and decision makers who want to develop, deploy or use DTN networks."
"This paper presents a general approach for enhancing slope limiters on non-uniform rectilinear grids. Slope limiters are mathematical methods used to enforce stability and prevent unphysical solutions in numerical simulations, such as computational fluid dynamics. The study focuses on the use of slope limiters on non-uniform rectilinear grids, which are widely used in engineering and scientific applications but pose challenges for slope limiting algorithms. The proposed approach uses a combination of grid adaptation techniques and novel slope limiters to improve the accuracy and robustness of numerical simulations on non-uniform grids. The results show that the proposed approach can effectively resolve the limitations of existing slope limiters and provide reliable solutions for a range of test problems. The findings of this study are expected to have important implications for the development of numerical methods for simulations on non-uniform grids.",This research introduces a general approach to enhance slope limiters on irregular grids that vary in size. Slope limiters are mathematical tools to ensure stability and avoid physically unrealistic results in simulation codes like CFD. This work focuses on how to use these limiters on irregular grids which are very common in engineering and science but pose difficulties for limiter algorithms. Proposed here is an approach using both adaptation techniques for grids and new limiters to improve accuracy and robustness of simulation results on irregular grids. Results indicate that this method effectively overcomes shortcomings of current limiters and provides high quality results on a variety of test problems. Results of this research are anticipated to have significant implications for the development of numerical methods for simulations on irregular grids.
"This paper examines the current state of electronic health record (EHR) interoperability in Tanzania's hospitals, and identifies the key issues, challenges, and opportunities that need to be addressed in order to improve the interoperability of EHR systems in the country. Through a comprehensive literature review and interviews with healthcare professionals and stakeholders, the authors highlight the major barriers to EHR interoperability in Tanzania, including a lack of standardization, inadequate infrastructure and technology, and a lack of trained personnel. The authors also explore the potential benefits of improving EHR interoperability, including improved patient care, reduced medical errors, and enhanced public health surveillance. Finally, the authors provide recommendations for policymakers, healthcare organizations, and technology vendors to help improve EHR interoperability in Tanzania, including the development of standards and guidelines, investment in technology and infrastructure, and the implementation of training programs for healthcare professionals. This research provides important insights into the state of EHR interoperability in Tanzania, and will be of interest to healthcare professionals, policymakers, and technology vendors working in the field of health information technology.","This study looks at current interoperability among electronic health records (EHRs) in Tanzanian hospitals. It identifies critical problems, challenges and opportunities to enhance interoperability. By reviewing a broad literature and interviewing health workers and other stakeholders, the authors point out major obstacles to interoperability including a lack of standards, inadequate infrastructure and technology and insufficient trained staff. They also look at possible benefits such as better patient care, fewer medical errors and improved public health surveillance. Finally they suggest recommendations to policymakers, hospitals and technology companies to improve interoperability through development of standards and guidelines, investments in technology and infrastructure and implementation of training programs. Results from this research are valuable for those working in health IT and related areas who wish to understand current status of interoperability in Tanzania."
"This paper presents a novel method for efficient blind compressed sensing in magnetic resonance imaging (MRI). The proposed method leverages sparsifying transforms to achieve compression while preserving the important features of the data. The use of sparsifying transforms allows for a reduced number of measurements to be taken, leading to faster acquisition times and reduced costs. Additionally, the method includes convergence guarantees, ensuring that the reconstructed image accurately represents the original data. The effectiveness of the proposed method was evaluated through experiments on MRI data, demonstrating improved performance compared to existing methods. This work advances the state of the art in blind compressed sensing and has potential applications in a variety of imaging modalities.",This paper introduces a new approach to efficiently compress using blind compressed sensing for Magnetic Resonance Imaging (MRI). Proposed method uses transforms to reduce data features but retain important ones. Using such transforms results in lower measurement counts which accelerates acquisition and lowers cost. Method includes convergence assurances ensuring that reconstructions closely match original data. Performance is also validated through experiments with MRI data showing better results compared to prior methods. Results advance current research in blind compressed sensing and suggest future application diversity across different imaging types.
"This paper explores the impact of catastrophic collisions and collision avoidance on swarming behavior. The paper first provides an overview of swarming behavior in animal and artificial systems, and then discusses the various factors that can impact swarming, including catastrophic collisions and collision avoidance. The paper then presents a series of simulations and experiments that demonstrate the impact of catastrophic collisions and collision avoidance on swarming behavior, and analyzes the results to draw conclusions about the relationship between these factors and the overall effectiveness of swarming. The results suggest that catastrophic collisions and effective collision avoidance are critical components of successful swarming behavior, and that these factors can have a significant impact on the overall performance of a swarm.",This paper looks at how collisions and avoiding such collisions affect swarm behavior. It starts by giving a broad overview of swarm behavior in both natural and artificial systems. Next it talks about various things that influence swarm behavior including major collisions and avoidance of collisions. Then there is simulation and experiment work showing how collisions and avoidance impacts swarm behavior and analysis of results draws conclusions regarding relationships among these factors and effectiveness of swarms as a whole. Results show that collisions and good avoidance are key elements for successful swarm behavior and this matters quite a lot for performance of the swarm as a whole.
"This paper presents a comprehensive study on the prediction reliability of graph neural networks (GNNs) for virtual screening. The study was conducted to evaluate the performance of GNNs in predicting the binding affinity of small molecules with protein targets. The authors tested several state-of-the-art GNN models on a diverse set of protein-ligand complexes and compared their performance to other commonly used methods in virtual screening. The results showed that GNNs achieved comparable or superior performance compared to traditional methods, with high predictive accuracy and robustness. The authors also identified important factors that can affect the reliability of GNN predictions, such as the choice of hyperparameters and the quality of training data. The findings of this study provide valuable insights into the potential of GNNs for virtual screening and highlight their utility as a reliable tool for drug discovery.",This paper studies carefully the reliability of Graph Neural Networks (GNNs) for virtual screening and evaluates their performance on a wide variety of protein ligand pairs. Authors test different top performing GNN models against other common methods and show that they perform comparably or better in terms of accuracy and stability. Results also reveal important factors that influence reliability including choice of hyperparameters and quality of training data. Insights are provided into promising potential for GNNs and utility as reliable tools for new drug discovery.
"This paper presents a novel approach for anti-isomorphism mapping of unordered labeled trees, called ""Anti-Tai Mapping"". The proposed method addresses the challenge of finding a one-to-one correspondence between nodes in two unordered labeled trees, while preserving the structural and label similarities between the nodes. The approach is based on the concept of ""tree tai-mapping"", which is modified to handle the unordered nature of the trees and to enforce the anti-isomorphism constraint. The performance of the Anti-Tai Mapping method was evaluated on several datasets and compared with existing methods, demonstrating its superiority in terms of accuracy and computational efficiency. The results of this study have potential applications in various fields, such as pattern recognition, machine learning, and computational biology.",This paper introduces an innovative method for creating one-to-one correspondences among unordered trees that preserve both their structural similarity and labels; this method is called Anti-Tai Mapping. We develop this from the idea of tree tai mapping but modify it to deal specifically with unordered trees and to meet anti isomorphism constraints. Performance evaluation against other methods was conducted using several data sets and we showed that this new method excels in accuracy and speed. Results from this research have application in many areas including pattern recognition and machine learning.
This paper presents an investigation into the automated identification of security discussions in microservices systems. The study includes both industrial surveys and experimental results to provide a comprehensive analysis of the current state of the field. The surveys were conducted with industry professionals to gather their perspectives on the importance and challenges of security in microservices systems. The experimental results focus on the development and evaluation of a machine learning-based approach for automatically identifying security discussions in microservices systems. The results of the study provide insights into the current state of security in microservices systems and suggest that automated identification of security discussions is a promising area for future research and development.,This paper investigates automation for identifying security discussions in systems composed of microservices. Results from both industrial surveys and experiments contribute to an integrated analysis of this field. Surveys are carried out among professionals in industry to understand how important and difficult security is in such systems. Experimental work focuses on building and testing a machine learning approach for automatic detection of security discussions within such systems. Results of the study also point to areas where future research and development can be focused; they show that identifying security discussions through automation looks promising.
This paper proposes a new framework for counting and localization in crowds that utilizes a purely point-based approach. The current methods for counting and localizing individuals in crowds rely on region-based techniques which are limited by their assumptions about crowd behavior and can be prone to errors. The proposed point-based framework aims to overcome these limitations by treating each person in a crowd as a discrete point and using this information to make more accurate counts and localizations. The framework has been tested and validated using real-world data and has shown promising results in improving the accuracy of crowd counting and localization. This research provides a new perspective on the current limitations of region-based techniques and offers a potential solution to these limitations through a point-based approach.,This paper introduces a new framework for counting and localization in crowds. This new framework uses only points and does not rely on regions; previous approaches have relied heavily on region based techniques and have thus made some incorrect assumptions about crowd dynamics and therefore are prone to mistakes. The new framework attempts to resolve this by considering each person as an isolated point and using that data to count and localize them better. Results have been tested using actual data and show promising improvements in accuracy. This research also points out current weaknesses of region based techniques and suggests that an approach using points is a potential way to solve these problems.
"In this paper, the authors present a novel method for interpolation called Neurint which uses neural ordinary differential equations (ODEs). The method utilizes a parametric approach to approximate the solution of a system of ODEs and learns the parameters through backpropagation, enabling the model to interpolate between any two points in the input space. The authors conduct experiments to compare Neurint with traditional interpolation methods and show that Neurint outperforms these methods in terms of accuracy and generalizability. The results of this study demonstrate the potential of Neurint as a promising tool for various interpolation tasks in machine learning and beyond.",This paper introduces a new method for interpolation called Neurint. It uses neural ordinary differential equations (ODEs) and takes an approach where it approximates solutions of systems of ODEs by using parameters learned via back propagation. Thus allowing the model to interpolate between any two points within the input space. Experiments are conducted to compare Neurint against traditional methods of interpolation and show Neurint excels in both accuracy and generalization. Results from this research indicate Neurint has great potential as a promising tool for interpolation tasks across different disciplines.
"This paper presents an investigation of efficient labeling algorithms for solving the adjacent quadratic shortest path problem. The authors analyze the performance of various labeling algorithms, including the A* and Dijkstra algorithms, and propose a novel hybrid approach combining elements from both methods. The experimental results show that the proposed hybrid algorithm outperforms existing methods in terms of computation time and solution quality. The findings of this study contribute to the development of more efficient algorithms for solving the adjacent quadratic shortest path problem and have implications for applications in fields such as transportation, logistics, and network design.",This paper looks into efficient algorithms for labeling to solve the adjacent quadratic shortest path problem. Authors compare different labeling algorithms like A* and Dijkstra and introduce a new hybrid approach which combines features of both. Results show the proposed hybrid algorithm excels in terms of both speed and solution quality compared to previous work. Results are important because they improve algorithms for this kind of problems and have ramifications for transportation logistics and other network design areas.
"Scene parsing is a crucial task in computer vision, where the goal is to segment and categorize objects in an image or video. In this paper, we propose a novel approach to scene parsing using multiscale feature learning, purity trees, and optimal covers. Our approach first uses multiscale feature learning to extract features from an image at multiple scales, capturing both local and global context. Then, we employ purity trees to hierarchically divide the image into regions, grouping similar pixels and objects. Finally, we use optimal covers to assign each region to a semantic class, resulting in a comprehensive scene parse. Our approach is tested on benchmark datasets and demonstrates state-of-the-art performance in scene parsing accuracy.","This paper introduces a new method for scene parsing which involves learning features at different scales, using purity trees for hierarchical segmentation of images and using optimal coverages for class assignment. At first we extract features from the image at various levels of scale so that we can capture both local and global context. After that we sort regions based on purity trees so that similar pixels and objects are grouped together. Finally, using optimal coverages we assign each region to a semantic class and this results in high quality scene parsing. Results show our method performs at leading edge compared to benchmarks."
"The lattice Boltzmann method (LBM) is a powerful computational fluid dynamics (CFD) tool that simulates fluid flow by solving the Boltzmann equation. In this paper, we present a new approach to massively parallelize the LBM on non-uniform grids. Non-uniform grids are more flexible than uniform grids and can better resolve complex geometries, but they have been challenging to parallelize. Our approach takes advantage of the data-parallel nature of the LBM to parallelize the simulation across multiple compute nodes. We implement the LBM on non-uniform grids using a hybrid MPI/OpenMP approach, which balances computational load and communication overhead. The performance of our implementation is evaluated on a range of test cases and compared to existing parallel implementations. Our results show that our parallel algorithm can scale efficiently up to a large number of compute nodes and achieve significant speedup over existing methods, demonstrating the viability of our approach for large-scale CFD simulations.","Using lattice Boltzmann method (LBM), a powerful tool for computational fluid dynamics (CFD) simulations, we introduce a new approach to parallelize this method on irregular grids. Irregular grids provide advantages such as flexibility and better resolution of complex shapes; however, they have proven difficult to parallelize effectively. By exploiting parallelism inherent in LBM we develop an approach for distributing simulations among different processors. We use a mixed MPI and OpenMP approach to run LBM codes on irregular grids and balance workload and communication costs. Performance is measured using various test problems and compared against other parallel implementations. Results indicate that our parallel algorithm scales well and outperforms previous work at high node counts, thus showing feasibility for large scale CFD simulations."
"This paper presents a modified scheme for a quantum sealed-bid auction using a multiparty circular quantum key agreement. The proposed scheme is designed to address the limitations of traditional sealed-bid auctions, which are vulnerable to security threats such as bid manipulation and information leakage. By incorporating principles of quantum cryptography, the proposed scheme provides increased security for the auction process. The multiparty circular quantum key agreement enhances the security of the auction by ensuring that all parties involved in the process have secure communication channels and that the bids are kept confidential. Results from simulations and experiments indicate that the proposed scheme is efficient and secure, offering a promising solution for conducting secure sealed-bid auctions. The findings of this research have important implications for the design and implementation of secure electronic auction systems.","This paper introduces an improved scheme for sealed bid auctions using a protocol that includes multiparty circular quantum key distribution. Proposed scheme aims to overcome limitations of traditional auctions which are prone to attacks such as bid tampering and leaking information. By leveraging principles of quantum cryptography, the scheme ensures higher security for auction transactions. Multiparty circular quantum key distribution further improves security through provision of secure channels and confidentiality of bids. Results from simulation and experimentation show that the proposed method works well and securely; this suggests a promising way to conduct secure auctions. Results of this research are significant for designing and implementing secure electronic auction systems."
"In this paper, we introduce a novel approach for designing faster sublinear algorithms using conditional sampling. Sublinear algorithms are a class of algorithms that can process large datasets efficiently without having to read the entire dataset. Our approach is based on conditional sampling, which allows us to obtain unbiased estimates of specific functions with high probability using only a small subset of the data. This leads to significant speedup in the computation time while maintaining the accuracy of the results. Our proposed method has been tested on several real-world datasets and showed improvement over existing sublinear algorithms. This work opens up new avenues for research in the area of sublinear algorithms and has practical applications in various domains including machine learning, data mining, and big data analytics.","We present here an innovative approach for designing faster sub linear algorithms using conditional sampling. Sub linear algorithms allow fast processing of large data sets by not reading every element directly. Our approach relies on conditional sampling which enables obtaining accurate estimations of specific functions with high probability by sampling only a small fraction of the data. This leads to a notable reduction in computation time alongside ensuring high accuracy. Results from our experiments conducted on diverse real data sets demonstrate improvements compared to current algorithms. This study paves the way forward for future research into sub linear algorithms and has practical relevance for different disciplines such as machine learning, data mining and big data analysis."
"This paper proposes a joint optimization approach for computing resource allocation in three-tier IoT fog networks that combines Stackelberg game and matching. The three-tier architecture of IoT fog networks consists of cloud, fog, and edge layers, each with their own computing resources. The Stackelberg game models the interactions between the layers as a leader-follower relationship, while the matching algorithm finds the optimal allocation of resources based on the network's constraints and objectives. Simulation results demonstrate the effectiveness of the proposed approach in achieving efficient resource allocation, reduced latency, and improved network performance compared to traditional methods. This work sheds light on the potential of combining game theory and optimization techniques for addressing resource allocation challenges in multi-tier IoT networks.","This paper suggests an integrated approach for computing resource allocation using Stackelberg games and matching algorithms for IoT fog networks which have three layers: cloud, fog, and edge; each layer has distinct computational resources. Using Stackelberg games the authors model interaction among these layers as a leader and follower relationship; meanwhile they use matching algorithms to optimize resource allocation based on constraints and goals of the network. Results from simulations show this proposed method is effective at improving efficiency of resource allocation, reducing latency and enhancing network performance relative to conventional approaches. Results also indicate promising potential to combine game theory and optimization techniques for solving resource allocation issues across multiple tiers of IoT networks."
"In this paper, the authors propose a new beamforming technique called ""Layered Group Sparse Beamforming (LGSB)"" for cache-enabled green wireless networks. LGSB is designed to address the challenge of balancing energy efficiency and data rate in wireless networks with caching capabilities. The technique is based on grouping users into different layers and allocating sparse beamforming resources to each layer. The results of simulations show that LGSB provides significant improvements in energy efficiency while maintaining high data rates compared to traditional beamforming techniques. The findings of this study can contribute to the development of more sustainable and energy-efficient wireless networks.","Authors propose a new method called Layered Group Sparse Beamforming (LGSB) in this paper for green wireless networks that include caching capabilities. This new method aims at optimizing both energy efficiency and data throughput. Based on dividing users into layers, LGSB allocates sparse beamforming resources to each layer. Results from simulations demonstrate that LGSB significantly improves energy efficiency and maintains high data throughput compared to traditional methods. Findings of this work contribute to development of networks that are more sustainable and energy efficient."
"This paper proposes a novel approach for clustering stream data based on the concept of density mountain evolution. The approach utilizes the notion of density peaks to track the evolution of clusters over time and identify the evolution patterns of these density peaks. This method allows for a more dynamic and adaptive clustering of stream data, compared to traditional methods that often assume a static cluster structure. The performance of the proposed method was evaluated through experiments on synthetic and real-world data sets, and the results showed that the approach outperforms existing methods in terms of accuracy, efficiency, and scalability. The findings of this study contribute to the advancement of clustering techniques for stream data and have potential applications in various domains, such as online market segmentation and fraud detection.","This paper introduces a new way of clustering streaming data based on the idea of evolving density mountains. It uses the notion of peak density to follow changes of clusters over time and recognize patterns of density peaks. Compared to previous methods that generally assume fixed cluster structures, this new approach is much more responsive and adaptive. Performance of the proposed method has been tested using both synthetic and real data sets and results show that this approach excels in terms of accuracy, efficiency and scalability. Results of this research advance clustering methods for streaming data and open up possibilities for use in different fields such as market segmentation online and fraud detection."
"This paper investigates the potential of using machine learning techniques to improve the efficiency of boolean satisfiability problem (SAT) solving. SAT is a well-known NP-complete problem and is widely used in various fields, including hardware and software verification, scheduling, and cryptography. The paper proposes a novel approach that leverages machine learning models to guide the search process of traditional SAT solvers. The results show that the proposed approach significantly improves the solving time of SAT instances compared to state-of-the-art solvers. The proposed approach also achieves competitive results on a range of standard benchmark datasets, demonstrating the effectiveness of the approach in practical scenarios. This research provides insights into the application of machine learning in solving computationally hard problems and has potential for further advancements in this field.","This paper looks into using machine learning methods to enhance the efficiency of solving Boolean Satisfiability Problems (SAT). SAT is well known as an NP complete problem and is used extensively in many areas such as hardware/software verification, scheduling and cryptography. A new method is proposed here which uses machine learning models to direct the search process of traditional SAT solvers. Results indicate that this new approach greatly reduces solving time for SAT instances when compared to leading solvers. Performance also matches well against standard benchmark sets across a wide range of cases showing effectiveness. This work sheds light on applying machine learning to hard computational problems and opens doors to future development in this domain."
"The IEEE Computational Intelligence and Games (CIG) 2017 Game Data Mining Competition aimed to find the best solution for predicting player behavior in a video game. In this paper, the winning solution is presented and analyzed. The approach used in the winning solution was based on machine learning techniques, specifically gradient boosting and deep neural networks. The results showed that the winning solution outperformed other competitors by a significant margin in terms of prediction accuracy. The findings from this study demonstrate the potential of combining machine learning techniques for predicting player behavior in video games and highlights the importance of careful feature selection and model architecture design. The authors also provide insights into the limitations and challenges of the approach and suggest areas for future research.",The winning team at IEEE CIG 2017 competition on mining data related to video games focused on predicting player behavior. Results show their winning solution performed better than other teams by a big margin in terms of accuracy. Results from this work show promise for using machine learning techniques to predict player behavior; the paper also looks closely at what features work best and how to design models. Authors also point out some drawbacks of their approach and give suggestions for future research.
"This paper proposes a meta-transfer objective for learning to disentangle causal mechanisms. The objective is based on the idea of transfer learning, where knowledge learned from one task is used to improve performance on a related but different task. The proposed objective aims to transfer the ability to disentangle causal mechanisms from one problem to another. The objective is evaluated on a series of experiments and compared to existing methods for disentangling causal mechanisms. The results show that the proposed meta-transfer objective outperforms existing methods in terms of accuracy and generalization performance, demonstrating its effectiveness in improving the ability to disentangle causal mechanisms.",This paper suggests a new objective for separating causal mechanisms. This goal draws on ideas from transfer learning which transfers knowledge from tasks into performance improvement for closely related tasks. The new objective uses this idea to transfer the skill of separating mechanisms. Results of several experiments compare this objective to other methods. Results indicate that using this objective leads to higher performance and better generalization performance and thus shows its effectiveness at improving ability to separate mechanisms.
"This paper presents a novel approach for decoding in communication systems, referred to as SRGRAND (Symbol Reliability Guided Random Additive Noise Decoding). The conventional decoding methods rely solely on channel channel state information (CSI) to perform the decoding. However, in practical communication systems, channel CSI is often unavailable or outdated, which results in performance degradation. The proposed SRGRAND approach leverages symbol reliability information in addition to channel CSI to enhance the decoding performance. The symbol reliability information is calculated based on the statistical properties of the transmitted symbols and the channel noise, which provides additional information about the quality of each symbol. This information is then used to guide the decoding process and improve the error rate. Simulation results show that the SRGRAND approach outperforms the conventional decoding methods and provides a more robust performance in practical communication systems.",This paper introduces a new way to decode information in communications called SRGRAND (Symbol Reliability Guided Random Addition Noise Decoder). Conventional decoding uses only information about channel states to perform decoding but in real systems this information is often not available or outdated which leads to reduced performance. Proposed SRGRAND uses additional symbol reliability information along with channel states to improve performance. Symbol reliability is calculated using statistical characteristics of transmitted symbols and channel noise and gives information about quality of each symbol. This information is then used to direct the decoding process and reduce errors. Results from simulations show SRGRAND outperforms standard decoding methods and performs better overall.
"Semi-supervised Generative Adversarial Networks (ssGANs) are a popular approach for unsupervised learning tasks in computer vision and natural language processing. However, their application to regression problems has been limited due to the difficulty in defining a suitable loss function. In this paper, we propose a new approach to generalize ssGANs to regression problems by using a feature contrasting loss function. The proposed method effectively leverages both labeled and unlabeled data to learn a mapping from the input space to the output space. We evaluate the performance of the proposed method on various regression benchmarks and compare it with existing methods. The results show that the proposed method outperforms the state-of-the-art methods in terms of both accuracy and stability. This work opens up new possibilities for ssGANs in regression problems and demonstrates their potential for solving real-world problems.",Semi supervised GANs have been widely used for unsupervised learning tasks in computer vision and NLP. But they have not been applied much to regression problems because it is hard to define an appropriate loss function. This paper introduces a new approach to extend GANs to regression. We use contrastive features as loss function and propose a way to leverage both labeled and unlabeled data to learn mappings between input and output spaces. Performance is evaluated on different benchmarks and compared against other methods; results indicate that this new method surpasses current best methods in accuracy and reliability. This research opens up new avenues for GANs to tackle regression tasks and shows promise for dealing with practical issues.
"The paper presents a novel approach to semantic segmentation called Criss-Cross Attention Network (CCNet). The method is based on the idea of criss-cross attention mechanism, which has shown promising results in various computer vision tasks. In CCNet, the criss-cross attention mechanism is utilized to capture long-range dependencies between pixels, which are critical for accurate semantic segmentation. The network architecture consists of a feature extraction backbone, a criss-cross attention module, and a segmentation head. The criss-cross attention module utilizes both spatial and channel-wise attention to model the inter-dependencies between pixels. The experiments conducted on several benchmark datasets demonstrate the effectiveness of CCNet, achieving state-of-the-art performance with a relatively lightweight network architecture. The results suggest that the proposed criss-cross attention mechanism is a promising approach for semantic segmentation and can be further extended to other computer vision tasks.","The paper introduces a new approach to semantic segmentation named CCNet. Based on the idea of cross attention mechanism that excels at various computer vision tasks, CCNet employs this mechanism to exploit long range dependency among pixels; these dependencies are important for segmentation accuracy. Architecture includes a backbone for feature extraction, an attention module and a segmentation head. This module uses both spatial and channel attention to model dependencies among pixels. Experiments on benchmark datasets show that CCNet is effective and performs well with lightweight design. Results indicate that proposed mechanism for cross attention is promising for segmentation and may also improve other vision tasks."
"This paper presents a study on the development of an efficient object detection model for real-time unmanned aerial vehicle (UAV) applications. With the growing demand for UAVs in various industries, there is a need for efficient and reliable object detection algorithms that can be used in real-time scenarios. The proposed model is based on a combination of deep learning and computer vision techniques, which are optimized to provide accurate and fast object detection in real-time UAV applications. The study includes a comprehensive evaluation of the proposed model on a benchmark dataset, and the results show that it outperforms existing state-of-the-art object detection models in terms of both accuracy and processing speed. The model is also tested in various real-world UAV applications, including surveillance and inspection tasks, and the results demonstrate its effectiveness and practicality. This research contributes to the field of UAV object detection by providing a highly efficient and accurate model for real-time applications.",This paper reports a study about development of a high performance model for efficient real time use with unmanned aerial vehicles (UAVs). There has been growing interest from different industries in using UAVs and this paper addresses the requirement for high quality and reliable detection algorithms suitable for real time use. Proposed method combines deep learning and computer vision techniques which are fine tuned to perform accurate and quick object detection for real time UAVs. Results of comprehensive testing on benchmark data set show that this new method excels compared to current leading models for both accuracy and processing speed. Results are also shown in diverse practical tasks like surveillance and inspections; effectiveness and practicability of this model is demonstrated. This research advances UAV object detection by introducing a very efficient and accurate model for practical real time applications.
"This paper presents a quantitative analysis of cryptocurrency egalitarianism, exploring how the distribution of cryptocurrency holdings among users reflects or diverges from principles of egalitarianism. To do so, we analyzed transaction data from a sample of the most widely used cryptocurrencies, including Bitcoin, Ethereum, and Litecoin. Our approach involves constructing a Gini coefficient for each cryptocurrency, which measures the level of inequality in its distribution of wealth. We also examined the concentration of wealth among the top percentile of cryptocurrency holders and assessed the impact of various factors, such as mining, trading, and adoption rates, on the level of egalitarianism in each cryptocurrency's ecosystem. Our findings suggest that, while cryptocurrency is often touted as a decentralized and democratized form of currency, the reality is that the distribution of wealth in many cryptocurrencies is highly unequal. However, we also identify several factors that could help promote greater egalitarianism in the cryptocurrency space, including measures to address concentration of wealth and the promotion of fair mining practices. Overall, our research sheds light on the extent to which cryptocurrency adheres to principles of egalitarianism and provides insights into potential strategies for promoting a more equitable distribution of wealth in this emerging financial ecosystem.","This paper conducts a quantitative study of egalitarianism among cryptocurrency users by analyzing data on transactions of leading cryptocurrencies like Bitcoin, Ethereum and Litecoin. We construct Gini coefficients for each cryptocurrency to gauge wealth inequality. We also look at concentration of wealth among top percentiles of holders and assess impact of different factors like mining and adoption rates. We find that while cryptocurrency is frequently seen as decentralized and democratic, wealth distribution among many cryptocurrencies is very uneven. We also identify factors that might contribute to greater equality in cryptocurrency ecosystems such as reducing concentration of wealth and promoting fair mining practices. Overall we illuminate how closely cryptocurrency aligns with egalitarian principles and suggest ways to promote a more equal wealth distribution in this new financial system."
"This paper investigates the problem of simple and approximately optimal pricing for proportional complementarities. Proportional complementarities arise in various economic settings, where the value of a good to a consumer increases as the adoption of the good by others increases. The paper focuses on finding pricing strategies that maximize the total surplus generated by the market while being simple and easy to implement. The paper presents a novel approach that combines the principles of optimal pricing with approximation methods to derive pricing formulas that are both optimal and practical. The approach is tested through simulation and the results show that the proposed pricing formulas lead to near-optimal results, while being much simpler than traditional optimal pricing methods. The findings of this paper contribute to a better understanding of how to design pricing strategies for proportional complementarities and provide valuable insights for practitioners and policymakers.",This study examines pricing strategy that maximizes total surplus and is simple to implement. Such strategy is important because proportional complementarity occurs in different economic contexts where the benefit from having something rises with increasing adoption by others. We propose an innovative method combining principles of optimal pricing with approximation techniques. Through simulations we test this method and find results indicate near optimality compared to traditional methods but our proposed formulas are also much simpler. Results of this research offer insight into designing pricing strategies for this type of complementarity and are useful for people who make policies and business decisions.
"This paper investigates the design flow of digital microfluidic biochips (DMFBs) with the aim of improving their fault-tolerance. DMFBs have the potential to revolutionize the field of bioanalysis by enabling miniaturization and automation of complex laboratory protocols. However, the high complexity and large-scale integration of DMFBs can lead to various faults and errors, which can compromise their functionality and reliability. To improve fault-tolerance in DMFBs, the proposed design flow comprises four main steps: fault modeling, fault analysis, fault-tolerant synthesis, and fault-tolerant testing. The fault modeling step involves identifying possible faults and errors in the DMFB design and their root causes. The fault analysis step assesses the impact of faults on the DMFB and its overall performance. The fault-tolerant synthesis step involves generating a fault-tolerant design that can mitigate the identified faults. Finally, the fault-tolerant testing step involves validating the performance and reliability of the fault-tolerant DMFB design. The proposed design flow is demonstrated through the design of a droplet-based DMFB for protein crystallization, which involves a complex sequence of biochemical reactions. Fault injection experiments are conducted to validate the fault-tolerant design, and the results show that the proposed approach can effectively improve the fault-tolerance of DMFBs. The contributions of this paper include a comprehensive design flow for fault-tolerant DMFBs, a case study demonstrating the effectiveness of the proposed approach, and a discussion on the potential limitations and future directions of the proposed design flow. The proposed design flow can be used as a framework for the development of reliable and robust DMFBs for a range of bioanalysis applications.","This paper studies a design process for digital microfluidic biochips (DMFBs) aimed at enhancing their tolerance to faults. These chips have great potential for revolutionizing bioanalysis because they allow for miniaturization and automation of complex lab procedures. But due to high levels of complexity and integration into larger systems, DMFBs often suffer from different kinds of faults and errors that can undermine functionality and reliability. To improve fault tolerance, we propose a design process comprising four major steps: modeling faults and causes, assessing impact on performance, synthesizing fault tolerance, and performing fault testing. Modeling faults involves identifying possible faults and causes within designs of DMFBs. Analysis evaluates how faults affect performance and overall function. Synthesis generates designs that reduce identified faults. Finally, testing validates performance and reliability of resulting designs. We demonstrate this approach using a case study involving protein crystallization and other complicated biochemical reactions. We inject faults into the system and test to verify our fault tolerant design; results show this approach effectively improves fault tolerance. Contributions of this work include a comprehensive design process for reliable DMFBs, a demonstration of effectiveness of proposed method, and discussion of limitations and future developments. Proposed framework could serve as a foundation for developing high quality DMFBs for diverse applications in bioanalysis."
"The purpose of this paper is to present a new approach for behavior planning in connected autonomous vehicles using feedback deep reinforcement learning. The study focuses on developing a system that can learn from past experiences and make informed decisions in real-time to ensure safe and efficient navigation on the roads. The proposed approach utilizes a combination of deep reinforcement learning and feedback control, where the feedback control component provides additional stability and robustness to the system. The effectiveness of the proposed approach is evaluated through a series of simulations and experiments, which show significant improvement in terms of safety and efficiency compared to traditional behavior planning methods. The results of the study demonstrate the potential of the proposed approach in providing a practical solution for behavior planning in connected autonomous vehicles.",This paper introduces a new way to plan behavior of connected autonomous cars using feedback deep reinforcement learning. We focus on developing systems that learn from previous experience and make smart decisions in real time to achieve safe and efficient driving on roads. Our proposal combines deep reinforcement learning and feedback control; the latter component adds extra stability and robustness. Effectiveness is tested via simulations and experiments which show marked improvement in safety and efficiency compared to current methods. Results suggest this approach has practical value for behavior planning in connected cars.
"This paper presents a novel approach to fast Bayesian calibration of brain tumor models using a geometry-aware neural solver. The proposed method leverages the power of deep learning to efficiently solve the inverse problem of mapping MRI scans to physical parameters of the tumor, while accounting for the underlying geometrical constraints. The proposed solver is trained using a combination of simulated and real data, and the results demonstrate significant improvements in accuracy and computation time compared to traditional optimization-based methods. This work provides a promising solution to the challenging problem of estimating the physical properties of brain tumors from medical images, and has potential applications in personalized medicine and radiation therapy planning.",This paper introduces a new approach for fast Bayesian calibration of brain tumor models. A solver that respects geometric constraints is proposed which uses deep learning power to efficiently solve the inverse problem of matching MR images to physical tumor parameters. Results show marked improvements over traditional optimization methods both in terms of accuracy and speed. This work therefore opens up promising prospects for estimation of physical properties of brain tumors based on medical images and might have important implications for personalized medicine and radiation treatment planning.
"This paper explores the use of self-supervised learning techniques to improve the representation learning of tabular data. Traditional methods for learning representations from tables rely on handcrafted features or pre-defined embeddings, which may not capture the underlying structure of the data. To overcome these limitations, we propose a self-supervised learning approach that leverages the inherent structure and relationships within the table to learn more informative representations. Specifically, we introduce two self-supervised tasks that focus on predicting missing values and reconstructing corrupted tables. We demonstrate the effectiveness of our approach on several real-world datasets, showing that it outperforms existing methods for tabular data representation learning. Our findings suggest that self-supervised learning can provide a promising avenue for enhancing the representation of tabular data and improving downstream applications such as classification and regression.","This paper investigates use of unsupervised learning techniques to improve learning of representation in tabular data. Most current approaches rely on features or fixed embeddings defined by hand that do not fully exploit the data's intrinsic structure. To address this shortcoming, we introduce an unsupervised learning method which exploits internal structure and relationships within tables to learn more meaningful representations. Specifically, we present two tasks focused on predicting missing values and reconstructing corrupted tables. We show effectiveness through performance on different real datasets and find that this new method outperforms current methods for learning representations for tabular data. Results indicate that using unsupervised learning has promise for better representation of tables and higher performance for downstream tasks such as classification and regression."
"This paper presents a study of few-shot audio classification, which aims to classify audio signals into a limited number of categories with only a small amount of labeled data. The study focuses on exploring the effectiveness of various few-shot learning techniques, such as transfer learning and meta-learning, in addressing the challenge of limited labeled data. The research includes an extensive evaluation of different models and techniques on several benchmark datasets, using various metrics such as accuracy, precision, recall, and F1-score. The results demonstrate that transfer learning and meta-learning can significantly improve the classification performance in few-shot scenarios, compared to conventional approaches. The study also investigates the impact of different factors such as the size of labeled data, the number of categories, and the complexity of audio signals on the classification performance. The findings of this study provide valuable insights into the design of effective few-shot audio classification models, which have important applications in speech recognition, music analysis, and audio-based surveillance systems.","This paper investigates a task of classifying audio signals into limited categories using very little labeled data. The goal is to evaluate various methods for few shot learning including transfer learning and meta learning to address this difficulty. Different models and techniques are evaluated on various benchmarks using metrics like accuracy, precision, recall and F1 score. Results show that transfer learning and meta learning greatly improve classification performance compared to regular methods. Factors such as quantity of labeled data, category number and signal complexity also affect performance and this work examines their effect too. Results of this research offer useful guidance for designing effective classifiers for tasks like speech recognition and surveillance based on audio."
"Ride-hailing services have become a popular mode of transportation in many urban cities around the world. In Nigeria, ORide is one of the prominent ride-hailing services that provide affordable transportation to commuters. However, the anonymity of ORide drivers has been a long-standing issue that raises concerns about user safety and trust. In this paper, we present an empirical study that explores the perceptions of ORide users towards driver anonymity and its implications for user safety. We conducted a survey of 500 ORide users in Lagos, Nigeria, to gather their opinions on driver anonymity and the factors that influence their decision to use the service. Our findings show that a significant number of ORide users are concerned about driver anonymity and would prefer to have more information about their drivers, such as their names and photos. We also found that users' perception of driver anonymity is influenced by their prior experience with the service, gender, and age. Additionally, our study reveals the potential safety implications of driver anonymity, such as difficulty in identifying drivers in case of accidents or security breaches. Based on our findings, we provide recommendations for ORide and other ride-hailing services on how to address the issue of driver anonymity while maintaining user safety and privacy. Our study contributes to the growing body of literature on ride-hailing services in developing countries and sheds light on the importance of balancing user privacy and safety in the design of such services.","Ride hailing services have become very common in many big cities all over the world. In Nigeria, ORide is one of the leading ride hailing services that offer affordable transportation to people who need to commute. One major concern regarding ORide drivers has been their anonymity which worries about the safety and trust of the users. This paper looks at how users perceive anonymity of drivers and the effects on safety. We conducted a survey of 500 users in Lagos, Nigeria to learn what they think about anonymity of drivers and what influences their decisions to use the service. Results show that many users feel uncomfortable about anonymity and want to know more about drivers like their names and pictures. Users also report that anonymity affects their feelings differently based on past experience with the service, gender and age. The results also indicate that anonymity may pose some risks to safety; namely difficulty distinguishing drivers in case of accidents or security issues. We provide recommendations to both ORide and other ride hailing companies to improve anonymity while ensuring safety. Results of this study help fill a gap in the literature on ride hailing services in developing countries and highlight the balance needed between privacy and safety when designing such services."
"This paper explores the concept of intermittent status updating in wireless sensor networks. In particular, it focuses on the joint scheduling of sensing and retransmission operations to optimize the efficiency of status updates. The authors propose a new scheduling algorithm that balances the trade-off between sensing accuracy and energy consumption by dynamically adjusting the sensing and retransmission intervals based on network conditions. Simulation results demonstrate the effectiveness of the proposed algorithm in terms of reduced energy consumption and improved status update accuracy compared to traditional scheduling approaches. The findings of this study have important implications for the design and deployment of wireless sensor networks in various applications, including environmental monitoring, industrial automation, and healthcare.","This paper looks into how to do frequent updates using sensors through wireless networks. It particularly studies scheduling for both sensing and retransmission operations to maximize efficiency of these updates. Authors suggest a new scheduling algorithm that seeks a balance between sensing accuracy and energy use by changing sensing and transmission intervals according to network conditions. Results from simulation show that proposed method reduces energy consumption and improves accuracy of updates compared to current scheduling methods. Findings are significant for designing and deploying networks for diverse applications such as environmental monitoring, industrial control, and health care."
"This paper presents a novel approach to cell-free massive multiple-input multiple-output (MIMO) detection using distributed expectation propagation (DEP). Cell-free massive MIMO is a promising technology for achieving high spectral efficiency in wireless communication networks, but its detection algorithms can be computationally expensive and difficult to implement. DEP offers a distributed and computationally efficient solution to this problem by breaking down the detection problem into smaller subproblems that can be solved in parallel by multiple processing units. The proposed approach is evaluated through simulations and compared with other state-of-the-art detection methods. Results show that the DEP approach achieves comparable or better performance than other methods, while also being more efficient in terms of computational complexity and memory usage. Furthermore, the distributed nature of the approach enables it to scale to larger networks with minimal overhead. The paper also discusses potential applications of the DEP approach beyond cell-free massive MIMO, such as in distributed optimization and inference problems. Overall, the results suggest that the DEP approach is a promising method for improving the performance and efficiency of cell-free massive MIMO detection and has potential for broader applications in distributed computation.","This paper introduces a new approach to detecting cells free massive multiple input multiple output (MIMO). Cell free massive MIMO is very promising for wireless communications because of high spectral efficiency, but its detection algorithms are often computationally expensive and hard to implement. Distributed expectation propagation (DEP) provides an efficient solution by dividing detection into smaller parts which can be solved simultaneously by different processors. Results show DEP performs similarly or better compared to other recent approaches. Compared to others, DEP uses less computation and memory. Also because it is distributed, scaling up to large networks incurs little overhead. The authors also mention that the DEP technique might be applicable in other contexts including distributed optimization and inference. Overall results indicate DEP is promising both for improvement of performance and efficiency of detection and that it might have broader use in distributed computing too."
"This paper proposes a novel approach to address the challenge of object detection with rare classes and slices in the context of targeted active learning using submodular mutual information. The proposed method, called Talisman, is designed to iteratively select informative samples to label, with the aim of improving the performance of object detectors on rare classes and slices. The Talisman framework employs a submodular function to measure the mutual information between the labeled and unlabeled samples, and utilizes this measure to select the most informative samples for labeling. Additionally, the proposed approach incorporates the notion of target classes and slices, which enables the system to prioritize the selection of samples from the rare classes and slices of interest. The proposed method was evaluated on several benchmark datasets, and the results demonstrate the effectiveness of the Talisman framework in improving the performance of object detectors on rare classes and slices. The experimental results also show that the proposed approach outperforms several state-of-the-art active learning methods, especially in the scenarios with rare classes and slices. Overall, the proposed Talisman framework provides a promising approach for targeted active learning in the context of object detection, especially for the challenging scenario of rare classes and slices.","This paper introduces a new approach to tackle the problem of detecting objects in rare categories and slices using targeted active learning and utilizing submodular mutual information. We call this approach Talisman. Talisman aims at improving performance of object detectors through iterative selection of informative samples to label. Using a sub modular function we measure mutual information between labeled and unlabeled samples and use this measure to choose most informative samples for labelling. Our approach also considers target classes and slices and uses them to give priority to choosing samples of those specific rare ones of interest. Performance has been measured on different benchmark datasets and results show effectiveness of Talisman in improving performance on rare categories and slices. Results also indicate our method outperforms many leading active learning approaches particularly when dealing with rare categories and slices. In summary, Talisman offers promising solution for targeted active learning context especially for difficult situation with rare categories and slices."
"This paper proposes a novel approach for attribute artifacts removal in geometry-based point cloud compression. The proposed approach utilizes a two-step process, first performing point cloud downsampling using a quadtree-based method, followed by an attribute artifact removal step. The artifact removal step employs a local regression method that uses the spatial and attribute information of neighboring points to estimate and remove the artifact. The effectiveness of the proposed approach is evaluated on several benchmark datasets, and the results show that it outperforms state-of-the-art methods in terms of both objective metrics and visual quality. The proposed approach is computationally efficient and can be easily integrated into existing point cloud compression frameworks. The results of this research have significant implications for the development of more efficient and effective compression techniques for point cloud data.",This paper introduces a new way to remove artifacts from geometry based point clouds through compression. The new approach uses a two step process; first downsampling the point cloud using a quad tree method and then removing artifacts. Artifact removal relies on local regression which uses spatial and attribute information of neighboring points to estimate and eliminate those artifacts. Results on test datasets are evaluated and show better performance compared to current best methods in both objective scores and visual quality. The approach is fast and easy to integrate with existing compression frameworks. Results also indicate important advancements for developing better compression techniques for point cloud data.
"This paper presents tight bounds for randomized load balancing on arbitrary network topologies. Load balancing is a fundamental problem in distributed computing, and randomized algorithms are widely used to solve it. However, previous work has only provided loose bounds for the performance of these algorithms on arbitrary network topologies. To address this gap, we propose a new analysis framework that enables us to derive tight bounds for the expected maximum load in a randomized load balancing algorithm. Our framework is based on a novel application of the classic balls-and-bins problem, which allows us to bound the probability of any bin receiving too many balls. We then use this result to derive tight bounds for the expected maximum load, which hold for any network topology. We also present experimental results that demonstrate the effectiveness of our analysis framework. Specifically, we show that our framework can accurately predict the performance of randomized load balancing algorithms on a variety of network topologies, including real-world internet topologies. Our results provide important insights into the performance of randomized load balancing on arbitrary network topologies, and can inform the design and analysis of distributed systems that rely on load balancing.","This paper studies tight bounds for randomized load balancing on any network layout. Load balancing is an important problem in distributed computing and random algorithms have been widely used to solve it. Previous research has given loose bounds for such algorithms for general network layouts. We introduce a new analysis framework that lets us derive tight bounds for expected maximum load for randomized load balancing. This new framework uses a novel approach drawing from classic balls and bins problems to bound probabilities that any container gets too many items. Using this result we derive tight bounds for expected maximum load which apply to all kinds of network layouts. We also report experimental results showing how well our analysis framework works. Specifically, we show the framework can accurately forecast performance of load balancing algorithms on different network layouts, including realistic internet layouts. Results give significant insight into performance of load balancing on any layout and can be used to design and analyze distributed systems relying on load balancing."
"This paper investigates medium chemical reaction in diffusion-based molecular communication in a two-way relaying scenario. Diffusion-based molecular communication is a promising paradigm for communication in nanoscale devices and systems, where the use of electromagnetic waves is not feasible. In this study, a two-way relaying scenario is considered, where two nanomachines exchange information through the diffusion of molecules in a medium. The medium is modeled as a compartmentalized system, where each compartment represents a different environment with different reaction rates. The paper proposes a mathematical framework to model the reaction rates of the medium and the molecular signals, and derives the analytical expressions for the bit error rate (BER) and the capacity of the communication system. The proposed framework is used to analyze the effect of medium reactions on the performance of the communication system. The simulation results demonstrate that the medium reaction significantly affects the BER and the capacity of the system, and the proposed framework can be used to optimize the communication system by adjusting the reaction rates of the medium. The findings of this study can be useful for the design and optimization of diffusion-based molecular communication systems, especially in scenarios where medium chemical reactions are significant.",This paper studies chemical reactions at diffusion level using molecular communication in a scenario involving bidirectional relaying. Using this method of communication is attractive for nanoscale devices and systems because electromagnetic waves cannot be used. Here we consider a scenario with two nanodevices exchanging information via diffusion of molecules in a medium. Medium compartments represent distinct environments which have different reaction rates. A mathematical framework is developed to model reaction rates of medium and molecular signals and derive analytical expressions for bit error rate (BER) and capacity of the communication system. We apply this framework to assess how medium reaction influences performance of the system. Simulation results show that reaction of medium has a strong effect on BER and capacity and that the proposed framework can be used to improve system performance by tuning reaction rates. Results from this work may prove useful for designing and optimizing systems based on diffusion communication especially in scenarios where chemical reactions in medium play an important role.
"In this paper, we propose a novel approach to correlated signal inference using free energy exploration. The method involves the estimation of the free energy landscape of a set of correlated signals and the exploration of this landscape to identify the most likely configuration of the signals. The approach is based on the assumption that the free energy is a good proxy for the entropy of the system and that the exploration of the free energy landscape provides a way to infer the underlying structure of the correlated signals. To validate the proposed approach, we apply it to a variety of simulated and real-world data sets and compare its performance to existing methods. The results demonstrate that the free energy exploration approach is effective in inferring the structure of correlated signals and outperforms existing methods in a number of cases. The approach has the potential to be applied to a wide range of problems in signal processing and machine learning, including the analysis of complex systems, the inference of hidden variables, and the discovery of patterns in high-dimensional data.","This paper introduces a new method for inferring signals which are correlated. This method uses free energy exploration to estimate the landscape of free energy for a group of correlated signals and then explores that landscape to identify the most probable configuration of those signals. The hypothesis here is that free energy serves as an excellent proxy for entropy and that exploring the free energy landscape can infer the structure underlying the correlated signals. We test this method by applying it to both simulated and real data and comparing its performance against other methods. Results show that free energy exploration proves effective at inferring structure and performs better than competing methods in many cases. This method could be useful for solving diverse problems such as analyzing complicated systems, inferring hidden variables, and discovering regularities in high dimensional data."
"As software systems become more complex, ensuring their deployability and reliability has become a critical concern. Record and replay (R&R) techniques have emerged as a promising approach for improving software system deployability by capturing and reproducing execution traces of the system in a controlled environment. However, existing R&R techniques have limitations in terms of scalability, precision, and overhead. In this extended technical report, we present an engineering approach for R&R that addresses these limitations. We first identify the key requirements for engineering R&R systems, including support for multi-tier applications, efficient data collection and storage, and low overhead during recording and replaying. We then present our R&R system architecture, which leverages virtualization and distributed tracing to achieve high scalability and precision. Our system also employs novel techniques to reduce overhead during recording and replaying, including adaptive sampling and selective replay. We evaluate the effectiveness of our R&R system through a series of experiments on a set of real-world applications. Our results demonstrate that our system can capture and reproduce complex execution traces with high accuracy and low overhead, while also providing insights into the behavior of the system under different conditions. We also show that our system can be used to identify and diagnose performance and reliability issues in the system. In conclusion, our engineering approach to R&R provides a practical solution for improving software system deployability and reliability, especially for large-scale, complex systems. Our system can be used by software engineers and developers to diagnose and debug issues in the system, and to ensure that the system behaves as expected in different environments. Future work will focus on integrating our R&R system with existing software development and testing workflows, and exploring new applications for R&R in areas such as security and compliance.","As software becomes increasingly complex, deployability and reliability are now very important. Replay and record techniques have been introduced as promising approaches to improve deployability by capturing and reproducing system execution traces. However, existing techniques face problems in terms of scalability and overhead. This report introduces an engineering approach to these techniques which solves these problems. Key features include support for multi tier apps and efficient data collection and storage. We also use virtualization and distributed tracing for scalability and precision. We reduce overhead by using techniques like adaptive sampling and selective replay. We test effectiveness with real world applications; results show high accuracy and low overhead along with insight into system behavior under different conditions. We find that the system can pinpoint and diagnose performance and reliability issues. In conclusion, this engineering approach improves deployability and reliability for large scale complex systems. Software engineers and developers can use this system for debugging and to ensure consistent behavior across environments. Future work aims to integrate this system into workflow processes and investigate new uses like security and compliance."
"This paper examines the effectiveness of two common heuristics for solving vehicle routing problems (VRPs): sequence optimization and set optimization. Sequence optimization involves optimizing the order in which customers are visited by each vehicle, while set optimization involves partitioning the set of customers into clusters that are visited by different vehicles. To compare the two approaches, we conducted a series of computational experiments on a set of benchmark instances of VRPs. We tested both heuristics using a variety of problem sizes and configurations, and evaluated their performance based on solution quality, computational time, and scalability. Our results show that the choice of heuristic depends on the specific characteristics of the problem. For small to medium-sized instances with relatively homogeneous customer demands, sequence optimization tends to be more effective, while for larger and more complex instances with heterogeneous demands and multiple depots, set optimization outperforms sequence optimization. However, we also found that a hybrid approach combining both heuristics can be effective for certain types of VRPs. Overall, this study provides insights into the strengths and weaknesses of different heuristics for VRPs, and offers guidance for practitioners and researchers in selecting the most appropriate approach for a given problem.","This paper investigates effectiveness of two common heuristics used to solve Vehicle Routing Problems (VRP): sequencing and clustering. Sequencing focuses on optimizing order of visiting customers by each vehicle whereas clustering groups customers into sets that are visited by different vehicles. We conduct a series of computational tests on benchmark VRP instances. We test both heuristics using different problem sizes and configurations and measure performance based on solution quality, computation time and scalability. Results show that heuristic choice depends on specific problem characteristics. For smaller and medium sized instances with similar customer demand, sequencing works better but clustering performs better for larger and more complex instances with diverse customer demand and multiple depots. We also find that hybrid approach combining both heuristics performs well for some VRP types. Overall, this work gives insight into comparative strengths and weaknesses of different heuristics and suggests best practices for practitioners and researchers regarding selection of proper approach for specific problem."
"This paper presents a novel approach to the task of interpretable visual question answering (VQA) by leveraging dependency trees to encode visual and textual information. The proposed method uses a graph-based representation of the dependencies between words in the question and image features to perform reasoning. The experiments demonstrate that the dependency trees effectively capture the relationships between the visual and textual inputs, leading to improved performance on VQA benchmarks compared to state-of-the-art methods. Furthermore, the interpretability of the approach is demonstrated by visualizing the reasoning process in the form of the dependency trees. The results of this study provide a new direction for developing interpretable VQA systems that can explain their predictions to human users.","This paper introduces a novel method for VQA that utilizes dependency trees to integrate visual and textual information. Using a graph representation of dependencies among words in questions and features extracted from images, the proposed method reasons through the information. Experiments show that dependency trees successfully capture relations between visual and text inputs resulting in better performance on benchmark tests compared to leading methods. Interpreting results is also shown by visualization of reasoning using these dependency trees; results of this research point toward promising new directions for building VQA systems that can explain their predictions to humans."
"This paper proposes a low-complexity full-diversity detection scheme for multi-user multiple-input multiple-output (MIMO) multiple-access channels. The proposed scheme is designed to achieve full-diversity gain without sacrificing the low-complexity advantage of conventional detection schemes. Specifically, the scheme exploits the inherent structure of the channel matrix and performs iterative detection to achieve full-diversity gain with low complexity. The performance of the proposed scheme is evaluated through simulations and compared with conventional detection schemes in terms of bit error rate (BER) and computational complexity. The simulation results show that the proposed scheme achieves full-diversity gain and outperforms conventional detection schemes in terms of BER, while maintaining low computational complexity. The proposed scheme can be a promising solution for practical MIMO communication systems that require high data rate and low computational complexity.",This paper introduces a new detection method with low computational cost that works on channels where multiple users send information using multiple transmitters and receivers. This method uses the natural structure of the channel and employs iterative detection to obtain full diversity gain but avoids the high computational costs of usual detection methods. Results from simulations show that this new method obtains full diversity and does better in terms of bit error rate than other detection methods while also being less complex computationally. This new method has potential as a good solution for systems that need to communicate at high data rates and have low computational capacity.
"This paper investigates the phenomenon of asymmetric evaluations of erasure and undetected error probabilities in digital communication systems. Specifically, the study focuses on the ways in which users tend to perceive and evaluate these two types of errors differently, and the factors that contribute to such differences. The paper begins by introducing the concept of erasure and undetected errors, and explaining the significance of these errors in digital communication systems. It then reviews existing literature on user perceptions of these two types of errors, and highlights the lack of consensus in the field. The research methodology involves conducting a series of experiments that assess participants' evaluations of erasure and undetected error probabilities in a simulated digital communication environment. The study collects both quantitative and qualitative data, including measures of perceived severity, subjective probability estimates, and verbal explanations of participants' evaluations. The results of the experiments indicate that users tend to evaluate erasure and undetected errors differently, with undetected errors generally being considered more severe and having a greater impact on user trust. The study also identifies several factors that contribute to these differences, including the role of system design, user experience, and individual differences in risk perception. Overall, this paper contributes to our understanding of how users evaluate errors in digital communication systems, and provides insights for designers and engineers to create more effective and trustworthy systems.","This paper looks at how people perceive and rate differently erasure and undetected errors in digital communication systems. Specifically it explores reasons why users have different views towards these two kinds of mistakes and the reasons behind them. At first the paper introduces what erasure and undetected errors are and explains their significance. Then it surveys previous research into user perception of these two types of errors and points out that there is little agreement among researchers. Research methodology includes conducting a series of experiments that measure participant evaluation of probabilities of these two types of errors within a simulated communication environment. Results include quantitative and qualitative data such as severity perception and subjective probability ratings and participants' explanations of their evaluations. Results show that generally users view undetected errors as more severe and impactful on trust. Factors leading to this difference are also identified including role of system design, user experience and individual differences in risk perception. In summary this paper advances understanding of user evaluation of errors in digital communication systems and offers suggestions for designers and engineers to improve design quality and trustworthiness."
"The abandoned side of the internet refers to domain names that have expired and are no longer in use. This paper investigates the phenomenon of hijacking these expired domain names and the associated internet resources. The research focuses on the motivations and methods used by hijackers to seize control of these abandoned assets, as well as the potential consequences for the previous owners and the internet as a whole. The results of the study suggest that the hijacking of expired domain names is a growing problem, with a significant impact on the stability and security of the internet. The paper concludes by offering recommendations for policy-makers and domain registrars to address this issue and protect the rights of domain name owners.",The abandoned part of the web consists of domain names that have expired and stopped being used. This study looks at how people take control of those unused domain names and related internet resources. It explores reasons and methods that hijackers use to acquire control of those abandoned assets and also considers what impact this has on former owners and the internet. Results show that hijacking expired domains is becoming more common and has major effects on internet stability and security. Concluding remarks offer suggestions to policymakers and registrars to deal with this issue and safeguard rights of domain owners.
"This paper proposes a novel approach for anomaly detection using multi-criteria similarity-based techniques and Pareto depth analysis. The proposed method addresses the limitations of existing anomaly detection techniques by considering multiple similarity criteria and analyzing data points' Pareto depth scores. Pareto depth is a measure of how far a data point is from the set of non-dominated points, representing the best trade-offs between different criteria. By using Pareto depth analysis, the proposed method identifies anomalies that deviate significantly from the best trade-offs and provides a better understanding of the trade-offs among different criteria. The effectiveness of the proposed method is evaluated using several real-world datasets, and the results demonstrate its superiority over existing state-of-the-art anomaly detection techniques. The proposed approach can be used in various domains, such as finance, healthcare, and cybersecurity, where detecting anomalies is crucial for ensuring system integrity and security. The research provides a valuable contribution to the field of anomaly detection, and the proposed method has the potential to be integrated into various real-world applications.","This paper introduces a new approach for anomaly detection that uses similarity based on multiple criteria and Pareto depth analysis. We address shortcomings of current methods by using Pareto depth scores to evaluate data points. Pareto depth measures how far away an observation is from other observations which are not dominated by any others; this represents good tradeoffs among different criteria. Using Pareto depth we find anomalies that diverge markedly from these tradeoffs and thereby understand better tradeoffs among different criteria. Performance evaluation using diverse real datasets demonstrates this approach outperforms leading recent anomaly detection methods. This new approach is useful across many areas like finance, health care, and cybersecurity where identifying anomalies is important to ensure system integrity and safety. Results show this research makes a significant contribution to anomaly detection and suggest integration into practical applications."
This paper proposes a deep reinforcement learning approach to the joint optimization of spectrum and energy efficiency in C-V2X communication networks while considering the security aspect. The study takes a novel approach by utilizing a deep reinforcement learning algorithm to solve the complex optimization problem and provides an optimal solution that balances the trade-off between spectrum and energy efficiency while enhancing the security of the network. The results of the simulation experiments demonstrate the effectiveness and superiority of the proposed approach compared to the traditional optimization methods. The findings of this study provide valuable insights into the joint optimization of spectrum and energy efficiency with security considerations in C-V2X communication networks.,This paper introduces a deep reinforcement learning method for joint optimization of spectrum and energy efficiency in C-V2X communication networks and also considers security aspects. This new work uses a deep learning algorithm to tackle this complex optimization problem and yields an optimal solution which strikes a balance between spectrum and energy efficiency as well as improving network security. Results from simulation experiments show this new approach is effective and superior compared to traditional optimization approaches. Findings of this research offer important guidance on joint optimization of spectrum and energy efficiency along with security in C-V2X networks.
"This paper presents a multi-class classification model for vulnerabilities in smart contracts using an Attention-based Bi-directional Long Short-Term Memory (AWD-LSTM) network with a pre-trained encoder inspired by natural language processing. The goal of this study is to address the challenge of accurately identifying different types of vulnerabilities in smart contract code, which is crucial for ensuring the security and reliability of decentralized applications. The model uses a pre-trained encoder from natural language processing to extract features from smart contract code and an AWD-LSTM network for classification. The results of the experiments show that the proposed approach outperforms existing methods and provides a promising solution for multi-class classification of vulnerabilities in smart contracts.",This paper introduces a model for classifying different kinds of vulnerabilities in smart contract code using Attention based Bidirectional LSTM networks with encoders inspired by natural language processing. The aim of this research is to overcome the challenge of distinguishing among various types of vulnerabilities so that decentralized applications can be secure and reliable. The model uses an encoder pretrained on NLP for feature extraction from code and uses Attention LSTM networks for classification. Results of tests indicate that this new approach performs better than previous methods and offers a promising way to classify vulnerabilities among many classes.
"Adaptive motion planning is a critical aspect of autonomous systems, particularly in robotics. This paper presents a new approach to motion planning using artificial potential fields and a prior path. The method utilizes a combination of a potential field-based algorithm and a prior path to overcome the limitations of traditional potential field methods, such as local minima and slow convergence. The proposed approach is evaluated using simulations and experiments with a real-world robotic system, demonstrating improved performance compared to traditional potential field methods. The results suggest that the use of a prior path can enhance the performance of potential field-based motion planning algorithms, making them more suitable for real-world applications.",Adaptive motion planning is crucial for autonomous systems especially robots. We introduce a new way to plan movement by mixing artificial potential fields with a prior path. We use an algorithm combining these features to avoid problems like local minima and slow convergence that are typical of standard potential field methods. Results from both simulation and real robotic systems show higher performance when compared to conventional potential field approaches; results indicate that using prior paths improves performance of planning algorithms based on potential fields for practical use.
"The paper provides a review of Ultra-Reliable Low Latency Communications (URLLC) for Massive Machine Type Communications (MMTC) with the aim of enabling critical MMTC applications. The paper first provides an overview of MMTC and its requirements, including low latency, high reliability, and massive scalability. The paper then focuses on URLLC as a solution to meet the stringent requirements of critical MMTC applications, such as autonomous vehicles and industrial automation. The paper presents the key concepts and design principles of URLLC, including reliability and latency enhancement techniques, channel coding and modulation schemes, and network architecture. The paper also discusses the challenges and limitations of implementing URLLC in MMTC systems and provides insights into the future direction of research and development in this area. The paper concludes by summarizing the main findings and providing recommendations for further research.","This paper reviews Ultra Reliable Low Latency Communications (URLLC) for Massive Machine Type Communication (MMTC) with the goal of supporting critical MMTC applications. First, it introduces MMTC and its demands which include low latency and high reliability along with scalability at very high levels. Then the paper concentrates on URLLC as a way to fulfill strict requirements for important applications such as autonomous driving and industrial automation. Key ideas and design principles are discussed here including ways to enhance reliability and reduce latency, coding and modulation schemes, and network architecture. Challenges and limits to deploying URLLC in MMTC networks are also addressed and some thoughts about future research directions are presented. In conclusion the paper summarizes results and suggests further research avenues."
"This paper presents a new approach to cooperative verification by using collective invariant generation. The method is based on the idea that multiple agents can work together to generate invariants, or mathematical representations of the desired properties of a system, which can then be used to verify that the system behaves as expected. The authors show that this approach is more effective than traditional verification methods, as it allows for a more thorough exploration of the system's behavior and can find errors that would be missed by other methods. The results of the study demonstrate the effectiveness of the approach in a variety of real-world cases and highlight its potential for widespread adoption in the field of verification.",This paper introduces an innovative way to do cooperative verification using collective invariant generation. The idea is that different agents work together to generate invariants; mathematical expressions that represent system desired properties and can later serve as a basis for verification that the system works as intended. Results indicate this new method is better than standard verification approaches because it thoroughly explores system behavior and can catch errors others might miss. Results show effectiveness in diverse real world examples and point toward broad use in verification.
"This paper presents a novel stereo camera visual simultaneous localization and mapping (SLAM) approach for outdoor construction sites containing large dynamic objects. The proposed method employs hierarchical masking to exclude the dynamic objects from the SLAM estimation process and improve accuracy. The masking process is performed in multiple stages, starting with a coarse mask and refining it through subsequent stages to obtain a more precise mask. Additionally, the paper introduces a motion-state classification module to distinguish between static and dynamic objects and to improve the robustness of the masking process. The effectiveness of the proposed method is demonstrated through extensive experimentation in real-world outdoor construction scenarios. The results show that the proposed method outperforms the state-of-the-art visual SLAM approaches in terms of accuracy and robustness in the presence of large dynamic objects.","This study introduces a new SLAM approach using stereo cameras for outdoor construction sites that contain big moving objects. A key feature of this method is hierarchical masking which eliminates those moving objects to improve accuracy. This masking process is carried out in several steps; we start with a coarse mask and refine it step by step to get finer masks. Additionally, the paper also proposes a module for distinguishing between stationary and mobile objects and enhances robustness of masking process. Effectiveness of the proposed method is validated with experiments on real outdoor construction sites. Results show the new method performs better than leading visual SLAM approaches in terms of both accuracy and robustness when there are many moving things around."
"This paper investigates the concept of ""different strokes for different folks"" in the context of pre-training approaches for diverse dialogue tasks. The authors examine various pre-training methods and evaluate their effectiveness on different types of dialogue tasks. The results show that there is no one-size-fits-all solution, and that different pre-training approaches are better suited for different types of dialogue tasks. The authors conclude that it is important to carefully consider the specific characteristics of each task when choosing a pre-training approach, in order to achieve optimal results.",This study looks at how different training methods work for different kinds of dialog tasks. They investigate various ways to pretrain and assess their effectiveness on diverse tasks. Results indicate that there isn't one best method; different methods perform better for different kinds of tasks. Authors conclude that careful consideration should be given to task specifics when picking a method for pretraining so that best performance can be achieved.
This paper presents a newton-type method for the optimal control of switched systems that leverages the structure of the systems to improve computation efficiency. The proposed method is compared with existing methods and shown to produce solutions with higher accuracy and faster convergence. The results demonstrate the effectiveness of the structure-exploiting newton-type method in addressing the challenges associated with the optimal control of switched systems and offer promising directions for future research in this area.,This paper introduces a Newton method tailored for switching system optimal control which exploits structural features of these systems for computational efficiency. Results are compared against previous work and show better solution accuracy and faster convergence. Results indicate this method exploiting structural features effectively addresses challenges in switching systems control and suggest promising directions for further research.
"In this paper, we explore the relationships between Artin automorphisms, cyclotomic function fields, and folded list-decodable codes. Artin automorphisms, also known as Artin maps, are certain algebraic maps that play a crucial role in number theory and algebraic geometry. On the other hand, cyclotomic function fields are algebraic extensions of the field of rational numbers, obtained by adding the roots of unity to the field. Finally, folded list-decodable codes are a class of error-correcting codes with specific properties, widely used in the field of communication engineering. We demonstrate that Artin automorphisms can be applied to cyclotomic function fields to obtain folded list-decodable codes with improved performance compared to previously known methods. Our results suggest that a deeper understanding of Artin automorphisms and their applications can lead to new advances in the design of efficient error-correcting codes. Our findings have potential applications in various fields, such as cryptography, communication engineering, and computer science. The paper is accompanied by numerical simulations that validate our results and highlight the potential of our approach. Our study opens up new avenues for further research and provides a valuable contribution to the fields of number theory, algebraic geometry, and coding theory.","This paper studies the relationship between Artin automorphisms, cyclotomic function fields and folded list decodable codes. Artin automorphisms are significant maps in number theory and algebraic geometry; they are closely related to Artin maps. Cyclotomic function fields contain rational numbers and extra roots of unity. Folded list decodable codes have special properties and are commonly used in communication engineering. We show that applying Artin automorphisms to cyclotomic fields produces high performing codes that outperform previous approaches. Results indicate that deeper understanding of automorphisms may result in new advances in code design. Results have implications in cryptography, communications engineering and computer science. Numerical simulations confirm results and emphasize practical potential of our approach. We open new lines of research and make an important contribution to number theory, algebraic geometry and coding theory."
"This paper explores the need for traceability in neuroimaging analyses and proposes a solution for ensuring the robustness and reproducibility of results. The increasing complexity and heterogeneity of neuroimaging data pose significant challenges for researchers, including the potential for errors, inconsistencies, and incomplete documentation of data processing and analysis steps. The paper argues that a traceable and transparent workflow is crucial for maintaining the integrity and reliability of neuroimaging results, and outlines a comprehensive approach to achieve this goal. The proposed solution involves the integration of existing tools and best practices from the fields of software engineering and data management, with a focus on automation, version control, and metadata tracking. The paper provides a detailed evaluation of the proposed solution and its potential benefits, including increased efficiency, reduced risk of errors, and improved collaboration and sharing of results. The findings of this research have important implications for the neuroimaging community and contribute to the broader efforts to improve the reliability and transparency of scientific research.","This paper looks at why traceability matters for neuroimaging research and suggests a method to ensure strong results and consistent results. Neuroimaging data is getting more complicated and diverse, which can cause researchers trouble such as mistakes, inconsistencies, and poor record keeping of procedures. Traceability and transparency are key for preserving quality and reliability of results and the paper presents a comprehensive approach to achieve this. The suggested solution combines best practices and tools from software engineering and data management; emphasis is placed on automation, version control and tracking of metadata. Detailed evaluation of the solution is presented along with advantages like higher efficiency, lower error risk and enhanced collaboration and sharing of results. Results of this work have important implications for the community doing neuroimaging and they advance efforts towards improving reliability and transparency overall."
"This paper presents a study on resolving multi-party privacy conflicts in social media. Social media platforms, with their vast user base and rich personal data, have become a prime target for privacy violations. As such, there is a growing need for effective mechanisms to resolve multi-party privacy conflicts in these platforms. The paper proposes a privacy conflict resolution framework that considers the privacy preferences of multiple parties involved in a conflict. The framework combines game theory, computational social choice, and privacy-enhancing technologies to reach a mutually acceptable solution. The paper also provides an experimental evaluation of the proposed framework and demonstrates its efficacy in resolving multi-party privacy conflicts in a realistic social media setting. The findings of the study contribute to the development of privacy-preserving social media platforms and offer practical insights for practitioners and policymakers in the field.","This paper studies how to resolve privacy conflicts among many users on social media. Social media sites have grown into targets for privacy breaches due to their large user base and rich personal information. Because of this, there is growing demand for effective methods to handle privacy conflicts. The authors propose a framework for handling privacy conflicts that considers privacy preferences of multiple conflicting parties. This framework uses tools from game theory, computational social choice and privacy enhancing technology to arrive at a solution acceptable to all. Experimental evaluation shows effectiveness of the proposed framework in dealing with privacy conflicts in realistic social media. Results of this research advance privacy preservation on social media and offer practical advice for those who design such systems and policy makers."
"The study of learning and inferring relations in cortical networks is a crucial aspect of understanding the workings of the brain. In this paper, we present a new computational framework that models the process of learning and inference of relations in cortical networks. Our approach is based on the theory of deep learning, which uses artificial neural networks to analyze large amounts of data. The model is trained on a dataset of sensory inputs and corresponding target outputs to learn the relationship between these inputs and outputs. Our results demonstrate that the proposed framework is capable of accurately inferring the underlying relationships in cortical networks, providing a new avenue for investigating the functioning of the brain. The findings of this study have important implications for the development of new artificial intelligence systems and the advancement of our understanding of brain function.",Studying how the brain learns and infers relations from cortical networks is critical for understanding brain functioning. We introduce a new computational framework that models learning and inference of relations in these networks. Our approach relies on deep learning theory which utilizes artificial neural networks to process large datasets. The framework is trained using a dataset containing sensory inputs paired with targets and learns the relationship between them. Results show that this proposed framework accurately infers underlying relationships within cortical networks opening up new avenues for investigation into brain function. Implications are significant for development of new AI systems and deeper understanding of brain functioning.
"This paper examines the use of neologisms (newly created words or phrases) on Facebook. The study analyzed a large corpus of data collected from Facebook posts and comments to identify the most common neologisms and their usage patterns. The results showed that neologisms on Facebook are primarily used to express emotions, create humor, and emphasize a sense of belonging among users. The study also found that neologisms are more commonly used by younger users and that they spread quickly through the platform due to its high level of connectivity. This research sheds light on the role of social media in shaping language and highlights the importance of monitoring the development of neologisms in this context.","This research investigates use of new terms and expressions on Facebook. A large body of data was collected from posts and comments to find out which new terms were used most and to understand patterns of their use. Results show that people use new words mainly for expressing feelings, making jokes and emphasizing feeling part of the group. It also turns out that new words are used more by younger people and spread fast because of the high connectivity. This work reveals how social media shape language; it points out that keeping an eye on new words is important."
"This paper presents a parallel algorithm for counting triangles in networks with large degrees. The proposed algorithm leverages the parallel processing capabilities of modern computing systems to efficiently handle the high degree nodes in the network, which are a major bottleneck in traditional triangle counting algorithms. The algorithm is designed to be scalable, allowing for the processing of large-scale networks in a reasonable amount of time. The experimental results show that the proposed algorithm significantly outperforms existing triangle counting methods in terms of speed and accuracy. The results demonstrate the effectiveness of the parallel approach for tackling the problem of triangle counting in networks with large degrees and provide new insights into the design of efficient algorithms for graph analysis.","This paper introduces an algorithm designed for parallel processing that counts triangles in networks with very high degree nodes. Leveraging modern computers, this method handles these high degree nodes efficiently compared to previous methods; traditionally, such nodes are a big obstacle. The new method scales well and can process large networks quickly. Results indicate that performance metrics for this parallel approach are much better than those of competing methods. Performance results validate this parallel way of solving triangle count problem on networks with high degree nodes and suggest new ways to design effective algorithms for graph analysis."
"This paper presents a novel approach to understanding the causal structure of a system through a categorical semantic framework. The framework utilizes category theory, a branch of mathematics, to define the relationships between variables in a system and the effects of changing one variable on another. The central contribution of the paper is the introduction of a categorical causal model that provides a way to reason about the causal relationships between variables in a compact and intuitive way. The paper also presents several examples to demonstrate the usefulness of the categorical causal model in different domains, including physics, biology, and social sciences. The results show that the categorical semantic framework provides a powerful tool for formalizing and reasoning about causal relationships in a clear and consistent manner.","This paper introduces a new method to understand causal structures using a semantic framework based on category theory. Category theory is used as a mathematical branch to define relationships among variables within systems and to study effects resulting from changes to one variable versus another. The main contribution of this paper is a categorical model of causality which allows us to reason about relationships among variables concisely and intuitively. The authors present various examples to show how useful this categorical causal model can be in diverse fields such as physics, biology and social sciences. Results indicate that this semantic framework with category theory offers a strong tool for formalization and clear consistency reasoning about causal relationships."
"This paper explores the process of scaling out acid applications with operation partitioning. Acid (Atomicity, Consistency, Isolation, Durability) is a crucial characteristic for database transactions, ensuring that operations are performed in a secure and reliable manner. However, as databases grow in size and complexity, it becomes increasingly difficult to maintain Acid properties. This paper proposes the use of operation partitioning as a means of scaling out acid applications, allowing for improved performance and increased reliability. The methodology of this research involves the implementation of operation partitioning in a real-world acid application, evaluating its impact on various performance metrics. The results demonstrate that operation partitioning is a promising approach for scaling out acid applications, offering improved performance and increased reliability without sacrificing the essential Acid properties.","This paper studies ways to scale out applications that depend on Acid properties by partitioning operations. Acid refers to important characteristics like Atomicity, Consistency, Isolation and Durability which are critical for database transactions to ensure security and reliability. But as databases get larger and more complex, it becomes harder to preserve Acid properties. In this paper we suggest partitioning operations as a way to scale such applications and achieve higher performance and reliability. Methodology includes implementing this partitioning in an actual Acid application and assessing different performance metrics. Results show that operation partitioning is promising and provides higher performance and reliability while still maintaining fundamental Acid properties."
"This paper presents a novel approach for efficiently handling congestion in RDMA-based datacenter networks. The approach, called Dart, divides the network into multiple subnets and specializes each subnet to handle specific types of traffic. By doing so, Dart can quickly respond to congestion and reduce the latency of critical applications. The evaluation of Dart shows that it outperforms state-of-the-art congestion control methods in terms of both throughput and fairness, while maintaining low latency for real-time applications. The results of this research demonstrate that Dart is a promising solution for addressing congestion in RDMA-based datacenter networks.",This paper introduces a new approach for efficient handling of congestion in networks based on RDMA. This approach named Dart splits the network into different subnets and specializes them to handle different kinds of traffic. Dart responds quickly to congestion and reduces latency for critical applications. Results show Dart excels compared to leading congestion control techniques in both throughput and fairness; at the same time low latency is maintained for real time applications. Results also show Dart is promising as a solution for dealing with congestion in RDMA networks.
"This paper presents a novel approach for image classification and semantic segmentation using structured binary neural networks. The proposed method uses binary weights and activations, reducing the memory and computational requirements while maintaining high accuracy. The paper demonstrates that this structure enhances the accuracy of image classification compared to traditional neural networks with floating-point weights and activations. Additionally, the method is extended to semantic segmentation tasks and showed significant improvement over existing methods. The results of extensive experiments on benchmark datasets confirm the efficacy of the proposed method. The structured binary neural network approach provides a promising solution for accurate image classification and semantic segmentation with efficient computational and memory requirements.","This paper introduces a new method for image classification and segmentation using structured binary neural networks. Using binary weights and activations, this approach reduces memory and computational load while retaining high performance. Results show this structure outperforms conventional networks that use floating point weights and activations. The authors also extend this work to segmentation tasks and find marked improvements over previous work. Extensive experiments on standard data sets confirm the effectiveness of this proposed method. Use of structured binary networks thus promises to improve classification and segmentation performance with low cost in terms of computation and memory."
"In this paper, we propose a novel method for combinatorial Bayesian optimization that leverages the graph cartesian product to address the challenge of optimizing high-dimensional, discrete, and non-separable functions. We demonstrate that our method can efficiently navigate the search space and achieve superior performance compared to existing algorithms on a range of benchmark problems. Additionally, our method is shown to be scalable to large datasets, making it suitable for real-world applications in a variety of domains. The results of this research contribute to the growing field of Bayesian optimization and have potential implications for a wide range of optimization problems in machine learning and other areas.","This paper introduces a new approach to combinatorial Bayesian optimization that uses the graph Cartesian product to handle high dimensional, discrete and non separable functions. Results show that this approach excels at exploring the search space and outperforms other methods on diverse benchmarks. Scalability studies show that this method scales well to large data sets and therefore is practical for use across many different fields. This research adds to an expanding body of work on Bayesian optimization and has broad implications for solving various optimization tasks including those in machine learning."
"This paper focuses on the problem of energy plan selection in energy markets with retail choice. The authors introduce two competitive online algorithms for energy consumers to make decisions about whether to stay with their current energy plan or switch to a different one. The first algorithm, called ""Stay-or-Switch"", takes into account the historical energy consumption and costs of the consumer to determine the optimal strategy. The second algorithm, called ""Adaptive Stay-or-Switch"", adapts to changing market conditions and adjusts the energy plan selection accordingly. The performance of these algorithms is evaluated using simulations based on real-world energy market data. The results show that both algorithms outperform existing approaches and can lead to significant cost savings for energy consumers.","This paper considers choosing energy plans in markets that let customers shop around and selects two competing algorithms for consumers to decide whether to stick with their current plans or change them. First algorithm, ""Stay or Switch,"" looks at past energy use and costs to determine best strategy. Second algorithm called ""Adaptive Stay or Switch"" changes as conditions change and picks new energy plans accordingly. Performance of these algorithms is evaluated using simulations based on actual energy market data. Results indicate both perform better than previous methods and reduce costs for consumers significantly."
"Iterative optimization heuristics (IOHs) are a class of optimization algorithms that are widely used to solve complex problems in various domains. The performance and scalability of these algorithms are critical to their success, and benchmarking and profiling are essential tools for evaluating and improving IOHs. This paper presents iohprofiler, a new benchmarking and profiling tool for IOHs that is designed to meet the specific requirements of this class of algorithms. iohprofiler provides a comprehensive set of benchmark problems, performance metrics, and profiling tools to enable researchers and practitioners to evaluate and improve the performance and scalability of IOHs. The paper provides an overview of the design and implementation of iohprofiler, and presents experimental results that demonstrate its effectiveness in benchmarking and profiling a variety of IOHs. These results highlight the benefits of using iohprofiler for evaluating and improving the performance of IOHs and make it a valuable tool for researchers and practitioners in the field.","Iterative optimization heuristics (IOHs) are very useful methods that perform well on hard problems across different disciplines. Performance and scalability are key factors affecting whether IOHs succeed or fail. For assessing and improving IOHs, benchmarking and profiling are important. This paper introduces a new benchmarking and profiling tool called iohprofiler specifically designed for IOHs. It includes a range of benchmark tasks, performance metrics and profiling tools so users can evaluate and enhance performance and scalability of IOHs. We describe design and implementation details and present experiments showing effectiveness at benchmarking and profiling diverse IOHs. Results show advantages of using iohprofiler for evaluation and enhancement of IOHs and therefore the tool is valuable for research and practice."
"This paper examines the evolution patterns of groups in running races. Using data collected from various races, we analyze the changes in group size and composition over time, as well as the factors that contribute to the formation and dissolution of these groups. We find that groups tend to form naturally during the early stages of a race, and that their size and composition are influenced by factors such as pacing strategies, social dynamics, and environmental conditions. Furthermore, we observe that groups tend to break apart as the race progresses, with individual runners assuming their own pace and strategy. Our findings shed light on the complex dynamics of group behavior in endurance events, and have implications for the design of race courses and training programs.","This study looks at how groups evolve in running races. Using information gathered from different races we look into changes in size and composition of groups through time along with factors that cause formation and dissolution of these groups. We find that groups typically form at the beginning of a race and that size and composition are influenced by things like pacing strategies, social interaction and environmental conditions. Moreover we observe that as the race goes on, groups tend to dissolve as individual runners set their own pace and strategy. Results show the complex behavior of group dynamics in endurance competitions and they suggest important considerations for designing race courses and training programs."
"This paper proposes a novel approach for music artist classification using Convolutional Recurrent Neural Networks (CRNNs). The model is designed to extract both temporal and spectral features from raw audio signals, allowing it to capture both short-term and long-term patterns in the audio data. We evaluate our model on a large-scale music dataset and compare it to state-of-the-art methods. Our results show that the proposed CRNN-based approach achieves superior performance in terms of accuracy and efficiency, demonstrating the potential of deep learning techniques for music classification tasks.","This paper introduces an innovative way to classify musical artists using Convolutional Recurrent Neural Networks (CRNNs). The model extracts both temporal and spectral features directly from raw audio signal and captures both near term and long term features. We test this method using a large dataset and we compare against leading techniques. Results show that compared to others, our method based on CRNNs outperforms in terms of accuracy and effectiveness. This indicates potential for using deep learning in musical classification tasks."
"This paper explores the prevalence and functional roles of heterogeneous network motifs in complex systems. Network motifs are recurring patterns of interconnections between nodes in a network, and they have been extensively studied in the context of homogeneous motifs where all nodes in the motif have the same functional role. However, many real-world networks are heterogeneous, with nodes having different functional roles and properties. In this paper, we introduce a method for identifying heterogeneous motifs in networks and analyze their prevalence across a range of real-world systems, including biological, social, and technological networks. Our analysis reveals that heterogeneous motifs are significantly more prevalent than homogeneous motifs in many real-world systems, and they play important functional roles in information processing and regulation. We find that many heterogeneous motifs are involved in modular organization, signal propagation, and regulatory feedback loops. Moreover, we demonstrate that the same heterogeneous motifs can be found in multiple systems, suggesting that they represent fundamental building blocks of complex networks. Our findings have important implications for the design and understanding of complex systems. By considering the functional roles of heterogeneous motifs, we can gain insights into the mechanisms that underlie the behavior of real-world systems, and develop more effective strategies for controlling and manipulating these systems. Furthermore, our method for identifying heterogeneous motifs can be applied to a wide range of networks, enabling a deeper understanding of the complex interplay between structure and function in complex systems.","This paper studies how common and important specific arrangements of connections among elements (nodes) are in complex systems. Motifs are recurring configurations of links among nodes; they have received much study focusing on homogenous motifs where all nodes play similar roles. However, most real world networks are heterogeneous with nodes differing by function and other characteristics. Here we present a method for finding heterogeneous motifs and assess their prevalence across various real systems such as biology, society and technology. Analysis shows that these heterogeneous motifs are very common and play key roles in processing information and regulation. Many also contribute to modular organization and signal propagation along with regulatory feedback circuits. We also show that these motifs recur in diverse systems, suggesting they serve as basic building blocks for complex networks. Results have significant implications: by considering functional roles of motifs, we learn about mechanisms underlying system behavior and develop better ways to manage them. Moreover, the approach for detecting heterogeneous motifs works broadly and allows further exploration of relationships between structure and function in complex systems."
"This paper proposes a novel approach for bidirectional updates on fine-grained medical data using blockchain technology. Traditional methods of updating medical records are often time-consuming, error-prone, and lack transparency. The proposed system aims to address these issues by leveraging the security and immutability properties of blockchain to enable efficient and transparent bidirectional updates on medical data. The system architecture consists of a blockchain network, a smart contract, and a user interface. The smart contract defines the rules for updating medical data, including the conditions for granting access and the types of updates allowed. The user interface provides an intuitive and user-friendly way for patients and healthcare providers to interact with the system and update the medical records. To ensure privacy, the system implements a fine-grained access control mechanism that allows patients to grant different levels of access to different healthcare providers based on their roles and responsibilities. Additionally, the system employs data encryption and decryption techniques to protect sensitive medical data. We evaluated the proposed system by implementing a proof-of-concept prototype and conducting a series of experiments to measure its performance and security. The experimental results demonstrate that the system is highly efficient and can handle a large number of transactions per second. Moreover, the system is highly secure, and its immutability properties ensure that the medical data is tamper-proof. In conclusion, our proposed blockchain-based bidirectional updates system provides an innovative solution for updating medical records efficiently and transparently. The system's security and privacy features make it suitable for deployment in real-world healthcare settings, and its scalability and performance characteristics ensure that it can handle a large volume of transactions.","This paper introduces a new method for bidirectional updates of fine granularity in medical data using blockchain technology. Common ways of updating medical information are slow, prone to mistakes and lack transparency. Our proposed system addresses this problem by making use of blockchain security and immutability to achieve fast and transparent bidirectional updates on medical records. The architecture includes a blockchain network, smart contract and user interface. Smart contracts define rules for updating data including conditions for granting access and types of updates allowed. Interface design is easy and user friendly for interaction between patients and providers. Privacy protection is ensured through fine grained access control that allows different levels of access according to role and responsibility. Encryption and decryption techniques are also used to protect sensitive medical data. Performance and security were tested by building a prototype and conducting experiments. Results show high performance with high transaction rates and high security as well. Immutability ensures tamperproof medical data. Conclusion: This blockchain approach for bidirectional updates is an innovative solution to efficiently and transparently manage medical records. Features such as security and privacy make it fit for practical use and scalability performance characteristics ensure handling large volumes of transactions."
"This paper explores the use of machine learning algorithms to predict routing congestion in FPGA high-level synthesis. Routing congestion is a significant bottleneck in the design process of FPGAs and can cause delays and increased costs. Therefore, accurately predicting routing congestion is crucial to optimize design parameters and ensure the timely delivery of FPGA-based products. The proposed methodology combines machine learning models, including decision trees and random forests, with traditional congestion prediction metrics to predict routing congestion during the high-level synthesis process. The research also investigates the impact of using different feature extraction techniques and machine learning algorithms on the accuracy of congestion prediction. Experimental results show that the proposed approach achieves a high prediction accuracy, outperforming traditional congestion prediction methods. Additionally, the results show that feature extraction techniques, such as principal component analysis (PCA) and independent component analysis (ICA), can significantly improve prediction accuracy. This research provides an efficient and effective method for predicting routing congestion in FPGA high-level synthesis. The proposed approach can be used to optimize design parameters and reduce design iterations, ultimately reducing development costs and accelerating product delivery. The approach also has the potential to be applied to other FPGA design optimization problems, demonstrating the value of integrating machine learning in FPGA design.","This paper studies how machine learning algorithms can be used to forecast congestion related to routing during high level synthesis in Field Programmable Gate Arrays (FPGAs). Congestion is a major barrier in FPGA design and can result in delay and higher cost. Thus, accurate predictions of congestion are essential to optimize design parameters and meet deadlines for delivering FPGA based products. Proposed methodology uses decision trees and random forests along with traditional metrics to forecast congestion during synthesis process. The study also evaluates the performance of various feature extraction techniques and algorithms on accuracy of congestion forecasting. Experimental results show that this proposed approach performs very well and outperforms traditional congestion prediction methods. Results indicate that PCA and ICA among others perform much better at improving forecast accuracy. This research introduces a practical and effective way to predict congestion during high synthesis in FPGA design. Proposed approach can help optimize parameters and minimize iteration steps which reduces development cost and accelerates product delivery. This method can also be applicable to other optimization problems in FPGA design and shows the benefits of integrating machine learning into design."
"Farsighted collusion is a phenomenon in game theory where two or more players, in the pursuit of a common goal, make decisions that may not be in their immediate best interest but lead to a better outcome in the long run. This concept has been widely studied in various fields, including economics, psychology, and computer science. In this paper, we apply the concept of farsighted collusion to the stable marriage problem, a classical problem in the field of algorithms and discrete mathematics. The stable marriage problem involves finding a stable matching between two sets of elements, such that no two elements would both prefer each other over their assigned partners. We examine the conditions under which farsighted collusion can occur in stable marriage problems and demonstrate how it can lead to more stable outcomes. Our results provide new insights into the role of farsighted behavior in matching problems and have important implications for the design of algorithms and mechanisms for solving stable marriage problems in practical settings.","Farsighted collusion is a phenomenon in game theory where multiple players act together toward a shared goal and they choose actions which do not benefit them immediately, but result in a better long term outcome. This idea has been investigated widely in different disciplines like economics, psychology and computer science. This paper looks at this concept specifically in the context of stable marriage problems - a classic case from algorithmics and discrete math. Stable marriage problems deal with pairing elements from two sets so that no pair prefers each other over what they are matched against. We analyze when and under what conditions such collusion happens and show that it leads to improved stability. Results shed light on the significance of foresight in matching and are significant for designing algorithms and mechanisms for practical applications in this domain."
"This paper aims to examine the changing landscape of political discussions on Twitter between 2014 and 2019. The study utilizes Twitter data to analyze the trend of political discussions and identify the significant shifts that have occurred. The research findings show that Twitter has become an increasingly important platform for political discussions, with a growing number of users participating in these conversations. Additionally, the study reveals that the nature of political discussions on Twitter has changed, with users becoming more polarized and the tone of discussions becoming more confrontational. The paper concludes that the changes in political discussions on Twitter reflect broader shifts in the political landscape, with an increasing number of individuals using social media to voice their opinions and engage in political discussions. The research highlights the importance of monitoring the changing landscape of political discussions on social media and the need for continued research to understand the impact of these changes on political discourse and democracy.",This research studies political conversations on Twitter from 2014 to 2019; it uses Twitter data to track trends and discern significant changes. Results indicate that Twitter has become increasingly important for political discussions and participation by users is rising. Also observed are changes in character of these discussions with users becoming more polarized and debates taking a harsher tone. Conclusions state that Twitter discussion patterns reflect larger changes in politics as more people use social media for expression and engagement. Findings stress importance of monitoring changing dynamics of political discussions on social media and ongoing research into effect on political discourse and democracy.
"This paper presents a decomposition-based multi-objective evolutionary algorithm (MOEA) design under two algorithm frameworks. The first framework focuses on designing a MOEA based on decomposition techniques, which are used to transform a multi-objective optimization problem into a set of sub-problems. The second framework focuses on incorporating decomposition techniques into existing MOEAs to improve their performance. The proposed algorithms are tested on a set of benchmark multi-objective optimization problems, and their performance is compared to that of state-of-the-art MOEAs. The results show that the proposed algorithms have better performance in terms of convergence and spread, and are able to find a set of well-distributed solutions for multi-objective optimization problems. The findings of this study provide useful insights for the design of MOEAs, and demonstrate the effectiveness of decomposition-based MOEAs in solving multi-objective optimization problems.",This paper introduces a new design of a Multi Objective Evolutionary Algorithm (MOEA) using two different algorithm frameworks. One framework is about designing an MOEA based on decomposition methods; they use decomposition techniques to convert a multi objective optimization problem into a series of subproblems. The other framework considers embedding these decomposition techniques into existing MOEAs to enhance performance. Proposed algorithms are tested on a suite of benchmark problems and compared against top performing MOEAs. Results show that these new algorithms perform better in convergence and diversity; they are able to obtain diverse good quality solutions for optimization problems with multiple objectives. Results from this research offer valuable insight for developing MOEAs and show effectiveness of decomposition MOEAs at solving optimization problems with multiple objectives.
"This paper presents a novel heterogeneous graph embedding framework, called MTHetGNN, for multivariate time series forecasting. MTHetGNN combines the strengths of graph neural networks and attention mechanisms to model the complex relationships among multiple time series and their dependencies. The framework employs a heterogeneous graph to represent the different types of relationships between the time series and applies graph convolutional neural networks to learn the representations of the nodes in the graph. The attention mechanism is used to capture the importance of each node in the forecasting process. The proposed framework is evaluated on several multivariate time series forecasting tasks and demonstrates significant improvement over the existing state-of-the-art methods in terms of accuracy and computational efficiency. This study provides a new approach to address the challenge of multivariate time series forecasting and has potential applications in various fields, such as finance, energy, and transportation.","This paper introduces a new heterogeneous graph embedding framework named MTHetGNN which is designed for forecasting multivariate time series. This framework integrates strengths of both graph neural nets and attention mechanisms to model complicated relationships among different series and also captures their dependencies. The framework uses a heterogeneous graph to represent relationships among these series and uses graph convolutional nets to extract features from nodes of the graph. Attention mechanism is used to emphasize the importance of individual nodes in the forecast process. Evaluation results show that this new framework outperforms leading methods not only in terms of accuracy but also in efficiency. Results suggest that this study proposes a new way to handle forecasting of multivariate time series and this research might be applied in diverse fields including finance, energy and transport."
"This paper presents a novel approach to deriving theorems in implicational linear logic, using a declarative method. Linear logic is a type of mathematical logic that is particularly well-suited for modeling various computational systems, due to its ability to naturally handle the concepts of resource consumption and resource management. However, the task of deriving theorems in linear logic can be challenging, especially for those who are not familiar with its proof rules and strategies. To address this challenge, the authors propose a declarative method for theorem derivation, which is based on a high-level, algebraic representation of linear logic proofs. The key idea of this method is to use a set of rewrite rules that capture the essential structure of linear logic proofs, and to apply these rules in a systematic, automated manner, until the desired theorem is derived. The results of the study show that this method is effective and efficient for deriving theorems in implicational linear logic, and that it has the potential to be applied to a wide range of other logics and theorem proving tasks.","This paper introduces a new way to derive theorems in implicational linear logic through declarative methods. Linear logic is an ideal form of logic for modelling systems because it handles concepts of resource consumption and management very well. But deriving theorems is hard, especially if you're unfamiliar with its proof rules and strategies. To deal with this problem, researchers propose a method for theorem derivation that uses a high level algebraic representation of proofs. The core idea of this method is to use a set of rewrite rules that focus on essential structure of proofs and systematically automate application of such rules until the desired result is achieved. Results indicate this method works well and efficiently at deriving theorems in implication logic and also suggests it could be useful for many different kinds of logics and tasks."
"Temporal pattern mining is a crucial task in many fields such as finance, health, and engineering, where large amounts of time series data are generated regularly. Traditional methods of temporal pattern mining are inefficient and may lead to high computational costs when dealing with big time series data. Mutual information has been widely used in various fields to measure the dependence between two variables, and it has been shown to be effective in reducing the dimensionality of data. In this paper, we propose an efficient approach to temporal pattern mining in big time series data using mutual information. The proposed approach consists of three stages: data pre-processing, mutual information-based feature selection, and temporal pattern mining. In the pre-processing stage, we filter the data to remove irrelevant data points and perform data normalization. In the feature selection stage, we use mutual information to select the most relevant features for temporal pattern mining. Finally, in the temporal pattern mining stage, we apply a modified version of the PrefixSpan algorithm to extract frequent temporal patterns from the selected features. We demonstrate the effectiveness of our approach on several real-world datasets and compare our results with existing state-of-the-art methods. Our experiments show that our approach outperforms the existing methods in terms of efficiency and accuracy, and can handle big time series data with high-dimensional features. The proposed approach can be applied in various domains, including finance, health, and engineering, to extract valuable temporal patterns from large amounts of time series data efficiently.","Temporal pattern mining is important for many areas such as finance, health, and engineering because we regularly generate lots of time series data. Traditional ways of mining these patterns are slow and costly when handling big data. Mutual information is useful to assess how closely related two variables are and is very successful at reducing data dimensions. This paper introduces an efficient way to mine temporal patterns from big time series data by using mutual information. The method consists of three steps: preprocessing data, selecting important features based on mutual information, and then mining patterns. During preprocessing we remove irrelevant data and normalize what remains. Feature selection uses mutual information to pick important features that best relate to mining patterns. Finally, we use a modified PrefixSpan algorithm to extract frequent patterns from selected features. We validate performance on real datasets and compare against leading methods. Results show that our method excels both in efficiency and accuracy; it works well with high dimensional features too. Proposed method applies across diverse fields including finance, health and engineering to efficiently extract meaningful patterns from large time series data."
"This paper focuses on fair division problems under heterogeneous matroid constraints, which arise in various allocation scenarios where resources have different properties and priorities. We propose a new algorithmic framework that guarantees both fairness and efficiency by exploiting the structure of matroid constraints and the concept of envy-freeness up to one good (EF1). Our approach extends existing results on matroid intersection and partitioning, and adapts well to settings with multiple agents and indivisible items. We also provide a theoretical analysis of the algorithm, proving that it achieves a constant approximation ratio in terms of social welfare and EF1 envy-freeness. Our experiments on synthetic and real-world datasets demonstrate the practical effectiveness of the proposed method, outperforming state-of-the-art algorithms in terms of both objective metrics and subjective evaluations by human participants. Overall, our work contributes to the growing literature on fair allocation mechanisms and provides a useful tool for solving real-world problems with complex constraints.","This paper concentrates on problems of fair division when there are differing attributes and preferences among resources. We introduce an algorithmic framework that ensures both fairness and efficiency through leveraging structures related to matroids and EF1 (envy freedom up to one item). The proposed approach builds upon previous work on intersections and partitions and is flexible enough for multiple agents and indivisible goods. We also analyze the performance theoretically and prove that it attains an approximation ratio of constants in terms of social welfare and EF1. Empirical results using both simulated and real data show that this proposed solution performs very well; it outperforms leading algorithms based on both objective metrics and judgments made by human observers. Overall, we add to research into fair mechanisms and offer practical tools to handle complex resource allocations."
"This paper proposes a deep learning-based approach for detecting Distributed Denial of Service (DDoS) and Denial of Service (DoS) attacks in Internet of Things (IoT) networks. The proposed approach utilizes a deep convolutional neural network, specifically the Residual Network (ResNet), to learn features and classify network traffic patterns associated with normal and attack behaviors. The ResNet model is trained on a large-scale dataset of network traffic samples, which include various types of attacks and normal network traffic. The proposed approach is evaluated on a real-world IoT network dataset, and the results show that the ResNet-based model achieves high detection accuracy and outperforms several baseline models. The proposed approach is also shown to be effective in detecting zero-day attacks, which are new and previously unknown types of attacks that can evade traditional signature-based detection methods. Furthermore, the proposed approach is shown to have a low false positive rate, which reduces the likelihood of triggering false alarms and the subsequent disruption of legitimate network traffic. Overall, the results demonstrate the effectiveness of deep learning-based approaches for detecting DDoS and DoS attacks in IoT networks and highlight the potential of using ResNet for this purpose.",This paper introduces an approach using deep learning to detect attacks such as DDoS and DoS in IoT networks. We use residual networks (ResNet) as a classifier for distinguishing between normal traffic and attack behavior by learning features from large data sets containing diverse attack types and regular traffic. Results show that our model based on ResNets performs very well compared to baselines. Performance is particularly good at detecting new attacks (zero days). False positives are rare. Overall results suggest that deep learning methods work well for detecting such attacks and ResNet proves useful.
"With the increasing demand for high-speed connectivity and mobile network coverage, drone-mounted base stations have emerged as a promising solution to enhance network coverage in remote and disaster-stricken areas. However, handover management, which refers to the process of transferring a user's connection from one base station to another, remains a critical issue for drone-mounted base stations due to their high mobility and limited coverage. In this paper, we propose a group handover strategy for drone-mounted base stations in mobile networks. The proposed group handover strategy aims to minimize the handover latency and ensure seamless connectivity for multiple users when the drone-mounted base station is in motion. The strategy is based on a proactive handover approach, where the base station initiates the handover process before losing the connection with the user. The proposed strategy also considers the location and velocity of the drone-mounted base station and the quality of the communication link between the user and the base station. To evaluate the performance of the proposed strategy, we conducted a simulation study using a realistic urban scenario. The results show that the proposed strategy can reduce the handover latency by up to 50% compared to the traditional reactive handover approach. Moreover, the proposed strategy can improve the overall network throughput and reduce the number of handover failures. In conclusion, our study demonstrates the effectiveness of the proposed group handover strategy for drone-mounted base stations in mobile networks. The proposed strategy can enhance the quality of service and provide seamless connectivity for users in areas with limited network coverage. Further research can explore the integration of the proposed strategy with other handover management techniques and evaluate its performance in different deployment scenarios.","With growing need for fast connections and good mobile network coverage, drones equipped with base stations are becoming promising means to improve coverage in remote and disaster areas. However, handing over connections from one station to another due to high mobility and limited range remains a key challenge for these drones. This paper proposes a strategy for group handovers for drones. The strategy seeks to minimize latency and maintain smooth connections for multiple users during motion. It uses a proactive approach where the station initiates handover before loss of connection. Also considered is station location and speed along with quality of user link. Results of simulation studies conducted using real urban scenarios showed that proposed strategy reduces latency by up to 50% compared to reactive approach. Throughput improvement and reduction of failure events were also noted. In summary, this study shows effectiveness of proposed strategy for drones in mobile networks; this strategy enhances service quality and connectivity in places with weak coverage. Future research should consider integration with other handover management methods and testing different deployment scenarios."
"DeepCert is a new method for verifying the robustness of neural network image classifiers in contextually relevant scenarios. It addresses the challenge of ensuring that these classifiers can accurately identify objects in real-world situations, where images may be degraded, occluded, or otherwise altered from their training data. DeepCert uses a combination of formal verification techniques and machine learning to provide a probabilistic guarantee of robustness for a given classifier, with respect to a specified set of transformations. The method is evaluated on a range of benchmark datasets, and results demonstrate significant improvements over existing methods for robustness verification. This research demonstrates the feasibility of DeepCert for improving the confidence and trust in neural network image classifiers for practical applications.","DeepCert is a new approach for certifying robustness of classifiers that recognize images in practical contexts. It tackles the problem of ensuring that such classifiers perform reliably when encountering images in reality which might be noisy or distorted compared to what they were trained on. Using both formal verification and machine learning techniques, Deep Cert gives probabilistic guarantees about the robustness of a given classifier for specified sets of transformations. Results are evaluated against various benchmark datasets and show marked improvement over previous methods that certify robustness. Results indicate feasibility of Deep Cert for enhancing confidence and trustworthiness of image classifiers in practice."
"This paper proposes a novel approach to scalable and efficient training of multitask multilingual models. Multitask multilingual models are becoming increasingly popular in natural language processing due to their ability to handle multiple languages and tasks simultaneously. However, training such models is often time-consuming and computationally expensive, limiting their practicality. The proposed approach leverages the power of multi-node distributed training and adaptive optimization techniques to accelerate training time and reduce memory usage. Specifically, we introduce a new technique called task-aware gradient accumulation, which effectively balances the computational resources allocated to each task during training. Additionally, we employ an adaptive optimizer that dynamically adjusts learning rates and momentum values based on the characteristics of each task. Experimental results on a large-scale dataset demonstrate that our approach achieves state-of-the-art performance on several tasks, including language modeling, machine translation, and part-of-speech tagging, while significantly reducing training time and memory usage. Our approach provides a promising direction for the scalable and efficient training of multitask multilingual models, which could have significant implications for real-world natural language processing applications.","This paper introduces a new approach for fast and efficient training of multitask models that work across different languages. These models are increasingly used in natural language processing because they can handle multiple tasks together. But currently this training is very time consuming and expensive, making them impractical. We use distributed training on multiple nodes and adaptive optimization methods to speed up the training process and reduce memory use. Specifically, we introduce a new method called task aware gradient accumulation which efficiently allocates computational resources among different tasks during training. We also use adaptive optimizers that adjust learning rates and momentum dynamically according to each task's features. Results on a large dataset show that this new approach outperforms other methods on language modeling, machine translation, and POS tagging tasks and significantly reduces training time and memory usage. Overall, this new approach opens promising directions toward scalable and efficient training of multitask models that have important practical value in real world natural language processing tasks."
"This paper proposes a novel deep learning model for traffic forecasting, called CDGNet. CDGNet is a cross-time dynamic graph-based model that incorporates both spatial and temporal dependencies in traffic data. Specifically, CDGNet employs a dynamic graph convolutional neural network to capture spatial correlations among different regions in traffic networks, and a recurrent neural network with a time attention mechanism to capture temporal patterns in traffic data. To evaluate the performance of CDGNet, experiments are conducted on real-world traffic datasets, and compared with state-of-the-art traffic forecasting models. The experimental results demonstrate that CDGNet outperforms other models in terms of both short-term and long-term traffic forecasting accuracy. CDGNet can effectively capture complex spatiotemporal correlations in traffic data, and produce accurate traffic forecasts with high efficiency. Overall, this research presents a promising approach for traffic forecasting, which has important applications in traffic management, urban planning, and intelligent transportation systems. CDGNet can contribute to the development of smarter and more efficient transportation systems, and help to reduce traffic congestion, improve safety, and enhance the quality of life in urban areas.","This paper introduces a new deep learning model for forecasting traffic flows. This model is named CDGNet and it combines spatial and temporal dependencies into traffic data using dynamic graphs. Specifically, CDGNet uses dynamic graph convolutional neural network to discern correlation between various locations within traffic networks and also recurrent neural networks with a time attention mechanism to recognize trends in traffic data. Performance of CDGNet is evaluated on real traffic data and compared against top performing forecasting models. Results show that CDGNet excels both in short and long term forecasting accuracy. CDGNet efficiently captures complicated spatio temporal correlations in traffic data and produces accurate forecasts at high speed. Overall this research suggests a promising way of forecasting traffic; this has practical implications such as better traffic management, urban planning and intelligent transportation systems. CDGNet contributes to development of smarter transportation systems and improves quality of life in cities by reducing congestion and enhancing safety."
"This paper investigates the problem of constructing a critical geometric graph (CGG) in dense wireless sensor networks (WSNs) using a distributed approach. CGG is a fundamental structure in WSNs that captures the geometric relationships between nodes and is widely used in various applications. However, constructing CGG in dense WSNs is a challenging task due to the large number of nodes and the limited communication range. To address this problem, we propose a novel distributed algorithm that leverages the properties of Delaunay triangulation and Gabriel graph to construct the CGG in a distributed manner. The proposed algorithm works in an iterative fashion and consists of three main phases: node selection, local construction, and global synchronization. In the node selection phase, each node selects a set of candidate neighbors based on the Delaunay triangulation. In the local construction phase, each node constructs a local subgraph using the selected neighbors and Gabriel graph. Finally, in the global synchronization phase, the local subgraphs are merged to construct the final CGG. We evaluate the proposed algorithm through extensive simulations and compare it with existing approaches. The simulation results demonstrate that the proposed algorithm can construct the CGG with high accuracy and efficiency, even in dense WSNs. Furthermore, the proposed algorithm outperforms the existing approaches in terms of construction time and energy consumption. Overall, the proposed distributed algorithm provides a promising solution for constructing the CGG in dense WSNs, which can be used in various WSN applications such as localization, routing, and coverage control.","This paper studies how to construct critical graphs geometrically (CGG) in dense sensor networks (WSNs) via a distributed approach. CGGs are important structures in WSNs that reflect spatial relationships among nodes and are widely utilized for many tasks. However, building CGGs in densely populated networks is hard because of large numbers of nodes and small transmission range. Thus, we introduce a new distributed algorithm based on properties of Delaunay triangulation and Gabriel graph. This algorithm iterates in stages and comprises three major phases: selecting nodes, constructing locally and synchronizing globally. Each node chooses neighbor sets according to Delaunay triangulation during selection stage. Nodes then construct local subgraphs with selected neighbors using Gabriel graph in construction phase. At last, merging these local graphs results in final CGG in synchronization phase. Results of extensive simulations show that our algorithm produces high quality CGGs efficiently even in dense networks. Moreover, compared to previous methods, it performs better regarding both construction time and energy usage. Overall, our distributed algorithm offers promising solutions to building CGGs in dense networks and could be useful for localization, routing and other control tasks in WSNs."
"In today's world, communication is more pervasive than ever before. Large-scale communication networks like social media, instant messaging platforms, and online forums have become ubiquitous, generating vast amounts of conversational data every day. Overhearing, the act of observing and analyzing conversations between individuals, has emerged as a valuable tool for understanding and predicting behavior in these networks. However, as the scale of these networks continues to grow, there is a need for more efficient methods for analyzing conversational data. In this paper, we present a framework for representing conversations that is designed to facilitate scalable overhearing. Our framework leverages the graph structure of communication networks to create a compact representation of conversational data that can be efficiently analyzed. We use a combination of graph clustering and sequence modeling techniques to identify conversation topics and represent conversations as sequences of topic vectors. By representing conversations in this way, we are able to reduce the dimensionality of the data and improve the efficiency of overhearing algorithms. To evaluate our framework, we conduct experiments on a large-scale dataset of online forum conversations. We compare the performance of our framework to baseline approaches and show that our framework is able to achieve better accuracy and efficiency in overhearing tasks. We also demonstrate the scalability of our framework by analyzing datasets of varying sizes, showing that our approach can handle datasets with millions of messages. Overall, our framework provides a novel approach to representing conversations for scalable overhearing. By leveraging the structure of communication networks and reducing the dimensionality of conversational data, our approach enables more efficient analysis of large-scale communication networks.","Today, communication pervades society more than ever before. Platforms such as social media and instant messaging along with forums generate huge volumes of daily conversations. Observing and analyzing interactions among people has become a useful means to understand and predict behaviors within these networks. As these networks continue to grow larger, there is a growing need for more efficient ways of studying conversation data. This paper introduces a framework for representing conversations that aims to promote scalability for overhearing. The framework uses graph structure of communication networks to produce compressed representations of conversation data that are easier to analyze quickly. Techniques such as clustering and sequence modeling are combined to recognize topics and represent conversations as sequences of topic vectors. Through this representation, we can reduce data dimensions and improve efficiency of overhearing algorithms. We test our framework using a large dataset of online forum conversations. Comparisons are made against baselines and results show higher performance and efficiency. Scalability is demonstrated by processing different size datasets containing millions of messages. Overall, we introduce a new way of representing conversations for efficient scalability in overhearing through exploiting network structures and compressing data dimensions. Leveraging structure and reducing data dimensions allows for more effective analysis of large communication networks."
"This paper explores the concept of random caching based cooperative transmission in heterogeneous wireless networks. The authors propose a novel method for increasing the efficiency of wireless data transmission by utilizing random caching techniques at the intermediate nodes in the network. This approach combines the benefits of cooperative transmission and caching, resulting in improved network performance and reduced latency. The authors conduct extensive simulations and evaluations to demonstrate the effectiveness of the proposed method. The results show that the proposed random caching based cooperative transmission scheme significantly improves the network performance in terms of network throughput and energy efficiency compared to traditional cooperative transmission methods. The study also provides insights into the impact of various network parameters, such as the number of caches and the size of the cache, on the performance of the proposed method. These results provide valuable information for the design and optimization of future wireless networks. In conclusion, this paper provides a promising solution for improving the performance of wireless networks through the integration of random caching and cooperative transmission techniques. The results of this study have important implications for the design of future wireless networks and can be used to guide the development of new technologies in this area.","This paper studies random caching together with cooperative transmission for use in heterogeneous wireless networks. Authors present a new method which aims to improve wireless data transfer efficiency by using random caching at intermediate nodes in the network. This approach merges cooperative transmission advantages with caching benefits to yield better overall performance and lower latency. Authors conduct extensive simulation tests to verify the efficacy of their proposed approach. Results indicate that the proposed method based on random caching and cooperation greatly enhances throughput and energy efficiency compared to traditional cooperation methods. Insights are also provided regarding how different network parameters like cache numbers and cache size impact performance. Results of this research offer valuable guidance for designing future wireless networks. Conclusively, this work introduces a promising strategy to enhance wireless network performance via integration of caching and cooperation techniques. Results from this study have important ramifications for network design and can serve as a basis for further technological development in this domain."
"This paper proposes an efficient simulation method for predicting the temperature evolution of overhead transmission lines. The method combines an analytical solution for heat transfer with numerical weather prediction (NWP) data to accurately simulate the temperature distribution along the line. The proposed method is computationally efficient, making it suitable for real-time monitoring and control of transmission lines. The effectiveness of the proposed method is demonstrated through a case study, where it is shown to accurately predict the temperature of a transmission line under varying weather conditions. The results of this study highlight the potential of the proposed simulation method to improve the reliability and efficiency of overhead transmission line operations.",This paper introduces an efficient computational method that predicts temperature evolution along overhead power lines. The approach blends analytical solutions for heat transfer with numerical weather prediction data to simulate temperatures along the line. Results show this proposed approach saves computing time and can be used in real time for monitoring and control of transmission lines. Results from a case study clearly show accuracy in predicting temperature variation under different weather conditions. Results indicate this new simulation method has great potential to enhance reliability and efficiency of operation for overhead power lines.
"This paper proposes a Bayesian optimization method that incorporates domain knowledge for optimizing the performance of the ATRIAS biped, a two-legged robot. Bayesian optimization is a popular technique for global optimization of black-box functions, but its performance can be improved by leveraging relevant domain knowledge. The proposed method uses a Bayesian optimization framework that incorporates domain knowledge in the form of prior beliefs about the parameters that affect the performance of the ATRIAS biped. These prior beliefs are incorporated into the Bayesian optimization algorithm through the choice of the prior distribution. The method is evaluated on a set of simulation experiments, where it is compared with standard Bayesian optimization and other popular optimization algorithms. The experimental results demonstrate that the proposed method outperforms other methods in terms of the number of function evaluations required to obtain an optimal solution. The method also provides a better understanding of the ATRIAS biped's behavior by identifying the most important parameters and their interaction effects. The results suggest that incorporating domain knowledge into Bayesian optimization can lead to more efficient optimization and a better understanding of the system under consideration. Overall, this research contributes to the growing body of literature on Bayesian optimization and demonstrates its potential for optimizing complex systems such as bipedal robots. It also highlights the importance of incorporating domain knowledge to improve the performance of optimization algorithms.","This paper introduces a Bayesian optimization method that includes domain knowledge to optimize performance of ATRIAS biped, a two legged robot. Standard Bayesian optimization is very good for optimizing black box functions globally but it can be improved by using domain knowledge. Proposed method uses a Bayesian optimization framework that incorporates prior beliefs about parameters affecting performance of ATRIAS biped. Prior beliefs are integrated into the optimization algorithm through selection of prior distributions. Method is tested via simulations comparing to standard Bayesian optimization and other leading algorithms. Results show this new approach performs better by requiring fewer function evaluations to reach optimal solutions. It also clarifies important parameters and interactions among them. Results indicate that integrating domain knowledge into Bayesian optimization leads to higher efficiency and deeper insight into systems being optimized. Overall contribution to literature on Bayesian optimization and demonstration of effectiveness in optimizing complex systems like bipeds. Also underscores value of incorporating knowledge specific to the system being optimized."
"This paper explores the learning of coordination policies for robotic swarms. Robotic swarms consist of a large number of relatively simple robots that can work together to accomplish complex tasks. However, achieving coordination in such swarms is challenging due to the inherent complexity of the system and the lack of a centralized control mechanism. In this paper, we propose a reinforcement learning approach to learn coordination policies for robotic swarms. Our approach involves using a centralized critic and decentralized actors to learn the coordination policies. The centralized critic observes the global state of the swarm and provides feedback to the decentralized actors, which take local actions based on their observations of the local environment. The feedback from the centralized critic is used to update the policy parameters for the decentralized actors, which gradually learn to coordinate their actions with those of the other robots in the swarm. We evaluate our approach on a set of benchmark tasks for robotic swarms, including formation control, obstacle avoidance, and cooperative transport. Our results show that our approach is effective at learning coordination policies for robotic swarms, achieving high performance on these tasks while also being able to generalize to new environments. Overall, our work demonstrates the potential of reinforcement learning for learning coordination policies in robotic swarms, and provides a foundation for further research in this area. The ability to learn coordination policies in this way could enable the development of more sophisticated and efficient robotic swarms that can accomplish complex tasks in a variety of environments.","This paper studies coordination policies learned by robot swarms. Robot swarms include many simple robots which can cooperate to perform complex tasks. Coordination among such swarms is hard because of inherent complexity and absence of centralized controllers. Here we propose a reinforcement learning method to learn coordination policies. We use a central critic and distributed actors to learn coordination. Central critic observes overall swarm state and gives feedback to actors who act locally based on observation of local environment. Feedback from critic updates actor policy parameters leading to coordinated actions among members of the swarm. We test our approach on formation control, obstacle avoidance and cooperative transport benchmark tasks. Results show our approach learns good coordination policies efficiently and generalizes well to new situations. Overall, we show promise of reinforcement learning for learning coordination among robots and provide a basis for further research. Through this, we also open the door to developing more advanced and efficient robot swarms which can tackle diverse environments."
"The paper presents Serket, a novel architecture for connecting stochastic models in order to realize a large-scale cognitive model. The architecture is based on a modular design that allows for the integration of multiple models into a single system. Serket takes advantage of the strengths of stochastic models, such as their ability to handle uncertainty and make probabilistic predictions, while also addressing their limitations, such as the difficulty of integrating multiple models. The architecture is evaluated on a range of tasks, including language understanding and generation, decision making, and perception. The results show that Serket outperforms existing approaches in terms of accuracy, scalability, and interpretability. The paper concludes by discussing the implications of the Serket architecture for the development of more advanced cognitive models and its potential applications in areas such as artificial intelligence and robotics.","This paper introduces Serket which is a new architecture for connecting stochastic models so that we can build larger cognitive systems. Based on modular design, this architecture integrates multiple models into one system. Using advantages like flexibility with uncertainty and probabilistic predictions, Serket overcomes the disadvantages like integrating multiple models. Performance is measured across many tasks including language understanding and generation, decision making and perception. Results show high performance metrics such as accuracy and interpretability; Serket excels compared to other methods. At the end, this work discusses implications of Serket architecture and considers future development of advanced cognitive systems along with practical applications such as AI and robotics."
"This paper examines the ethical debate surrounding actuarial risk assessment and the use of interventions to modify predicted outcomes. The traditional view of actuarial risk assessment has been to use predictive algorithms to identify individuals at high risk for negative outcomes and allocate resources accordingly. However, this approach raises ethical concerns about the potential for discrimination and unequal treatment. The paper argues that reframing the debate to focus on interventions rather than predictions can shift the conversation towards a more ethical and equitable approach to actuarial risk assessment. The authors propose that interventions should be designed to address the root causes of risk and promote positive outcomes, rather than simply mitigating predicted negative outcomes. The paper concludes by highlighting the importance of considering the ethical implications of actuarial risk assessment and the need for ongoing dialogue about the appropriate use of interventions in this field.","This paper looks into ethical debates over actuarial risk assessment and using interventions to change forecasted results. Traditionally, actuarial assessment uses predictive algorithms to find people who are likely to have bad results and then allocates resources accordingly. But this practice raises ethical questions about possible discrimination and unfair treatment. The paper suggests refocusing the discussion toward interventions instead of just prediction as a way to move toward a fairer approach to assessing risk through actuarial means. Interventions should target the underlying causes of risk and enhance positive results rather than just reducing forecasted negatives. In conclusion, the paper stresses that consideration of ethics is important when assessing risk via actuarial means and that there needs to be ongoing dialogue about appropriate use of interventions in this area."
"This paper presents a framework for enriching lexical semantic resources with distributional semantics. The framework aims to address the limitations of traditional lexical resources, such as wordnets and ontologies, by incorporating distributional information from large text corpora. The proposed framework consists of three main components: (1) data collection, (2) feature extraction, and (3) semantic enrichment. The data collection component involves the retrieval of large amounts of text data from various sources, including online forums, news articles, and social media platforms. The feature extraction component involves the use of algorithms such as word2vec and GloVe to compute vector representations of words in the text data. Finally, the semantic enrichment component involves the integration of these vector representations into existing lexical semantic resources, such as wordnets and ontologies, to enhance their semantic coverage and accuracy. The framework is evaluated on a set of benchmark tasks, including word similarity and analogy tasks, and the results show that the proposed framework significantly improves the performance of existing lexical semantic resources. The framework provides a practical solution for enhancing lexical semantic resources and has the potential to impact a wide range of natural language processing tasks.","This paper introduces a framework for improving lexical semantic resources using distributional semantics. The framework seeks to overcome shortcomings of traditional resources like WordNet and Ontologies by incorporating distributional information from large text corpora. This framework contains three core parts: (1) data collection, (2) feature extraction and (3) semantic enrichment. Data collection gathers large volumes of text from diverse sources including forums, news articles, and social media. Feature extraction uses methods such as word2vec and GloVe to extract vector representations of words within collected data. Finally, this enriched vector representation is integrated into existing resources like WordNet and Ontologies to increase their coverage and accuracy. Performance of benchmark tasks like similarity and analogies is tested against this framework and results show marked improvement over existing resources. Overall the framework offers practical solutions for enhancement of lexical semantic resources and could influence many NLP tasks."
"This paper proposes a novel approach to sequence-to-sequence learning using a recurrent neural network-based semantic variational autoencoder (RNN-SVAE). The model leverages the encoding-decoding architecture of traditional autoencoders while also incorporating variational inference and a recurrent neural network for improved sequential modeling. The RNN-SVAE is capable of learning meaningful and latent representations of input sequences, allowing for effective generation of output sequences with a high degree of semantic coherence. Experimental results show that the proposed model outperforms traditional sequence-to-sequence models on various text generation tasks, demonstrating its effectiveness in capturing the underlying structure and meaning of sequences.",This work introduces a new method for sequence learning through use of a recurrent neural network based on Semantic Variational Autoencoder (RNN SVAE). The model uses an encoding decoding structure similar to traditional autoencoders but incorporates variational inference and a recurrent neural network to improve sequence modeling. This RNN SVAE learns meaningful latent representations of input sequences and can generate output sequences with very high semantic coherence. Results from experiments indicate this proposed method performs better than previous sequence models on diverse text generation tasks; it excels at extracting underlying structure and meaning of sequences.
This paper proposes a novel approach for detecting deception attacks in networked control systems using a sequential detection framework with watermarking. The proposed method involves embedding a unique watermark into the control system's input signal and then monitoring the output signal for any deviations from the expected behavior. A sequential hypothesis testing algorithm is used to detect the presence of a deception attack based on the watermark's correlation with the output signal. Simulation results demonstrate the effectiveness of the proposed method in detecting various types of deception attacks with high accuracy while maintaining low false alarm rates. This research provides a promising solution for enhancing the security and reliability of networked control systems against malicious attacks.,This paper introduces a new method for detecting deceptive attacks in networked control systems by using a sequential detection framework along with watermark embedding. The suggested approach embeds a unique watermark into control system inputs and monitors output for deviation from expected behavior. Detection of attacks uses sequential hypothesis testing algorithm based on correlation between watermark and output. Results show that this method effectively detects different kinds of attacks accurately at high levels and with very low false alarms. This work offers promising solutions for improving security and reliability of control systems from harmful attacks.
"This paper presents a scalable shared-memory parallel simplex algorithm for solving large-scale linear programming problems. The proposed algorithm is designed to exploit the parallelism inherent in modern multi-core processors and uses efficient data structures and algorithms to optimize performance. The algorithm employs a task-based parallelism model, in which tasks are dynamically scheduled and executed on available processors. The parallelism is achieved by dividing the problem into smaller subproblems, which are then solved independently using the simplex method. The proposed algorithm also uses load balancing techniques to ensure that the workload is distributed evenly among all the available processors. To evaluate the proposed algorithm, we conducted a series of experiments using several benchmark linear programming problems with different sizes and characteristics. The experimental results show that the parallel simplex algorithm is highly scalable and achieves significant speedups over the sequential simplex algorithm. The performance of the algorithm is further improved by using advanced optimization techniques, such as lazy updates and column-wise data layout. The proposed algorithm is expected to have a significant impact on the field of linear programming, especially in large-scale optimization problems. The algorithm is highly flexible and can be easily extended to handle various types of linear programming problems, including mixed-integer programming, network flow problems, and more. Overall, the research presented in this paper demonstrates the feasibility and effectiveness of using shared-memory parallelism to solve large-scale linear programming problems, paving the way for future research in this area.",This paper introduces an algorithm designed to scale well for solving large linear programming problems using shared memory. Proposed algorithm takes advantage of parallelism in modern multicore processors and uses efficient data structures and algorithms to improve performance. Algorithm uses dynamic scheduling and execution of tasks on processors. Tasks divide the problem into small pieces and solve them independently using simplex method. Load balancing techniques are used to distribute workloads evenly across processors. Results from a series of experiments using different benchmark problems demonstrate scalability and significant speedup compared to sequential simplex algorithm. Performance improvements include use of advanced optimizations such as lazy updates and columnar data layout. Results suggest that using shared memory parallelism will have a big effect on linear programming. Flexible enough to handle different types of LPs including mixed integer programming and network flow problems. Results of this study indicate that using parallelism is feasible and effective for solving large LP problems and opens up avenues for future research. Overall this work shows promise for advancing research in this domain.
"This paper proposes a novel method called ""Zoom-SVD"" for extracting key patterns in an arbitrary time range from large data sets. The method is designed to be both fast and memory-efficient, making it suitable for use in analyzing time-series data sets with a large number of observations. The Zoom-SVD approach is based on an extension of the singular value decomposition (SVD) algorithm, which enables efficient computation of the most important modes of variation in the data. The method also incorporates a zooming mechanism, which allows the user to focus on specific time intervals of interest, and only compute the SVD on those portions of the data. The performance of the Zoom-SVD method is evaluated on a range of real-world data sets, demonstrating its effectiveness in identifying key patterns in various time-series applications, including weather forecasting, financial analysis, and speech recognition. The results show that the Zoom-SVD method is significantly faster and more memory-efficient than existing SVD-based methods, while also achieving comparable or superior accuracy in detecting patterns of interest. Overall, this research presents a promising approach for efficient and effective analysis of time-series data in a variety of fields.","This paper introduces a new method called Zoom SVD for extracting key features within any given time span from large datasets. Designed for speed and low memory usage, this method excels at analysis of high observation count time series. Zoom SVD relies on extending Singular Value Decomposition (SVD), which computes significant modes of variation efficiently. It also includes a zooming feature allowing users to concentrate on specific time periods and computing SVD only on relevant parts of data. Performance of Zoom SVD is tested on diverse real world datasets and results show effectiveness in pattern identification across different domains such as weather prediction, finance and speech recognition. Results indicate that Zoom SVD outperforms competing SVD methods by being much faster and using less memory while matching or exceeding performance at detecting relevant features. Overall this research proposes promising ways to analyze time series data broadly."
"This paper presents a novel approach to face recognition using transformed shape features. The proposed algorithm is based on the transformation of facial features into a new shape representation, which is then used as input to a recognition model. This transformation process takes into account the inherent variability of facial features, such as changes in lighting, pose, and expression, to produce a robust representation of the face. Experiments were conducted on several publicly available datasets and the results show that the proposed approach outperforms existing state-of-the-art face recognition algorithms in terms of accuracy and robustness. The results demonstrate that the transformed shape features approach is a promising direction for future face recognition research.","This paper introduces an innovative method for face recognition through transformed shape features. Proposed algorithm uses transformation of facial features into new shape representations as input to a recognition model. Transformation accounts for variability inherent in features such as illumination, pose and expression so as to generate a robust face representation. Results of experiments on diverse public datasets show proposed approach excels current top performance in accuracy and robustness. Results indicate transformed shape feature approach is promising for future research."
"This paper proposes the use of neural networks as a tool to analyze attack patterns and generate effective solutions to potential threats. Attack patterns are a critical aspect of cybersecurity, and identifying and addressing these patterns is a key challenge faced by security professionals. In this study, we develop a neural network-based approach that leverages machine learning techniques to analyze and understand patterns of cyber-attacks. The proposed model utilizes both supervised and unsupervised learning techniques to extract meaningful features from large-scale datasets of attack patterns. The neural network then employs these features to identify and categorize potential threats, and generate effective solutions for mitigating the impact of these threats. Our experiments demonstrate that the proposed approach can effectively analyze attack patterns and generate actionable solutions, with high accuracy and efficiency. The proposed model has the potential to improve cybersecurity practices by providing a comprehensive and automated solution for detecting and addressing security threats.",This paper introduces neural networks as an important tool for analyzing attack patterns and generating solutions for handling potential threats. Patterns of attacks are critical in cybersecurity and identifying and dealing with these patterns poses a major challenge to security experts. We develop a method using neural networks based on machine learning techniques; we extract meaningful features from large datasets of attack patterns using both supervised and unsupervised learning methods. Neural networks then use these features to identify and classify potential threats and generate effective solutions to reduce threat impact. Results show that our proposed method performs well at analysis and solution generation with high precision and speed. Proposed model could enhance cybersecurity practice by providing a complete and automated solution for detection and dealing with security threats.
"The Internet of Things (IoT) is rapidly expanding, with billions of devices connected to the internet. This expansion brings numerous security challenges, as many IoT devices are resource-constrained and lack robust security mechanisms. The OSCAR (Object Security Architecture for the Internet of Things) architecture is proposed to address these security challenges. OSCAR is a lightweight security architecture that uses object-oriented security mechanisms to protect IoT devices from various types of attacks. The proposed OSCAR architecture consists of three main components: object modeling, object security, and object communication. Object modeling is used to describe the structure and behavior of IoT objects, including their security attributes. Object security is responsible for protecting the objects and their attributes from various security threats. Object communication defines the protocols and procedures used for secure communication between IoT objects. The proposed OSCAR architecture is evaluated using several performance metrics, including memory and processing overhead, communication latency, and energy consumption. The experimental results show that OSCAR provides robust security while incurring minimal overhead. OSCAR also supports various security services, including confidentiality, integrity, authenticity, and access control. The OSCAR architecture is compared to several existing IoT security architectures, and the results show that OSCAR provides better security and performance compared to these architectures. Finally, the proposed OSCAR architecture is implemented on several IoT devices, including sensors, actuators, and gateways, demonstrating its practicality and effectiveness. In conclusion, the proposed OSCAR architecture provides a lightweight and effective security solution for the IoT, which can be applied to various IoT devices and networks. The use of object-oriented security mechanisms makes OSCAR adaptable and scalable, addressing the security challenges posed by the IoT.","The IoT is rapidly growing with billions of devices connected online. Such an expansion brings up lots of security issues because many IoT devices have limited resources and do not have strong security mechanisms. A new architecture called OSCAR is introduced to handle these security concerns. OSCAR is a light security architecture that uses object oriented security mechanisms to prevent different kinds of attacks against IoT devices. The proposed OSCAR includes three major parts: object modeling, object security and object communication. Object modeling describes the structure and behavior of IoT objects along with their security features. Object security is tasked with protecting those objects and their features against different security risks. Communication protocols and procedures are defined for secure exchange of information among IoT objects. Performance metrics such as memory and processing overhead, latency and energy consumption are used to evaluate this proposed architecture. Experimental results indicate that OSCAR provides strong security while requiring little overhead. OSCAR supports diverse security services including confidentiality, integrity, authentication and access control. Compared to other existing IoT security architectures, OSCAR performs better. Implementation has been done on various devices like sensors, actuators and gateways to show practicality and effectiveness. In short, this proposed OSCAR architecture offers lightweight and effective security solutions for IoT that work well for different devices and networks. Using object security makes OSCAR adaptable and scalable and thus addresses security issues brought by IoT."
"In recent years, 5G technology has become a crucial aspect of wireless communication systems, providing high-speed data transfer and reduced latency. The increasing demand for 5G services in dense urban areas has created a need for effective approaches to selecting radio access technologies (RATs) for 5G networks. This paper presents an in-depth analysis of the key factors that influence RAT selection for 5G dense wireless networks, such as network architecture, network density, available spectrum, and user requirements. The study proposes a novel RAT selection approach that leverages machine learning algorithms to predict the most suitable RAT for a given scenario based on real-time network conditions. The approach considers various metrics, such as network traffic, interference, and network coverage, to make informed RAT selection decisions. The proposed approach is evaluated through extensive simulations and experiments, and the results show that it outperforms conventional RAT selection methods in terms of network performance and user satisfaction. This research provides valuable insights into the challenges of RAT selection for 5G dense wireless networks and offers a promising solution for operators and network designers to optimize their networks and improve the overall user experience. The proposed approach is expected to have a significant impact on the development of 5G networks and contribute to the advancement of the field of wireless communication systems.","In recent years 5G technology has become a critical element of wireless communications systems and it enables high speed data transfer and low latency. Recently, there is a strong demand for 5G services especially in dense urban areas, which has led to the need to use effective methods to select radio access technologies (RATs) for 5G networks. This paper carries out an in-depth study on the factors that influence selection of RATs for 5G networks with high density such as network architecture, network density, availability of spectrum and user needs. A new method of RAT selection is proposed that uses machine learning algorithms to forecast the most suitable RAT based on real time network conditions. It takes into account many metrics like network traffic, interference and network coverage to make informed selections. Results from extensive simulations and experiments show this new approach performs better than traditional selection methods in terms of performance and user satisfaction. This research offers important insights into challenges of RAT selection in high density 5G networks and also provides promising solutions for operators and network designers to optimize networks and improve overall user experience. This approach is expected to have a great impact on development of 5G networks and advance the field of wireless communication systems."
"This paper presents Lamassu, a storage-efficient host-side encryption technique that allows sensitive data to be stored on a client device without compromising its security. Lamassu is designed to provide confidentiality, integrity, and authenticity of the data, while minimizing the storage space required for the encrypted data. To achieve its goal, Lamassu employs a novel approach of combining authenticated encryption with a technique called inline encryption. This allows Lamassu to encrypt data inline with its creation, without requiring additional storage space for the encrypted data. The technique also ensures that the data is not tampered with during transmission and storage. The paper presents a thorough analysis of the security properties of Lamassu, including a formal security proof, and a performance evaluation that demonstrates its effectiveness in terms of storage efficiency, speed, and security. The authors also discuss the practical applications of Lamassu in various settings, including cloud storage, mobile devices, and network communication. Overall, the results of this research demonstrate the potential of Lamassu as an effective technique for storage-efficient host-side encryption, with promising practical applications in various settings.","This paper introduces Lamassu, a method for secure storage of sensitive information at the client side that reduces storage requirements. Lamassu secures confidentiality, integrity and authentication of data. Lamassu uses a new approach combining authenticated encryption with inline encryption. This approach enables encryption along with creation of data without requiring extra storage space for encrypted data. Data is also protected from tampering during storage and transmission. Results show that Lamassu achieves good performance in terms of storage efficiency, speed and security. The authors also describe some practical uses for Lamassu such as cloud storage and communications over networks. Overall this study indicates that Lamassu has great promise as an efficient method of host side storage encryption with broad application potential."
"This paper presents an overview of tensor decomposition techniques and their applications in signal processing and machine learning. Tensor decomposition is a powerful tool for analyzing high-dimensional data, which arises in many real-world problems. The paper first provides a brief introduction to tensors and tensor decomposition, including various tensor formats and the mathematical foundations of tensor decomposition. Then, it discusses the main tensor decomposition methods, including Tucker decomposition, canonical polyadic decomposition (CPD), and PARAFAC2. For each method, the paper describes its algorithm, advantages, and limitations. In addition, the paper discusses several applications of tensor decomposition in signal processing and machine learning, such as tensor completion, tensor regression, and tensor clustering. Finally, the paper concludes with some future directions and challenges in the field of tensor decomposition. Overall, this paper provides a comprehensive overview of tensor decomposition and its potential applications in signal processing and machine learning.","This paper surveys tensor decomposition techniques and their use cases in signal processing and machine learning. Decomposing tensors is a powerful means to analyze high dimensional data which frequently occurs in real life. The survey begins with a basic introduction of tensors and decomposition methods. It introduces different tensor formats along with the mathematics that support tensor decomposition. Following this introduction, the paper focuses on the main decomposition methods, namely Tucker decomposition, CPD, and PARAFAC2. Each method receives detailed discussion regarding algorithms, benefits and shortcomings. Furthermore, examples of application include tensor completion, regression and clustering. Lastly, the paper reflects on future research prospects and challenges related to this topic. In summary, this paper offers a broad review of tensor decomposition and its relevance to signal processing and learning."
"The theory of matchings in set families has been an active area of research since the seminal work of Erds and Kleitman in 1971. In this paper, we investigate two fundamental problems in this area: the maximum matching problem and the maximal matching problem. The maximum matching problem seeks to find a matching of maximum cardinality in a given set family. We provide a new algorithm that achieves an improved upper bound on the number of edges in the matching, which improves upon the previously best-known bounds. Our algorithm also has the advantage of being faster than existing algorithms, making it practical for larger set families. The maximal matching problem seeks to find a matching that cannot be extended to a larger matching. We show that this problem is closely related to the combinatorial structure of set families and introduce a new measure called the maximal matching degree. Using this measure, we prove a new upper bound on the maximal matching degree, which has implications for the existence of maximal matchings in certain classes of set families. Our results extend the work of Erds and Kleitman and shed new light on the structure of matchings in set families. They have implications for a variety of applications in computer science, operations research, and other fields.","Since important work by Erdos and Kleitman in 1971, research has continued into the theory of matchings in set families. We study two core issues here: one about finding as many matches as possible (maximum matching) and another about finding the largest match that can't be expanded further (maximal matching). We present an improved algorithm for maximum matching; this new algorithm finds better upper bounds on the number of edges matched compared to previous methods. This new algorithm runs faster and is thus practical for large sets of families. For maximal matching we show this is closely tied to combinatorial structure within families and we define a new metric called maximal matching degree. We establish new upper bounds for this degree through this metric and these bounds have implications regarding existence of maximal matchings in some classes of families. Results here build on work by Erdos and Kleitman and we discover new insights into matching structures. These results have relevance in diverse areas including computer science and operations research."
"This paper proposes Augury, a time-series based application for the analysis and forecasting of system and network performance metrics. Augury employs advanced machine learning techniques such as neural networks and time-series decomposition to identify trends, patterns, and anomalies in performance metrics. Augury also provides a comprehensive set of forecasting models that enable system administrators to predict future performance metrics accurately. The effectiveness of Augury was evaluated using real-world data collected from multiple systems and networks. The results indicate that Augury outperforms existing solutions in terms of accuracy, scalability, and speed. Furthermore, Augury provides valuable insights into the underlying causes of performance issues, which can be used to optimize system and network performance. Augury has the potential to improve the efficiency and reliability of complex systems and networks, and to reduce the cost and effort associated with performance monitoring and management.","This paper introduces Augury which is a new tool designed for analyzing and predicting performance metrics of systems and networks. Augury uses sophisticated learning algorithms including neural networks and time series decomposition to recognize trends and irregularities in performance metrics. Augury also includes a rich selection of predictive models which allow system administrators to forecast performance accurately. Results obtained by using real data from different systems and networks show that Augury excels at accuracy, scalability and speed compared to other solutions. Additionally, Augury delivers important understanding of root causes of performance problems; this understanding can help to optimize performance. Augury should enhance efficiency and reliability of complicated systems and networks and reduce costs and efforts related to monitoring and management."
"This paper examines the packing and covering properties of subspace codes for error control in random linear network coding. We analyze the trade-off between the minimum distance and the maximum rate of subspace codes and provide constructions of subspace codes with optimal packing and covering properties. Specifically, we propose a method for constructing subspace codes with high minimum distance that achieves the optimal trade-off between the minimum distance and the maximum rate. We also investigate the covering radius of subspace codes and show that our constructions achieve the optimal covering radius in certain regimes. Our results have implications for the design of practical subspace codes for error control in random linear network coding.",This research looks at the packing and covering characteristics of subspace codes for use in error control through random linear network coding. We study the balance between minimum distance and maximal rate and present constructions of high quality subspace codes that meet the best possible tradeoff between these. Specifically we propose a method for making codes with high minimum distances that optimize this tradeoff. We also consider the covering radius and show that our designs perform optimally under certain conditions. Results are important for designing effective subspace codes for error correction via random linear network coding.
"This paper presents an analysis of the two most promising mobile broadband solutions for South Asia: WiMAX and LTE. The paper aims to determine which technology would be the better solution for providing reliable and high-speed mobile broadband connectivity to the region. The study examines the current state of mobile broadband infrastructure in South Asia, and the unique challenges that the region presents, such as population density and geography. The paper then provides an in-depth analysis of WiMAX and LTE, including their technical specifications, deployment requirements, and performance characteristics. The study also includes a comparison of the two technologies based on factors such as coverage, capacity, latency, and cost. The paper concludes with a recommendation for the best mobile broadband solution for South Asia, taking into account the specific needs of the region. The findings of this paper will be useful for policymakers, mobile network operators, and other stakeholders in the telecommunications industry who are seeking to improve mobile broadband connectivity in South Asia.","This research analyses two leading broadband mobile options for South Asia: WiMAX and LTE. The aim is to decide which technology works best for dependable and fast mobile broadband service delivery for the region. We look at current infrastructure for mobile broadband and challenges specific to South Asia like population density and geography. We compare and contrast these technologies in detail: focusing on their technical specs, deployment requirements and performance attributes. Factors compared include coverage, capacity, latency and cost. Results conclude which technology excels for South Asia and takes regional needs into account. This paper will help policymakers and other players in the telecom sector improve mobile broadband connectivity in South Asia."
"This paper presents a comprehensive study of bloated dependencies in the Maven ecosystem. The study aims to understand the extent and impact of bloated dependencies on the build process and overall performance of Maven projects. The study employs a combination of static code analysis and dynamic profiling to identify and quantify the impact of bloated dependencies. The results of the study show that bloated dependencies are a common problem in Maven projects and can significantly slow down the build process and increase the memory footprint of the build system. The paper also provides recommendations for mitigating the impact of bloated dependencies, including the use of dependency management techniques and the adoption of lightweight alternatives to bloated dependencies. The study provides valuable insights for developers, build engineers, and managers who are looking to optimize their Maven build processes and improve the performance of their projects.","This research thoroughly investigates bloated dependencies in the Maven ecosystem. We aim to understand how much and what effect such dependencies have on Maven project builds and overall performance. Using both static code analysis and dynamic profiling, we pinpoint and measure this effect. Results indicate that bloated dependencies are common and they greatly reduce build speed and increase system memory usage. Recommendations include better management of dependencies and using lighter alternatives for dependencies that are large. Results offer important guidance to developers, build engineers and managers seeking to optimize Maven build processes and improve project performance."
"This paper investigates the factors that make a good summary in an educational context and the focus of automatic summarization technology. The study aims to determine the importance of key elements such as conciseness, relevance, and coherence in summarization and how they affect the effectiveness of summarization in an educational setting. The paper also explores the current state of automatic summarization technology, including its limitations and potential for improvement. The results of the study provide insights into the effective use of summarization in education and the future development of automatic summarization technology.","This research looks into what makes summaries good in an educational context and focuses on automatic summarization technology. It aims to assess how important certain elements such as conciseness, relevance and coherence are to summarization and how those elements influence effectiveness in education. The paper also looks at current status of technology related to automatic summarization including its drawbacks and possibilities for enhancement. Results from this study offer insights into how summarization is used effectively in education and future developments of automatic summarization technology."
"This paper explores the utilization of heterogeneity in robust federated best-arm identification. The authors aim to address the challenge of selecting the best arm in a multi-armed bandit problem in a federated setting where the arms are distributed across multiple agents and may have different distributions. The proposed solution leverages the heterogeneity of the agents to design a robust and efficient algorithm for best-arm identification. The algorithm is tested on various simulation scenarios and the results demonstrate its effectiveness in comparison to other existing methods. The findings of this study contribute to the development of robust algorithms for federated multi-armed bandit problems, which have applications in various fields including personalized recommendation systems and online advertising.","This paper investigates the use of heterogeneity for robust best choice identification in federated settings. Researchers seek to solve the challenge of choosing the best option when faced with multiple options (arms) that are spread among different parties (agents) and each has its own distribution. Proposed solution uses differences among these agents to design an algorithm that identifies the best choice robustly and efficiently. Results from diverse simulation cases show effectiveness compared to other methods. Findings advance robust algorithms useful for federated multi choice bandits, useful for many fields such as personalized recommendation systems and online ads."
"Frequency estimation is a critical problem in various signal processing applications. In this paper, we propose a novel approach to frequency estimation with one-sided error, which is a type of estimation problem where the estimated value is required to be within a specified range, either above or below the true value. The proposed method is based on the use of a modified version of the periodogram, combined with a statistical hypothesis test. The proposed approach is compared with existing frequency estimation methods in terms of accuracy, robustness, and computational complexity, and the results demonstrate that the proposed method outperforms existing methods in a variety of scenarios. The proposed method is a valuable tool for researchers and practitioners who are working on signal processing applications where frequency estimation with one-sided error is a critical requirement.","Frequency estimation is important for many different tasks in signal processing. This paper introduces a new method for estimating frequencies that have an upper limit (one-sided error) - that is, we need to estimate frequencies such that they lie within some defined limits either above or below the actual frequency. This new method uses modified periodograms alongside a statistical hypothesis test. Results comparing this new approach against other methods show that this new method works better overall under different conditions; it is therefore valuable for people working on signal processing tasks that demand precise frequency estimation with upper limits."
"Fishing is a popular leisure activity, but anglers often face challenges in finding and catching fish. This study aimed to understand the anglers' fishing problem and identify potential solutions. A survey was conducted with a sample of experienced anglers to gather information about their fishing experiences, the challenges they face, and their preferred methods for finding and catching fish. The results showed that the most common challenges faced by anglers include limited access to fishing locations, poor water quality, and declining fish populations. To address these challenges, the study recommends improving access to fishing locations, implementing conservation efforts to protect fish populations and their habitats, and providing education and resources for anglers on sustainable fishing practices. The findings of this study can inform management and policy decisions to support the sustainable use of fishing resources and improve the fishing experience for anglers.","Fishing is a popular pastime for leisure. But anglers regularly encounter difficulties both in finding and catching fish. This research aimed to understand angler problems and suggest solutions. We surveyed experienced anglers to get information on their fishing experiences and challenges and preferred methods of finding and catching fish. Results show that anglers commonly face difficulties such as insufficient access to fishing spots, poor water quality, and reduction in fish populations. Recommendations from this study include enhancing access to fishing spots, implementing conservation efforts to protect fish and their habitat, and educating anglers on sustainable fishing practices. Results of this study also provide information useful for managing and making policies to support sustainable use of fishing resources and improve angler experience."
"This paper explores the concept of processes, roles, and their interactions within organizational settings. The study aims to gain a deeper understanding of the dynamics of these elements and how they impact organizational performance. Through an extensive review of relevant literature, the paper identifies the various types of processes and roles and the functions they perform. It also examines the different interactions between processes and roles, including their mutual dependencies, conflicts, and collaborations. The research methodology includes qualitative and quantitative approaches, such as case studies, surveys, and statistical analysis. The findings highlight the crucial role of processes and roles in driving organizational success and the importance of effective interactions between them. The paper concludes with practical recommendations for organizations to optimize their processes, roles, and interactions for improved performance.","This paper investigates processes, roles and their interrelations within organizational contexts. Its goal is to deepen understanding of dynamic relations among these elements and the effect on performance. By thoroughly reviewing related literature, this paper identifies different process types and roles along with functions they perform. It also looks at interaction among processes and roles including dependencies, conflicts and cooperation. Research method uses both qualitative and quantitative approaches like case studies and surveys together with statistical analyses. Results show that processes and roles are essential for success and effective collaboration matters most. Conclusions offer practical advice for better optimization of processes, roles and interactions for higher performance."
"This paper presents a new approach for optimizing spread dynamics on graphs by utilizing message passing. The method is based on a combination of graph theory and probability theory to accurately model the spread of information or influence through a network. The authors demonstrate the efficacy of their method through extensive simulations and real-world experiments on various graph structures, showing significant improvements in spread optimization compared to existing methods. The results of this work have important implications for various fields, including social network analysis, marketing, and network design.","This paper introduces a new approach for optimizing propagation dynamics using message passing. Using both graph theory and probability theory together, they develop an accurate model of how information or influence spreads through networks. Authors show effectiveness through simulation experiments and real data tests on diverse networks; results indicate marked improvement over previous approaches. Results are relevant to many different areas including social network analysis, marketing and network design."
"This paper presents a novel algorithm for determining optimal electricity pricing in a smart microgrid network. The algorithm is based on a multi-objective optimization framework that considers both economic and technical factors. The economic objective seeks to maximize the revenue of the microgrid operator, while the technical objective minimizes the total energy cost to the consumers. The proposed algorithm incorporates real-time demand and supply information to dynamically adjust electricity prices, taking into account factors such as renewable energy generation, energy storage capacity, and load management strategies. The performance of the algorithm was evaluated through simulation studies and compared to traditional pricing methods. The results showed that the proposed algorithm was able to significantly improve the overall efficiency of the microgrid network and provide substantial economic benefits to both the operator and consumers. This study contributes to the development of smart grid technologies and provides a practical tool for optimizing electricity pricing in microgrid networks.","This paper introduces a new algorithm for optimal pricing of electricity in a smart microgrid network. The algorithm employs a multi objective optimization framework which takes into account both economic and technical aspects. Economic goals aim at maximizing revenue for the microgrid operator while technical goals seek to minimize total costs for consumers. Proposed algorithm uses real time information about demand and supply to adjust prices dynamically considering factors like renewable energy generation, storage capacities and load management strategies. Performance evaluation has been done using simulation studies and compared to traditional pricing methods. Results indicate that this proposed algorithm achieves significant improvements in efficiency of the microgrid system and provides strong financial benefits for both the operator and consumers. Contribution of this research includes advancement in smart grid technology and offering a practical method for pricing optimization in microgrid systems."
"In recent years, there has been growing interest in the study of computational complexity of various variants of combinatorial voter control problems in elections. Combinatorial voter control refers to the problem of manipulating the outcome of an election by making strategic changes to the votes of a subset of voters. This paper provides a comprehensive analysis of the computational complexity of several variants of this problem. First, we consider the classic voter control problem, where the goal is to make a minimum number of vote changes to ensure a desired candidate wins the election. We show that this problem is NP-hard, meaning that finding the optimal solution is computationally infeasible for large instances of the problem. Next, we explore the complexity of variants of this problem that take into account constraints such as budget limitations or restrictions on the number of vote changes that can be made to a single voter. We show that these variants are also NP-hard, and provide algorithms for finding approximate solutions to these problems. Finally, we consider the computational complexity of the voter control problem under different election systems, including single-winner elections, multi-winner elections, and proportional representation elections. We show that the complexity of the problem varies depending on the election system and provide algorithms for solving the problem under each system. This paper provides a comprehensive analysis of the computational complexity of various variants of combinatorial voter control problems in elections, and provides insights into the design of algorithms for solving these problems.","In recent years there has grown increasing interest in studying the computational complexity of different variants of combinatorial problems related to voting control in elections. Voting control refers to the task of influencing election outcomes through strategic changes to the votes of some subset of voters. We conduct a detailed analysis of the computational hardness of several variants of this problem. We start by considering the basic problem which aims to minimize changes in votes to secure a desired candidate's victory. We prove this problem is NP hard and hence computationally impractical for large instances. We then look at variants of this problem that include budget limits or restrictions on how many votes a single voter can change. We show these variants are also NP hard and present algorithms for finding approximate solutions. We also investigate computational hardness for different types of election systems like single winner, multiple winners and proportional representation and show complexity depends on system type and propose algorithms to solve the problem under each system. This paper therefore studies thoroughly the computational complexity of voting control variants and gives insight into designing algorithms."
"This paper presents a method for estimating subgraph frequencies from egocentrically sampled data, with or without attributes. The method is based on the observation that subgraph frequencies can be estimated using the product of the subgraph frequencies in the sampled data and the ratio of the number of subgraphs in the full graph to the number of subgraphs in the sampled data. The proposed method is evaluated on synthetic and real-world graphs and is shown to be effective in estimating subgraph frequencies with high accuracy. The results demonstrate the utility of the proposed method in various applications such as graph pattern mining and network analysis.",This paper proposes a method for estimating subgraph frequencies from data sampled from individual perspectives. This approach uses ratios of subgraph counts in full graphs compared to sampled data along with frequencies inside sampled data to estimate subgraph frequency. The approach is tested on both synthetic and real graphs and shows good performance in terms of accuracy. Results show usefulness of this approach for diverse tasks such as mining graph patterns and network analysis.
"The paper presents Visual Genome, a new platform that connects language and vision using dense image annotations obtained through crowdsourcing. The paper describes the process of annotating and creating a large-scale dataset of images with detailed scene descriptions, object and relationship annotations, and question-answer pairs. The authors evaluate the quality of the data by comparing it to existing datasets and demonstrate its potential for various computer vision and natural language processing tasks. They also discuss the challenges and future directions for the platform, including improving data quality and expanding the scope of annotations. The paper concludes that Visual Genome provides a valuable resource for research in both computer vision and natural language processing and has the potential to significantly advance the field.","This paper introduces Visual Genome as a new platform which combines language and vision through dense annotation from crowd workers. It details how the authors annotate large sets of images and create a database that contains detailed scene descriptions, object and relationship annotations along with question/answer pairs. Quality is assessed by comparing against other datasets and showing capability for different computer vision and NLP tasks. Challenges and future development are discussed; improvements to data quality and expansion of annotations are among them. In conclusion, this paper asserts that Visual Genome offers significant value for research in both fields and could drive progress forward quite a bit."
"This paper explores the use of distributed scheduling algorithms for event analytics in a hybrid edge-cloud environment. The objective is to balance the computation load between edge devices and the cloud while ensuring timely and accurate event analytics. The proposed approach utilizes a combination of task-splitting and load balancing algorithms to dynamically distribute the event analytics tasks based on the available resources at the edge and in the cloud. The experimental results demonstrate that the proposed approach effectively balances the computation load, reduces the latency, and improves the overall efficiency of the event analytics process. This research provides a practical solution for event analytics in edge-cloud environments, which is critical for many emerging Internet of Things (IoT) and cyber-physical systems applications.","This research investigates how to use distributed scheduling algorithms for event analytics in environments that mix edge devices and clouds. The goal is to fairly divide computational loads among both sides and ensure timely and precise analytics of events. A method combining splitting tasks and load balancing algorithms is used to dynamically distribute analytics work according to resource availability both at the edge and in the cloud. Experimental results show that this approach successfully balances computing loads, decreases latency, and enhances overall efficiency of analytics processes. Results indicate this research offers practical solutions to analytics in edge cloud environments which are important for various new IoT and cyber physical system applications."
"This paper presents a novel deep bilateral learning approach for real-time image enhancement. The proposed method leverages the power of deep learning to enhance the visual quality of an image while preserving important features such as edges and textures. The method uses a bilateral filter to enforce smoothness and preserve edges, and a deep neural network to learn a mapping from low-quality to high-quality images. Experiments on a benchmark dataset demonstrate that the proposed approach outperforms state-of-the-art image enhancement methods in terms of both visual quality and computational efficiency. The results of this research could have significant impact in a variety of applications, including photography, computer vision, and image processing.","This paper introduces a new deep bilateral learning approach for enhancing images in real time. Proposed approach makes use of deep learning to improve visual quality while retaining key features like edges and texture. Bilateral filtering is used to ensure smoothness and preservation of edges and deep neural nets are used to map low quality to high quality images. Results on a benchmark dataset show proposed approach excels visually and computationally compared to top methods. Results of this work have broad potential implications including photography, vision and image processing."
"This paper presents a framework for entity-specific ranking of knowledge base properties. The proposed method is evaluated on two case studies: one using a doctoral advisor dataset and the other using a medical condition dataset. The results demonstrate that the entity-specific ranking framework outperforms traditional methods in terms of accuracy and relevance. The findings have significant implications for improving the quality and performance of knowledge-based systems, particularly in domains such as academic advising and medical diagnosis. The extended version of the paper provides a more in-depth analysis of the proposed method and its implementation, along with a thorough evaluation of its effectiveness on the two case studies.",This paper introduces a framework for ranking specific entities based on knowledge base properties. Results from evaluations using two datasets are presented: one dataset uses data about advisors for PhD students and another uses data about medical conditions. Results show that the new ranking framework outperforms traditional methods in both accuracy and relevance. Results have significant implications for bettering overall system quality and performance especially in areas like academic advising and diagnosis of medical conditions. An extended version of this paper contains a detailed analysis of the proposed method and its implementation along with an extensive evaluation of effectiveness using both datasets.
"Color constancy is the ability of the human visual system to perceive the color of an object accurately under different lighting conditions. In computer vision, this is a challenging problem that has been addressed using various algorithms such as gray-world, white-patch, and shade-of-gray. However, these algorithms do not perform well in all scenarios and require significant computational resources. In this paper, we formulate camera-adaptive color constancy as a few-shot meta-learning problem. The proposed approach leverages a meta-learning framework to enable the model to quickly adapt to new cameras and lighting conditions using only a small number of examples. Experiments on a variety of datasets demonstrate that the proposed approach outperforms traditional color constancy algorithms in terms of accuracy and computational efficiency. This work provides a promising solution for camera-adaptive color constancy and has the potential to impact a wide range of applications in computer vision and image processing.","Color constancy is the capability of the visual system of humans to accurately perceive colors under diverse lighting conditions. In computer vision research this is a hard problem that has been tackled by applying various methods like gray world, white patch, and shade of gray. However, none of these methods performs consistently across all situations and they consume much computing power. We consider camera adaptation as a few shot meta learning problem. Proposed method uses a meta learning framework to enable quick adaptation of models to new cameras and lighting using just a few examples. Results on different datasets show the proposed approach excels traditional algorithms both in performance and efficiency. This study offers a promising solution for adaptive color constancy and opens avenues for application development in fields such as computer vision and image processing."
"This paper explores the concept of liveness in broadcast networks. Liveness refers to the real-time transmission of content, as opposed to pre-recorded or on-demand material. The paper examines how the notion of liveness has evolved over time, particularly in relation to the rise of digital media and the increasing use of social media platforms. The paper provides a comprehensive review of the literature on liveness, drawing on both theoretical and empirical studies from a range of disciplines. It also presents original research on the perception and interpretation of liveness among different audiences, using both qualitative and quantitative methods. The findings suggest that the definition of liveness is not fixed, but rather varies depending on cultural, technological, and social factors. Moreover, audiences may have different expectations and interpretations of liveness depending on the type of content being broadcast and the context in which it is consumed. The paper concludes by arguing that liveness remains a crucial aspect of broadcast media, particularly in the context of live events and breaking news. However, as digital media continues to evolve, the concept of liveness may need to be re-examined in order to account for new forms of engagement and interactivity.","This paper investigates the concept of live broadcasting in networks today. Live means real time transmission of content versus prerecorded or demand on demand stuff. We look at how this idea has changed over time especially with the growth of digital media and increased use of social media platforms. We review the literature comprehensively and mix theory and empirical studies from various disciplines. Original research also looks into audience perceptions and interpretations of live broadcasts using both qualitative and quantitative methods. Results show that definition of live is flexible and varies based on cultural, technological and social factors. Audiences may also have different views regarding what constitutes live depending on the content and context in which it is consumed. Conclusion: live broadcasting remains important especially for live events and breaking news; however, as digital media keeps evolving we might need to rethink live again in light of new forms of engagement and interaction."
"This paper presents a generative learning approach for spatio-temporal modeling in connected vehicular networks. The proposed approach combines deep learning techniques with generative models to effectively capture the complex spatial and temporal dependencies present in connected vehicular network data. The method is evaluated on a real-world dataset and demonstrates superior performance compared to traditional machine learning methods. The results of this study provide a promising direction for future research on spatio-temporal modeling in connected vehicular networks and have potential applications in traffic prediction and control, as well as in the design of intelligent transportation systems.",This paper introduces a new approach for generating learning that focuses on modeling spatial and temporal relationships for connected vehicles. This new approach blends deep learning techniques with generative models to effectively model complex dependency relationships that occur in datasets of connected vehicle networks. Results are based on real world data and show clear superiority over traditional machine learning approaches. Results of this research open up exciting avenues for further work on modeling spatial and temporal relationships and have potential use cases for traffic forecasting and control along with designing intelligent transportation systems.
"Adversarial attacks on deep neural networks have become a significant security concern in recent years. In this paper, we propose a new approach for defending against such attacks by utilizing the shape information of the input data. Our method consists of pre-processing the input data to preserve its original shape and then feeding it into a deep neural network that has been trained to recognize the shape. Our experiments show that this approach is effective in defending against various types of adversarial attacks, including white-box and black-box attacks, with a significant improvement in overall accuracy compared to traditional defense methods. Our results demonstrate the effectiveness of incorporating shape information in defending against adversarial attacks, and suggest that this approach has the potential to be a promising direction for future research in this field.",Recent years have seen deep neural networks facing serious security concerns due to adversarial attacks. This paper introduces a new approach for countering these attacks through exploiting information about shape of the inputs. We process the input data beforehand to retain its original shape and feed it into a network that has learned to identify shapes. Experiments reveal that our method works well against different kinds of attacks including both white box and black box ones; overall performance significantly improves over conventional defenses. Results indicate effectiveness of using shape information to counter attacks and point towards promising future research directions in this domain.
"Explanatory machine learning refers to the use of algorithms to understand and interpret the predictions made by machine learning models. While these algorithms can provide valuable insights into how models make decisions, they can also have harmful consequences if used improperly. This paper provides an overview of the beneficial and harmful aspects of explanatory machine learning, with a focus on the ethical implications of its use. On the beneficial side, explanatory machine learning can improve transparency and accountability in decision-making processes by providing clear and concise explanations of how models arrived at their predictions. This can help stakeholders understand and trust the decisions made by these models, leading to more effective and efficient outcomes. However, there are also harmful aspects to consider. Explanatory machine learning can perpetuate biases and reinforce discriminatory practices if it is trained on biased data. Additionally, the explanations generated by these algorithms can be misleading, leading to incorrect decisions based on incomplete or inaccurate information. The ethical implications of explanatory machine learning are significant, and it is important for practitioners to consider these implications when designing and deploying these algorithms. This paper concludes with a call for further research and discussion to ensure that the benefits of explanatory machine learning are realized while minimizing its potential harm.","Explanatory machine learning uses algorithms to explain predictions of machine learning models; both positive and negative results should be considered here. If applied incorrectly, this technique can cause harm. This paper examines both positives and negatives of using explanatory ML with emphasis on ethical concerns. On the plus side, explanatory ML enhances transparency and accountability by offering clear and succinct reasons for model outcomes. Stakeholders gain understanding and trust, resulting in better performance and efficiency. However, it poses risks as well. If ML models are trained on biased data, this technique could reproduce and even intensify discrimination. Moreover, explanations produced by such algorithms may be deceptive and lead to wrong decisions based on insufficient or inaccurate information. Ethical issues associated with explanatory ML are significant and practitioners need to think carefully about these issues when designing and deploying algorithms. Ultimately the paper calls for further study and discourse to realize benefits without increasing harms."
"This paper investigates the use of Local Interpretable Model-Agnostic Explanations with Outlier Detection (LIMEOUT) to improve the fairness of machine learning (ML) models. LIMEOUT is a recently developed approach that combines LIME, which explains the predictions of ML models at the local level, with outlier detection, which identifies instances that are far from the training data distribution. The paper demonstrates the effectiveness of LIMEOUT in improving the fairness of ML models by analyzing its performance on several benchmark datasets that exhibit different types of fairness-related challenges. The results show that LIMEOUT can significantly improve the fairness of ML models by identifying and mitigating bias in the predictions. Furthermore, the paper shows that LIMEOUT can be used to provide transparent and interpretable explanations for the decision-making process of ML models, which can enhance their trustworthiness and accountability. Overall, the paper suggests that LIMEOUT is a promising approach for making ML models fairer and more transparent, which can help mitigate the ethical concerns associated with their use in various domains.","This study uses Local Interpretability Model Agnostic Explanations with Outlier Detection (LIMEOUT) to improve fairness of machine learning (ML) models. Recently developed LIMEOUT integrates LIME that explains model predictions locally with outlier detection which detects points far from training distribution. Results show effectiveness of LIMEOUT in terms of fairness using benchmark datasets with diverse fairness challenges. Results show significant improvements in fairness due to identification and mitigation of bias in prediction decisions. Also, results indicate LIMEOUT can be used to explain decision making processes of ML models transparently and interpretably thus enhancing trustworthiness and accountability. In summary, this paper asserts that LIMEOUT is promising approach towards making ML models fair and transparent thereby reducing ethical concerns across domains."
"Permutations are widely used in various fields, such as cryptography, computer science, and mathematics. Understanding the relations between permutations is important for many applications, but determining these relations can be a challenging task. In this paper, we investigate the testability of relations between permutations. Specifically, we focus on the problem of testing whether two permutations are equal, inverse, or conjugate to each other, given access to their image and inverse images under some fixed set of points. We provide efficient algorithms for these testing problems and show that they have low query complexity, i.e., the number of queries to the input permutations needed by the algorithm is relatively small. Our algorithms are based on a combination of group theory and graph theory techniques, and their analysis involves tools from probabilistic combinatorics and algebraic geometry. We also provide lower bounds for the query complexity of these testing problems, showing that our algorithms are optimal up to constant factors. Our results have implications for the study of permutation groups and their applications in computer science and cryptography.","Permutations are used everywhere including cryptography and computer science and math. Relations among them are key for many applications but identifying these relations is hard. This paper looks at how to test those relations. Specifically, we study whether two permutations are equal, inverses or conjugates to each other and we give efficient algorithms for this testing. These algorithms use few queries to inputs; we use techniques from both group theory and graph theory and we use tools from probabilistic combinatorics and algebraic geometry to analyze these algorithms. We also prove lower bounds on the number of queries which shows our algorithms are nearly optimal up to small constants. Results of this work have impact on research into permutation groups and their relevance to CS and crypto."
"This paper presents a new approach to object re-identification (re-ID) using transformer-based models, referred to as TransReID. The proposed method leverages the self-attention mechanism of transformers to capture the long-range dependencies between feature maps and global contextual information. The experimental results demonstrate the superiority of TransReID over state-of-the-art re-ID methods on several benchmark datasets. The paper also analyzes the impact of different components of the TransReID framework, including the number of layers, the size of the model, and the use of additional data augmentation techniques. The findings provide insights into the effectiveness of the proposed method and its potential for improving the performance of re-ID tasks in practical applications.","This paper introduces a new approach to object re-identification by using transformer models and is called TransReID. The authors exploit the attention mechanisms within transformers to take advantage of long range relationships among features and to incorporate global context. Results show that TransReID outperforms other leading methods on various benchmark datasets. Authors also study how different aspects of the TransReID framework affect performance including number of layers used, model size, and usage of augmentations. Results shed light on effectiveness and potential improvements for practical use."
"This paper conducts a bibliometric analysis literature review to examine the research trends and patterns related to the digital agricultural revolution. The review covers studies published from 2011 to 2021 in several databases, including Scopus, Web of Science, and Google Scholar. The analysis focuses on the growth of research in this field, the most cited authors and publications, and the main research themes and trends. The findings suggest that research on the digital agricultural revolution has been growing steadily over the past decade, with a particular emphasis on precision agriculture, sensor technologies, and big data analytics. The most cited authors and publications are mainly from developed countries, indicating a significant gap in research from developing countries. The study highlights the need for further research to address the challenges and opportunities of the digital agricultural revolution, particularly in developing countries.","This study uses bibliometric analysis to conduct a review of literature on the research trends and patterns associated with the digital agricultural revolution. Reviews use data from Scopus, Web of Science and Google Scholar covering papers published between 2011 and 2021. Analysis looks at the rise of research on this topic, top cited authors and publications as well as key research themes and trends. Results indicate steady growth in research over the last decade with a focus on precision agriculture, sensors and big data analytics. Top cited researchers and papers are mostly from developed countries; this suggests an important gap in research from developing countries. This research also highlights future research needs to deal with challenges and opportunities of this digital agricultural revolution especially in developing countries."
"The Ethics and Society Review of Artificial Intelligence Research (ESR) is a comprehensive examination of the moral and ethical implications of artificial intelligence (AI) research and development. This paper provides an in-depth analysis of the current state of AI ethics, including discussions of the various ethical frameworks and principles that have been proposed to guide AI development, as well as the challenges and limitations of these frameworks. The paper also explores the role of social and political institutions in shaping AI ethics, including the role of governments, corporations, and non-governmental organizations. The paper concludes by presenting recommendations for future AI research, including the need for greater collaboration between AI researchers, ethicists, and society, and the importance of a proactive approach to AI ethics that takes into account the social, political, and economic impacts of AI on society.","Comprehensive review of ethics and society related to AI research is ESR. This paper analyzes current ethics of AI and looks at different frameworks and principles that have been put forward to guide development of AI. It also looks at the difficulties and limits of these frameworks. The paper also considers how social and political institutions shape AI ethics including roles of governments, companies and NGOs. In conclusion the paper recommends future research should involve greater collaboration among AI researchers, ethicists and society and emphasize proactive approach to ethics of AI considering impact of AI on society politically, socially and economically."
"The increasing need for continuous environmental monitoring has led to the development of energy-efficient sensor nodes that can harvest energy from the environment. In this paper, we present eico, an energy-harvesting long-range environmental sensor node that co-optimizes energy and information. Eico incorporates a number of novel features including an adaptive power management system that optimizes the energy consumption of the node, a dynamic sensing module that adjusts the sensing frequency based on the level of activity in the environment, and a machine learning algorithm that predicts energy availability and adjusts the sensing frequency accordingly. We evaluate the performance of eico through a series of experiments, and demonstrate that eico achieves a longer lifetime and higher sensing accuracy than existing energy-harvesting sensor nodes. Our results suggest that eico has the potential to be a game-changer in the field of environmental monitoring, enabling continuous and accurate data collection with minimal energy consumption.","Growing demand for continuous monitoring of the environment has resulted in developing energy efficient sensor nodes that can harvest energy from surroundings. This paper introduces eico, an energy harvesting node designed to optimize both energy and information. Eico includes several new features such as adaptive power management which minimizes energy consumption of the node; a flexible sensing module whose sensing frequency is adjusted according to activity levels in the environment; and a learning algorithm that forecasts energy availability and automatically adjusts sensing frequency. Performance evaluations show eico outperforms other energy harvesting sensors by achieving much longer lifetimes and high accuracy in sensing. Results indicate that eico is promising for revolutionizing environmental monitoring tasks because it collects continuous and accurate data using very little energy."
"This paper presents a new approach to modelling epistemic uncertainty using Bayesian confidence calibration. Epistemic uncertainty arises from a lack of knowledge or understanding about a system, which can result in significant errors when making predictions. Bayesian methods have been used extensively in the literature to model epistemic uncertainty, but they can suffer from overconfidence when the model is misspecified or when the data is limited. To address this issue, we propose a new method based on Bayesian confidence calibration, which aims to better calibrate the uncertainty estimates produced by Bayesian models. Specifically, we introduce a calibration function that adjusts the uncertainty estimates produced by the Bayesian model based on the reliability of the data and the degree of model misspecification. We demonstrate the effectiveness of our approach on synthetic and real-world datasets and compare it to other popular methods for modelling epistemic uncertainty. Our results show that our approach produces more accurate and reliable uncertainty estimates, which can be valuable for decision-making under uncertainty. We conclude by discussing the potential applications of our method in various domains, including finance, engineering, and healthcare.","This paper introduces a new way to model uncertainty due to lack of knowledge or understanding through Bayesian calibration. Such uncertainty causes significant prediction errors. Calibration has been used often for modeling this type of uncertainty but there are problems such as high confidence despite wrong specification or limited data. Therefore, we introduce a new method using Bayesian calibration aiming to improve the calibration of uncertainty estimates from Bayesian models. Specifically, we introduce a calibration function that adjusts uncertainty estimates from Bayesian models based on data reliability and degree of misspecification. Results show that this new method generates more accurate and reliable estimates compared to other leading approaches for uncertainty modeling. Applications of this new method are also discussed including finance, engineering and health care. Conclusions include advantages of using this approach."
"This paper investigates the improvement of loadability in distribution systems using genetic algorithm optimization techniques. The aim of the research is to optimize the load transfer between distribution feeders and improve the overall system loadability. The study uses a realistic distribution system model and proposes a genetic algorithm-based approach to solve the optimization problem. The proposed algorithm is applied to a range of load profiles, and its effectiveness is evaluated based on the improvement in loadability achieved. The results show that the genetic algorithm approach can effectively improve load transfer and increase the loadability of the distribution system. The study also compares the performance of the proposed approach with other existing methods and shows that the genetic algorithm-based approach outperforms them in terms of loadability improvement. The findings of this study can be used by distribution system operators to optimize their load transfer strategies and improve system performance.","This paper looks at how we can improve load transfer capability in distribution systems through use of genetic algorithms. The main goal is to maximize overall system loadability. Using realistic models of distribution systems, the authors propose an approach using genetic algorithms to solve optimization problems. Results from this method are tested against different load profiles and performance is judged on improved loadability. Results indicate that this genetic algorithm approach successfully improves load transfer and increases loadability of systems. Performance comparisons to other methods are made and results show that using genetic algorithms leads to better improvements in loadability. Results from this work could be useful for distribution system operators to refine load transfer strategy and enhance performance."
"This paper presents an analysis of the throughput performance of wireless powered cognitive radio networks (WPCRNs) using compressive sensing (CS) and matrix completion (MC) techniques. The study focuses on the utilization of surplus energy from primary users to power secondary users in WPCRNs and the impact of different system parameters, such as the number of primary and secondary users, the compression ratio of CS and the MC rank, on the overall throughput performance of the system. The authors use simulations to evaluate the proposed approach and compare its performance with existing methods. The results show that the combination of CS and MC techniques can significantly enhance the throughput performance of WPCRNs and provide better energy efficiency compared to traditional methods. The findings of this study can be useful for future research in the field of WPCRNs and provide insights into the design of efficient and effective communication systems.","This paper studies throughput performance of wireless powered cognitive radio networks (WPCRNs) via CS and MC methods. The work considers how excess energy from main users is used to power secondary users in WPCRNs and looks at the influence of key parameters including numbers of primary and secondary users, CS compression ratios and MC ranks on system throughput performance. Results are obtained by simulation and compared against current methods. Results indicate that integration of CS and MC methods greatly improves performance and also leads to higher energy efficiency. Results here contribute to further research in this area and offer insight into designing good communications systems."
"This paper investigates the Conditional Lucas & Kanade (CLK) algorithm, a popular computer vision technique for estimating the optical flow between two frames in a video sequence. Specifically, we propose a novel extension to the original Lucas & Kanade (LK) algorithm by incorporating a spatially varying weighting matrix that conditions the computation of the optical flow on local image features. This extension improves the accuracy and robustness of the original LK algorithm by accounting for variations in the image data, such as illumination changes and occlusions. To evaluate the performance of the proposed CLK algorithm, we conducted experiments on several standard benchmark datasets and compared its results to those of other state-of-the-art optical flow methods. Our experiments demonstrate that the CLK algorithm achieves superior accuracy and robustness compared to the original LK algorithm and other competing methods, particularly in challenging scenarios with large displacements and occlusions. Furthermore, we demonstrate the applicability of the CLK algorithm in real-world applications such as visual odometry, where accurate and robust estimation of the motion of a moving camera is essential. Our results show that the CLK algorithm outperforms other optical flow methods in terms of both accuracy and computational efficiency. Overall, our research highlights the effectiveness of the proposed CLK algorithm as an improved version of the original LK algorithm, and its potential to advance the field of computer vision in various real-world applications.","This paper investigates the Conditional Lucas and Kanade (CLK) method which is widely used in computer vision for computing optical flow between frames of videos. We introduce a new development of this method by integrating a spatially variable weight matrix that conditions flow calculation based on local features of images. This development improves the accuracy and robustness of the original Lucas and Kanade (LK) method through taking into account variations in image data like lighting changes and occlusions. Performance evaluation was conducted using different standard benchmarks and results were compared against other leading methods; results showed that the CLK method performs better than LK method and others especially under difficult conditions with high displacements and occlusions. Moreover, we show that CLK works well in practical uses such as visual odometry where precise and reliable estimation of camera movement is necessary. Results indicate that CLK excels other methods both in accuracy and computational efficiency. In summary, our work emphasizes the advantages of the proposed CLK method as an improvement over LK and its promise for diverse practical applications in computer vision."
"This paper presents a new dataset called ""Amigos"" that is designed for affect, personality, and mood research on individuals and groups. The dataset consists of audio recordings, video recordings, and self-reported questionnaires collected from a diverse sample of participants in various social contexts. The paper provides a detailed description of the dataset, including the data collection procedures, the measures of affect, personality, and mood, and the ethical considerations involved in the study. The paper also presents several examples of how the dataset can be used for research in different areas such as social psychology, affective computing, and human-computer interaction. Overall, the Amigos dataset provides a valuable resource for researchers interested in studying affect, personality, and mood in both individual and social contexts.","This paper introduces a new dataset named ""Amigos"" that aims at research on affect, personality and mood for both individuals and groups. Data are collected via recordings of audio and video along with self report questionnaires from diverse participants interacting in varied contexts. Detailed descriptions are given about the dataset including collection procedures, measures for affect, personality and mood together with ethical considerations. The paper also gives some examples of use of this dataset in different fields such as social psychology, affective computing and HCI. Ultimately, Amigos dataset serves as a useful resource for researchers who are interested in conducting research related to affect, personality and mood at individual and social levels."
"This paper investigates the connectivity properties of random annulus graphs and the geometric block model. These models are commonly used in network science to model real-world networks with underlying geometric structure. In particular, we focus on the critical threshold for connectivity, which is the probability at which the network transitions from being disconnected to being connected. We first introduce the random annulus graph, which is a random graph generated by connecting points in an annulus with probability determined by the Euclidean distance between the points. We prove that the critical threshold for connectivity in the random annulus graph is determined by a simple function of the parameters of the annulus. Next, we consider the geometric block model, which is a random graph generated by partitioning a d-dimensional space into blocks and connecting points within the same block with some probability. We show that the critical threshold for connectivity in the geometric block model is determined by the product of the block probabilities and the volume of the blocks. Finally, we compare the critical thresholds for connectivity in the random annulus graph and the geometric block model. We demonstrate that the critical threshold in the geometric block model is always greater than or equal to that in the random annulus graph. This result has important implications for the design and analysis of networked systems with underlying geometric structure, such as wireless sensor networks and social networks. Overall, this paper provides important insights into the connectivity properties of random graphs with underlying geometric structure.","This paper looks at connectivity properties of random annuli graphs and the geometric block model. Both these models are frequently used in network science for modeling networks that have underlying geometric structure. Specifically, we focus on determining critical threshold probabilities at which networks go from disconnected to connected. We start by introducing random annuli graphs: they consist of randomly placing points inside an annulus and connecting those points if the Euclidean distance between them meets a certain probability criterion. We prove that connectivity critical threshold depends simply on parameters of the annulus. We then consider the geometric block model: here we split a d dimensional space into blocks and connect points within the same block with a fixed probability. We show that connectivity critical threshold depends on product of block probabilities and block volume. Lastly we compare critical thresholds for connectivity of random annuli graphs and geometric block model and find that critical threshold for model based on blocks is always higher than or equal to that of the annuli graph. Results have major implications for designing and analyzing systems that include networks with embedded geometry like wireless sensor networks and social networks. Overall this work reveals key features of connectivity in random graphs having underlying geometric features."
"The Fast Fourier Transform (FFT) is a crucial tool in a wide range of scientific and engineering applications, including signal processing, image processing, and scientific simulations. As the size of the data increases, the computational cost of computing the FFT also increases, making it necessary to develop efficient parallel algorithms for the FFT. In this paper, we present a new parallel algorithm for the multidimensional FFT using Advanced Message Passing Interface (MPI) on high-performance computing systems. We propose a two-stage approach that combines a distributed 1D FFT algorithm with a block-cyclic data distribution scheme. Our algorithm exploits the properties of the block-cyclic distribution to minimize the communication overhead and balance the workload among the processors. We also use the latest features of MPI to optimize the communication and reduce the synchronization overhead. We demonstrate the performance and scalability of our algorithm on a range of hardware platforms, including large-scale clusters and supercomputers. Our experiments show that our algorithm achieves near-linear speedup up to thousands of processors, and outperforms existing parallel FFT libraries for large-scale data. Our approach is highly efficient and can significantly reduce the computation time for the multidimensional FFT, enabling faster and more accurate scientific simulations and data analysis in various fields.","The Fast Fourier Transform (FFT) is an important tool used across many different fields such as signal processing, image processing and scientific simulations. When dealing with larger datasets, computation costs increase and hence efficient parallel algorithms are needed to compute FFT. This paper introduces a new parallel algorithm based on MPI for multidimensional FFT. We use a two stage approach that uses a distributed 1D FFT along with a scheme for block cyclic distribution of data. We exploit the advantages of this cyclic distribution to reduce communication overhead and balance workloads among processors. We use recent features of MPI to improve communication and minimize synchronization overheads. Performance and scalability results were obtained on diverse hardware including large clusters and supercomputers. Experiments showed our algorithm achieves near linear speedup up to thousands of processors and performs better than other parallel FFT libraries for large data sets. Overall we have developed an efficient method which greatly reduces computation time and thus enhances faster and more accurate simulations and analyses in diverse areas."
"This paper presents an approximate dynamic programming (ADP) approach to community recovery management following a natural disaster. Traditional recovery management approaches have focused on pre-defined and static recovery plans, which may not be applicable or effective in dynamic and complex post-disaster scenarios. The proposed ADP approach is designed to learn from past disaster events and dynamically adapt recovery strategies based on the evolving state of the affected community. The ADP approach combines data-driven machine learning techniques with a dynamic optimization framework to provide adaptive decision-making capabilities for recovery management. The framework considers multiple objectives, such as minimizing recovery time, maximizing resource utilization, and minimizing costs, while accounting for the uncertainties and constraints associated with the recovery process. The ADP approach is also able to incorporate feedback from the community and recovery stakeholders, enabling the system to learn from its own performance and improve over time. To evaluate the effectiveness of the ADP approach, a case study is conducted on a hypothetical community impacted by a major earthquake. The results demonstrate that the proposed approach is able to provide effective recovery strategies that outperform traditional static recovery plans. Moreover, the ADP approach is able to quickly adapt to changes in the community and recovery environment, enabling efficient and effective recovery management. Overall, this research highlights the potential benefits of applying ADP to community recovery management, providing decision-makers with a powerful tool to better prepare for and respond to natural disasters. The ADP approach is able to learn from past experiences, dynamically adapt to changes in the recovery environment, and effectively balance multiple recovery objectives, ultimately resulting in more resilient and sustainable communities.","This paper introduces an approach using Approximate Dynamic Programming (ADP) for disaster recovery management. Traditional methods typically rely on fixed and unchanging plans that do not work well in complex and changing situations after disasters occur. Proposed here is an ADP approach that learns from past disasters and adapts dynamically to changes in the status of affected communities. Combining machine learning techniques with a dynamic optimization framework, ADP provides flexible decision making capabilities for recovery management. This framework takes into account various goals such as reducing recovery time, increasing resource use, and reducing cost. It also considers uncertainties and limits related to recovery processes. The ADP approach incorporates feedback from community members and stakeholders so the system can learn and improve performance over time. Results from a case study show that the new method performs much better than traditional static recovery plans. Furthermore, ADP quickly responds to shifts in the community and recovery conditions and manages recovery efficiently and effectively. In sum, this research demonstrates the advantage of using ADP for disaster recovery management and offers policymakers strong tools to better handle natural disasters. ADP excels at adapting to past lessons learned, balancing diverse recovery objectives, and producing more resilient and sustainable communities."
"This paper proposes a novel approach for optimizing the trajectories of cooperative dual-band UAV swarms in order to improve their overall performance. The proposed approach leverages the unique capabilities of dual-band UAVs, which can operate in two different frequency bands simultaneously, to enhance the swarm's communication and sensing capabilities. The trajectory optimization problem is formulated as a multi-objective optimization problem that considers several objectives, including minimizing the total time of the mission, minimizing the energy consumption of the UAVs, and maximizing the quality of the communication and sensing links between the UAVs. The proposed approach utilizes a genetic algorithm to search for the optimal trajectories that satisfy the specified objectives. The paper presents simulation results that demonstrate the effectiveness of the proposed approach in improving the performance of the cooperative dual-band UAV swarm. The simulation results show that the proposed approach can significantly reduce the mission time and energy consumption while also improving the quality of the communication and sensing links. The paper also discusses the practical implications of the proposed approach and identifies several areas for future research. Overall, this research contributes to the growing body of literature on cooperative UAV swarms and provides a promising approach for optimizing the trajectories of dual-band UAVs in real-world applications.",This paper introduces a new method for optimizing trajectories of cooperative UAV swarms operating at dual frequencies that aims to improve performance overall. The paper takes advantage of the special features of dual frequency UAVs to boost both communication and sensing capability. Formulating the trajectory optimization as a multi objective problem the authors consider various goals such as reducing mission time and saving energy of the UAVs and improving quality of communication and sensing among them. Genetic algorithms are used to search for optimal trajectories that meet these goals. Results from simulations show the effectiveness of the introduced approach and demonstrate reductions in mission time and energy consumption along with higher quality of communication and sensing. Practical implications are discussed and some avenues for future research are suggested. In summary this research advances knowledge of cooperative UAV swarms and offers promising ways to optimize dual frequency UAV trajectories for real world applications.
"Reinforcement learning (RL) is a popular machine learning approach for decision making problems, where an agent learns from the consequences of its actions to maximize a reward signal. However, in many real-world applications, the reward signal is subject to noise or perturbations, which can lead to suboptimal or undesirable behavior. This paper presents a comprehensive study of RL with perturbed rewards, focusing on two main aspects: (1) the impact of reward perturbations on the learning process and the performance of RL algorithms, and (2) the design of robust RL algorithms that are able to handle reward perturbations effectively. The study explores different forms of reward perturbations, such as Gaussian noise, stochastic reward functions, and reward tampering attacks, and evaluates various RL algorithms, including value-based, policy-based, and actor-critic methods, under different perturbation scenarios. The results show that reward perturbations can have a significant impact on the learning process and the performance of RL algorithms, and highlight the importance of designing robust RL algorithms that can effectively handle reward perturbations. The paper provides insights and guidelines for future research in this area and has implications for a wide range of applications, including robotics, gaming, autonomous systems, and more.","Reinforcement learning (RL) is one of the most used approaches for decision making tasks; agents learn through their actions and aim to maximize the reward signals. However, in practice the reward signal often contains noise or disturbances which may result in suboptimal or poor behavior. This work studies RL with perturbed rewards and focuses on two key aspects: (1) how perturbations affect learning and performance of different RL algorithms and (2) designing robust RL algorithms that cope well with perturbations. We consider different types of perturbations like Gaussian noise and random reward functions and we test different kinds of algorithms such as value based ones and actor critics under different conditions. Results suggest that perturbations can substantially affect learning and algorithm performance and highlight that robustness against perturbations is important. The paper also gives ideas for future research and implications span across diverse areas including robotics, gaming and autonomous systems among others."
"This paper presents a lower bound for the Boolean Satisfiability (SAT) problem on deterministic Turing machines. The SAT problem is a fundamental problem in computer science, which asks whether a given boolean formula can be satisfied by some assignment of truth values to its variables. The lower bound established in this paper states that any deterministic Turing machine algorithm that solves the SAT problem requires at least 2^n / poly(n) time, where n is the number of variables in the input formula and poly(n) is some polynomial function of n. To prove this lower bound, the paper introduces a novel technique for constructing hard instances of the SAT problem that require a large number of steps to solve on a Turing machine. The technique is based on the notion of ""expanding"" formulas, which involves adding redundant clauses to the formula in such a way that any satisfying assignment must satisfy a large subset of the added clauses. The paper then shows that any deterministic Turing machine algorithm for the SAT problem must spend a significant amount of time checking these redundant clauses, which leads to the lower bound. The significance of this result is that it provides a strong lower bound for the SAT problem on deterministic Turing machines, which is a widely studied problem in theoretical computer science. The lower bound implies that there is a fundamental limit to the efficiency of any deterministic algorithm for solving the SAT problem, which has important implications for the design and analysis of algorithms in general. The paper concludes by discussing some potential avenues for future research based on the techniques developed in this work.",This paper establishes a lower bound for deterministic Turing machine algorithms that solve the Boolean Satisfiability (SAT) problem. SAT is a core question in computer science asking if there exists an assignment of truth values that satisfies a given Boolean formula. This paper asserts that any deterministic Turing machine solving SAT requires at least time 2^n divided by some polynomial function p(n) of n; here n denotes the number of variables in the input formula and p(n) is some polynomial. Proving this lower bound uses a new technique for constructing hard SAT instances requiring many steps. Expanding formulas are used here: extra clauses are added redundantly so any solution must satisfy a large part of them. Showing that deterministic algorithms need to check these extra clauses very slowly proves the bound. Results are important because they set a strong lower limit for efficient deterministic solutions to SAT: this problem has been extensively studied in theoretical computer science. Lower bounds imply there is a fundamental barrier to efficiency for deterministic algorithms generally. Concluding remarks discuss further research directions suggested by this work.
"In this paper, we present a parallel implementation of the Ensemble Kalman Filter (EnKF) based on a modified Cholesky decomposition. The EnKF is a popular data assimilation method that uses a Monte Carlo approach to estimate the state of a complex dynamic system. However, its implementation can be computationally expensive, especially for large-scale problems. To address this issue, we propose a parallel implementation of the EnKF that leverages the modified Cholesky decomposition to distribute the computation across multiple processors. The modified Cholesky decomposition is a matrix factorization method that provides an efficient way to compute the square root of a positive-definite matrix. By using this technique, our parallel implementation of the EnKF is able to significantly reduce the computation time without sacrificing accuracy. The performance of the parallel implementation is evaluated using numerical experiments, and the results show that it is able to achieve substantial speed-ups compared to a sequential implementation of the EnKF. This work has implications for a wide range of applications, including weather forecasting, climate modeling, and seismic imaging, where the EnKF is used to process large amounts of data in real-time.","We present here an implementation of Ensemble Kalman Filter (EnKF) parallelized through use of a modified Cholesky decomposition. EnKF is a method widely used for data assimilation that applies Monte Carlo techniques to estimate states of complex systems; however, it is computationally expensive for large scale problems. We solve this by proposing a parallel implementation of EnKF that distributes computation among different processors. Modified Cholesky decomposition is a matrix factorization method that enables fast computation of square root of positive definite matrices; we use this to reduce computation time while maintaining accuracy. Performance of parallel implementation is assessed via numerical experiments; results show that it achieves significant speed ups relative to sequential implementation. Results have broad applicability, including for tasks like weather forecasting and seismic imaging where high data throughput is needed."
"The increasing popularity of YouTube as a platform for content sharing has raised concerns about the potential risks to children from exposure to harmful or inappropriate content. This paper focuses on the detection, characterization, and analysis of child unsafe content and promoters on YouTube. We conducted a large-scale analysis of the Kidstube dataset, consisting of over 100,000 videos, to identify content that poses a risk to children, including violent, sexual, and misleading content. We also analyzed the behavior of promoters who exploit children's vulnerabilities for their benefit, including through the use of deceptive advertising and manipulation of search algorithms. Our results provide insights into the prevalence of child unsafe content on YouTube, the tactics used by promoters to attract young audiences, and the potential impact on children's wellbeing. The findings highlight the need for improved detection and removal of harmful content, as well as better regulation of advertising practices on the platform to protect children from harm.","As YouTube has become increasingly popular for sharing content, there are growing concerns regarding risks for children due to exposure to dangerous or inappropriate material. This paper focuses on identifying and analyzing unsafe content and those who profit from children by exploiting them. We examined a large dataset of videos containing over 100 thousand items to find content that endangers children such as violence, sexuality, and deception. We also looked at how profiteers manipulate children by using deceptive advertising and search algorithms. Results show prevalence of dangerous content on YouTube, tactics used by profiteers to target younger viewers and potential consequences for children. Findings underscore necessity for improvements in detection and removal of harmful content alongside better regulation of ads to safeguard children from harm."
This paper focuses on the proof search specifications of bisimulation and modal logics for the -calculus. The -calculus is a formal system for modeling the dynamic behavior of distributed systems. The paper examines the use of bisimulation and modal logics as proof search specifications for verifying the correctness of -calculus models. The authors present a detailed analysis of the proof search algorithms and their implementation in the context of the -calculus. The results of the study demonstrate the effectiveness of using bisimulation and modal logics as proof search specifications for verifying the correctness of -calculus models. The findings of the research have important implications for the design and implementation of verification tools for distributed systems.,This paper looks at specification proofs of bisimulation and modal logic for  calculus.  calculus is a formal system for dynamic modeling of distributed systems. The paper studies how bisimulation and modal logic are used as proof search specifications to verify correctness of  calculus models. Authors analyze carefully the algorithms of proof search in context of  calculus and implement them. Results show that bisimulation and logic are effective to use for verification correctness of model. Findings from this research are significant for designing and implementing tools for verification of distributed systems.
