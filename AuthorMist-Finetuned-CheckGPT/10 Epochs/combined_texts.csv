text,label,source
"Since important work by Erdos and Kleitman in 1971, research has continued into the theory of matchings in set families. We study two core issues here: one about finding as many matches as possible (maximum matching) and another about finding the largest match that can't be expanded further (maximal matching). We present an improved algorithm for maximum matching; this new algorithm finds better upper bounds on the number of edges matched compared to previous methods. This new algorithm runs faster and is thus practical for large sets of families. For maximal matching we show this is closely tied to combinatorial structure within families and we define a new metric called maximal matching degree. We establish new upper bounds for this degree through this metric and these bounds have implications regarding existence of maximal matchings in some classes of families. Results here build on work by Erdos and Kleitman and we discover new insights into matching structures. These results have relevance in diverse areas including computer science and operations research.",1,AI
"The field of Artificial Intelligence (AI) and, in particular, the Machine Learning area, counts on a wide range of performance metrics and benchmark data sets to assess the problem-solving effectiveness of its solutions. However, the appearance of research centres, projects or institutions addressing AI solutions from a multidisciplinary and multi-stakeholder perspective suggests a new approach to assessment comprising ethical guidelines, reports or tools and frameworks to help both academia and business to move towards a responsible conceptualisation of AI. They all highlight the relevance of three key aspects: (i) enhancing cooperation among the different stakeholders involved in the design, deployment and use of AI; (ii) promoting multidisciplinary dialogue, including different domains of expertise in this process; and (iii) fostering public engagement to maximise a trusted relation with new technologies and practitioners. In this paper, we introduce the Observatory on Society and Artificial Intelligence (OSAI), an initiative grew out of the project AI4EU aimed at stimulating reflection on a broad spectrum of issues of AI (ethical, legal, social, economic and cultural). In particular, we describe our work in progress around OSAI and suggest how this and similar initiatives can promote a wider appraisal of progress in AI. This will give us the opportunity to present our vision and our modus operandi to enhance the implementation of these three fundamental dimensions.",0,Human
"In this paper, we present the design, implementation and our year-long maintenance experience of SNSAPI, a Python-based middleware which unifies the interfaces and data structures of heterogeneous Social Networking Services (SNS). Unlike most prior works, our middleware is user-oriented and requires zero infrastructure support. It enables a user to readily conduct online social activities in a programmable, cross-platform fashion while gradually reducing the dependence on centralized Online Social Networks (OSN). More importantly, as the SNSAPI middleware can be used to support decentralized social networking services via conventional communication channels such as RSS or Email, it enables the deployment of Decentralized Social Networks (DSN) in an incremental, ad hoc manner. To demonstrate the viability of such type of DSNs, we have deployed an experimental 6000-node SNSAPI-based DSN on PlanetLab and evaluate its performance by replaying traces of online social activities collected from a mainstream OSN. Our results show that, with only mild resource consumption, the SNSAPI-based DSN can achieve acceptable forwarding latency comparable to that of a centralized OSN. We also develop an analytical model to characterize the trade-offs between resource consumption and message forwarding delay in our DSN. Via 20 parameterized experiments on PlanetLab, we have found that the empirical measurement results match reasonably with the performance predicted by our analytical model.",0,Human
"Fueled by the rapid development of communication networks and sensors in portable devices, today many mobile users are invited by content providers to sense and send back real-time useful information (e.g., traffic observations and sensor data) to keep the freshness of the providers' content updates. However, due to the sampling cost in sensing and transmission, an individual may not have the incentive to contribute the real-time information to help a content provider reduce the age of information (AoI). Accordingly, we propose dynamic pricing for the provider to offer age-dependent monetary returns and encourage users to sample information at different rates over time. This dynamic pricing design problem needs to balance the monetary payments to users and the AoI evolution over time, and is challenging to solve especially under the incomplete information about users' arrivals and their private sampling costs. For analysis tractability, we linearize the nonlinear AoI evolution in the constrained dynamic programming problem, by approximating the dynamic AoI reduction as a time-average term and solving the approximate dynamic pricing in closed-form. Then, we estimate this approximate term based on Brouwer's fixed-point theorem. Finally, we provide the steady-state analysis of the optimized approximate dynamic pricing scheme for an infinite time horizon, and show that the pricing scheme can be further simplified to an e-optimal version without recursive computing over time.",0,Human
"This paper introduces Lamassu, a method for secure storage of sensitive information at the client side that reduces storage requirements. Lamassu secures confidentiality, integrity and authentication of data. Lamassu uses a new approach combining authenticated encryption with inline encryption. This approach enables encryption along with creation of data without requiring extra storage space for encrypted data. Data is also protected from tampering during storage and transmission. Results show that Lamassu achieves good performance in terms of storage efficiency, speed and security. The authors also describe some practical uses for Lamassu such as cloud storage and communications over networks. Overall this study indicates that Lamassu has great promise as an efficient method of host side storage encryption with broad application potential.",1,AI
"Successful design of human-in-the-loop control systems requires appropriate models for human decision makers. Whilst most paradigms adopted in the control systems literature hide the (limited) decision capability of humans, in behavioral economics individual decision making and optimization processes are well-known to be affected by perceptual and behavioral biases. Our goal is to enrich control engineering with some insights from behavioral economics research through exposing such biases in control-relevant settings. This paper addresses the following two key questions: 1) How do behavioral biases affect decision making? 2) What is the role played by feedback in human-in-the-loop control systems? Our experimental framework shows how individuals behave when faced with the task of piloting an UAV under risk and uncertainty, paralleling a real-world decision-making scenario. Our findings support the notion of humans in Cyberphysical Systems underlying behavioral biases regardless of -- or even because of -- receiving immediate outcome feedback. We observe substantial shares of drone controllers to act inefficiently through either flying excessively (overconfident) or overly conservatively (underconfident). Furthermore, we observe human-controllers to self-servingly misinterpret random sequences through being subject to a ""hot hand fallacy"". We advise control engineers to mind the human component in order not to compromise technological accomplishments through human issues.",0,Human
"This paper studies radio propagation mechanisms that impact handoffs, air interface design, beam steering, and MIMO for 5G mobile communication systems. Knife edge diffraction (KED) and a creeping wave linear model are shown to predict diffraction loss around typical building objects from 10 to 26 GHz, and human blockage measurements at 73 GHz are shown to fit a double knife-edge diffraction (DKED) model which incorporates antenna gains. Small-scale spatial fading of millimeter wave received signal voltage amplitude is generally Ricean-distributed for both omnidirectional and directional receive antenna patterns under both line-of-sight (LOS) and non-line-of-sight (NLOS) conditions in most cases, although the log-normal distribution fits measured data better for the omnidirectional receive antenna pattern in the NLOS environment. Small-scale spatial autocorrelations of received voltage amplitudes are shown to fit sinusoidal exponential and exponential functions for LOS and NLOS environments, respectively, with small decorrelation distances of 0.27 cm to 13.6 cm (smaller than the size of a handset) that are favorable for spatial multiplexing. Local area measurements using cluster and route scenarios show how the received signal changes as the mobile moves and transitions from LOS to NLOS locations, with reasonably stationary signal levels within clusters. Wideband mmWave power levels are shown to fade from 0.4 dB/ms to 40 dB/s, depending on travel speed and surroundings.",0,Human
"Let $G=(V,E)$ be an $n$-nodes non-negatively real-weighted undirected graph. In this paper we show how to enrich a {\em single-source shortest-path tree} (SPT) of $G$ with a \emph{sparse} set of \emph{auxiliary} edges selected from $E$, in order to create a structure which tolerates effectively a \emph{path failure} in the SPT. This consists of a simultaneous fault of a set $F$ of at most $f$ adjacent edges along a shortest path emanating from the source, and it is recognized as one of the most frequent disruption in an SPT. We show that, for any integer parameter $k \geq 1$, it is possible to provide a very sparse (i.e., of size $O(kn\cdot f^{1+1/k})$) auxiliary structure that carefully approximates (i.e., within a stretch factor of $(2k-1)(2|F|+1)$) the true shortest paths from the source during the lifetime of the failure. Moreover, we show that our construction can be further refined to get a stretch factor of $3$ and a size of $O(n \log n)$ for the special case $f=2$, and that it can be converted into a very efficient \emph{approximate-distance sensitivity oracle}, that allows to quickly (even in optimal time, if $k=1$) reconstruct the shortest paths (w.r.t. our structure) from the source after a path failure, thus permitting to perform promptly the needed rerouting operations. Our structure compares favorably with previous known solutions, as we discuss in the paper, and moreover it is also very effective in practice, as we assess through a large set of experiments.",0,Human
"The nuclear norm minimization (NNM) is commonly used to approximate the matrix rank by shrinking all singular values equally. However, the singular values have clear physical meanings in many practical problems, and NNM may not be able to faithfully approximate the matrix rank. To alleviate the above-mentioned limitation of NNM, recent studies have suggested that the weighted nuclear norm minimization (WNNM) can achieve a better rank estimation than NNM, which heuristically set the weight being inverse to the singular values. However, it still lacks a rigorous explanation why WNNM is more effective than NMM in various applications. In this paper, we analyze NNM and WNNM from the perspective of group sparse representation (GSR). Concretely, an adaptive dictionary learning method is devised to connect the rank minimization and GSR models. Based on the proposed dictionary, we prove that NNM and WNNM are equivalent to L1-norm minimization and the weighted L1-norm minimization in GSR, respectively. Inspired by enhancing sparsity of the weighted L1-norm minimization in comparison with L1-norm minimization in sparse representation, we thus explain that WNNM is more effective than NMM. By integrating the image nonlocal self-similarity (NSS) prior with the WNNM model, we then apply it to solve the image denoising problem. Experimental results demonstrate that WNNM is more effective than NNM and outperforms several state-of-the-art methods in both objective and perceptual quality.",0,Human
"Growing demand for continuous monitoring of the environment has resulted in developing energy efficient sensor nodes that can harvest energy from surroundings. This paper introduces eico, an energy harvesting node designed to optimize both energy and information. Eico includes several new features such as adaptive power management which minimizes energy consumption of the node; a flexible sensing module whose sensing frequency is adjusted according to activity levels in the environment; and a learning algorithm that forecasts energy availability and automatically adjusts sensing frequency. Performance evaluations show eico outperforms other energy harvesting sensors by achieving much longer lifetimes and high accuracy in sensing. Results indicate that eico is promising for revolutionizing environmental monitoring tasks because it collects continuous and accurate data using very little energy.",1,AI
Semi supervised GANs have been widely used for unsupervised learning tasks in computer vision and NLP. But they have not been applied much to regression problems because it is hard to define an appropriate loss function. This paper introduces a new approach to extend GANs to regression. We use contrastive features as loss function and propose a way to leverage both labeled and unlabeled data to learn mappings between input and output spaces. Performance is evaluated on different benchmarks and compared against other methods; results indicate that this new method surpasses current best methods in accuracy and reliability. This research opens up new avenues for GANs to tackle regression tasks and shows promise for dealing with practical issues.,1,AI
"This paper introduces a new method for bidirectional updates of fine granularity in medical data using blockchain technology. Common ways of updating medical information are slow, prone to mistakes and lack transparency. Our proposed system addresses this problem by making use of blockchain security and immutability to achieve fast and transparent bidirectional updates on medical records. The architecture includes a blockchain network, smart contract and user interface. Smart contracts define rules for updating data including conditions for granting access and types of updates allowed. Interface design is easy and user friendly for interaction between patients and providers. Privacy protection is ensured through fine grained access control that allows different levels of access according to role and responsibility. Encryption and decryption techniques are also used to protect sensitive medical data. Performance and security were tested by building a prototype and conducting experiments. Results show high performance with high transaction rates and high security as well. Immutability ensures tamperproof medical data. Conclusion: This blockchain approach for bidirectional updates is an innovative solution to efficiently and transparently manage medical records. Features such as security and privacy make it fit for practical use and scalability performance characteristics ensure handling large volumes of transactions.",1,AI
"This paper introduces a new algorithm for optimal pricing of electricity in a smart microgrid network. The algorithm employs a multi objective optimization framework which takes into account both economic and technical aspects. Economic goals aim at maximizing revenue for the microgrid operator while technical goals seek to minimize total costs for consumers. Proposed algorithm uses real time information about demand and supply to adjust prices dynamically considering factors like renewable energy generation, storage capacities and load management strategies. Performance evaluation has been done using simulation studies and compared to traditional pricing methods. Results indicate that this proposed algorithm achieves significant improvements in efficiency of the microgrid system and provides strong financial benefits for both the operator and consumers. Contribution of this research includes advancement in smart grid technology and offering a practical method for pricing optimization in microgrid systems.",1,AI
"Farsighted collusion is a phenomenon in game theory where multiple players act together toward a shared goal and they choose actions which do not benefit them immediately, but result in a better long term outcome. This idea has been investigated widely in different disciplines like economics, psychology and computer science. This paper looks at this concept specifically in the context of stable marriage problems - a classic case from algorithmics and discrete math. Stable marriage problems deal with pairing elements from two sets so that no pair prefers each other over what they are matched against. We analyze when and under what conditions such collusion happens and show that it leads to improved stability. Results shed light on the significance of foresight in matching and are significant for designing algorithms and mechanisms for practical applications in this domain.",1,AI
"This paper introduces a new deep bilateral learning approach for enhancing images in real time. Proposed approach makes use of deep learning to improve visual quality while retaining key features like edges and texture. Bilateral filtering is used to ensure smoothness and preservation of edges and deep neural nets are used to map low quality to high quality images. Results on a benchmark dataset show proposed approach excels visually and computationally compared to top methods. Results of this work have broad potential implications including photography, vision and image processing.",1,AI
"This paper compares empirically four learning algorithms for collision prediction in a simulated 2D grid world: Q learning, SARSA, Expected SARSA and Double Q learning. Comparisons are made on a range of hyperparameters and performance is measured using metrics such as mean squared error and average reward. Results show that Expected SARSA performs best with regards to mean squared error and average reward whereas Double Q learning does poorly. Performance variation among algorithms is also observed to depend strongly on hyperparameters and sensitivity to them varies between algorithms. Overall this work conducts valuable empirical comparisons among these learning algorithms for collision prediction and suggests important insights regarding their relative merits and demerits. Insights gained are valuable for research and practice aiming at similar prediction tasks and may guide choice of appropriate algorithms for specific contexts.",1,AI
"Multi-view detection incorporates multiple camera views to alleviate occlusion in crowded scenes, where the state-of-the-art approaches adopt homography transformations to project multi-view features to the ground plane. However, we find that these 2D transformations do not take into account the object's height, and with this neglection features along the vertical direction of same object are likely not projected onto the same ground plane point, leading to impure ground-plane features. To solve this problem, we propose VFA, voxelized 3D feature aggregation, for feature transformation and aggregation in multi-view detection. Specifically, we voxelize the 3D space, project the voxels onto each camera view, and associate 2D features with these projected voxels. This allows us to identify and then aggregate 2D features along the same vertical line, alleviating projection distortions to a large extent. Additionally, because different kinds of objects (human vs. cattle) have different shapes on the ground plane, we introduce the oriented Gaussian encoding to match such shapes, leading to increased accuracy and efficiency. We perform experiments on multiview 2D detection and multiview 3D detection problems. Results on four datasets (including a newly introduced MultiviewC dataset) show that our system is very competitive compared with the state-of-the-art approaches. %Our code and data will be open-sourced.Code and MultiviewC are released at https://github.com/Robert-Mar/VFA.",0,Human
This paper reports a study about development of a high performance model for efficient real time use with unmanned aerial vehicles (UAVs). There has been growing interest from different industries in using UAVs and this paper addresses the requirement for high quality and reliable detection algorithms suitable for real time use. Proposed method combines deep learning and computer vision techniques which are fine tuned to perform accurate and quick object detection for real time UAVs. Results of comprehensive testing on benchmark data set show that this new method excels compared to current leading models for both accuracy and processing speed. Results are also shown in diverse practical tasks like surveillance and inspections; effectiveness and practicability of this model is demonstrated. This research advances UAV object detection by introducing a very efficient and accurate model for practical real time applications.,1,AI
This paper introduces a new approach to efficiently compress using blind compressed sensing for Magnetic Resonance Imaging (MRI). Proposed method uses transforms to reduce data features but retain important ones. Using such transforms results in lower measurement counts which accelerates acquisition and lowers cost. Method includes convergence assurances ensuring that reconstructions closely match original data. Performance is also validated through experiments with MRI data showing better results compared to prior methods. Results advance current research in blind compressed sensing and suggest future application diversity across different imaging types.,1,AI
"This paper introduces a new approach to combinatorial Bayesian optimization that uses the graph Cartesian product to handle high dimensional, discrete and non separable functions. Results show that this approach excels at exploring the search space and outperforms other methods on diverse benchmarks. Scalability studies show that this method scales well to large data sets and therefore is practical for use across many different fields. This research adds to an expanding body of work on Bayesian optimization and has broad implications for solving various optimization tasks including those in machine learning.",1,AI
"Age-Related Macular Degeneration (AMD) is an asymptomatic retinal disease which may result in loss of vision. There is limited access to high-quality relevant retinal images and poor understanding of the features defining sub-classes of this disease. Motivated by recent advances in machine learning we specifically explore the potential of generative modeling, using Generative Adversarial Networks (GANs) and style transferring, to facilitate clinical diagnosis and disease understanding by feature extraction. We design an analytic pipeline which first generates synthetic retinal images from clinical images; a subsequent verification step is applied. In the synthesizing step we merge GANs (DCGANs and WGANs architectures) and style transferring for the image generation, whereas the verified step controls the accuracy of the generated images. We find that the generated images contain sufficient pathological details to facilitate ophthalmologists' task of disease classification and in discovery of disease relevant features. In particular, our system predicts the drusen and geographic atrophy sub-classes of AMD. Furthermore, the performance using CFP images for GANs outperforms the classification based on using only the original clinical dataset. Our results are evaluated using existing classifier of retinal diseases and class activated maps, supporting the predictive power of the synthetic images and their utility for feature extraction. Our code examples are available online.",0,Human
"This paper introduces a new method for exploring labeled graphs guided by user labels. This differs from previous methods which use a constant proportion of labels; this new method lets users change this proportion according to their specific needs and goals. The authors developed an algorithm that computes the best proportion of labels dynamically depending on the exploration progress. They consider different factors like graph density and label distribution along with user feedback to compute this proportion. Experiments on real data and synthetic data showed this method outperforms previous methods in both efficiency and accuracy especially when the best proportion is uncertain or varies over time. An experiment with users also found good results. In summary, this new method allows flexible and efficient navigation of large and complicated graphs useful for diverse applications such as social network analysis and bioinformatics.",1,AI
"This paper introduces a new approach to object re-identification by using transformer models and is called TransReID. The authors exploit the attention mechanisms within transformers to take advantage of long range relationships among features and to incorporate global context. Results show that TransReID outperforms other leading methods on various benchmark datasets. Authors also study how different aspects of the TransReID framework affect performance including number of layers used, model size, and usage of augmentations. Results shed light on effectiveness and potential improvements for practical use.",1,AI
"Despite the tremendous success of BitTorrent, its swarming system suffers from a fundamental limitation: lower or no availability of unpopular contents. Recently, Menasche et al. has shown that bundling is a promising solution to mitigate this availability problem; it improves the availability and reduces download times for unpopular contents by combining multiple files into a single swarm. There also have been studies on bundling strategies and performance issues in bundled swarms. In spite of the recent surge of interest in the benefits of and strategies for bundling, there are still little empirical grounding for understanding, describing, and modeling it. This is the first empirical study that measures and analyzes how prevalent contents bundling is in BitTorrent and how peers access the bundled contents, in comparison to the other non-bundled (i.e., single-filed) ones. To our surprise, we found that around 70% of BitTorrent swarms contain multiple files, which indicate that bundling has become widespread for contents sharing. We also show that the amount of bytes shared in bundled swarms is estimated to be around 85% out of all the BitTorrent contents logged in our datasets. Inspired from our findings, we raise and discuss three important research questions in the field of file sharing systems as well as future contents-oriented networking: i) bundling strategies, ii) bundling-aware sharing systems in BitTorrent, and iii) implications on content-oriented networking.",0,Human
This paper introduces a new approach for fast Bayesian calibration of brain tumor models. A solver that respects geometric constraints is proposed which uses deep learning power to efficiently solve the inverse problem of matching MR images to physical tumor parameters. Results show marked improvements over traditional optimization methods both in terms of accuracy and speed. This work therefore opens up promising prospects for estimation of physical properties of brain tumors based on medical images and might have important implications for personalized medicine and radiation treatment planning.,1,AI
"This paper focuses on security of edge nodes in the Internet of Things (IoT) context. We investigate possible security attacks that can happen at edge nodes, which are small devices located near network edges collecting and processing data from other IoT devices. As machine learning becomes increasingly common in IoT, there is growing concern about system security because learning algorithms are vulnerable to different types of attacks. This paper looks into various types of attacks such as inference attacks, evasion attacks and poisoning attacks. We analyze impact of these attacks on performance of learning algorithms and how they can undermine security of IoT networks. Countermeasures proposed here aim to defend against these attacks. Countermeasures include using secure communication protocols, implementing security measures within machine learning algorithms and using secure hardware and software systems. Authors also point out challenges to effective implementation of these countermeasures and offer suggestions for further research. Overall this paper presents an in depth look at security threats related to learning systems in IoT and offers practical ways to mitigate those risks. Findings and recommendations from this work should be useful for practitioners and researchers to enhance security of IoT networks and to deploy learning algorithms safely.",1,AI
"Millions of drivers worldwide have enjoyed financial benefits and work schedule flexibility through a ride-sharing economy, but meanwhile they have suffered from the lack of a sense of identity and career achievement. Equipped with social identity and contest theories, financially incentivized team competitions have been an effective instrument to increase drivers' productivity, job satisfaction, and retention, and to improve revenue over cost for ride-sharing platforms. While these competitions are overall effective, the decisive factors behind the treatment effects and how they affect the outcomes of individual drivers have been largely mysterious. In this study, we analyze data collected from more than 500 large-scale team competitions organized by a leading ride-sharing platform, building machine learning models to predict individual treatment effects. Through a careful investigation of features and predictors, we are able to reduce out-sample prediction error by more than 24%. Through interpreting the best-performing models, we discover many novel and actionable insights regarding how to optimize the design and the execution of team competitions on ride-sharing platforms. A simulated analysis demonstrates that by simply changing a few contest design options, the average treatment effect of a real competition is expected to increase by as much as 26%. Our procedure and findings shed light on how to analyze and optimize large-scale online field experiments in general.",0,Human
"This paper looks at comprehensive studies comparing systems for selecting features for algorithm selection focusing on evaluating performance of different methods for choosing best algorithms for specific optimization problems. Using a set of benchmark problems and a suite of algorithms, the study measures performance of these selection methods. Results show that systems that use features perform much better than using one algorithm alone. Results also reveal strengths and weaknesses of different selection methods and stress the importance of feature engineering. Findings from this research will help guide development of future systems for black box numerical optimization.",1,AI
"As software becomes increasingly complex, deployability and reliability are now very important. Replay and record techniques have been introduced as promising approaches to improve deployability by capturing and reproducing system execution traces. However, existing techniques face problems in terms of scalability and overhead. This report introduces an engineering approach to these techniques which solves these problems. Key features include support for multi tier apps and efficient data collection and storage. We also use virtualization and distributed tracing for scalability and precision. We reduce overhead by using techniques like adaptive sampling and selective replay. We test effectiveness with real world applications; results show high accuracy and low overhead along with insight into system behavior under different conditions. We find that the system can pinpoint and diagnose performance and reliability issues. In conclusion, this engineering approach improves deployability and reliability for large scale complex systems. Software engineers and developers can use this system for debugging and to ensure consistent behavior across environments. Future work aims to integrate this system into workflow processes and investigate new uses like security and compliance.",1,AI
"Reinforcement learning (RL) is one of the most used approaches for decision making tasks; agents learn through their actions and aim to maximize the reward signals. However, in practice the reward signal often contains noise or disturbances which may result in suboptimal or poor behavior. This work studies RL with perturbed rewards and focuses on two key aspects: (1) how perturbations affect learning and performance of different RL algorithms and (2) designing robust RL algorithms that cope well with perturbations. We consider different types of perturbations like Gaussian noise and random reward functions and we test different kinds of algorithms such as value based ones and actor critics under different conditions. Results suggest that perturbations can substantially affect learning and algorithm performance and highlight that robustness against perturbations is important. The paper also gives ideas for future research and implications span across diverse areas including robotics, gaming and autonomous systems among others.",1,AI
"Current performance-driven building design methods are not widely adopted outside the research field for several reasons that make them difficult to integrate into a typical design process. In the early design phase, in particular, the time-intensity and the cognitive load associated with optimization and form parametrization are incompatible with design exploration, which requires quick iteration. This research introduces a novel method for performance-driven geometry generation that can afford interaction directly in the 3d modeling environment, eliminating the need for explicit parametrization, and is multiple orders faster than the equivalent form optimization. The method uses Machine Learning techniques to train a generative model offline. The generative model learns a distribution of optimal performing geometries and their simulation contexts based on a dataset that addresses the performance(s) of interest. By navigating the generative model's latent space, geometries with the desired characteristics can be quickly generated. A case study is presented, demonstrating the generation of a synthetic dataset and the use of a Variational Autoencoder (VAE) as a generative model for geometries with optimal solar gain. The results show that the VAE-generated geometries perform on average at least as well as the optimized ones, suggesting that the introduced method shows a feasible path towards more intuitive and interactive early-phase performance-driven design assistance.",0,Human
This paper introduces a new approach for generating learning that focuses on modeling spatial and temporal relationships for connected vehicles. This new approach blends deep learning techniques with generative models to effectively model complex dependency relationships that occur in datasets of connected vehicle networks. Results are based on real world data and show clear superiority over traditional machine learning approaches. Results of this research open up exciting avenues for further work on modeling spatial and temporal relationships and have potential use cases for traffic forecasting and control along with designing intelligent transportation systems.,1,AI
This paper introduces a new way of compressing categorical features using submodular optimization. Features of this kind are very common in practice but they often cause high computational cost and storage demands in machine learning. Our approach uses submodular optimization to select an important subset of representative categories which capture most of the information from raw data. We formulate the problem as a maximization task using submodularity and develop a greedy algorithm for efficient solution finding. We also give theoretical assurance about quality of results from our algorithm. Results show that this method compares favorably against others in terms of both compression ratio and accuracy on different benchmarks. Our method is easy to integrate into current pipelines and has promise to greatly enhance efficiency and scalability for models working with categorical features.,1,AI
"This paper considers choosing energy plans in markets that let customers shop around and selects two competing algorithms for consumers to decide whether to stick with their current plans or change them. First algorithm, ""Stay or Switch,"" looks at past energy use and costs to determine best strategy. Second algorithm called ""Adaptive Stay or Switch"" changes as conditions change and picks new energy plans accordingly. Performance of these algorithms is evaluated using simulations based on actual energy market data. Results indicate both perform better than previous methods and reduce costs for consumers significantly.",1,AI
"Volume transmission is an important neural communication pathway in which neurons in one brain region influence the neurotransmitter concentration in the extracellular space of a distant brain region. In this paper, we apply asymptotic analysis to a stochastic partial differential equation model of volume transmission to calculate the neurotransmitter concentration in the extracellular space. Our model involves the diffusion equation in a three-dimensional domain with interior holes that randomly switch between being either sources or sinks. These holes model nerve varicosities that alternate between releasing and absorbing neurotransmitter, according to when they fire action potentials. In the case that the holes are small, we compute analytically the first two nonzero terms in an asymptotic expansion of the average neurotransmitter concentration. The first term shows that the concentration is spatially constant to leading order and that this constant is independent of many details in the problem. Specifically, this constant first term is independent of the number and location of nerve varicosities, neural firing correlations, and the size and geometry of the extracellular space. The second term shows how these factors affect the concentration at second order. Interestingly, the second term is also spatially constant under some mild assumptions. We verify our asymptotic results by high-order numerical simulation using radial basis function-generated finite differences.",0,Human
"This study looks at current interoperability among electronic health records (EHRs) in Tanzanian hospitals. It identifies critical problems, challenges and opportunities to enhance interoperability. By reviewing a broad literature and interviewing health workers and other stakeholders, the authors point out major obstacles to interoperability including a lack of standards, inadequate infrastructure and technology and insufficient trained staff. They also look at possible benefits such as better patient care, fewer medical errors and improved public health surveillance. Finally they suggest recommendations to policymakers, hospitals and technology companies to improve interoperability through development of standards and guidelines, investments in technology and infrastructure and implementation of training programs. Results from this research are valuable for those working in health IT and related areas who wish to understand current status of interoperability in Tanzania.",1,AI
"This paper introduces a new method to improve security for systems of Industry 4.0 using Quorum Based Cloud Edge Distributed Deep Neural Network System (QUDOS). Recently, there is a growing demand for secure and efficient data processing because cloud services have become very popular in Industry 4.0. Existing solutions for security of such systems are inadequate to handle the complicated nature of threats. Authors propose QUDOS, a framework based on quorums for secure execution of deep neural networks (DNNs) in cloud and edge computing environments. QUDOS combines benefits from both clouds and edges to deliver secure and efficient processing of data in Industry 4.0. Using a consensus mechanism based on quorums ensures secure and decentralized execution of DNNs across different edge devices and cloud. This framework secures data processing by distributing execution of DNNs among different devices which reduces risks of single points of failure and data breaches. Secure multiparty computation techniques are used to ensure privacy and confidentiality of sensitive data during execution. Performance and security of QUDOS are evaluated experimentally and simulation results show high accuracy and low latency compared to current top solutions. Results also indicate that QUDOS outperforms previous methods in accuracy and latency; therefore it is appealing for secure and efficient data processing in Industry 4.0 systems. Conclusion: Authors present QUDOS as a promising way to enhance security of Industry 4.0 systems and introduce a secure and efficient framework for executing DNNs in cloud and edge computing environments.",1,AI
This paper investigates design of constellation for Non Coherent MIMO Multiple Access Channels. The main goal is to improve system performance by optimizing constellation design for data transmission. A new joint design method which considers both channel state information and user pairing is proposed. Performance of this method is assessed via simulations and compared against other approaches. Results indicate clear improvement of Bit Error Rate (BER) especially at high SNR levels. Concluding that this joint constellation design shows promise and applicability in practice.,1,AI
"Single-frame infrared small target (SIRST) detection aims at separating small targets from clutter backgrounds. With the advances of deep learning, CNN-based methods have yielded promising results in generic object detection due to their powerful modeling capability. However, existing CNN-based methods cannot be directly applied for infrared small targets since pooling layers in their networks could lead to the loss of targets in deep layers. To handle this problem, we propose a dense nested attention network (DNANet) in this paper. Specifically, we design a dense nested interactive module (DNIM) to achieve progressive interaction among high-level and low-level features. With the repeated interaction in DNIM, infrared small targets in deep layers can be maintained. Based on DNIM, we further propose a cascaded channel and spatial attention module (CSAM) to adaptively enhance multi-level features. With our DNANet, contextual information of small targets can be well incorporated and fully exploited by repeated fusion and enhancement. Moreover, we develop an infrared small target dataset (namely, NUDT-SIRST) and propose a set of evaluation metrics to conduct comprehensive performance evaluation. Experiments on both public and our self-developed datasets demonstrate the effectiveness of our method. Compared to other state-of-the-art methods, our method achieves better performance in terms of probability of detection (Pd), false-alarm rate (Fa), and intersection of union (IoU).",0,Human
"Chatter identification and detection in machining processes has been an active area of research in the past two decades. Part of the challenge in studying chatter is that machining equations that describe its occurrence are often nonlinear delay differential equations. The majority of the available tools for chatter identification rely on defining a metric that captures the characteristics of chatter, and a threshold that signals its occurrence. The difficulty in choosing these parameters can be somewhat alleviated by utilizing machine learning techniques. However, even with a successful classification algorithm, the transferability of typical machine learning methods from one data set to another remains very limited. In this paper we combine supervised machine learning with Topological Data Analysis (TDA) to obtain a descriptor of the process which can detect chatter. The features we use are derived from the persistence diagram of an attractor reconstructed from the time series via Takens embedding. We test the approach using deterministic and stochastic turning models, where the stochasticity is introduced via the cutting coefficient term. Our results show a 97% successful classification rate on the deterministic model labeled by the stability diagram obtained using the spectral element method. The features gleaned from the deterministic model are then utilized for characterization of chatter in a stochastic turning model where there are very limited analysis methods.",0,Human
"This paper introduces an algorithm designed for parallel processing that counts triangles in networks with very high degree nodes. Leveraging modern computers, this method handles these high degree nodes efficiently compared to previous methods; traditionally, such nodes are a big obstacle. The new method scales well and can process large networks quickly. Results indicate that performance metrics for this parallel approach are much better than those of competing methods. Performance results validate this parallel way of solving triangle count problem on networks with high degree nodes and suggest new ways to design effective algorithms for graph analysis.",1,AI
"This paper investigates a task of classifying audio signals into limited categories using very little labeled data. The goal is to evaluate various methods for few shot learning including transfer learning and meta learning to address this difficulty. Different models and techniques are evaluated on various benchmarks using metrics like accuracy, precision, recall and F1 score. Results show that transfer learning and meta learning greatly improve classification performance compared to regular methods. Factors such as quantity of labeled data, category number and signal complexity also affect performance and this work examines their effect too. Results of this research offer useful guidance for designing effective classifiers for tasks like speech recognition and surveillance based on audio.",1,AI
"Unstructured data from diverse sources, such as social media and aerial imagery, can provide valuable up-to-date information for intelligent situation assessment. Mining these different information sources could bring major benefits to applications such as situation awareness in disaster zones and mapping the spread of diseases. Such applications depend on classifying the situation across a region of interest, which can be depicted as a spatial ""heatmap"". Annotating unstructured data using crowdsourcing or automated classifiers produces individual classifications at sparse locations that typically contain many errors. We propose a novel Bayesian approach that models the relevance, error rates and bias of each information source, enabling us to learn a spatial Gaussian Process classifier by aggregating data from multiple sources with varying reliability and relevance. Our method does not require gold-labelled data and can make predictions at any location in an area of interest given only sparse observations. We show empirically that our approach can handle noisy and biased data sources, and that simultaneously inferring reliability and transferring information between neighbouring reports leads to more accurate predictions. We demonstrate our method on two real-world problems from disaster response, showing how our approach reduces the amount of crowdsourced data required and can be used to generate valuable heatmap visualisations from SMS messages and satellite images.",0,Human
"Authors propose and assess an approach to study use of software in the social sciences using a knowledge graph. They emphasize importance of understanding tools and methods used in research and show how this graph allows integration and overview of software use in this discipline. They report results of their study, collecting and analyzing data from different places such as academic journals, software repositories and online discussion boards. Results indicate patterns and connections among software users, research topics and they show benefits for improving and innovating use of software in social sciences. In conclusion, authors say that graph approach useful for understanding software use and provides basis for further research and development work in this domain.",1,AI
"Knowledge Graphs (KGs) have been integrated in several models of recommendation to augment the informational value of an item by means of its related entities in the graph. Yet, existing datasets only provide explicit ratings on items and no information is provided about user opinions of other (non-recommendable) entities. To overcome this limitation, we introduce a new dataset, called the MindReader, providing explicit user ratings both for items and for KG entities. In this first version, the MindReader dataset provides more than 102 thousands explicit ratings collected from 1,174 real users on both items and entities from a KG in the movie domain. This dataset has been collected through an online interview application that we also release open source. As a demonstration of the importance of this new dataset, we present a comparative study of the effect of the inclusion of ratings on non-item KG entities in a variety of state-of-the-art recommendation models. In particular, we show that most models, whether designed specifically for graph data or not, see improvements in recommendation quality when trained on explicit non-item ratings. Moreover, for some models, we show that non-item ratings can effectively replace item ratings without loss of recommendation quality. This finding, thanks also to an observed greater familiarity of users towards common KG entities than towards long-tail items, motivates the use of KG entities for both warm and cold-start recommendations.",0,Human
This paper introduces neural networks as an important tool for analyzing attack patterns and generating solutions for handling potential threats. Patterns of attacks are critical in cybersecurity and identifying and dealing with these patterns poses a major challenge to security experts. We develop a method using neural networks based on machine learning techniques; we extract meaningful features from large datasets of attack patterns using both supervised and unsupervised learning methods. Neural networks then use these features to identify and classify potential threats and generate effective solutions to reduce threat impact. Results show that our proposed method performs well at analysis and solution generation with high precision and speed. Proposed model could enhance cybersecurity practice by providing a complete and automated solution for detection and dealing with security threats.,1,AI
This paper looks at how collisions and avoiding such collisions affect swarm behavior. It starts by giving a broad overview of swarm behavior in both natural and artificial systems. Next it talks about various things that influence swarm behavior including major collisions and avoidance of collisions. Then there is simulation and experiment work showing how collisions and avoidance impacts swarm behavior and analysis of results draws conclusions regarding relationships among these factors and effectiveness of swarms as a whole. Results show that collisions and good avoidance are key elements for successful swarm behavior and this matters quite a lot for performance of the swarm as a whole.,1,AI
"Existing color-guided depth super-resolution (DSR) approaches require paired RGB-D data as training samples where the RGB image is used as structural guidance to recover the degraded depth map due to their geometrical similarity. However, the paired data may be limited or expensive to be collected in actual testing environment. Therefore, we explore for the first time to learn the cross-modality knowledge at training stage, where both RGB and depth modalities are available, but test on the target dataset, where only single depth modality exists. Our key idea is to distill the knowledge of scene structural guidance from RGB modality to the single DSR task without changing its network architecture. Specifically, we construct an auxiliary depth estimation (DE) task that takes an RGB image as input to estimate a depth map, and train both DSR task and DE task collaboratively to boost the performance of DSR. Upon this, a cross-task interaction module is proposed to realize bilateral cross task knowledge transfer. First, we design a cross-task distillation scheme that encourages DSR and DE networks to learn from each other in a teacher-student role-exchanging fashion. Then, we advance a structure prediction (SP) task that provides extra structure regularization to help both DSR and DE networks learn more informative structure representations for depth recovery. Extensive experiments demonstrate that our scheme achieves superior performance in comparison with other DSR methods.",0,Human
This paper suggests a new objective for separating causal mechanisms. This goal draws on ideas from transfer learning which transfers knowledge from tasks into performance improvement for closely related tasks. The new objective uses this idea to transfer the skill of separating mechanisms. Results of several experiments compare this objective to other methods. Results indicate that using this objective leads to higher performance and better generalization performance and thus shows its effectiveness at improving ability to separate mechanisms.,1,AI
"Recently, the problem of inaccurate learning targets in crowd counting draws increasing attention. Inspired by a few pioneering work, we solve this problem by trying to predict the indices of pre-defined interval bins of counts instead of the count values themselves. However, an inappropriate interval setting might make the count error contributions from different intervals extremely imbalanced, leading to inferior counting performance. Therefore, we propose a novel count interval partition criterion called Uniform Error Partition (UEP), which always keeps the expected counting error contributions equal for all intervals to minimize the prediction risk. Then to mitigate the inevitably introduced discretization errors in the count quantization process, we propose another criterion called Mean Count Proxies (MCP). The MCP criterion selects the best count proxy for each interval to represent its count value during inference, making the overall expected discretization error of an image nearly negligible. As far as we are aware, this work is the first to delve into such a classification task and ends up with a promising solution for count interval partition. Following the above two theoretically demonstrated criterions, we propose a simple yet effective model termed Uniform Error Partition Network (UEPNet), which achieves state-of-the-art performance on several challenging datasets. The codes will be available at: https://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.",0,Human
"Low-precision representation of deep neural networks (DNNs) is critical for efficient deployment of deep learning application on embedded platforms, however, converting the network to low precision degrades its performance. Crucially, networks that are designed for embedded applications usually suffer from increased degradation since they have less redundancy. This is most evident for the ubiquitous MobileNet architecture which requires a costly quantization-aware training cycle to achieve acceptable performance when quantized to 8-bits. In this paper, we trace the source of the degradation in MobileNets to a shift in the mean activation value. This shift is caused by an inherent bias in the quantization process which builds up across layers, shifting all network statistics away from the learned distribution. We show that this phenomenon happens in other architectures as well. We propose a simple remedy - compensating for the quantization induced shift by adding a constant to the additive bias term of each channel. We develop two simple methods for estimating the correction constants - one using iterative evaluation of the quantized network and one where the constants are set using a short training phase. Both methods are fast and require only a small amount of unlabeled data, making them appealing for rapid deployment of neural networks. Using the above methods we are able to match the performance of training-based quantization of MobileNets at a fraction of the cost.",0,Human
"Because there is growing need for efficient computing, different approaches have been developed for allocating virtual machines (VMs) in cloud computing environments. This paper presents a new method aiming at minimizing total busy times of VMs while energy consumption becomes a major consideration. We formulate this into an optimization problem and design an algorithm that considers workloads and server energy consumption. The algorithm allocates VMs so that energy usage is balanced among servers while also minimizing total busy times. Performance evaluation using both real data and synthesized data shows that this proposed method works very well at reducing busy times and energy consumption significantly. Results from this research are important for designing efficient cloud computing systems.",1,AI
"This paper studies the relationship between Artin automorphisms, cyclotomic function fields and folded list decodable codes. Artin automorphisms are significant maps in number theory and algebraic geometry; they are closely related to Artin maps. Cyclotomic function fields contain rational numbers and extra roots of unity. Folded list decodable codes have special properties and are commonly used in communication engineering. We show that applying Artin automorphisms to cyclotomic fields produces high performing codes that outperform previous approaches. Results indicate that deeper understanding of automorphisms may result in new advances in code design. Results have implications in cryptography, communications engineering and computer science. Numerical simulations confirm results and emphasize practical potential of our approach. We open new lines of research and make an important contribution to number theory, algebraic geometry and coding theory.",1,AI
"This paper looks at why traceability matters for neuroimaging research and suggests a method to ensure strong results and consistent results. Neuroimaging data is getting more complicated and diverse, which can cause researchers trouble such as mistakes, inconsistencies, and poor record keeping of procedures. Traceability and transparency are key for preserving quality and reliability of results and the paper presents a comprehensive approach to achieve this. The suggested solution combines best practices and tools from software engineering and data management; emphasis is placed on automation, version control and tracking of metadata. Detailed evaluation of the solution is presented along with advantages like higher efficiency, lower error risk and enhanced collaboration and sharing of results. Results of this work have important implications for the community doing neuroimaging and they advance efforts towards improving reliability and transparency overall.",1,AI
"Reinforcement Learning is proving a successful tool that can manage urban intersections with a fraction of the effort required to curate traditional traffic controllers. However, literature on the introduction and control of pedestrians to such intersections is scarce. Furthermore, it is unclear what traffic state variables should be used as reward to obtain the best agent performance. This paper robustly evaluates 30 different Reinforcement Learning reward functions for controlling intersections serving pedestrians and vehicles covering the main traffic state variables available via modern vision-based sensors. Some rewards proposed in previous literature solely for vehicular traffic are extended to pedestrians while new ones are introduced. We use a calibrated model in terms of demand, sensors, green times and other operational constraints of a real intersection in Greater Manchester, UK. The assessed rewards can be classified in 5 groups depending on the magnitudes used: queues, waiting time, delay, average speed and throughput in the junction. The performance of different agents, in terms of waiting time, is compared across different demand levels, from normal operation to saturation of traditional adaptive controllers. We find that those rewards maximising the speed of the network obtain the lowest waiting time for vehicles and pedestrians simultaneously, closely followed by queue minimisation, demonstrating better performance than other previously proposed methods.",0,Human
"The ability to segment teeth precisely from digitized 3D dental models is an essential task in computer-aided orthodontic surgical planning. To date, deep learning based methods have been popularly used to handle this task. State-of-the-art methods directly concatenate the raw attributes of 3D inputs, namely coordinates and normal vectors of mesh cells, to train a single-stream network for fully-automated tooth segmentation. This, however, has the drawback of ignoring the different geometric meanings provided by those raw attributes. This issue might possibly confuse the network in learning discriminative geometric features and result in many isolated false predictions on the dental model. Against this issue, we propose a two-stream graph convolutional network (TSGCNet) to learn multi-view geometric information from different geometric attributes. Our TSGCNet adopts two graph-learning streams, designed in an input-aware fashion, to extract more discriminative high-level geometric representations from coordinates and normal vectors, respectively. These feature representations learned from the designed two different streams are further fused to integrate the multi-view complementary information for the cell-wise dense prediction task. We evaluate our proposed TSGCNet on a real-patient dataset of dental models acquired by 3D intraoral scanners, and experimental results demonstrate that our method significantly outperforms state-of-the-art methods for 3D shape segmentation.",0,Human
This paper introduces an approach based on recurrence relations for analysis of effective capacity for schemes of retransmissions in wireless communication systems. Effective capacity measures the highest attainable rate of data transmission while adhering to strict limits on delays. We propose a computational tool that finds effective capacity for various schemes like ARQ and HARQ protocols. Effectiveness of our method has been validated by numerical examples and simulations; results show high accuracy and efficiency. Results from this research are very useful for designing efficient schemes and serve as reference material for future work in wireless communications.,1,AI
"With the increasing interest in the content creation field in multiple sectors such as media, education, and entertainment, there is an increasing trend in the papers that uses AI algorithms to generate content such as images, videos, audio, and text. Generative Adversarial Networks (GANs) in one of the promising models that synthesizes data samples that are similar to real data samples. While the variations of GANs models, in general, have been covered to some extent in several survey papers, to the best of our knowledge, this is among the first survey papers that reviews the state-of-the-art video GANs models. This paper first categorized GANs review papers into general GANs review papers, image GANs review papers, and special field GANs review papers such as anomaly detection, medical imaging, or cybersecurity. The paper then summarizes the main improvements in GANs frameworks that are not initially developed for the video domain but have been adopted in multiple video GANs variations. Then, a comprehensive review of video GANs models is provided under two main divisions according to the presence or non-presence of a condition. The conditional models then further grouped according to the type of condition into audio, text, video, and image. The paper is concluded by highlighting the main challenges and limitations of the current video GANs models. A comprehensive list of datasets, applied loss functions, and evaluation metrics is provided in the supplementary material.",0,Human
The winning team at IEEE CIG 2017 competition on mining data related to video games focused on predicting player behavior. Results show their winning solution performed better than other teams by a big margin in terms of accuracy. Results from this work show promise for using machine learning techniques to predict player behavior; the paper also looks closely at what features work best and how to design models. Authors also point out some drawbacks of their approach and give suggestions for future research.,1,AI
"This paper introduces a new method to understand causal structures using a semantic framework based on category theory. Category theory is used as a mathematical branch to define relationships among variables within systems and to study effects resulting from changes to one variable versus another. The main contribution of this paper is a categorical model of causality which allows us to reason about relationships among variables concisely and intuitively. The authors present various examples to show how useful this categorical causal model can be in diverse fields such as physics, biology and social sciences. Results indicate that this semantic framework with category theory offers a strong tool for formalization and clear consistency reasoning about causal relationships.",1,AI
"This paper investigates the concept of live broadcasting in networks today. Live means real time transmission of content versus prerecorded or demand on demand stuff. We look at how this idea has changed over time especially with the growth of digital media and increased use of social media platforms. We review the literature comprehensively and mix theory and empirical studies from various disciplines. Original research also looks into audience perceptions and interpretations of live broadcasts using both qualitative and quantitative methods. Results show that definition of live is flexible and varies based on cultural, technological and social factors. Audiences may also have different views regarding what constitutes live depending on the content and context in which it is consumed. Conclusion: live broadcasting remains important especially for live events and breaking news; however, as digital media keeps evolving we might need to rethink live again in light of new forms of engagement and interaction.",1,AI
"This paper introduces a new method of deep metric learning for proxies to improve performance and scalability of recognition systems based on proxies. Previous methods use many proxy samples but this can be costly computationally and impractical for scalability. We propose ""Less is More"" learning of proxy examples into a compact feature space using deep graph metric learning models. Leveraging relationships among proxies we learn a low dimensional feature space that captures discriminative information per class with much fewer samples. Results show this method performs better than previous ones in both accuracy and efficiency using significantly less proxies. Our work points toward development of more efficient and scalable systems for object recognition.",1,AI
This paper looks into efficient algorithms for labeling to solve the adjacent quadratic shortest path problem. Authors compare different labeling algorithms like A* and Dijkstra and introduce a new hybrid approach which combines features of both. Results show the proposed hybrid algorithm excels in terms of both speed and solution quality compared to previous work. Results are important because they improve algorithms for this kind of problems and have ramifications for transportation logistics and other network design areas.,1,AI
"CNNs have excelled at performing place recognition over time, particularly when the neural network is optimized for localization in the current environmental conditions. In this paper we investigate the concept of feature map filtering, where, rather than using all the activations within a convolutional tensor, only the most useful activations are used. Since specific feature maps encode different visual features, the objective is to remove feature maps that are detract from the ability to recognize a location across appearance changes. Our key innovation is to filter the feature maps in an early convolutional layer, but then continue to run the network and extract a feature vector using a later layer in the same network. By filtering early visual features and extracting a feature vector from a higher, more viewpoint invariant later layer, we demonstrate improved condition and viewpoint invariance. Our approach requires image pairs for training from the deployment environment, but we show that state-of-the-art performance can regularly be achieved with as little as a single training image pair. An exhaustive experimental analysis is performed to determine the full scope of causality between early layer filtering and late layer extraction. For validity, we use three datasets: Oxford RobotCar, Nordland, and Gardens Point, achieving overall superior performance to NetVLAD. The work provides a number of new avenues for exploring CNN optimizations, without full re-training.",0,Human
"The paper introduces a new approach to semantic segmentation named CCNet. Based on the idea of cross attention mechanism that excels at various computer vision tasks, CCNet employs this mechanism to exploit long range dependency among pixels; these dependencies are important for segmentation accuracy. Architecture includes a backbone for feature extraction, an attention module and a segmentation head. This module uses both spatial and channel attention to model dependencies among pixels. Experiments on benchmark datasets show that CCNet is effective and performs well with lightweight design. Results indicate that proposed mechanism for cross attention is promising for segmentation and may also improve other vision tasks.",1,AI
"This paper investigates effectiveness of two common heuristics used to solve Vehicle Routing Problems (VRP): sequencing and clustering. Sequencing focuses on optimizing order of visiting customers by each vehicle whereas clustering groups customers into sets that are visited by different vehicles. We conduct a series of computational tests on benchmark VRP instances. We test both heuristics using different problem sizes and configurations and measure performance based on solution quality, computation time and scalability. Results show that heuristic choice depends on specific problem characteristics. For smaller and medium sized instances with similar customer demand, sequencing works better but clustering performs better for larger and more complex instances with diverse customer demand and multiple depots. We also find that hybrid approach combining both heuristics performs well for some VRP types. Overall, this work gives insight into comparative strengths and weaknesses of different heuristics and suggests best practices for practitioners and researchers regarding selection of proper approach for specific problem.",1,AI
"Permutations are used everywhere including cryptography and computer science and math. Relations among them are key for many applications but identifying these relations is hard. This paper looks at how to test those relations. Specifically, we study whether two permutations are equal, inverses or conjugates to each other and we give efficient algorithms for this testing. These algorithms use few queries to inputs; we use techniques from both group theory and graph theory and we use tools from probabilistic combinatorics and algebraic geometry to analyze these algorithms. We also prove lower bounds on the number of queries which shows our algorithms are nearly optimal up to small constants. Results of this work have impact on research into permutation groups and their relevance to CS and crypto.",1,AI
"Explanatory machine learning uses algorithms to explain predictions of machine learning models; both positive and negative results should be considered here. If applied incorrectly, this technique can cause harm. This paper examines both positives and negatives of using explanatory ML with emphasis on ethical concerns. On the plus side, explanatory ML enhances transparency and accountability by offering clear and succinct reasons for model outcomes. Stakeholders gain understanding and trust, resulting in better performance and efficiency. However, it poses risks as well. If ML models are trained on biased data, this technique could reproduce and even intensify discrimination. Moreover, explanations produced by such algorithms may be deceptive and lead to wrong decisions based on insufficient or inaccurate information. Ethical issues associated with explanatory ML are significant and practitioners need to think carefully about these issues when designing and deploying algorithms. Ultimately the paper calls for further study and discourse to realize benefits without increasing harms.",1,AI
This paper looks at specification proofs of bisimulation and modal logic for  calculus.  calculus is a formal system for dynamic modeling of distributed systems. The paper studies how bisimulation and modal logic are used as proof search specifications to verify correctness of  calculus models. Authors analyze carefully the algorithms of proof search in context of  calculus and implement them. Results show that bisimulation and logic are effective to use for verification correctness of model. Findings from this research are significant for designing and implementing tools for verification of distributed systems.,1,AI
"We consider Bayesian optimization of the output of a network of functions, where each function takes as input the output of its parent nodes, and where the network takes significant time to evaluate. Such problems arise, for example, in reinforcement learning, engineering design, and manufacturing. While the standard Bayesian optimization approach observes only the final output, our approach delivers greater query efficiency by leveraging information that the former ignores: intermediate output within the network. This is achieved by modeling the nodes of the network using Gaussian processes and choosing the points to evaluate using, as our acquisition function, the expected improvement computed with respect to the implied posterior on the objective. Although the non-Gaussian nature of this posterior prevents computing our acquisition function in closed form, we show that it can be efficiently maximized via sample average approximation. In addition, we prove that our method is asymptotically consistent, meaning that it finds a globally optimal solution as the number of evaluations grows to infinity, thus generalizing previously known convergence results for the expected improvement. Notably, this holds even though our method might not evaluate the domain densely, instead leveraging problem structure to leave regions unexplored. Finally, we show that our approach dramatically outperforms standard Bayesian optimization methods in several synthetic and real-world problems.",0,Human
"This paper presents a comprehensive survey of existing authentication and privacy-preserving schemes for 4G and 5G cellular networks. We start by providing an overview of existing surveys that deal with 4G and 5G communications, applications, standardization, and security. Then, we give a classification of threat models in 4G and 5G cellular networks in four categories, including, attacks against privacy, attacks against integrity, attacks against availability, and attacks against authentication. We also provide a classification of countermeasures into three types of categories, including, cryptography methods, humans factors, and intrusion detection methods. The countermeasures and informal and formal security analysis techniques used by the authentication and privacy preserving schemes are summarized in form of tables. Based on the categorization of the authentication and privacy models, we classify these schemes in seven types, including, handover authentication with privacy, mutual authentication with privacy, RFID authentication with privacy, deniable authentication with privacy, authentication with mutual anonymity, authentication and key agreement with privacy, and three-factor authentication with privacy. In addition, we provide a taxonomy and comparison of authentication and privacy-preserving schemes for 4G and 5G cellular networks in form of tables. Based on the current survey, several recommendations for further research are discussed at the end of this paper.",0,Human
"This paper conducts a quantitative study of egalitarianism among cryptocurrency users by analyzing data on transactions of leading cryptocurrencies like Bitcoin, Ethereum and Litecoin. We construct Gini coefficients for each cryptocurrency to gauge wealth inequality. We also look at concentration of wealth among top percentiles of holders and assess impact of different factors like mining and adoption rates. We find that while cryptocurrency is frequently seen as decentralized and democratic, wealth distribution among many cryptocurrencies is very uneven. We also identify factors that might contribute to greater equality in cryptocurrency ecosystems such as reducing concentration of wealth and promoting fair mining practices. Overall we illuminate how closely cryptocurrency aligns with egalitarian principles and suggest ways to promote a more equal wealth distribution in this new financial system.",1,AI
"This paper studies tight bounds for randomized load balancing on any network layout. Load balancing is an important problem in distributed computing and random algorithms have been widely used to solve it. Previous research has given loose bounds for such algorithms for general network layouts. We introduce a new analysis framework that lets us derive tight bounds for expected maximum load for randomized load balancing. This new framework uses a novel approach drawing from classic balls and bins problems to bound probabilities that any container gets too many items. Using this result we derive tight bounds for expected maximum load which apply to all kinds of network layouts. We also report experimental results showing how well our analysis framework works. Specifically, we show the framework can accurately forecast performance of load balancing algorithms on different network layouts, including realistic internet layouts. Results give significant insight into performance of load balancing on any layout and can be used to design and analyze distributed systems relying on load balancing.",1,AI
"Cyclus archetypes are important in the study of nuclear fuel cycles. This paper reviews different ones that have been studied by other researches. We start by defining what they are and explain why they matter. Then we take a broad look at different types of archetypes such as once through, closed and open. We also look at benefits and drawbacks of each one and note any limitations or assumptions involved. Finally we discuss how this knowledge could affect future studies into nuclear fuel cycles and sustainable development of nuclear power systems.",1,AI
"This paper introduces an improved scheme for sealed bid auctions using a protocol that includes multiparty circular quantum key distribution. Proposed scheme aims to overcome limitations of traditional auctions which are prone to attacks such as bid tampering and leaking information. By leveraging principles of quantum cryptography, the scheme ensures higher security for auction transactions. Multiparty circular quantum key distribution further improves security through provision of secure channels and confidentiality of bids. Results from simulation and experimentation show that the proposed method works well and securely; this suggests a promising way to conduct secure auctions. Results of this research are significant for designing and implementing secure electronic auction systems.",1,AI
"Compile-time information flow analysis has been a promising technique for protecting confidentiality and integrity of private data. In the last couple of decades, a large number of information flow security tools in the form of run-time execution-monitors or static type systems have been developed for programming languages to analyze information flow security policies. However, existing flow analysis tools lack in precision and usability, which is the primary reason behind not being widely adopted in real application development. In this paper, we propose a compile-time information flow analysis for an imperative program based on a hybrid (mutable + immutable) labelling approach that enables a user to detect information flow-policy breaches and modify the program to overcome violations. We have developed an information flow security analyzer for a dialect of Python language, PyX, called Pifthon using the said approach. The flow-analyzer aids in identifying possible misuse of the information in sequential PyX programs corresponding to a given information flow policy (IFP). Pifthon has distinct advantages like reduced labelling overhead that ameliorates usability, covers a wide range of PyX programs that include termination-and progress-sensitive channels, in contrast to other approaches in the literature. The proposed flow analysis is proved to be sound under the classical non-interference property. Further, case study and experience in the usage of Pifthon are provided.",0,Human
This paper gives an extensive review of recent advances in querying paths in compressed graphs. It starts with a short introduction on why graph compression matters and where it finds use. The paper goes into detail about different approaches for compressing graphs such as adjacency lists and matrices and levels. The focus is on algorithms that process compressed graphs specifically and on balancing costs of time and space. Both exact and approximate algorithms are considered and performance metrics are compared using real data and simulated data. In conclusion the paper addresses remaining questions and points towards possible future research directions.,1,AI
"This paper studies how to resolve privacy conflicts among many users on social media. Social media sites have grown into targets for privacy breaches due to their large user base and rich personal information. Because of this, there is growing demand for effective methods to handle privacy conflicts. The authors propose a framework for handling privacy conflicts that considers privacy preferences of multiple conflicting parties. This framework uses tools from game theory, computational social choice and privacy enhancing technology to arrive at a solution acceptable to all. Experimental evaluation shows effectiveness of the proposed framework in dealing with privacy conflicts in realistic social media. Results of this research advance privacy preservation on social media and offer practical advice for those who design such systems and policy makers.",1,AI
"We study exploration in stochastic multi-armed bandits when we have access to a divisible resource that can be allocated in varying amounts to arm pulls. We focus in particular on the allocation of distributed computing resources, where we may obtain results faster by allocating more resources per pull, but might have reduced throughput due to nonlinear scaling. For example, in simulation-based scientific studies, an expensive simulation can be sped up by running it on multiple cores. This speed-up however, is partly offset by the communication among cores, which results in lower throughput than if fewer cores were allocated per trial to run more trials in parallel. In this paper, we explore these trade-offs in two settings. First, in a fixed confidence setting, we need to find the best arm with a given target success probability as quickly as possible. We propose an algorithm which trades off between information accumulation and throughput and show that the time taken can be upper bounded by the solution of a dynamic program whose inputs are the gaps between the sub-optimal and optimal arms. We also prove a matching hardness result. Second, we present an algorithm for a fixed deadline setting, where we are given a time deadline and need to maximize the probability of finding the best arm. We corroborate our theoretical insights with simulation experiments that show that the algorithms consistently match or outperform baseline algorithms on a variety of problem instances.",0,Human
"Vulnerable software represents a tremendous threat to modern information systems. Vulnerabilities in widespread applications may be used to spread malware, steal money and conduct target attacks. To address this problem, developers and researchers use different approaches of dynamic and static software analysis; one of these approaches is called fuzzing. Fuzzing is performed by generating and sending potentially malformed data to an application under test. Since first appearance in 1988, fuzzing has evolved a lot, but issues which addressed to effectiveness evaluation have not fully investigated until now. In our research, we propose a novel approach of fuzzing effectiveness evaluation, taking into account semantics of executed code along with a quantitative assessment. For this purpose, we use specific metrics of source code complexity assessment adapted to perform analysis of machine code. We conducted effectiveness evaluation of these metrics on 104 widespread applications with known vulnerabilities. As a result of these experiments, we were able to identify a set of metrics that are more suitable to find bugs. In addition, we conducted separate experiments on 7 applications without known vulnerabilities by using the set of metrics. The experimental results confirmed that proposed approach can be applied to increase performance of the fuzzing. Moreover, the tools helped detect two critical zero day (previously unknown) vulnerabilities in the wide-spread applications.",0,Human
"Distributed storage systems employ codes to provide resilience to failure of multiple storage disks. Specifically, an $(n, k)$ MDS code stores $k$ symbols in $n$ disks such that the overall system is tolerant to a failure of up to $n-k$ disks. However, access to at least $k$ disks is still required to repair a single erasure. To reduce repair bandwidth, array codes are used where the stored symbols or packets are vectors of length $\ell$. MDS array codes have the potential to repair a single erasure using a fraction $1/(n-k)$ of data stored in the remaining disks. We introduce new methods of analysis which capitalize on the translation of the storage system problem into a geometric problem on a set of operators and subspaces. In particular, we ask the following question: for a given $(n, k)$, what is the minimum vector-length or sub-packetization factor $\ell$ required to achieve this optimal fraction? For \emph{exact recovery} of systematic disks in an MDS code of low redundancy, i.e. $k/n > 1/2$, the best known explicit codes \cite{WTB12} have a sub-packetization factor $\ell$ which is exponential in $k$. It has been conjectured \cite{TWB12} that for a fixed number of parity nodes, it is in fact necessary for $\ell$ to be exponential in $k$. In this paper, we provide a new log-squared converse bound on $k$ for a given $\ell$, and prove that $k \le 2\log_2\ell\left(\log_{\delta}\ell+1\right)$, for an arbitrary number of parity nodes $r = n-k$, where $\delta = r/(r-1)$.",0,Human
"This paper introduces a new deep learning model for forecasting traffic flows. This model is named CDGNet and it combines spatial and temporal dependencies into traffic data using dynamic graphs. Specifically, CDGNet uses dynamic graph convolutional neural network to discern correlation between various locations within traffic networks and also recurrent neural networks with a time attention mechanism to recognize trends in traffic data. Performance of CDGNet is evaluated on real traffic data and compared against top performing forecasting models. Results show that CDGNet excels both in short and long term forecasting accuracy. CDGNet efficiently captures complicated spatio temporal correlations in traffic data and produces accurate forecasts at high speed. Overall this research suggests a promising way of forecasting traffic; this has practical implications such as better traffic management, urban planning and intelligent transportation systems. CDGNet contributes to development of smarter transportation systems and improves quality of life in cities by reducing congestion and enhancing safety.",1,AI
"The IoT is rapidly growing with billions of devices connected online. Such an expansion brings up lots of security issues because many IoT devices have limited resources and do not have strong security mechanisms. A new architecture called OSCAR is introduced to handle these security concerns. OSCAR is a light security architecture that uses object oriented security mechanisms to prevent different kinds of attacks against IoT devices. The proposed OSCAR includes three major parts: object modeling, object security and object communication. Object modeling describes the structure and behavior of IoT objects along with their security features. Object security is tasked with protecting those objects and their features against different security risks. Communication protocols and procedures are defined for secure exchange of information among IoT objects. Performance metrics such as memory and processing overhead, latency and energy consumption are used to evaluate this proposed architecture. Experimental results indicate that OSCAR provides strong security while requiring little overhead. OSCAR supports diverse security services including confidentiality, integrity, authentication and access control. Compared to other existing IoT security architectures, OSCAR performs better. Implementation has been done on various devices like sensors, actuators and gateways to show practicality and effectiveness. In short, this proposed OSCAR architecture offers lightweight and effective security solutions for IoT that work well for different devices and networks. Using object security makes OSCAR adaptable and scalable and thus addresses security issues brought by IoT.",1,AI
This paper introduces a model for classifying different kinds of vulnerabilities in smart contract code using Attention based Bidirectional LSTM networks with encoders inspired by natural language processing. The aim of this research is to overcome the challenge of distinguishing among various types of vulnerabilities so that decentralized applications can be secure and reliable. The model uses an encoder pretrained on NLP for feature extraction from code and uses Attention LSTM networks for classification. Results of tests indicate that this new approach performs better than previous methods and offers a promising way to classify vulnerabilities among many classes.,1,AI
"Recent advances in artificial intelligence (AI) and machine learning have created a general perception that AI could be used to solve complex problems, and in some situations over-hyped as a tool that can be so easily used. Unfortunately, the barrier to realization of mass adoption of AI on various business domains is too high because most domain experts have no background in AI. Developing AI applications involves multiple phases, namely data preparation, application modeling, and product deployment. The effort of AI research has been spent mostly on new AI models (in the model training stage) to improve the performance of benchmark tasks such as image recognition. Many other factors such as usability, efficiency and security of AI have not been well addressed, and therefore form a barrier to democratizing AI. Further, for many real world applications such as healthcare and autonomous driving, learning via huge amounts of possibility exploration is not feasible since humans are involved. In many complex applications such as healthcare, subject matter experts (e.g. Clinicians) are the ones who appreciate the importance of features that affect health, and their knowledge together with existing knowledge bases are critical to the end results. In this paper, we take a new perspective on developing AI solutions, and present a solution for making AI usable. We hope that this resolution will enable all subject matter experts (eg. Clinicians) to exploit AI like data scientists.",0,Human
This paper investigates automation for identifying security discussions in systems composed of microservices. Results from both industrial surveys and experiments contribute to an integrated analysis of this field. Surveys are carried out among professionals in industry to understand how important and difficult security is in such systems. Experimental work focuses on building and testing a machine learning approach for automatic detection of security discussions within such systems. Results of the study also point to areas where future research and development can be focused; they show that identifying security discussions through automation looks promising.,1,AI
"Several cybersecurity domains, such as ransomware detection, forensics and data analysis, require methods to reliably identify encrypted data fragments. Typically, current approaches employ statistics derived from byte-level distribution, such as entropy estimation, to identify encrypted fragments. However, modern content types use compression techniques which alter data distribution pushing it closer to the uniform distribution. The result is that current approaches exhibit unreliable encryption detection performance when compressed data appears in the dataset. Furthermore, proposed approaches are typically evaluated over few data types and fragment sizes, making it hard to assess their practical applicability. This paper compares existing statistical tests on a large, standardized dataset and shows that current approaches consistently fail to distinguish encrypted and compressed data on both small and large fragment sizes. We address these shortcomings and design EnCoD, a learning-based classifier which can reliably distinguish compressed and encrypted data. We evaluate EnCoD on a dataset of 16 different file types and fragment sizes ranging from 512B to 8KB. Our results highlight that EnCoD outperforms current approaches by a wide margin, with accuracy ranging from ~82 for 512B fragments up to ~92 for 8KB data fragments. Moreover, EnCoD can pinpoint the exact format of a given data fragment, rather than performing only binary classification like previous approaches.",0,Human
"In system identification, estimating parameters of a model using limited observations results in poor identifiability. To cope with this issue, we propose a new method to simultaneously select and estimate sensitive parameters as key model parameters and fix the remaining parameters to a set of typical values. Our method is formulated as a nonlinear least squares estimator with L1-regularization on the deviation of parameters from a set of typical values. First, we provide consistency and oracle properties of the proposed estimator as a theoretical foundation. Second, we provide a novel approach based on Levenberg-Marquardt optimization to numerically find the solution to the formulated problem. Third, to show the effectiveness, we present an application identifying a biomechanical parametric model of a head position tracking task for 10 human subjects from limited data. In a simulation study, the variances of estimated parameters are decreased by 96.1% as compared to that of the estimated parameters without L1-regularization. In an experimental study, our method improves the model interpretation by reducing the number of parameters to be estimated while maintaining variance accounted for (VAF) at above 82.5%. Moreover, the variances of estimated parameters are reduced by 71.1% as compared to that of the estimated parameters without L1-regularization. Our method is 54 times faster than the standard simplex-based optimization to solve the regularized nonlinear regression.",0,Human
"Conventional RGB-D salient object detection methods aim to leverage depth as complementary information to find the salient regions in both modalities. However, the salient object detection results heavily rely on the quality of captured depth data which sometimes are unavailable. In this work, we make the first attempt to solve the RGB-D salient object detection problem with a novel depth-awareness framework. This framework only relies on RGB data in the testing phase, utilizing captured depth data as supervision for representation learning. To construct our framework as well as achieving accurate salient detection results, we propose a Ubiquitous Target Awareness (UTA) network to solve three important challenges in RGB-D SOD task: 1) a depth awareness module to excavate depth information and to mine ambiguous regions via adaptive depth-error weights, 2) a spatial-aware cross-modal interaction and a channel-aware cross-level interaction, exploiting the low-level boundary cues and amplifying high-level salient channels, and 3) a gated multi-scale predictor module to perceive the object saliency in different contextual scales. Besides its high performance, our proposed UTA network is depth-free for inference and runs in real-time with 43 FPS. Experimental evidence demonstrates that our proposed network not only surpasses the state-of-the-art methods on five public RGB-D SOD benchmarks by a large margin, but also verifies its extensibility on five public RGB SOD benchmarks.",0,Human
"Properly designed precoders can significantly improve the spectral efficiency of multiple-input multiple-output (MIMO) relay systems. In this paper, we investigate joint source and relay precoding design based on the mean-square-error (MSE) criterion in MIMO two-way relay systems, where two multi-antenna source nodes exchange information via a multi-antenna amplify-and-forward relay node. This problem is non-convex and its optimal solution remains unsolved. Aiming to find an efficient way to solve the problem, we first decouple the primal problem into three tractable sub-problems, and then propose an iterative precoding design algorithm based on alternating optimization. The solution to each sub-problem is optimal and unique, thus the convergence of the iterative algorithm is guaranteed. Secondly, we propose a structured precoding design to lower the computational complexity. The proposed precoding structure is able to parallelize the channels in the multiple access (MAC) phase and broadcast (BC) phase. It thus reduces the precoding design to a simple power allocation problem. Lastly, for the special case where only a single data stream is transmitted from each source node, we present a source-antenna-selection (SAS) based precoding design algorithm. This algorithm selects only one antenna for transmission from each source and thus requires lower signalling overhead. Comprehensive simulation is conducted to evaluate the effectiveness of all the proposed precoding designs.",0,Human
"In the last decade, scenario-based serious-games have become a main tool for learning new skills and capabilities. An important factor in the development of such systems is the overhead in time, cost and human resources to manually create the content for these scenarios. We focus on how to create content for scenarios in medical, military, commerce and gaming applications where maintaining the integrity and coherence of the content is integral for the system's success. To do so, we present an automatic method for generating content about everyday activities through combining computer science techniques with the crowd. We use the crowd in three basic ways: to capture a database of scenarios of everyday activities, to generate a database of likely replacements for specific events within that scenario, and to evaluate the resulting scenarios. We found that the generated scenarios were rated as reliable and consistent by the crowd when compared to the scenarios that were originally captured. We also compared the generated scenarios to those created by traditional planning techniques. We found that both methods were equally effective in generated reliable and consistent scenarios, yet the main advantages of our approach is that the content we generate is more varied and much easier to create. We have begun integrating this approach within a scenario-based training application for novice investigators within the law enforcement departments to improve their questioning skills.",0,Human
This paper introduces a new way to identify sources in systems where particles spread both by moving (advection) and diffusing randomly (diffusion). It combines machine learning with a method using Green functions and inverse problems. Supervised learning algorithms are used to estimate unknown release sources based on observed concentration data. Then Green function method is used to verify accuracy of estimated sources and reconstruct system parameters. Results show this new approach excels at both accuracy and speed compared to previous methods and thus promises to be a useful tool for finding sources in practical systems.,1,AI
"This paper studies random caching together with cooperative transmission for use in heterogeneous wireless networks. Authors present a new method which aims to improve wireless data transfer efficiency by using random caching at intermediate nodes in the network. This approach merges cooperative transmission advantages with caching benefits to yield better overall performance and lower latency. Authors conduct extensive simulation tests to verify the efficacy of their proposed approach. Results indicate that the proposed method based on random caching and cooperation greatly enhances throughput and energy efficiency compared to traditional cooperation methods. Insights are also provided regarding how different network parameters like cache numbers and cache size impact performance. Results of this research offer valuable guidance for designing future wireless networks. Conclusively, this work introduces a promising strategy to enhance wireless network performance via integration of caching and cooperation techniques. Results from this study have important ramifications for network design and can serve as a basis for further technological development in this domain.",1,AI
This paper introduces a new way of designing efficient artificial neurons using superconducting wires. The architecture includes a superconducting loop with a junction for adjustable response to inputs. The junction performs as an activation function and low power usage is obtained by using superconductivity to eliminate resistance in the loop. We develop a detailed model for the proposed neuron and perform extensive simulations to validate its performance characteristics. Results indicate high accuracy and much lower power consumption compared to other designs. We also develop a method for integrating many neurons into a larger network and demonstrate that this neuron architecture scales well to create high performance networks that consume less power. Overall this work opens promising avenues for development of energy efficient neural networks applicable to diverse fields including machine learning and cognitive computing.,1,AI
"This paper presents a discrete-time option pricing model that is rooted in Reinforcement Learning (RL), and more specifically in the famous Q-Learning method of RL. We construct a risk-adjusted Markov Decision Process for a discrete-time version of the classical Black-Scholes-Merton (BSM) model, where the option price is an optimal Q-function, while the optimal hedge is a second argument of this optimal Q-function, so that both the price and hedge are parts of the same formula. Pricing is done by learning to dynamically optimize risk-adjusted returns for an option replicating portfolio, as in the Markowitz portfolio theory. Using Q-Learning and related methods, once created in a parametric setting, the model is able to go model-free and learn to price and hedge an option directly from data, and without an explicit model of the world. This suggests that RL may provide efficient data-driven and model-free methods for optimal pricing and hedging of options, once we depart from the academic continuous-time limit, and vice versa, option pricing methods developed in Mathematical Finance may be viewed as special cases of model-based Reinforcement Learning. Further, due to simplicity and tractability of our model which only needs basic linear algebra (plus Monte Carlo simulation, if we work with synthetic data), and its close relation to the original BSM model, we suggest that our model could be used for benchmarking of different RL algorithms for financial trading applications",0,Human
"We consider a decentralized networked control system (DNCS) consisting of a remote controller and a collection of linear plants, each associated with a local controller. Each local controller directly observes the state of its co-located plant and can inform the remote controller of the plant's state through an unreliable uplink channel. The downlink channels from the remote controller to local controllers were assumed to be perfect. The objective of the local controllers and the remote controller is to cooperatively minimize the infinite horizon time average of expected quadratic cost. The finite horizon version of this problem was solved in our prior work [2]. The optimal strategies in the finite horizon case were shown to be characterized by coupled Riccati recursions. In this paper, we show that if the link failure probabilities are below certain critical thresholds, then the coupled Riccati recursions of the finite horizon solution reach a steady state and the corresponding decentralized strategies are optimal. Above these thresholds, we show that no strategy can achieve finite cost. We exploit a connection between our DNCS Riccati recursions and the coupled Riccati recursions of an auxiliary Markov jump linear system to obtain our results. Our main results in Theorems 1 and 2 explicitly identify the critical thresholds for the link failure probabilities and the optimal decentralized control strategies when all link failure probabilities are below their thresholds.",0,Human
"We explore the value of weak labels in learning transferable representations for medical images. Compared to hand-labeled datasets, weak or inexact labels can be acquired in large quantities at significantly lower cost and can provide useful training signals for data-hungry models such as deep neural networks. We consider weak labels in the form of pseudo-labels and propose a semi-weakly supervised contrastive learning (SWCL) framework for representation learning using semi-weakly annotated images. Specifically, we train a semi-supervised model to propagate labels from a small dataset consisting of diverse image-level annotations to a large unlabeled dataset. Using the propagated labels, we generate a patch-level dataset for pretraining and formulate a multi-label contrastive learning objective to capture position-specific features encoded in each patch. We empirically validate the transfer learning performance of SWCL on seven public retinal fundus datasets, covering three disease classification tasks and two anatomical structure segmentation tasks. Our experiment results suggest that, under very low data regime, large-scale ImageNet pretraining on improved architecture remains a very strong baseline, and recently proposed self-supervised methods falter in segmentation tasks, possibly due to the strong invariant constraint imposed. Our method surpasses all prior self-supervised methods and standard cross-entropy training, while closing the gaps with ImageNet pretraining.",0,Human
"The analysis of the structure of musical pieces is a task that remains a challenge for Artificial Intelligence, especially in the field of Deep Learning. It requires prior identification of structural boundaries of the music pieces. This structural boundary analysis has recently been studied with unsupervised methods and \textit{end-to-end} techniques such as Convolutional Neural Networks (CNN) using Mel-Scaled Log-magnitude Spectograms features (MLS), Self-Similarity Matrices (SSM) or Self-Similarity Lag Matrices (SSLM) as inputs and trained with human annotations. Several studies have been published divided into unsupervised and \textit{end-to-end} methods in which pre-processing is done in different ways, using different distance metrics and audio characteristics, so a generalized pre-processing method to compute model inputs is missing. The objective of this work is to establish a general method of pre-processing these inputs by comparing the inputs calculated from different pooling strategies, distance metrics and audio characteristics, also taking into account the computing time to obtain them. We also establish the most effective combination of inputs to be delivered to the CNN in order to establish the most efficient way to extract the limits of the structure of the music pieces. With an adequate combination of input matrices and pooling strategies we obtain a measurement accuracy $F_1$ of 0.411 that outperforms the current one obtained under the same conditions.",0,Human
"A common writing style for statistical results are the recommendations of the American Psychology Association, known as APA-style. However, in practice, writing styles vary as reports are not 100% following APA-style or parameters are not reported despite being mandatory. In addition, the statistics are not reported in isolation but in context of experimental conditions investigated and the general topic. We address these challenges by proposing a flexible pipeline STEREO based on active wrapper induction and unsupervised aspect extraction. We applied our pipeline to the over 100,000 documents in the CORD-19 dataset. It required only 0.25% of the corpus (about 500 documents) to learn statistics extraction rules that cover 95% of the sentences in CORD-19. The statistic extraction has nearly 100% precision on APA-conform and 95% precision on non-APA writing styles. In total, we were able to extract 113k reported statistics, of which only <1% is APA conform. We could extract in 46% the correct conditions from APA-conform reports (30% for non-APA). The best model for topic extraction achieves a precision of 75% on statistics reported in APA style (73% for non-APA conform). We conclude that STEREO is a good foundation for automatic statistic extraction and future developments for scientific paper analysis. Particularly the extraction of non-APA conform reports is important and allows applications such as giving feedback to authors about what is missing and could be changed.",0,Human
"We present here an implementation of Ensemble Kalman Filter (EnKF) parallelized through use of a modified Cholesky decomposition. EnKF is a method widely used for data assimilation that applies Monte Carlo techniques to estimate states of complex systems; however, it is computationally expensive for large scale problems. We solve this by proposing a parallel implementation of EnKF that distributes computation among different processors. Modified Cholesky decomposition is a matrix factorization method that enables fast computation of square root of positive definite matrices; we use this to reduce computation time while maintaining accuracy. Performance of parallel implementation is assessed via numerical experiments; results show that it achieves significant speed ups relative to sequential implementation. Results have broad applicability, including for tasks like weather forecasting and seismic imaging where high data throughput is needed.",1,AI
"This paper concentrates on problems of fair division when there are differing attributes and preferences among resources. We introduce an algorithmic framework that ensures both fairness and efficiency through leveraging structures related to matroids and EF1 (envy freedom up to one item). The proposed approach builds upon previous work on intersections and partitions and is flexible enough for multiple agents and indivisible goods. We also analyze the performance theoretically and prove that it attains an approximation ratio of constants in terms of social welfare and EF1. Empirical results using both simulated and real data show that this proposed solution performs very well; it outperforms leading algorithms based on both objective metrics and judgments made by human observers. Overall, we add to research into fair mechanisms and offer practical tools to handle complex resource allocations.",1,AI
"Visual exploration of high-dimensional real-valued datasets is a fundamental task in exploratory data analysis (EDA). Existing methods use predefined criteria to choose the representation of data. There is a lack of methods that (i) elicit from the user what she has learned from the data and (ii) show patterns that she does not know yet. We construct a theoretical model where identified patterns can be input as knowledge to the system. The knowledge syntax here is intuitive, such as ""this set of points forms a cluster"", and requires no knowledge of maths. This background knowledge is used to find a Maximum Entropy distribution of the data, after which the system provides the user data projections in which the data and the Maximum Entropy distribution differ the most, hence showing the user aspects of the data that are maximally informative given the user's current knowledge. We provide an open source EDA system with tailored interactive visualizations to demonstrate these concepts. We study the performance of the system and present use cases on both synthetic and real data. We find that the model and the prototype system allow the user to learn information efficiently from various data sources and the system works sufficiently fast in practice. We conclude that the information theoretic approach to exploratory data analysis where patterns observed by a user are formalized as constraints provides a principled, intuitive, and efficient basis for constructing an EDA system.",0,Human
"The spectral gap $\gamma$ of a finite, ergodic, and reversible Markov chain is an important parameter measuring the asymptotic rate of convergence. In applications, the transition matrix $P$ may be unknown, yet one sample of the chain up to a fixed time $n$ may be observed. We consider here the problem of estimating $\gamma$ from this data. Let $\pi$ be the stationary distribution of $P$, and $\pi_\star = \min_x \pi(x)$. We show that if $n = \tilde{O}\bigl(\frac{1}{\gamma \pi_\star}\bigr)$, then $\gamma$ can be estimated to within multiplicative constants with high probability. When $\pi$ is uniform on $d$ states, this matches (up to logarithmic correction) a lower bound of $\tilde{\Omega}\bigl(\frac{d}{\gamma}\bigr)$ steps required for precise estimation of $\gamma$. Moreover, we provide the first procedure for computing a fully data-dependent interval, from a single finite-length trajectory of the chain, that traps the mixing time $t_{\text{mix}}$ of the chain at a prescribed confidence level. The interval does not require the knowledge of any parameters of the chain. This stands in contrast to previous approaches, which either only provide point estimates, or require a reset mechanism, or additional prior knowledge. The interval is constructed around the relaxation time $t_{\text{relax}} = 1/\gamma$, which is strongly related to the mixing time, and the width of the interval converges to zero roughly at a $1/\sqrt{n}$ rate, where $n$ is the length of the sample path.",0,Human
"Recommender System research suffers currently from a disconnect between the size of academic data sets and the scale of industrial production systems. In order to bridge that gap we propose to generate more massive user/item interaction data sets by expanding pre-existing public data sets. User/item incidence matrices record interactions between users and items on a given platform as a large sparse matrix whose rows correspond to users and whose columns correspond to items. Our technique expands such matrices to larger numbers of rows (users), columns (items) and non zero values (interactions) while preserving key higher order statistical properties. We adapt the Kronecker Graph Theory to user/item incidence matrices and show that the corresponding fractal expansions preserve the fat-tailed distributions of user engagements, item popularity and singular value spectra of user/item interaction matrices. Preserving such properties is key to building large realistic synthetic data sets which in turn can be employed reliably to benchmark Recommender Systems and the systems employed to train them. We provide algorithms to produce such expansions and apply them to the MovieLens 20 million data set comprising 20 million ratings of 27K movies by 138K users. The resulting expanded data set has 10 billion ratings, 864K items and 2 million users in its smaller version and can be scaled up or down. A larger version features 655 billion ratings, 7 million items and 17 million users.",0,Human
This paper investigates how spectral efficiency is affected when there is correlation among desired and interfering signals that follow Gamma Lognormal distributions. It looks at the effect of this correlation on wireless communications systems. Results indicate that spectral efficiency drops sharply due to strong correlation between desired and interfering signals and leads to substantial performance degradation. A theoretical framework is developed to characterize efficiency under such conditions and provides insight into balance between correlation and efficiency. Results of this research are significant because they influence design and optimization of wireless communication systems and suggest guidelines for development of better performing and more robust systems.,1,AI
"Today, communication pervades society more than ever before. Platforms such as social media and instant messaging along with forums generate huge volumes of daily conversations. Observing and analyzing interactions among people has become a useful means to understand and predict behaviors within these networks. As these networks continue to grow larger, there is a growing need for more efficient ways of studying conversation data. This paper introduces a framework for representing conversations that aims to promote scalability for overhearing. The framework uses graph structure of communication networks to produce compressed representations of conversation data that are easier to analyze quickly. Techniques such as clustering and sequence modeling are combined to recognize topics and represent conversations as sequences of topic vectors. Through this representation, we can reduce data dimensions and improve efficiency of overhearing algorithms. We test our framework using a large dataset of online forum conversations. Comparisons are made against baselines and results show higher performance and efficiency. Scalability is demonstrated by processing different size datasets containing millions of messages. Overall, we introduce a new way of representing conversations for efficient scalability in overhearing through exploiting network structures and compressing data dimensions. Leveraging structure and reducing data dimensions allows for more effective analysis of large communication networks.",1,AI
This study examines pricing strategy that maximizes total surplus and is simple to implement. Such strategy is important because proportional complementarity occurs in different economic contexts where the benefit from having something rises with increasing adoption by others. We propose an innovative method combining principles of optimal pricing with approximation techniques. Through simulations we test this method and find results indicate near optimality compared to traditional methods but our proposed formulas are also much simpler. Results of this research offer insight into designing pricing strategies for this type of complementarity and are useful for people who make policies and business decisions.,1,AI
This paper introduces a new approach for efficient handling of congestion in networks based on RDMA. This approach named Dart splits the network into different subnets and specializes them to handle different kinds of traffic. Dart responds quickly to congestion and reduces latency for critical applications. Results show Dart excels compared to leading congestion control techniques in both throughput and fairness; at the same time low latency is maintained for real time applications. Results also show Dart is promising as a solution for dealing with congestion in RDMA networks.,1,AI
"This paper looks into dynamic beliefs; that is, how subjective probabilities of events or propositions change over time. It studies how to track and model such beliefs both individually and among groups. Methods rely on Bayesian belief networks and probabilistic graphs, which have been successful frameworks for dealing with uncertainty and updating beliefs. Performance of these methods is evaluated via simulations and empirical studies and important factors affecting accuracy of extracting belief dynamics are also explored. Results of this research can help to develop better models for monitoring and understanding beliefs in practical situations like decision making, risk assessment and information sharing.",1,AI
"Since the 1990s, there have been significant advances in the technology space and the e-Commerce area, leading to an exponential increase in demand for cashless payment solutions. This has led to increased demand for credit cards, bringing along with it the possibility of higher credit defaults and hence higher delinquency rates, over a period of time. The purpose of this research paper is to build a contemporary credit scoring model to forecast credit defaults for unsecured lending (credit cards), by employing machine learning techniques. As much of the customer payments data available to lenders, for forecasting Credit defaults, is imbalanced (skewed), on account of a limited subset of default instances, this poses a challenge for predictive modelling. In this research, this challenge is addressed by deploying Synthetic Minority Oversampling Technique (SMOTE), a proven technique to iron out such imbalances, from a given dataset. On running the research dataset through seven different machine learning models, the results indicate that the Light Gradient Boosting Machine (LGBM) Classifier model outperforms the other six classification techniques. Thus, our research indicates that the LGBM classifier model is better equipped to deliver higher learning speeds, better efficiencies and manage larger data volumes. We expect that deployment of this model will enable better and timely prediction of credit defaults for decision-makers in commercial lending institutions and banks.",0,Human
This paper introduces an innovative way to do cooperative verification using collective invariant generation. The idea is that different agents work together to generate invariants; mathematical expressions that represent system desired properties and can later serve as a basis for verification that the system works as intended. Results indicate this new method is better than standard verification approaches because it thoroughly explores system behavior and can catch errors others might miss. Results show effectiveness in diverse real world examples and point toward broad use in verification.,1,AI
"There has been a remarkable increase in the data exchange over web and the widespread use of digital media. As a result, multimedia data transfers also had a boost up. The mounting interest with reference to digital watermarking throughout the last decade is certainly due to the increase in the need of copyright protection of digital content. This is also enhanced due to commercial prospective. Applications of video watermarking in copy control, broadcast monitoring, fingerprinting, video authentication, copyright protection etc is immensely rising. The main aspects of information hiding are capacity, security and robustness. Capacity deals with the amount of information that can be hidden. The skill of anyone detecting the information is security and robustness refers to the resistance to modification of the cover content before concealed information is destroyed. Video watermarking algorithms normally prefers robustness. In a robust algorithm it is not possible to eliminate the watermark without rigorous degradation of the cover content. In this paper, we introduce the notion of Video Watermarking and the features required to design a robust watermarked video for a valuable application. We review several algorithms, and introduce frequently used key techniques. The aim of this paper is to focus on the various domains of video watermarking techniques. The majority of the reviewed methods based on video watermarking emphasize on the notion of robustness of the algorithm.",0,Human
Symbolic Aggregate Approximation (SAX) has become popular for analyzing time series because it excels at both processing efficiency and capturing patterns. But limitations exist when it comes to detecting trend information at segments. We propose an improved SAX which integrates trend information aiming to enhance performance at capturing patterns in time series. Extensive testing and evaluation show that our new method performs better compared to traditional SAX both at global and local trends. Results from this study advance effective time series analysis methods and have practical significance for many real world tasks.,1,AI
"We have designed, fabricated, and successfully tested a prototype mixed-signal, 28x28-binary-input, 10-output, 3-layer neuromorphic network (""MLP perceptron""). It is based on embedded nonvolatile floating-gate cell arrays redesigned from a commercial 180-nm NOR flash memory. The arrays allow precise (~1%) individual tuning of all memory cells, having long-term analog-level retention and low noise. Each array performs a very fast and energy-efficient analog vector-by-matrix multiplication, which is the bottleneck for signal propagation in most neuromorphic networks. All functional components of the prototype circuit, including 2 synaptic arrays with 101,780 floating-gate synaptic cells, 74 analog neurons, and the peripheral circuitry for weight adjustment and I/O operations, have a total area below 1 mm^2. Its testing on the common MNIST benchmark set (at this stage, with a relatively low weight import precision) has shown a classification fidelity of 94.65%, close to the 96.2% obtained in simulation. The classification of one pattern takes less than 1 us time and ~20 nJ energy - both numbers much better than for digital implementations of the same task. Estimates show that this performance may be further improved using a better neuron design and a more advanced memory technology, leading to a >10^2 advantage in speed and a >10^4 advantage in energy efficiency over the state-of-the-art purely digital (GPU and custom) circuits, at classification of large, complex patterns.",0,Human
This paper introduces a new method for detecting deceptive attacks in networked control systems by using a sequential detection framework along with watermark embedding. The suggested approach embeds a unique watermark into control system inputs and monitors output for deviation from expected behavior. Detection of attacks uses sequential hypothesis testing algorithm based on correlation between watermark and output. Results show that this method effectively detects different kinds of attacks accurately at high levels and with very low false alarms. This work offers promising solutions for improving security and reliability of control systems from harmful attacks.,1,AI
"This study uses Local Interpretability Model Agnostic Explanations with Outlier Detection (LIMEOUT) to improve fairness of machine learning (ML) models. Recently developed LIMEOUT integrates LIME that explains model predictions locally with outlier detection which detects points far from training distribution. Results show effectiveness of LIMEOUT in terms of fairness using benchmark datasets with diverse fairness challenges. Results show significant improvements in fairness due to identification and mitigation of bias in prediction decisions. Also, results indicate LIMEOUT can be used to explain decision making processes of ML models transparently and interpretably thus enhancing trustworthiness and accountability. In summary, this paper asserts that LIMEOUT is promising approach towards making ML models fair and transparent thereby reducing ethical concerns across domains.",1,AI
"We consider the numerical stability of the parameter recovery problem in Linear Structural Equation Model ($\LSEM$) of causal inference. A long line of work starting from Wright (1920) has focused on understanding which sub-classes of $\LSEM$ allow for efficient parameter recovery. Despite decades of study, this question is not yet fully resolved. The goal of this paper is complementary to this line of work; we want to understand the stability of the recovery problem in the cases when efficient recovery is possible. Numerical stability of Pearl's notion of causality was first studied in Schulman and Srivastava (2016) using the concept of condition number where they provide ill-conditioned examples. In this work, we provide a condition number analysis for the $\LSEM$. First we prove that under a sufficient condition, for a certain sub-class of $\LSEM$ that are \emph{bow-free} (Brito and Pearl (2002)), the parameter recovery is stable. We further prove that \emph{randomly} chosen input parameters for this family satisfy the condition with a substantial probability. Hence for this family, on a large subset of parameter space, recovery is numerically stable. Next we construct an example of $\LSEM$ on four vertices with \emph{unbounded} condition number. We then corroborate our theoretical findings via simulations as well as real-world experiments for a sociology application. Finally, we provide a general heuristic for estimating the condition number of any $\LSEM$ instance.",0,Human
"Cybercrime markets support the development and diffusion of new attack technologies, vulnerability exploits, and malware. Whereas the revenue streams of cyber attackers have been studied multiple times in the literature, no quantitative account currently exists on the economics of attack acquisition and deployment. Yet, this understanding is critical to characterize the production of (traded) exploits, the economy that drives it, and its effects on the overall attack scenario. In this paper we provide an empirical investigation of the economics of vulnerability exploitation, and the effects of market factors on likelihood of exploit. Our data is collected first-handedly from a prominent Russian cybercrime market where the trading of the most active attack tools reported by the security industry happens. Our findings reveal that exploits in the underground are priced similarly or above vulnerabilities in legitimate bug-hunting programs, and that the refresh cycle of exploits is slower than currently often assumed. On the other hand, cybercriminals are becoming faster at introducing selected vulnerabilities, and the market is in clear expansion both in terms of players, traded exploits, and exploit pricing. We then evaluate the effects of these market variables on likelihood of attack realization, and find strong evidence of the correlation between market activity and exploit deployment. We discuss implications on vulnerability metrics, economics, and exploit measurement.",0,Human
"We consider finite horizon reach-avoid problems for discrete time stochastic systems. Our goal is to construct upper bound functions for the reach-avoid probability by means of tractable convex optimization problems. We achieve this by restricting attention to the span of Gaussian radial basis functions and imposing structural assumptions on the transition kernel of the stochastic processes as well as the target and safe sets of the reach-avoid problem. In particular, we require the kernel to be written as a Gaussian mixture density with each mean of the distribution being affine in the current state and input and the target and safe sets to be written as intersections of quadratic inequalities. Taking advantage of these structural assumptions, we formulate a recursion of semidefinite programs where each step provides an upper bound to the value function of the reach- avoid problem. The upper bounds provide a performance metric to which any suboptimal control policy can be compared, and can themselves be used to construct suboptimal control policies. We illustrate via numerical examples that even if the resulting bounds are conservative, the associated control policies achieve higher reach-avoid probabilities than heuristic controllers for problems of large state-input space dimensions (more than 20). The results presented in this paper, far exceed the limits of current approximation methods for reach-avoid problems in the specific class of stochastic systems considered.",0,Human
"This paper investigates the use of heterogeneity for robust best choice identification in federated settings. Researchers seek to solve the challenge of choosing the best option when faced with multiple options (arms) that are spread among different parties (agents) and each has its own distribution. Proposed solution uses differences among these agents to design an algorithm that identifies the best choice robustly and efficiently. Results from diverse simulation cases show effectiveness compared to other methods. Findings advance robust algorithms useful for federated multi choice bandits, useful for many fields such as personalized recommendation systems and online ads.",1,AI
This paper introduces an innovative method for creating one-to-one correspondences among unordered trees that preserve both their structural similarity and labels; this method is called Anti-Tai Mapping. We develop this from the idea of tree tai mapping but modify it to deal specifically with unordered trees and to meet anti isomorphism constraints. Performance evaluation against other methods was conducted using several data sets and we showed that this new method excels in accuracy and speed. Results from this research have application in many areas including pattern recognition and machine learning.,1,AI
This work introduces a new method for sequence learning through use of a recurrent neural network based on Semantic Variational Autoencoder (RNN SVAE). The model uses an encoding decoding structure similar to traditional autoencoders but incorporates variational inference and a recurrent neural network to improve sequence modeling. This RNN SVAE learns meaningful latent representations of input sequences and can generate output sequences with very high semantic coherence. Results from experiments indicate this proposed method performs better than previous sequence models on diverse text generation tasks; it excels at extracting underlying structure and meaning of sequences.,1,AI
"In Federated Learning (FL), a strong global model is collaboratively learned by aggregating the clients' locally trained models. Although this allows no need to access clients' data directly, the global model's convergence often suffers from data heterogeneity. This paper suggests that forgetting could be the bottleneck of global convergence. We observe that fitting on biased local distribution shifts the feature on global distribution and results in forgetting of global knowledge. We consider this phenomenon as an analogy to Continual Learning, which also faces catastrophic forgetting when fitted on the new task distribution. Based on our findings, we hypothesize that tackling down the forgetting in local training relives the data heterogeneity problem. To this end, we propose a simple yet effective framework Federated Local Self-Distillation (FedLSD), which utilizes the global knowledge on locally available data. By following the global perspective on local data, FedLSD encourages the learned features to preserve global knowledge and have consistent views across local models, thus improving convergence without compromising data privacy. Under our framework, we further extend FedLSD to FedLS-NTD, which only considers the not-true class signals to compensate noisy prediction of the global model. We validate that both FedLSD and FedLS-NTD significantly improve the performance in standard FL benchmarks in various setups, especially in the extreme data heterogeneity cases.",0,Human
Studying how the brain learns and infers relations from cortical networks is critical for understanding brain functioning. We introduce a new computational framework that models learning and inference of relations in these networks. Our approach relies on deep learning theory which utilizes artificial neural networks to process large datasets. The framework is trained using a dataset containing sensory inputs paired with targets and learns the relationship between them. Results show that this proposed framework accurately infers underlying relationships within cortical networks opening up new avenues for investigation into brain function. Implications are significant for development of new AI systems and deeper understanding of brain functioning.,1,AI
"This paper looks into ethical debates over actuarial risk assessment and using interventions to change forecasted results. Traditionally, actuarial assessment uses predictive algorithms to find people who are likely to have bad results and then allocates resources accordingly. But this practice raises ethical questions about possible discrimination and unfair treatment. The paper suggests refocusing the discussion toward interventions instead of just prediction as a way to move toward a fairer approach to assessing risk through actuarial means. Interventions should target the underlying causes of risk and enhance positive results rather than just reducing forecasted negatives. In conclusion, the paper stresses that consideration of ethics is important when assessing risk via actuarial means and that there needs to be ongoing dialogue about appropriate use of interventions in this area.",1,AI
This research report looks into correlation patterns between pollutants and child cancer incidence. It uses a large dataset of pollution levels and cancer cases in a particular region. It explains how they identified relationships among these variables and reports the outcomes of their analysis. Results show strong clustering of high levels of certain pollutants with increased cancer incidence in the study region. Results from this work add to the discussion about environmental pollution effects on health and offer important data to policy makers and public health authorities for regulatory decision making.,1,AI
"We propose NormalGAN, a fast adversarial learning-based method to reconstruct the complete and detailed 3D human from a single RGB-D image. Given a single front-view RGB-D image, NormalGAN performs two steps: front-view RGB-D rectification and back-view RGBD inference. The final model was then generated by simply combining the front-view and back-view RGB-D information. However, inferring backview RGB-D image with high-quality geometric details and plausible texture is not trivial. Our key observation is: Normal maps generally encode much more information of 3D surface details than RGB and depth images. Therefore, learning geometric details from normal maps is superior than other representations. In NormalGAN, an adversarial learning framework conditioned by normal maps is introduced, which is used to not only improve the front-view depth denoising performance, but also infer the back-view depth image with surprisingly geometric details. Moreover, for texture recovery, we remove shading information from the front-view RGB image based on the refined normal map, which further improves the quality of the back-view color inference. Results and experiments on both testing data set and real captured data demonstrate the superior performance of our approach. Given a consumer RGB-D sensor, NormalGAN can generate the complete and detailed 3D human reconstruction results in 20 fps, which further enables convenient interactive experiences in telepresence, AR/VR and gaming scenarios.",0,Human
"This paper introduces an approach using deep reinforcement learning (DRL) for content caching in VECNs along with permissioned blockchains. This DRL algorithm optimizes caching strategies per vehicle by taking into account things like free space, content popularity and network conditions. Permissioned blockchains handle distribution and sharing of cached content and assure secure transactions among participants in this system. Simulation results indicate this proposed DRL caching method performs better than traditional methods in hit rates and energy efficiency. Combining DRL and permissioned blockchains thus suggests a promising solution for caching content within VECNs and greatly enhances system performance.",1,AI
"We develop here a computationally effective approach for producing high-quality $\mathcal{H}_\infty$-approximations to large scale linear dynamical systems having multiple inputs and multiple outputs (MIMO). We extend an approach for $\mathcal{H}_\infty$ model reduction introduced by Flagg, Beattie, and Gugercin for the single-input/single-output (SISO) setting, which combined ideas originating in interpolatory $\mathcal{H}_2$-optimal model reduction with complex Chebyshev approximation. Retaining this framework, our approach to the MIMO problem has its principal computational cost dominated by (sparse) linear solves, and so it can remain an effective strategy in many large-scale settings. We are able to avoid computationally demanding $\mathcal{H}_\infty$ norm calculations that are normally required to monitor progress within each optimization cycle through the use of ""data-driven"" rational approximations that are built upon previously computed function samples. Numerical examples are included that illustrate our approach. We produce high fidelity reduced models having consistently better $\mathcal{H}_\infty$ performance than models produced via balanced truncation; these models often are as good as (and occasionally better than) models produced using optimal Hankel norm approximation as well. In all cases considered, the method described here produces reduced models at far lower cost than is possible with either balanced truncation or optimal Hankel norm approximation.",0,Human
"With the explosive growth in the number of pictures taken by smartphones, organizing and searching pictures has become important tasks. To efficiently fulfill these tasks, the key enabler is annotating images with proper keywords, with which keyword-based searching and organizing become available for images. Currently, smartphones usually synchronize photo albums with cloud storage platforms, and have their images annotated with the help of cloud computing. However, the ""offloading-to-cloud"" solution may cause privacy breach, since photos from smart photos contain various sensitive information. For privacy protection, existing research made effort to support cloud-based image annotation on encrypted images by utilizing cryptographic primitives. Nevertheless, for each annotation, it requires the cloud to perform linear checking on the large-scale encrypted dataset with high computational cost. This paper proposes a cloud-assisted privacy-preserving image annotation with randomized kd-forest, namely CPAR. With CPAR, users are able to automatically assign keywords to their images by leveraging the power of cloud with privacy protected. CPAR proposes a novel privacy-preserving randomized kd-forest structure, which significantly improves the annotation performance compared with existing research. Thorough analysis is carried out to demonstrate the security of CPAR. Experimental evaluation on the well-known IAPR TC-12 dataset validates the efficiency and effectiveness of CPAR.",0,Human
This study introduces FILIP as a new model which uses fine integration of language and image models for better performance in tasks involving computer vision and NLP. FILIP combines both language and visual recognition models and is trained using a large corpus of text and images. Evaluation results on various benchmarks show marked improvement compared to current models; this indicates the effectiveness of the fine integration approach to training models combining language and vision. Results indicate that FILIP could prove useful across a broad spectrum of tasks within AI.,1,AI
"This paper introduces Augury which is a new tool designed for analyzing and predicting performance metrics of systems and networks. Augury uses sophisticated learning algorithms including neural networks and time series decomposition to recognize trends and irregularities in performance metrics. Augury also includes a rich selection of predictive models which allow system administrators to forecast performance accurately. Results obtained by using real data from different systems and networks show that Augury excels at accuracy, scalability and speed compared to other solutions. Additionally, Augury delivers important understanding of root causes of performance problems; this understanding can help to optimize performance. Augury should enhance efficiency and reliability of complicated systems and networks and reduce costs and efforts related to monitoring and management.",1,AI
"Adversarial examples are often cited by neuroscientists and machine learning researchers as an example of how computational models diverge from biological sensory systems. Recent work has proposed adding biologically-inspired components to visual neural networks as a way to improve their adversarial robustness. One surprisingly effective component for reducing adversarial vulnerability is response stochasticity, like that exhibited by biological neurons. Here, using recently developed geometrical techniques from computational neuroscience, we investigate how adversarial perturbations influence the internal representations of standard, adversarially trained, and biologically-inspired stochastic networks. We find distinct geometric signatures for each type of network, revealing different mechanisms for achieving robust representations. Next, we generalize these results to the auditory domain, showing that neural stochasticity also makes auditory models more robust to adversarial perturbations. Geometric analysis of the stochastic networks reveals overlap between representations of clean and adversarially perturbed stimuli, and quantitatively demonstrates that competing geometric effects of stochasticity mediate a tradeoff between adversarial and clean performance. Our results shed light on the strategies of robust perception utilized by adversarially trained and stochastic networks, and help explain how stochasticity may be beneficial to machine and biological computation.",0,Human
This paper introduces a new way to plan behavior of connected autonomous cars using feedback deep reinforcement learning. We focus on developing systems that learn from previous experience and make smart decisions in real time to achieve safe and efficient driving on roads. Our proposal combines deep reinforcement learning and feedback control; the latter component adds extra stability and robustness. Effectiveness is tested via simulations and experiments which show marked improvement in safety and efficiency compared to current methods. Results suggest this approach has practical value for behavior planning in connected cars.,1,AI
"Regression analysis-based approaches have been widely studied for face recognition (FR) in the past several years. More recently, to better deal with some difficult conditions such as occlusions and illumination, nuclear norm based matrix regression methods have been proposed to characterize the low-rank structure of the error image, which generalize the one-dimensional, pixel-based error model to the two-dimensional structure. These methods, however, are inherently devised for grayscale image based FR and without exploiting the color information which is proved beneficial for FR of color face images. Benefiting from quaternion representation, which is capable of encoding the cross-channel correlation of color images, we propose a novel color FR method by formulating the color FR problem as a nuclear norm based quaternion matrix regression (NQMR). We further develop a more robust model called R-NQMR by using the logarithm of the nuclear norm, instead of the original nuclear norm, which adaptively assigns weights on different singular values, and then extend it to deal with the mixed noise. The proposed models, then, are solved using the effective alternating direction multiplier method (ADMM). Experiments on several public face databases demonstrate the superior performance and efficacy of the proposed approaches for color FR, especially for some difficult conditions (occlusion, illumination and mixed noise) over some state-of-the-art regression analysis-based approaches.",0,Human
This paper establishes a lower bound for deterministic Turing machine algorithms that solve the Boolean Satisfiability (SAT) problem. SAT is a core question in computer science asking if there exists an assignment of truth values that satisfies a given Boolean formula. This paper asserts that any deterministic Turing machine solving SAT requires at least time 2^n divided by some polynomial function p(n) of n; here n denotes the number of variables in the input formula and p(n) is some polynomial. Proving this lower bound uses a new technique for constructing hard SAT instances requiring many steps. Expanding formulas are used here: extra clauses are added redundantly so any solution must satisfy a large part of them. Showing that deterministic algorithms need to check these extra clauses very slowly proves the bound. Results are important because they set a strong lower limit for efficient deterministic solutions to SAT: this problem has been extensively studied in theoretical computer science. Lower bounds imply there is a fundamental barrier to efficiency for deterministic algorithms generally. Concluding remarks discuss further research directions suggested by this work.,1,AI
"This research investigates how to use distributed scheduling algorithms for event analytics in environments that mix edge devices and clouds. The goal is to fairly divide computational loads among both sides and ensure timely and precise analytics of events. A method combining splitting tasks and load balancing algorithms is used to dynamically distribute analytics work according to resource availability both at the edge and in the cloud. Experimental results show that this approach successfully balances computing loads, decreases latency, and enhances overall efficiency of analytics processes. Results indicate this research offers practical solutions to analytics in edge cloud environments which are important for various new IoT and cyber physical system applications.",1,AI
"Color constancy is the capability of the visual system of humans to accurately perceive colors under diverse lighting conditions. In computer vision research this is a hard problem that has been tackled by applying various methods like gray world, white patch, and shade of gray. However, none of these methods performs consistently across all situations and they consume much computing power. We consider camera adaptation as a few shot meta learning problem. Proposed method uses a meta learning framework to enable quick adaptation of models to new cameras and lighting using just a few examples. Results on different datasets show the proposed approach excels traditional algorithms both in performance and efficiency. This study offers a promising solution for adaptive color constancy and opens avenues for application development in fields such as computer vision and image processing.",1,AI
"Non-orthogonal multiple access (NOMA) has shown potential for scalable multicast of video data. However, one key drawback for NOMA-based video multicast is the limited number of layers allowed by the embedded successive interference cancellation algorithm, failing to meet satisfaction of heterogeneous receivers. We propose a novel receiver-driven superposed video multicast (Supcast) scheme by integrating Softcast, an analog-like transmission scheme, into the NOMA-based system to achieve high bandwidth efficiency as well as gradual decoding quality proportional to channel conditions at receivers. Although Softcast allows gradual performance by directly transmitting power-scaled transformation coefficients of frames, it suffers performance degradation due to discarding coefficients under insufficient bandwidth and its power allocation strategy cannot be directly applied in NOMA due to interference. In Supcast, coefficients are grouped into chunks, which are basic units for power allocation and superposition scheduling. By bisecting chunks into base-layer chunks and enhanced-layer chunks, the joint power allocation and chunk scheduling is formulated as a distortion minimization problem. A two-stage power allocation strategy and a near-optimal low-complexity algorithm for chunk scheduling based on the matching theory are proposed. Simulation results have shown the advantage of Supcast against Softcast as well as the reference scheme in NOMA under various practical scenarios.",0,Human
"Accurately modeling human decision-making in security is critical to thinking about when, why, and how to recommend that users adopt certain secure behaviors. In this work, we conduct behavioral economics experiments to model the rationality of end-user security decision-making in a realistic online experimental system simulating a bank account. We ask participants to make a financially impactful security choice, in the face of transparent risks of account compromise and benefits offered by an optional security behavior (two-factor authentication). We measure the cost and utility of adopting the security behavior via measurements of time spent executing the behavior and estimates of the participant's wage. We find that more than 50% of our participants made rational (e.g., utility optimal) decisions, and we find that participants are more likely to behave rationally in the face of higher risk. Additionally, we find that users' decisions can be modeled well as a function of past behavior (anchoring effects), knowledge of costs, and to a lesser extent, users' awareness of risks and context (R2=0.61). We also find evidence of endowment effects, as seen in other areas of economic and psychological decision-science literature, in our digital-security setting. Finally, using our data, we show theoretically that a ""one-size-fits""-all emphasis on security can lead to market losses, but that adoption by a subset of users with higher risks or lower costs can lead to market gains.",0,Human
"This paper introduces a new method for inferring signals which are correlated. This method uses free energy exploration to estimate the landscape of free energy for a group of correlated signals and then explores that landscape to identify the most probable configuration of those signals. The hypothesis here is that free energy serves as an excellent proxy for entropy and that exploring the free energy landscape can infer the structure underlying the correlated signals. We test this method by applying it to both simulated and real data and comparing its performance against other methods. Results show that free energy exploration proves effective at inferring structure and performs better than competing methods in many cases. This method could be useful for solving diverse problems such as analyzing complicated systems, inferring hidden variables, and discovering regularities in high dimensional data.",1,AI
"Authors propose a new method called Layered Group Sparse Beamforming (LGSB) in this paper for green wireless networks that include caching capabilities. This new method aims at optimizing both energy efficiency and data throughput. Based on dividing users into layers, LGSB allocates sparse beamforming resources to each layer. Results from simulations demonstrate that LGSB significantly improves energy efficiency and maintains high data throughput compared to traditional methods. Findings of this work contribute to development of networks that are more sustainable and energy efficient.",1,AI
"Wide Area Monitoring Systems (WAMS) utilizing synchrophasor measurements is considered one of the essential parts in smart grids that enable system operators to monitor, operate, and control power systems in wide geographical area. On the other hand, high-speed, reliable and scalable data communication infrastructure is crucial in both construction and operation of WAMS. Universal mobile Telecommunication System (UMTS), the 3G standard for mobile communication networks, was developed to provide high speed data transmission with reliable service performance for mobile users. Therefore, UMTS is considered a promising solution for providing a communication infrastructure for WAMS. 3G based EWAMS (Egyptian wide area Monitoring System) is designed and implemented in Egypt through deployment a number of frequency disturbance recorders (FDRs) devices on a live 220kV/500kV Egyptian grid in cooperation with the Egyptian Electricity Transmission Company (EETC). The developed EWAMS can gather information from 11 FDRs devices which are geographically dispersed throughout the boundary of the Egyptian power grid and to a remote data management center located at Helwan University. The communication performance for the developed EWAMS in terms of communication time delay, throughput, and percentage of wasted bandwidth are studied in this paper. The results showed that the system can achieve successfully the communication requirements needed by various wide area monitoring applications.",0,Human
This paper introduces a new detection method with low computational cost that works on channels where multiple users send information using multiple transmitters and receivers. This method uses the natural structure of the channel and employs iterative detection to obtain full diversity gain but avoids the high computational costs of usual detection methods. Results from simulations show that this new method obtains full diversity and does better in terms of bit error rate than other detection methods while also being less complex computationally. This new method has potential as a good solution for systems that need to communicate at high data rates and have low computational capacity.,1,AI
"This paper studies coordination policies learned by robot swarms. Robot swarms include many simple robots which can cooperate to perform complex tasks. Coordination among such swarms is hard because of inherent complexity and absence of centralized controllers. Here we propose a reinforcement learning method to learn coordination policies. We use a central critic and distributed actors to learn coordination. Central critic observes overall swarm state and gives feedback to actors who act locally based on observation of local environment. Feedback from critic updates actor policy parameters leading to coordinated actions among members of the swarm. We test our approach on formation control, obstacle avoidance and cooperative transport benchmark tasks. Results show our approach learns good coordination policies efficiently and generalizes well to new situations. Overall, we show promise of reinforcement learning for learning coordination among robots and provide a basis for further research. Through this, we also open the door to developing more advanced and efficient robot swarms which can tackle diverse environments.",1,AI
"Kernel machines often yield superior predictive performance on various tasks; however, they suffer from severe computational challenges. In this paper, we show how to overcome the important challenge of speeding up kernel machines. In particular, we develop a parallel block minimization framework for solving kernel machines, including kernel SVM and kernel logistic regression. Our framework proceeds by dividing the problem into smaller subproblems by forming a block-diagonal approximation of the Hessian matrix. The subproblems are then solved approximately in parallel. After that, a communication efficient line search procedure is developed to ensure sufficient reduction of the objective function value at each iteration. We prove global linear convergence rate of the proposed method with a wide class of subproblem solvers, and our analysis covers strongly convex and some non-strongly convex functions. We apply our algorithm to solve large-scale kernel SVM problems on distributed systems, and show a significant improvement over existing parallel solvers. As an example, on the covtype dataset with half-a-million samples, our algorithm can obtain an approximate solution with 96% accuracy in 20 seconds using 32 machines, while all the other parallel kernel SVM solvers require more than 2000 seconds to achieve a solution with 95% accuracy. Moreover, our algorithm can scale to very large data sets, such as the kdd algebra dataset with 8 million samples and 20 million features.",0,Human
"This paper looks at how people perceive and rate differently erasure and undetected errors in digital communication systems. Specifically it explores reasons why users have different views towards these two kinds of mistakes and the reasons behind them. At first the paper introduces what erasure and undetected errors are and explains their significance. Then it surveys previous research into user perception of these two types of errors and points out that there is little agreement among researchers. Research methodology includes conducting a series of experiments that measure participant evaluation of probabilities of these two types of errors within a simulated communication environment. Results include quantitative and qualitative data such as severity perception and subjective probability ratings and participants' explanations of their evaluations. Results show that generally users view undetected errors as more severe and impactful on trust. Factors leading to this difference are also identified including role of system design, user experience and individual differences in risk perception. In summary this paper advances understanding of user evaluation of errors in digital communication systems and offers suggestions for designers and engineers to improve design quality and trustworthiness.",1,AI
"Domain-Specific Languages (DSLs) help practitioners in contributing solutions to challenges of specific domains. The efficient development of user-friendly DSLs suitable for industrial practitioners with little expertise in modelling still is challenging. For such practitioners, who often do not model on a daily basis, there is a need to foster reduction of repetitive modelling tasks and providing simplified visual representations of DSL parts. For industrial language engineers, there is no methodical support for providing such guidelines or documentation as part of reusable language modules. Previous research either addresses the reuse of languages or guidelines for modelling. For the efficient industrial deployment of DSLs, their combination is essential: the efficient engineering of DSLs from reusable modules that feature integrated documentation and guidelines for industrial practitioners. To solve these challenges, we propose a systematic approach for the industrial engineering of DSLs based on the concept of reusable DSL Building Blocks, which rests on several years of experience in the industrial engineering of DSLs and their deployment to various organizations. We investigated our approach via focus group methods consisting of five participants from industry and research qualitatively. Ultimately, DSL Building Blocks support industrial language engineers in developing better usable DSLs and industrial practitioners in more efficiently achieving their modelling.",0,Human
"This paper suggests an integrated approach for computing resource allocation using Stackelberg games and matching algorithms for IoT fog networks which have three layers: cloud, fog, and edge; each layer has distinct computational resources. Using Stackelberg games the authors model interaction among these layers as a leader and follower relationship; meanwhile they use matching algorithms to optimize resource allocation based on constraints and goals of the network. Results from simulations show this proposed method is effective at improving efficiency of resource allocation, reducing latency and enhancing network performance relative to conventional approaches. Results also indicate promising potential to combine game theory and optimization techniques for solving resource allocation issues across multiple tiers of IoT networks.",1,AI
This paper looks into structured mappings for sharing information among many users through multiple access channels. Specifically this work focuses on whether structured mappings offer advantages like efficient communication of shared information and reducing interference along with increasing channel capacity. A theoretical framework is presented for designing such mappings which balance well between conveying common information and channel resource requirements. Practical algorithms are also developed based on sparse graph codes for constructing such mappings. Results from simulations show that this method performs better in terms of rate achieved and decoding complexity compared to previous approaches. At the end the authors conclude that structured mappings are promising solutions to enhance efficiency and capacity of channels and they have potential uses in wireless networks and multimedia communications as well as others.,1,AI
"This paper introduces a new approach to estimating depth unsupervisedly by using H-Net, which is a deep neural network with an attention mechanism, and using epipolar geometry. Depth estimation from stereo images is an important task in computer vision and has applications such as robotics and autonomous driving and 3D reconstruction. The proposed approach uses attention to focus on key parts of the images for estimating depth. Additionally, using epipolar geometry enforces geometric consistency on the estimated depths. Experimental results show this approach performs better compared to leading unsupervised methods on benchmark data sets. Also this approach is easy to adapt for other closely related tasks like flow estimation and super resolution. Results suggest that using H Net along with epipolar geometry is promising for unsupervised depth estimation with wide applicability in computer vision.",1,AI
"This paper introduces Serket which is a new architecture for connecting stochastic models so that we can build larger cognitive systems. Based on modular design, this architecture integrates multiple models into one system. Using advantages like flexibility with uncertainty and probabilistic predictions, Serket overcomes the disadvantages like integrating multiple models. Performance is measured across many tasks including language understanding and generation, decision making and perception. Results show high performance metrics such as accuracy and interpretability; Serket excels compared to other methods. At the end, this work discusses implications of Serket architecture and considers future development of advanced cognitive systems along with practical applications such as AI and robotics.",1,AI
"This paper introduces an innovative way to classify musical artists using Convolutional Recurrent Neural Networks (CRNNs). The model extracts both temporal and spectral features directly from raw audio signal and captures both near term and long term features. We test this method using a large dataset and we compare against leading techniques. Results show that compared to others, our method based on CRNNs outperforms in terms of accuracy and effectiveness. This indicates potential for using deep learning in musical classification tasks.",1,AI
"We present here an innovative approach for designing faster sub linear algorithms using conditional sampling. Sub linear algorithms allow fast processing of large data sets by not reading every element directly. Our approach relies on conditional sampling which enables obtaining accurate estimations of specific functions with high probability by sampling only a small fraction of the data. This leads to a notable reduction in computation time alongside ensuring high accuracy. Results from our experiments conducted on diverse real data sets demonstrate improvements compared to current algorithms. This study paves the way forward for future research into sub linear algorithms and has practical relevance for different disciplines such as machine learning, data mining and big data analysis.",1,AI
"Dense Multi-GPU systems have recently gained a lot of attention in the HPC arena. Traditionally, MPI runtimes have been primarily designed for clusters with a large number of nodes. However, with the advent of MPI+CUDA applications and CUDA-Aware MPI runtimes like MVAPICH2 and OpenMPI, it has become important to address efficient communication schemes for such dense Multi-GPU nodes. This coupled with new application workloads brought forward by Deep Learning frameworks like Caffe and Microsoft CNTK pose additional design constraints due to very large message communication of GPU buffers during the training phase. In this context, special-purpose libraries like NVIDIA NCCL have been proposed for GPU-based collective communication on dense GPU systems. In this paper, we propose a pipelined chain (ring) design for the MPI_Bcast collective operation along with an enhanced collective tuning framework in MVAPICH2-GDR that enables efficient intra-/inter-node multi-GPU communication. We present an in-depth performance landscape for the proposed MPI_Bcast schemes along with a comparative analysis of NVIDIA NCCL Broadcast and NCCL-based MPI_Bcast. The proposed designs for MVAPICH2-GDR enable up to 14X and 16.6X improvement, compared to NCCL-based solutions, for intra- and inter-node broadcast latency, respectively. In addition, the proposed designs provide up to 7% improvement over NCCL-based solutions for data parallel training of the VGG network on 128 GPUs using Microsoft CNTK.",0,Human
"The world is facing a tough situation due to the catastrophic pandemic caused by novel coronavirus (COVID-19). The number people affected by this virus are increasing exponentially day by day and the number has already crossed 6.4 million. As no vaccine has been discovered yet, the early detection of patients and isolation is the only and most effective way to reduce the spread of the virus. Detecting infected persons from chest X-Ray by using Deep Neural Networks, can be applied as a time and laborsaving solution. In this study, we tried to detect Covid-19 by classification of Covid-19, pneumonia and normal chest X-Rays. We used five different Convolutional Pre-Trained Neural Network models (VGG16, VGG19, Xception, InceptionV3 and Resnet50) and compared their performance. VGG16 and VGG19 shows precise performance in classification. Both models can classify between three kinds of X-Rays with an accuracy over 92%. Another part of our study was to find the impact of weather factors (temperature, humidity, sun hour and wind speed) on this pandemic using Decision Tree Regressor. We found that temperature, humidity and sun-hour jointly hold 85.88% impact on escalation of Covid-19 and 91.89% impact on death due to Covid-19 where humidity has 8.09% impact on death. We also tried to predict the death of an individual based on age, gender, country, and location due to COVID-19 using the LogisticRegression, which can predict death of an individual with a model accuracy of 94.40%.",0,Human
"This paper studies how machine learning algorithms can be used to forecast congestion related to routing during high level synthesis in Field Programmable Gate Arrays (FPGAs). Congestion is a major barrier in FPGA design and can result in delay and higher cost. Thus, accurate predictions of congestion are essential to optimize design parameters and meet deadlines for delivering FPGA based products. Proposed methodology uses decision trees and random forests along with traditional metrics to forecast congestion during synthesis process. The study also evaluates the performance of various feature extraction techniques and algorithms on accuracy of congestion forecasting. Experimental results show that this proposed approach performs very well and outperforms traditional congestion prediction methods. Results indicate that PCA and ICA among others perform much better at improving forecast accuracy. This research introduces a practical and effective way to predict congestion during high synthesis in FPGA design. Proposed approach can help optimize parameters and minimize iteration steps which reduces development cost and accelerates product delivery. This method can also be applicable to other optimization problems in FPGA design and shows the benefits of integrating machine learning into design.",1,AI
"This paper introduces a framework for improving lexical semantic resources using distributional semantics. The framework seeks to overcome shortcomings of traditional resources like WordNet and Ontologies by incorporating distributional information from large text corpora. This framework contains three core parts: (1) data collection, (2) feature extraction and (3) semantic enrichment. Data collection gathers large volumes of text from diverse sources including forums, news articles, and social media. Feature extraction uses methods such as word2vec and GloVe to extract vector representations of words within collected data. Finally, this enriched vector representation is integrated into existing resources like WordNet and Ontologies to increase their coverage and accuracy. Performance of benchmark tasks like similarity and analogies is tested against this framework and results show marked improvement over existing resources. Overall the framework offers practical solutions for enhancement of lexical semantic resources and could influence many NLP tasks.",1,AI
"This paper introduces an approach using Approximate Dynamic Programming (ADP) for disaster recovery management. Traditional methods typically rely on fixed and unchanging plans that do not work well in complex and changing situations after disasters occur. Proposed here is an ADP approach that learns from past disasters and adapts dynamically to changes in the status of affected communities. Combining machine learning techniques with a dynamic optimization framework, ADP provides flexible decision making capabilities for recovery management. This framework takes into account various goals such as reducing recovery time, increasing resource use, and reducing cost. It also considers uncertainties and limits related to recovery processes. The ADP approach incorporates feedback from community members and stakeholders so the system can learn and improve performance over time. Results from a case study show that the new method performs much better than traditional static recovery plans. Furthermore, ADP quickly responds to shifts in the community and recovery conditions and manages recovery efficiently and effectively. In sum, this research demonstrates the advantage of using ADP for disaster recovery management and offers policymakers strong tools to better handle natural disasters. ADP excels at adapting to past lessons learned, balancing diverse recovery objectives, and producing more resilient and sustainable communities.",1,AI
"This paper looks into how to do frequent updates using sensors through wireless networks. It particularly studies scheduling for both sensing and retransmission operations to maximize efficiency of these updates. Authors suggest a new scheduling algorithm that seeks a balance between sensing accuracy and energy use by changing sensing and transmission intervals according to network conditions. Results from simulation show that proposed method reduces energy consumption and improves accuracy of updates compared to current scheduling methods. Findings are significant for designing and deploying networks for diverse applications such as environmental monitoring, industrial control, and health care.",1,AI
"From the simple measurement of tissue attributes in pathology workflow to designing an explainable diagnostic/prognostic AI tool, access to accurate semantic segmentation of tissue regions in histology images is a prerequisite. However, delineating different tissue regions manually is a laborious, time-consuming and costly task that requires expert knowledge. On the other hand, the state-of-the-art automatic deep learning models for semantic segmentation require lots of annotated training data and there are only a limited number of tissue region annotated images publicly available. To obviate this issue in computational pathology projects and collect large-scale region annotations efficiently, we propose an efficient interactive segmentation network that requires minimum input from the user to accurately annotate different tissue types in the histology image. The user is only required to draw a simple squiggle inside each region of interest so it will be used as the guiding signal for the model. To deal with the complex appearance and amorph geometry of different tissue regions we introduce several automatic and minimalistic guiding signal generation techniques that help the model to become robust against the variation in the user input. By experimenting on a dataset of breast cancer images, we show that not only does our proposed method speed up the interactive annotation process, it can also outperform the existing automatic and interactive region segmentation models.",0,Human
"We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.",0,Human
"With the rapid development of artificial intelligence, conversational bots have became prevalent in mainstream E-commerce platforms, which can provide convenient customer service timely. To satisfy the user, the conversational bots need to understand the user's intention, detect the user's emotion, and extract the key entities from the conversational utterances. However, understanding dialogues is regarded as a very challenging task. Different from common language understanding, utterances in dialogues appear alternately from different roles and are usually organized as hierarchical structures. To facilitate the understanding of dialogues, in this paper, we propose a novel contextual dialogue encoder (i.e. DialogueBERT) based on the popular pre-trained language model BERT. Five self-supervised learning pre-training tasks are devised for learning the particularity of dialouge utterances. Four different input embeddings are integrated to catch the relationship between utterances, including turn embedding, role embedding, token embedding and position embedding. DialogueBERT was pre-trained with 70 million dialogues in real scenario, and then fine-tuned in three different downstream dialogue understanding tasks. Experimental results show that DialogueBERT achieves exciting results with 88.63% accuracy for intent recognition, 94.25% accuracy for emotion recognition and 97.04% F1 score for named entity recognition, which outperforms several strong baselines by a large margin.",0,Human
"Fishing is a popular pastime for leisure. But anglers regularly encounter difficulties both in finding and catching fish. This research aimed to understand angler problems and suggest solutions. We surveyed experienced anglers to get information on their fishing experiences and challenges and preferred methods of finding and catching fish. Results show that anglers commonly face difficulties such as insufficient access to fishing spots, poor water quality, and reduction in fish populations. Recommendations from this study include enhancing access to fishing spots, implementing conservation efforts to protect fish and their habitat, and educating anglers on sustainable fishing practices. Results of this study also provide information useful for managing and making policies to support sustainable use of fishing resources and improve angler experience.",1,AI
"We study the communication complexity of combinatorial auctions via interpolation mechanisms that interpolate between non-truthful and truthful protocols. Specifically, an interpolation mechanism has two phases. In the first phase, the bidders participate in some non-truthful protocol whose output is itself a truthful protocol. In the second phase, the bidders participate in the truthful protocol selected during phase one. Note that virtually all existing auctions have either a non-existent first phase (and are therefore truthful mechanisms), or a non-existent second phase (and are therefore just traditional protocols, analyzed via the Price of Anarchy/Stability).  The goal of this paper is to understand the benefits of interpolation mechanisms versus truthful mechanisms or traditional protocols, and develop the necessary tools to formally study them. Interestingly, we exhibit settings where interpolation mechanisms greatly outperform the optimal traditional and truthful protocols. Yet, we also exhibit settings where interpolation mechanisms are provably no better than truthful ones. Finally, we apply our new machinery to prove that the recent single-bid mechanism of Devanur et. al.~\cite{DevanurMSW15} (the only pre-existing interpolation mechanism in the literature) achieves the optimal price of anarchy among a wide class of protocols, a claim that simply can't be addressed by appealing just to machinery from communication complexity or the study of truthful mechanisms.",0,Human
"This paper introduces a new approach for optimizing propagation dynamics using message passing. Using both graph theory and probability theory together, they develop an accurate model of how information or influence spreads through networks. Authors show effectiveness through simulation experiments and real data tests on diverse networks; results indicate marked improvement over previous approaches. Results are relevant to many different areas including social network analysis, marketing and network design.",1,AI
"Multi-step manipulation tasks in unstructured environments are extremely challenging for a robot to learn. Such tasks interlace high-level reasoning that consists of the expected states that can be attained to achieve an overall task and low-level reasoning that decides what actions will yield these states. We propose a model-free deep reinforcement learning method to learn multi-step manipulation tasks. We introduce a Robotic Manipulation Network (RoManNet), which is a vision-based model architecture, to learn the action-value functions and predict manipulation action candidates. We define a Task Progress based Gaussian (TPG) reward function that computes the reward based on actions that lead to successful motion primitives and progress towards the overall task goal. To balance the ratio of exploration/exploitation, we introduce a Loss Adjusted Exploration (LAE) policy that determines actions from the action candidates according to the Boltzmann distribution of loss estimates. We demonstrate the effectiveness of our approach by training RoManNet to learn several challenging multi-step robotic manipulation tasks in both simulation and real-world. Experimental results show that our method outperforms the existing methods and achieves state-of-the-art performance in terms of success rate and action efficiency. The ablation studies show that TPG and LAE are especially beneficial for tasks like multiple block stacking. Code is available at: https://github.com/skumra/romannet",0,Human
"A sound Decision-Making (DM) process is key to the successful governance of software projects. In many Open Source Software Development (OSSD) communities, DM processes lie buried amongst vast amounts of publicly available data. Hidden within this data lie the rationale for decisions that led to the evolution and maintenance of software products. While there have been some efforts to extract DM processes from publicly available data, the rationale behind how the decisions are made have seldom been explored. Extracting the rationale for these decisions can facilitate transparency (by making them known), and also promote accountability on the part of decision-makers. This work bridges this gap by means of a large-scale study that unearths the rationale behind decisions from Python development email archives comprising about 1.5 million emails. This paper makes two main contributions. First, it makes a knowledge contribution by unearthing and presenting the rationale behind decisions made. Second, it makes a methodological contribution by presenting a heuristics-based rationale extraction system called Rationale Miner that employs multiple heuristics, and follows a data-driven, bottom-up approach to infer the rationale behind specific decisions (e.g., whether a new module is implemented based on core developer consensus or benevolent dictator's pronouncement). Our approach can be applied to extract rationale in other OSSD communities that have similar governance structures.",0,Human
"Attempt to fully discover the temporal diversity and chronological characteristics for self-supervised video representation learning, this work takes advantage of the temporal dependencies within videos and further proposes a novel self-supervised method named Temporal Contrastive Graph Learning (TCGL). In contrast to the existing methods that ignore modeling elaborate temporal dependencies, our TCGL roots in a hybrid graph contrastive learning strategy to jointly regard the inter-snippet and intra-snippet temporal dependencies as self-supervision signals for temporal representation learning. To model multi-scale temporal dependencies, our TCGL integrates the prior knowledge about the frame and snippet orders into graph structures, i.e., the intra-/inter- snippet temporal contrastive graphs. By randomly removing edges and masking nodes of the intra-snippet graphs or inter-snippet graphs, our TCGL can generate different correlated graph views. Then, specific contrastive learning modules are designed to maximize the agreement between nodes in different views. To adaptively learn the global context representation and recalibrate the channel-wise features, we introduce an adaptive video snippet order prediction module, which leverages the relational knowledge among video snippets to predict the actual snippet orders. Experimental results demonstrate the superiority of our TCGL over the state-of-the-art methods on large-scale action recognition and video retrieval benchmarks.",0,Human
"This paper introduces a new heterogeneous graph embedding framework named MTHetGNN which is designed for forecasting multivariate time series. This framework integrates strengths of both graph neural nets and attention mechanisms to model complicated relationships among different series and also captures their dependencies. The framework uses a heterogeneous graph to represent relationships among these series and uses graph convolutional nets to extract features from nodes of the graph. Attention mechanism is used to emphasize the importance of individual nodes in the forecast process. Evaluation results show that this new framework outperforms leading methods not only in terms of accuracy but also in efficiency. Results suggest that this study proposes a new way to handle forecasting of multivariate time series and this research might be applied in diverse fields including finance, energy and transport.",1,AI
"This paper introduces SCSGuard, a new approach using deep learning for spotting scams in smart contracts on Ethereum. Scams are problematic because they cost users big money. Previous methods usually rely on rules or static code analysis but SCSGuard uses a deep learning model to spot unusual patterns in bytecode. Performance is evaluated using a large dataset of known scams and clean code and training is done with supervised learning. Results show high performance compared to current best methods. Factors contributing to success include the choice of neural architecture, size and quality of dataset, and transfer learning. Limitations are also discussed; future work includes enhancing interpretability and evaluating robustness with adversarial examples. Overall we demonstrate that deep learning approaches have potential for detecting scams and suggest future research. We also think this system has great importance for making Ethereum ecosystem safer and more trustworthy. We hope future research continues this line.",1,AI
"This paper aims to conduct a comprehensive study on modeling beliefs in systems that change over time. It is divided into two sections: the first focuses on basic concepts. The authors start by defining belief and its importance within dynamic systems. They also review existing literature on modeling beliefs in these systems and discuss different approaches used. Next they introduce a framework for modeling beliefs in dynamic systems; this framework contains key elements including belief representation, updating belief and evaluating belief. It also considers aspects of uncertainty and ways of measuring this uncertainty. They also consider the challenges of modeling beliefs in systems that vary over time and list directions for future research. In summary, this paper lays a strong foundation for understanding how to model beliefs in systems that change. Authors review relevant literature and introduce a useful framework for both researchers and practitioners in this field. This paper is essential reading for anyone studying beliefs in changing systems.",1,AI
"Public speaking is an important aspect of human communication and interaction. The majority of computational work on public speaking concentrates on analyzing the spoken content, and the verbal behavior of the speakers. While the success of public speaking largely depends on the content of the talk, and the verbal behavior, non-verbal (visual) cues, such as gestures and physical appearance also play a significant role. This paper investigates the importance of visual cues by estimating their contribution towards predicting the popularity of a public lecture. For this purpose, we constructed a large database of more than $1800$ TED talk videos. As a measure of popularity of the TED talks, we leverage the corresponding (online) viewers' ratings from YouTube. Visual cues related to facial and physical appearance, facial expressions, and pose variations are extracted from the video frames using convolutional neural network (CNN) models. Thereafter, an attention-based long short-term memory (LSTM) network is proposed to predict the video popularity from the sequence of visual features. The proposed network achieves state-of-the-art prediction accuracy indicating that visual cues alone contain highly predictive information about the popularity of a talk. Furthermore, our network learns a human-like attention mechanism, which is particularly useful for interpretability, i.e. how attention varies with time, and across different visual cues by indicating their relative importance.",0,Human
"This study uses bibliometric analysis to conduct a review of literature on the research trends and patterns associated with the digital agricultural revolution. Reviews use data from Scopus, Web of Science and Google Scholar covering papers published between 2011 and 2021. Analysis looks at the rise of research on this topic, top cited authors and publications as well as key research themes and trends. Results indicate steady growth in research over the last decade with a focus on precision agriculture, sensors and big data analytics. Top cited researchers and papers are mostly from developed countries; this suggests an important gap in research from developing countries. This research also highlights future research needs to deal with challenges and opportunities of this digital agricultural revolution especially in developing countries.",1,AI
"This research investigates use of new terms and expressions on Facebook. A large body of data was collected from posts and comments to find out which new terms were used most and to understand patterns of their use. Results show that people use new words mainly for expressing feelings, making jokes and emphasizing feeling part of the group. It also turns out that new words are used more by younger people and spread fast because of the high connectivity. This work reveals how social media shape language; it points out that keeping an eye on new words is important.",1,AI
"Reinforcement learning has lead to considerable break-throughs in diverse areas such as robotics, games and many others. But the application to RL in complex real-world decision making problems remains limited. Many problems in operations management (inventory and revenue management, for example) are characterized by large action spaces and stochastic system dynamics. These characteristics make the problem considerably harder to solve for existing RL methods that rely on enumeration techniques to solve per step action problems. To resolve these issues, we develop Programmable Actor Reinforcement Learning (PARL), a policy iteration method that uses techniques from integer programming and sample average approximation. Analytically, we show that the for a given critic, the learned policy in each iteration converges to the optimal policy as the underlying samples of the uncertainty go to infinity. Practically, we show that a properly selected discretization of the underlying uncertain distribution can yield near optimal actor policy even with very few samples from the underlying uncertainty. We then apply our algorithm to real-world inventory management problems with complex supply chain structures and show that PARL outperforms state-of-the-art RL and inventory optimization methods in these settings. We find that PARL outperforms commonly used base stock heuristic by 44.7% and the best performing RL method by up to 12.1% on average across different supply chain environments.",0,Human
"Methods for neural network hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of model configurations. In this paper, we show that standard frequentist regression models can predict the final performance of partially trained model configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We empirically show that our performance prediction models are much more effective than prominent Bayesian counterparts, are simpler to implement, and are faster to train. Our models can predict final performance in both visual classification and language modeling domains, are effective for predicting performance of drastically varying model architectures, and can even generalize between model classes. Using these prediction models, we also propose an early stopping method for hyperparameter optimization and meta-modeling, which obtains a speedup of a factor up to 6x in both hyperparameter optimization and meta-modeling. Finally, we empirically show that our early stopping method can be seamlessly incorporated into both reinforcement learning-based architecture selection algorithms and bandit based search methods. Through extensive experimentation, we empirically show our performance prediction models and early stopping algorithm are state-of-the-art in terms of prediction accuracy and speedup achieved while still identifying the optimal model configurations.",0,Human
"This paper introduces a new method for image classification and segmentation using structured binary neural networks. Using binary weights and activations, this approach reduces memory and computational load while retaining high performance. Results show this structure outperforms conventional networks that use floating point weights and activations. The authors also extend this work to segmentation tasks and find marked improvements over previous work. Extensive experiments on standard data sets confirm the effectiveness of this proposed method. Use of structured binary networks thus promises to improve classification and segmentation performance with low cost in terms of computation and memory.",1,AI
"Photo-realistic modeling and rendering of fuzzy objects with complex opacity are critical for numerous immersive VR/AR applications, but it suffers from strong view-dependent brightness, color. In this paper, we propose a novel scheme to generate opacity radiance fields with a convolutional neural renderer for fuzzy objects, which is the first to combine both explicit opacity supervision and convolutional mechanism into the neural radiance field framework so as to enable high-quality appearance and global consistent alpha mattes generation in arbitrary novel views. More specifically, we propose an efficient sampling strategy along with both the camera rays and image plane, which enables efficient radiance field sampling and learning in a patch-wise manner, as well as a novel volumetric feature integration scheme that generates per-patch hybrid feature embeddings to reconstruct the view-consistent fine-detailed appearance and opacity output. We further adopt a patch-wise adversarial training scheme to preserve both high-frequency appearance and opacity details in a self-supervised framework. We also introduce an effective multi-view image capture system to capture high-quality color and alpha maps for challenging fuzzy objects. Extensive experiments on existing and our new challenging fuzzy object dataset demonstrate that our method achieves photo-realistic, globally consistent, and fined detailed appearance and opacity free-viewpoint rendering for various fuzzy objects.",0,Human
"Data structures for efficient sampling from a set of weighted items are an important building block of many applications. However, few parallel solutions are known. We close many of these gaps both for shared-memory and distributed-memory machines. We give efficient, fast, and practicable parallel algorithms for building data structures that support sampling single items (alias tables, compressed data structures). This also yields a simplified and more space-efficient sequential algorithm for alias table construction. Our approaches to sampling $k$ out of $n$ items with/without replacement and to subset (Poisson) sampling are output-sensitive, i.e., the sampling algorithms use work linear in the number of different samples. This is also interesting in the sequential case. Weighted random permutation can be done by sorting appropriate random deviates. We show that this is possible with linear work using a nonlinear transformation of these deviates. Finally, we give a communication-efficient, highly scalable approach to (weighted and unweighted) reservoir sampling. This algorithm is based on a fully distributed model of streaming algorithms that might be of independent interest. Experiments for alias tables and sampling with replacement show near linear speedups both for construction and queries using up to 158 threads of shared-memory machines. An experimental evaluation of distributed weighted reservoir sampling on up to 256 nodes (5120 cores) also shows good speedups.",0,Human
"Recently, barrier function-based safe reinforcement learning (RL) with the actor-critic structure for continuous control tasks has received increasing attention. It is still challenging to learn a near-optimal control policy with safety and convergence guarantees. Also, few works have addressed the safe RL algorithm design under time-varying safety constraints. This paper proposes a model-based safe RL algorithm for optimal control of nonlinear systems with time-varying state and control constraints. In the proposed approach, we construct a novel barrier-based control policy structure that can guarantee control safety. A multi-step policy evaluation mechanism is proposed to predict the policy's safety risk under time-varying safety constraints and guide the policy to update safely. Theoretical results on stability and robustness are proven. Also, the convergence of the actor-critic learning algorithm is analyzed. The performance of the proposed algorithm outperforms several state-of-the-art RL algorithms in the simulated Safety Gym environment. Furthermore, the approach is applied to the integrated path following and collision avoidance problem for two real-world intelligent vehicles. A differential-drive vehicle and an Ackermann-drive one are used to verify the offline deployment performance and the online learning performance, respectively. Our approach shows an impressive sim-to-real transfer capability and a satisfactory online control performance in the experiment.",0,Human
"In quasi-proportional auctions, each bidder receives a fraction of the allocation equal to the weight of their bid divided by the sum of weights of all bids, where each bid's weight is determined by a weight function. We study the relationship between the weight function, bidders' private values, number of bidders, and the seller's revenue in equilibrium. It has been shown that if one bidder has a much higher private value than the others, then a nearly flat weight function maximizes revenue. Essentially, threatening the bidder who has the highest valuation with having to share the allocation maximizes the revenue. We show that as bidder private values approach parity, steeper weight functions maximize revenue by making the quasi-proportional auction more like a winner-take-all auction. We also show that steeper weight functions maximize revenue as the number of bidders increases. For flatter weight functions, there is known to be a unique pure-strategy Nash equilibrium. We show that a pure-strategy Nash equilibrium also exists for steeper weight functions, and we give lower bounds for bids at an equilibrium. For a special case that includes the two-bidder auction, we show that the pure-strategy Nash equilibrium is unique, and we show how to compute the revenue at equilibrium. We also show that selecting a weight function based on private value ratios and number of bidders is necessary for a quasi-proportional auction to produce more revenue than a second-price auction.",0,Human
"Frequency estimation is important for many different tasks in signal processing. This paper introduces a new method for estimating frequencies that have an upper limit (one-sided error) - that is, we need to estimate frequencies such that they lie within some defined limits either above or below the actual frequency. This new method uses modified periodograms alongside a statistical hypothesis test. Results comparing this new approach against other methods show that this new method works better overall under different conditions; it is therefore valuable for people working on signal processing tasks that demand precise frequency estimation with upper limits.",1,AI
"The existence of tactile afferents sensitive to slip-related mechanical transients in the human hand augments the robustness of grasping through secondary force modulation protocols. Despite this knowledge and the fact that tactile-based slip detection has been researched for decades, robust slip detection is still not an out-of-the-box capability for any commercially available tactile sensor. This research seeks to bridge this gap with a comprehensive study addressing several aspects of slip detection. Key developments include a systematic data collection process yielding millions of sensory data points, the generalized conversion of multivariate-to-univariate sensor output, an insightful spectral analysis of the univariate sensor outputs, and the application of Long Short-Term Memory (LSTM) neural networks on the univariate signals to produce robust slip detectors from three commercially available sensors capable of tactile sensing. The sensing elements underlying these sensors vary in quantity, spatial arrangement, and mechanics, leveraging principles in electro-mechanical resistance, optics, and hydro-acoustics. Critically, slip detection performance of the tactile technologies is quantified through a measurement methodology that unveils the effects of data window size, sampling rate, material type, slip speed, and sensor manufacturing variability. Results indicate that the investigated commercial tactile sensors are inherently capable of high-quality slip detection.",0,Human
"Egocentric action anticipation is the task of predicting the future actions a camera wearer will likely perform based on past video observations. While in a real-world system it is fundamental to output such predictions before the action begins, past works have not generally paid attention to model runtime during evaluation. Indeed, current evaluation schemes assume that predictions can be made offline, and hence that computational resources are not limited. In contrast, in this paper, we propose a ``streaming'' egocentric action anticipation evaluation protocol which explicitly considers model runtime for performance assessment, assuming that predictions will be available only after the current video segment is processed, which depends on the processing time of a method. Following the proposed evaluation scheme, we benchmark different state-of-the-art approaches for egocentric action anticipation on two popular datasets. Our analysis shows that models with a smaller runtime tend to outperform heavier models in the considered streaming scenario, thus changing the rankings generally observed in standard offline evaluations. Based on this observation, we propose a lightweight action anticipation model consisting in a simple feed-forward 3D CNN, which we propose to optimize using knowledge distillation techniques and a custom loss. The results show that the proposed approach outperforms prior art in the streaming scenario, also in combination with other lightweight models.",0,Human
"In recent years 5G technology has become a critical element of wireless communications systems and it enables high speed data transfer and low latency. Recently, there is a strong demand for 5G services especially in dense urban areas, which has led to the need to use effective methods to select radio access technologies (RATs) for 5G networks. This paper carries out an in-depth study on the factors that influence selection of RATs for 5G networks with high density such as network architecture, network density, availability of spectrum and user needs. A new method of RAT selection is proposed that uses machine learning algorithms to forecast the most suitable RAT based on real time network conditions. It takes into account many metrics like network traffic, interference and network coverage to make informed selections. Results from extensive simulations and experiments show this new approach performs better than traditional selection methods in terms of performance and user satisfaction. This research offers important insights into challenges of RAT selection in high density 5G networks and also provides promising solutions for operators and network designers to optimize networks and improve overall user experience. This approach is expected to have a great impact on development of 5G networks and advance the field of wireless communication systems.",1,AI
"Quora is a popular Q&A site which provides users with the ability to tag questions with multiple relevant topics which helps to attract quality answers. These topics are not predefined but user-defined conventions and it is not so rare to have multiple such conventions present in the Quora ecosystem describing exactly the same concept. In almost all such cases, users (or Quora moderators) manually merge the topic pair into one of the either topics, thus selecting one of the competing conventions. An important application for the site therefore is to identify such competing conventions early enough that should merge in future. In this paper, we propose a two-step approach that uniquely combines the anomaly detection and the supervised classification frameworks to predict whether two topics from among millions of topic pairs are indeed competing conventions, and should merge, achieving an F-score of 0.711. We also develop a model to predict the direction of the topic merge, i.e., the winning convention, achieving an F-score of 0.898. Our system is also able to predict ~ 25% of the correct case of merges within the first month of the merge and ~ 40% of the cases within a year. This is an encouraging result since Quora users on average take 936 days to identify such a correct merge. Human judgment experiments show that our system is able to predict almost all the correct cases that humans can predict plus 37.24% correct cases which the humans are not able to identify at all.",0,Human
"This paper investigates the assembly of two kinds of tiles onto a lattice by using random placement and observes the formation of complex structures with emergent characteristics. Tiles are square shaped and have edges that are either sticky or nonsticky; these edges allow for different arrangements. Through random placement, researchers observe formation of different structures such as aperiodic tilings and quasi crystals. Analysis uses computer simulation and modeling to study assembly processes and assess effects of different variables like concentration of tiles and temperature on resultant structures. Results show promise for producing new materials with special properties through random assembly and contribute to the growing field of self assembling systems.",1,AI
"This paper studies ways to scale out applications that depend on Acid properties by partitioning operations. Acid refers to important characteristics like Atomicity, Consistency, Isolation and Durability which are critical for database transactions to ensure security and reliability. But as databases get larger and more complex, it becomes harder to preserve Acid properties. In this paper we suggest partitioning operations as a way to scale such applications and achieve higher performance and reliability. Methodology includes implementing this partitioning in an actual Acid application and assessing different performance metrics. Results show that operation partitioning is promising and provides higher performance and reliability while still maintaining fundamental Acid properties.",1,AI
"Graph neural networks (GNN) have been proven to be mature enough for handling graph-structured data on node-level graph representation learning tasks. However, the graph pooling technique for learning expressive graph-level representation is critical yet still challenging. Existing pooling methods either struggle to capture the local substructure or fail to effectively utilize high-order dependency, thus diminishing the expression capability. In this paper we propose HAP, a hierarchical graph-level representation learning framework, which is adaptively sensitive to graph structures, i.e., HAP clusters local substructures incorporating with high-order dependencies. HAP utilizes a novel cross-level attention mechanism MOA to naturally focus more on close neighborhood while effectively capture higher-order dependency that may contain crucial information. It also learns a global graph content GCont that extracts the graph pattern properties to make the pre- and post-coarsening graph content maintain stable, thus providing global guidance in graph coarsening. This novel innovation also facilitates generalization across graphs with the same form of features. Extensive experiments on fourteen datasets show that HAP significantly outperforms twelve popular graph pooling methods on graph classification task with an maximum accuracy improvement of 22.79%, and exceeds the performance of state-of-the-art graph matching and graph similarity learning algorithms by over 3.5% and 16.7%.",0,Human
This paper introduces a new method for optimizing trajectories of cooperative UAV swarms operating at dual frequencies that aims to improve performance overall. The paper takes advantage of the special features of dual frequency UAVs to boost both communication and sensing capability. Formulating the trajectory optimization as a multi objective problem the authors consider various goals such as reducing mission time and saving energy of the UAVs and improving quality of communication and sensing among them. Genetic algorithms are used to search for optimal trajectories that meet these goals. Results from simulations show the effectiveness of the introduced approach and demonstrate reductions in mission time and energy consumption along with higher quality of communication and sensing. Practical implications are discussed and some avenues for future research are suggested. In summary this research advances knowledge of cooperative UAV swarms and offers promising ways to optimize dual frequency UAV trajectories for real world applications.,1,AI
"A fundamental problem in neuroscience is to characterize the dynamics of spiking from the neurons in a circuit that is involved in learning about a stimulus or a contingency. A key limitation of current methods to analyze neural spiking data is the need to collapse neural activity over time or trials, which may cause the loss of information pertinent to understanding the function of a neuron or circuit. We introduce a new method that can determine not only the trial-to-trial dynamics that accompany the learning of a contingency by a neuron, but also the latency of this learning with respect to the onset of a conditioned stimulus. The backbone of the method is a separable two-dimensional (2D) random field (RF) model of neural spike rasters, in which the joint conditional intensity function of a neuron over time and trials depends on two latent Markovian state sequences that evolve separately but in parallel. Classical tools to estimate state-space models cannot be applied readily to our 2D separable RF model. We develop efficient statistical and computational tools to estimate the parameters of the separable 2D RF model. We apply these to data collected from neurons in the pre-frontal cortex (PFC) in an experiment designed to characterize the neural underpinnings of the associative learning of fear in mice. Overall, the separable 2D RF model provides a detailed, interpretable, characterization of the dynamics of neural spiking that accompany the learning of a contingency.",0,Human
"Iterative optimization heuristics (IOHs) are very useful methods that perform well on hard problems across different disciplines. Performance and scalability are key factors affecting whether IOHs succeed or fail. For assessing and improving IOHs, benchmarking and profiling are important. This paper introduces a new benchmarking and profiling tool called iohprofiler specifically designed for IOHs. It includes a range of benchmark tasks, performance metrics and profiling tools so users can evaluate and enhance performance and scalability of IOHs. We describe design and implementation details and present experiments showing effectiveness at benchmarking and profiling diverse IOHs. Results show advantages of using iohprofiler for evaluation and enhancement of IOHs and therefore the tool is valuable for research and practice.",1,AI
"Most distributed-memory bulk-synchronous parallel programs in HPC assume that compute resources are available continuously and homogeneously across the allocated set of compute nodes. However, long one-off delays on individual processes can cause global disturbances, so-called idle waves, by rippling through the system. This process is mainly governed by the communication topology of the underlying parallel code. This paper makes significant contributions to the understanding of idle wave dynamics. We study the propagation mechanisms of idle waves across the ranks of MPI-parallel programs. We present a validated analytic model for their propagation velocity with respect to communication parameters and topology, with a special emphasis on sparse communication patterns. We study the interaction of idle waves with MPI collectives and show that, depending on the implementation, a collective may be transparent to the wave. Finally we analyze two mechanisms of idle wave decay: topological decay, which is rooted in differences in communication characteristics among parts of the system, and noise-induced decay, which is caused by system or application noise. We show that noise-induced decay is largely independent of noise characteristics but depends only on the overall noise power. An analytic expression for idle wave decay rate with respect to noise power is derived. For model validation we use microbenchmarks and stencil algorithms on three different supercomputing platforms.",0,Human
"As YouTube has become increasingly popular for sharing content, there are growing concerns regarding risks for children due to exposure to dangerous or inappropriate material. This paper focuses on identifying and analyzing unsafe content and those who profit from children by exploiting them. We examined a large dataset of videos containing over 100 thousand items to find content that endangers children such as violence, sexuality, and deception. We also looked at how profiteers manipulate children by using deceptive advertising and search algorithms. Results show prevalence of dangerous content on YouTube, tactics used by profiteers to target younger viewers and potential consequences for children. Findings underscore necessity for improvements in detection and removal of harmful content alongside better regulation of ads to safeguard children from harm.",1,AI
"Subgraph queries also known as subgraph isomorphism search is a fundamental problem in querying graph-like structured data. It consists to enumerate the subgraphs of a data graph that match a query graph. This problem arises in many real-world applications related to query processing or pattern recognition such as computer vision, social network analysis, bioinformatic and big data analytic. Subgraph isomorphism search knows a lot of investigations and solutions mainly because of its importance and use but also because of its NP-completeness. Existing solutions use filtering mechanisms and optimise the order within witch the query vertices are matched on the data vertices to obtain acceptable processing times. However, existing approaches are iterative and generate several intermediate results. They also require that the data graph is loaded in main memory and consequently are not adapted to large graphs that do not fit into memory or are accessed by streams. To tackle this problem, we propose a new approach based on concepts widely different from existing works. Our approach distills the semantic and topological information that surround a vertex into a simple integer. This simple vertex encoding that can be computed and updated incrementally reduces considerably intermediate results and avoid to load the entire data graph into main memory. We evaluate our approach on several real-word datasets. The experimental results show that our approach is efficient and scalable.",0,Human
This research studies political conversations on Twitter from 2014 to 2019; it uses Twitter data to track trends and discern significant changes. Results indicate that Twitter has become increasingly important for political discussions and participation by users is rising. Also observed are changes in character of these discussions with users becoming more polarized and debates taking a harsher tone. Conclusions state that Twitter discussion patterns reflect larger changes in politics as more people use social media for expression and engagement. Findings stress importance of monitoring changing dynamics of political discussions on social media and ongoing research into effect on political discourse and democracy.,1,AI
This paper introduces a new way to decode information in communications called SRGRAND (Symbol Reliability Guided Random Addition Noise Decoder). Conventional decoding uses only information about channel states to perform decoding but in real systems this information is often not available or outdated which leads to reduced performance. Proposed SRGRAND uses additional symbol reliability information along with channel states to improve performance. Symbol reliability is calculated using statistical characteristics of transmitted symbols and channel noise and gives information about quality of each symbol. This information is then used to direct the decoding process and reduce errors. Results from simulations show SRGRAND outperforms standard decoding methods and performs better overall.,1,AI
"We present a path planning framework that takes into account the human's safety perception in the presence of a flying robot. The framework addresses two objectives: (i) estimation of the uncertain parameters of the proposed safety perception model based on test data collected using Virtual Reality (VR) testbed, and (ii) offline optimal control computation using the estimated safety perception model. Due to the unknown factors in the human tests data, it is not suitable to use standard regression techniques that minimize the mean squared error (MSE). We propose to use a Hidden Markov model (HMM) approach where human's attention is considered as a hidden state to infer whether the data samples are relevant to learn the safety perception model. The HMM approach improved log-likelihood over the standard least squares solution. For path planning, we use Bernstein polynomials for discretization, as the resulting path remains within the convex hull of the control points, providing guarantees for deconfliction with obstacles at low computational cost. An example of optimal trajectory generation using the learned human model is presented. The optimal trajectory generated using the proposed model results in reasonable safety distance from the human. In contrast, the paths generated using the standard regression model have undesirable shapes due to overfitting. The example demonstrates that the HMM approach has robustness to the unknown factors compared to the standard MSE model.",0,Human
This paper looks at hardness and approximation for probabilistic p centers when there are pressures. The p center problem is one of the classic ones in OR and CS; it finds the best location for p facilities in a network so that maximum distance to nearest facility is minimal. Probabilistic p centers extend this idea by considering probability distributions over demands at points in the network. Pressure means there are limitations or external things affecting decision making. First the paper defines probabilistic centers and related variants like standard and sensitive to pressure. Then it discusses computational complexity proving NP hardness via reductions from known hard problems. It introduces approximation algorithms for probabilistic centers with pressure and analyzes performance along with some numbers to show effectiveness. At last the authors summarize results and suggest future work. Results of this work contribute important insights into hardness and approximations for probabilistic centers under pressure and serve as useful references for researchers and professionals in OR and CS.,1,AI
"This paper investigates use of unsupervised learning techniques to improve learning of representation in tabular data. Most current approaches rely on features or fixed embeddings defined by hand that do not fully exploit the data's intrinsic structure. To address this shortcoming, we introduce an unsupervised learning method which exploits internal structure and relationships within tables to learn more meaningful representations. Specifically, we present two tasks focused on predicting missing values and reconstructing corrupted tables. We show effectiveness through performance on different real datasets and find that this new method outperforms current methods for learning representations for tabular data. Results indicate that using unsupervised learning has promise for better representation of tables and higher performance for downstream tasks such as classification and regression.",1,AI
"Unsupervised domain adaptation (UDA) aims to solve the problem of knowledge transfer from labeled source domain to unlabeled target domain. Recently, many domain adaptation (DA) methods use centroid to align the local distribution of different domains, that is, to align different classes. This improves the effect of domain adaptation, but domain differences exist not only between classes, but also between samples. This work rethinks what is the alignment between different domains, and studies how to achieve the real alignment between different domains. Previous DA methods only considered one distribution feature of aligned samples, such as full distribution or local distribution. In addition to aligning the global distribution, the real domain adaptation should also align the meso distribution and the micro distribution. Therefore, this study propose a double classifier method based on high confidence label (DCP). By aligning the centroid and the distribution between centroid and sample of different classifiers, the meso and micro distribution alignment of different domains is realized. In addition, in order to reduce the chain error caused by error marking, This study propose a high confidence marking method to reduce the marking error. To verify its versatility, this study evaluates DCP on digital recognition and target recognition data sets. The results show that our method achieves state-of-the-art results on most of the current domain adaptation benchmark datasets.",0,Human
This paper introduces a Newton method tailored for switching system optimal control which exploits structural features of these systems for computational efficiency. Results are compared against previous work and show better solution accuracy and faster convergence. Results indicate this method exploiting structural features effectively addresses challenges in switching systems control and suggest promising directions for further research.,1,AI
"In Knowledge Representation, it is crucial that knowledge engineers have a good understanding of the formal expressions that they write. What formal expressions state intuitively about the domain of discourse is studied in the theory of the informal semantics of a logic. In this paper we study the informal semantics of Answer Set Programming. The roots of answer set programming lie in the language of Extended Logic Programming, which was introduced initially as an epistemic logic for default and autoepistemic reasoning. In 1999, the seminal papers on answer set programming proposed to use this logic for a different purpose, namely, to model and solve search problems. Currently, the language is used primarily in this new role. However, the original epistemic intuitions lose their explanatory relevance in this new context. How answer set programs are connected to the specifications of problems they model is more easily explained in a classical Tarskian semantics, in which models correspond to possible worlds, rather than to belief states of an epistemic agent. In this paper, we develop a new theory of the informal semantics of answer set programming, which is formulated in the Tarskian setting and based on Frege's compositionality principle. It differs substantially from the earlier epistemic theory of informal semantics, providing a different view on the meaning of the connectives in answer set programming and on its relation to other logics, in particular classical logic.",0,Human
"The application of reinforcement learning algorithms onto real life problems always bears the challenge of filtering the environmental state out of raw sensor readings. While most approaches use heuristics, biology suggests that there must exist an unsupervised method to construct such filters automatically. Besides the extraction of environmental states, the filters have to represent them in a fashion that support modern reinforcement algorithms. Many popular algorithms use a linear architecture, so one should aim at filters that have good approximation properties in combination with linear functions. This thesis wants to propose the unsupervised method slow feature analysis (SFA) for this task. Presented with a random sequence of sensor readings, SFA learns a set of filters. With growing model complexity and training examples, the filters converge against trigonometric polynomial functions. These are known to possess excellent approximation capabilities and should therfore support the reinforcement algorithms well. We evaluate this claim on a robot. The task is to learn a navigational control in a simple environment using the least square policy iteration (LSPI) algorithm. The only accessible sensor is a head mounted video camera, but without meaningful filtering, video images are not suited as LSPI input. We will show that filters learned by SFA, based on a random walk video of the robot, allow the learned control to navigate successfully in ca. 80% of the test trials.",0,Human
This paper introduces an approach using deep learning to detect attacks such as DDoS and DoS in IoT networks. We use residual networks (ResNet) as a classifier for distinguishing between normal traffic and attack behavior by learning features from large data sets containing diverse attack types and regular traffic. Results show that our model based on ResNets performs very well compared to baselines. Performance is particularly good at detecting new attacks (zero days). False positives are rare. Overall results suggest that deep learning methods work well for detecting such attacks and ResNet proves useful.,1,AI
"Comprehensive review of ethics and society related to AI research is ESR. This paper analyzes current ethics of AI and looks at different frameworks and principles that have been put forward to guide development of AI. It also looks at the difficulties and limits of these frameworks. The paper also considers how social and political institutions shape AI ethics including roles of governments, companies and NGOs. In conclusion the paper recommends future research should involve greater collaboration among AI researchers, ethicists and society and emphasize proactive approach to ethics of AI considering impact of AI on society politically, socially and economically.",1,AI
"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",0,Human
"This paper analyzes filtering using consensus theory for agents that are interconnected in a network. The goal is to estimate a common signal from noisy measurements distributed among them. We present an algorithm for distributed filtering; each node updates its own estimate by combining its measurement with those from neighbors. There is a fusion step as well where these estimates are combined to get a global estimate of the signal. We study convergence properties of this algorithm and derive conditions under which the consensus estimate converges. We also look into how fusion affects accuracy of final results and derive error bounds. Through simulations we show effectiveness of the algorithm in noisy and changing environments for accurate estimation of common signals. The method applies to sensor networks, distributed control and systems of agents where common signal estimation is important.",1,AI
"Randomization-based Machine Learning methods for prediction are currently a hot topic in Artificial Intelligence, due to their excellent performance in many prediction problems, with a bounded computation time. The application of randomization-based approaches to renewable energy prediction problems has been massive in the last few years, including many different types of randomization-based approaches, their hybridization with other techniques and also the description of new versions of classical randomization-based algorithms, including deep and ensemble approaches. In this paper we review the most important characteristics of randomization-based machine learning approaches and their application to renewable energy prediction problems. We describe the most important methods and algorithms of this family of modeling methods, and perform a critical literature review, examining prediction problems related to solar, wind, marine/ocean and hydro-power renewable sources. We support our critical analysis with an extensive experimental study, comprising real-world problems related to solar, wind and hydro-power energy, where randomization-based algorithms are found to achieve superior results at a significantly lower computational cost than other modeling counterparts. We end our survey with a prospect of the most important challenges and research directions that remain open this field, along with an outlook motivating further research efforts in this exciting research field.",0,Human
"Using lattice Boltzmann method (LBM), a powerful tool for computational fluid dynamics (CFD) simulations, we introduce a new approach to parallelize this method on irregular grids. Irregular grids provide advantages such as flexibility and better resolution of complex shapes; however, they have proven difficult to parallelize effectively. By exploiting parallelism inherent in LBM we develop an approach for distributing simulations among different processors. We use a mixed MPI and OpenMP approach to run LBM codes on irregular grids and balance workload and communication costs. Performance is measured using various test problems and compared against other parallel implementations. Results indicate that our parallel algorithm scales well and outperforms previous work at high node counts, thus showing feasibility for large scale CFD simulations.",1,AI
"We present theory and algorithms for the computation of probability-weighted ""keep-out"" sets to assure probabilistically safe navigation in the presence of multiple rigid body obstacles with stochastic dynamics. Our forward stochastic reachability-based approach characterizes the stochasticity of the future obstacle states in a grid-free and recursion-free manner, using Fourier transforms and computational geometry. We consider discrete-time Markovian switched systems with affine parameter-varying stochastic subsystems (DMSP) as the obstacle dynamics, which includes Markov jump affine systems and discrete-time affine parameter-varying stochastic systems (DPV). We define a probabilistic occupancy function, to describe the probability that a given state is occupied by a rigid body obstacle with stochastic dynamics at a given time; keep-out sets are the super-level sets of this occupancy function. We provide sufficient conditions that ensure convexity and compactness of these keep-out sets for DPV obstacle dynamics. We also propose two computationally efficient algorithms to overapproximate the keep-out sets --- a tight polytopic approximation using projections, and an overapproximation using Minkowski sum. For DMSP obstacle dynamics, we compute a union of convex and compact sets that covers the potentially non-convex keep-out set. Numerical simulations show the efficacy of the proposed algorithms for a modified version of the classical unicycle dynamics, modeled as a DMSP.",0,Human
"Ride hailing services have become very common in many big cities all over the world. In Nigeria, ORide is one of the leading ride hailing services that offer affordable transportation to people who need to commute. One major concern regarding ORide drivers has been their anonymity which worries about the safety and trust of the users. This paper looks at how users perceive anonymity of drivers and the effects on safety. We conducted a survey of 500 users in Lagos, Nigeria to learn what they think about anonymity of drivers and what influences their decisions to use the service. Results show that many users feel uncomfortable about anonymity and want to know more about drivers like their names and pictures. Users also report that anonymity affects their feelings differently based on past experience with the service, gender and age. The results also indicate that anonymity may pose some risks to safety; namely difficulty distinguishing drivers in case of accidents or security issues. We provide recommendations to both ORide and other ride hailing companies to improve anonymity while ensuring safety. Results of this study help fill a gap in the literature on ride hailing services in developing countries and highlight the balance needed between privacy and safety when designing such services.",1,AI
"This paper introduces a new way to model uncertainty due to lack of knowledge or understanding through Bayesian calibration. Such uncertainty causes significant prediction errors. Calibration has been used often for modeling this type of uncertainty but there are problems such as high confidence despite wrong specification or limited data. Therefore, we introduce a new method using Bayesian calibration aiming to improve the calibration of uncertainty estimates from Bayesian models. Specifically, we introduce a calibration function that adjusts uncertainty estimates from Bayesian models based on data reliability and degree of misspecification. Results show that this new method generates more accurate and reliable estimates compared to other leading approaches for uncertainty modeling. Applications of this new method are also discussed including finance, engineering and health care. Conclusions include advantages of using this approach.",1,AI
"This paper introduces a new way of clustering streaming data based on the idea of evolving density mountains. It uses the notion of peak density to follow changes of clusters over time and recognize patterns of density peaks. Compared to previous methods that generally assume fixed cluster structures, this new approach is much more responsive and adaptive. Performance of the proposed method has been tested using both synthetic and real data sets and results show that this approach excels in terms of accuracy, efficiency and scalability. Results of this research advance clustering methods for streaming data and open up possibilities for use in different fields such as market segmentation online and fraud detection.",1,AI
This research looks at the packing and covering characteristics of subspace codes for use in error control through random linear network coding. We study the balance between minimum distance and maximal rate and present constructions of high quality subspace codes that meet the best possible tradeoff between these. Specifically we propose a method for making codes with high minimum distances that optimize this tradeoff. We also consider the covering radius and show that our designs perform optimally under certain conditions. Results are important for designing effective subspace codes for error correction via random linear network coding.,1,AI
"This paper looks at optimality of universal Bayesian sequence prediction for any given loss function and alphabet. It tries to establish conditions under which this Bayesian prediction method works well theoretically and gives a basis for proving optimality. First it explains what Bayesian prediction is and why it works. Then it defines the general loss function and alphabet and explains how they work in Bayesian prediction. Results are summarized and a theorem showing sufficient conditions for optimality is presented. Proving this theorem uses both math analysis and simulation results. Finally, conclusions are drawn about results and new lines of inquiry are pointed out.",1,AI
This paper introduces an innovative planning algorithm named e$ \mathbf{^3} $mop that integrates guided motion primitives pruning along with path optimization using a sparse banded structure. This algorithm is developed for high dimensional motion planning with complex constraints such as those relevant to robotics and autonomous systems. e$ \mathbf{^3} $mop begins by generating a set of primitives through heuristic guidance to reduce search space. Then this set of primitives is pruned using a new cost function that considers both the cost of the primitive and remaining path expected cost. This pruning greatly reduces consideration of primitives thus reducing computation time. Remaining primitives are used to construct a sparse banded graph capturing connectivity among them. Optimization of this graph uses a new method that exploits sparse banded structure to compute optimal path efficiently. Results show that e$ \mathbf{^3} $mop excels compared to leading algorithms regarding both performance and quality of solutions. Scalability results also show that the algorithm performs very well for large scale applications. Overall e$ \mathbf{^3} $mop represents a major advancement in motion planning and promises to have a broad impact across diverse application domains including robotics and autonomous systems.,1,AI
"This paper introduces a new way to improve security and robustness of communication for unmanned aerial vehicles (UAVs) using reconfigurable intelligent surfaces (RISs). The paper first reviews current challenges in UAV communications like interference and attack by jammers as well as weak signals. Next, the idea of RISs is introduced as a promising solution to these problems. Authors propose a secure and robust communication system for UAVs that uses RISs to adapt channel conditions on the fly. Performance of this proposed system is evaluated using simulations and experiments showing clear advantages such as improved signal quality, reliability and security compared to conventional UAV communication systems. Results from this study offer fresh insights into the value of RISs as critical technology for secure and reliable UAV communication.",1,AI
This paper proposes a method for estimating subgraph frequencies from data sampled from individual perspectives. This approach uses ratios of subgraph counts in full graphs compared to sampled data along with frequencies inside sampled data to estimate subgraph frequency. The approach is tested on both synthetic and real graphs and shows good performance in terms of accuracy. Results show usefulness of this approach for diverse tasks such as mining graph patterns and network analysis.,1,AI
"This paper introduces a Bayesian optimization method that includes domain knowledge to optimize performance of ATRIAS biped, a two legged robot. Standard Bayesian optimization is very good for optimizing black box functions globally but it can be improved by using domain knowledge. Proposed method uses a Bayesian optimization framework that incorporates prior beliefs about parameters affecting performance of ATRIAS biped. Prior beliefs are integrated into the optimization algorithm through selection of prior distributions. Method is tested via simulations comparing to standard Bayesian optimization and other leading algorithms. Results show this new approach performs better by requiring fewer function evaluations to reach optimal solutions. It also clarifies important parameters and interactions among them. Results indicate that integrating domain knowledge into Bayesian optimization leads to higher efficiency and deeper insight into systems being optimized. Overall contribution to literature on Bayesian optimization and demonstration of effectiveness in optimizing complex systems like bipeds. Also underscores value of incorporating knowledge specific to the system being optimized.",1,AI
"This paper introduces a new method called Zoom SVD for extracting key features within any given time span from large datasets. Designed for speed and low memory usage, this method excels at analysis of high observation count time series. Zoom SVD relies on extending Singular Value Decomposition (SVD), which computes significant modes of variation efficiently. It also includes a zooming feature allowing users to concentrate on specific time periods and computing SVD only on relevant parts of data. Performance of Zoom SVD is tested on diverse real world datasets and results show effectiveness in pattern identification across different domains such as weather prediction, finance and speech recognition. Results indicate that Zoom SVD outperforms competing SVD methods by being much faster and using less memory while matching or exceeding performance at detecting relevant features. Overall this research proposes promising ways to analyze time series data broadly.",1,AI
"The datasets of face recognition contain an enormous number of identities and instances. However, conventional methods have difficulty in reflecting the entire distribution of the datasets because a mini-batch of small size contains only a small portion of all identities. To overcome this difficulty, we propose a novel method called BroadFace, which is a learning process to consider a massive set of identities, comprehensively. In BroadFace, a linear classifier learns optimal decision boundaries among identities from a large number of embedding vectors accumulated over past iterations. By referring more instances at once, the optimality of the classifier is naturally increased on the entire datasets. Thus, the encoder is also globally optimized by referring the weight matrix of the classifier. Moreover, we propose a novel compensation method to increase the number of referenced instances in the training stage. BroadFace can be easily applied on many existing methods to accelerate a learning process and obtain a significant improvement in accuracy without extra computational burden at inference stage. We perform extensive ablation studies and experiments on various datasets to show the effectiveness of BroadFace, and also empirically prove the validity of our compensation method. BroadFace achieves the state-of-the-art results with significant improvements on nine datasets in 1:1 face verification and 1:N face identification tasks, and is also effective in image retrieval.",0,Human
"Temporal pattern mining is important for many areas such as finance, health, and engineering because we regularly generate lots of time series data. Traditional ways of mining these patterns are slow and costly when handling big data. Mutual information is useful to assess how closely related two variables are and is very successful at reducing data dimensions. This paper introduces an efficient way to mine temporal patterns from big time series data by using mutual information. The method consists of three steps: preprocessing data, selecting important features based on mutual information, and then mining patterns. During preprocessing we remove irrelevant data and normalize what remains. Feature selection uses mutual information to pick important features that best relate to mining patterns. Finally, we use a modified PrefixSpan algorithm to extract frequent patterns from selected features. We validate performance on real datasets and compare against leading methods. Results show that our method excels both in efficiency and accuracy; it works well with high dimensional features too. Proposed method applies across diverse fields including finance, health and engineering to efficiently extract meaningful patterns from large time series data.",1,AI
"Fog radio access networks (F-RANs), which consist of a cloud and multiple edge nodes (ENs) connected via fronthaul links, have been regarded as promising network architectures. The F-RAN entails a joint optimization of cloud and edge computing as well as fronthaul interactions, which is challenging for traditional optimization techniques. This paper proposes a Cloud-Enabled Cooperation-Inspired Learning (CECIL) framework, a structural deep learning mechanism for handling a generic F-RAN optimization problem. The proposed solution mimics cloud-aided cooperative optimization policies by including centralized computing at the cloud, distributed decision at the ENs, and their uplink-downlink fronthaul interactions. A group of deep neural networks (DNNs) are employed for characterizing computations of the cloud and ENs. The forwardpass of the DNNs is carefully designed such that the impacts of the practical fronthaul links, such as channel noise and signling overheads, can be included in a training step. As a result, operations of the cloud and ENs can be jointly trained in an end-to-end manner, whereas their real-time inferences are carried out in a decentralized manner by means of the fronthaul coordination. To facilitate fronthaul cooperation among multiple ENs, the optimal fronthaul multiple access schemes are designed. Training algorithms robust to practical fronthaul impairments are also presented. Numerical results validate the effectiveness of the proposed approaches.",0,Human
"This paper looks at effectiveness of hexaconv, a neural network architecture using hexagonal filters rather than usual square ones. The study compares performance of hexaconv against traditional square conv nets on different image classification problems. Results indicate hexaconv performs comparably or even excels in accuracy but requires fewer parameters and computational resources. The paper ends up suggesting hexaconv is an attractive alternative to square convs and especially valuable for classification involving regular hexagonal data like satellite photos or cellular networks.",1,AI
"This paper introduces a novel method for VQA that utilizes dependency trees to integrate visual and textual information. Using a graph representation of dependencies among words in questions and features extracted from images, the proposed method reasons through the information. Experiments show that dependency trees successfully capture relations between visual and text inputs resulting in better performance on benchmark tests compared to leading methods. Interpreting results is also shown by visualization of reasoning using these dependency trees; results of this research point toward promising new directions for building VQA systems that can explain their predictions to humans.",1,AI
"The goal of this paper is to establish which practical routing schemes for wireless networks are most suitable for wideband systems in the power-limited regime, which is, for example, a practically relevant mode of operation for the analysis of ultrawideband (UWB) mesh networks. For this purpose, we study the tradeoff between energy efficiency and spectral efficiency (known as the power-bandwidth tradeoff) in a wideband linear multihop network in which transmissions employ orthogonal frequency-division multiplexing (OFDM) modulation and are affected by quasi-static, frequency-selective fading. Considering open-loop (fixed-rate) and closed-loop (rate-adaptive) multihop relaying techniques, we characterize the impact of routing with spatial reuse on the statistical properties of the end-to-end conditional mutual information (conditioned on the specific values of the channel fading parameters and therefore treated as a random variable) and on the energy and spectral efficiency measures of the wideband regime. Our analysis particularly deals with the convergence of these end-to-end performance measures in the case of large number of hops, i.e., the phenomenon first observed in \cite{Oyman06b} and named as ``multihop diversity''. Our results demonstrate the realizability of the multihop diversity advantages in the case of routing with spatial reuse for wideband OFDM systems under wireless channel effects such as path-loss and quasi-static frequency-selective multipath fading.",0,Human
Recent years have seen deep neural networks facing serious security concerns due to adversarial attacks. This paper introduces a new approach for countering these attacks through exploiting information about shape of the inputs. We process the input data beforehand to retain its original shape and feed it into a network that has learned to identify shapes. Experiments reveal that our method works well against different kinds of attacks including both white box and black box ones; overall performance significantly improves over conventional defenses. Results indicate effectiveness of using shape information to counter attacks and point towards promising future research directions in this domain.,1,AI
"Automatic segmentation methods are an important advancement in medical image analysis. Machine learning techniques, and deep neural networks in particular, are the state-of-the-art for most medical image segmentation tasks. Issues with class imbalance pose a significant challenge in medical datasets, with lesions often occupying a considerably smaller volume relative to the background. Loss functions used in the training of deep learning algorithms differ in their robustness to class imbalance, with direct consequences for model convergence. The most commonly used loss functions for segmentation are based on either the cross entropy loss, Dice loss or a combination of the two. We propose the Unified Focal loss, a new hierarchical framework that generalises Dice and cross entropy-based losses for handling class imbalance. We evaluate our proposed loss function on five publicly available, class imbalanced medical imaging datasets: CVC-ClinicDB, Digital Retinal Images for Vessel Extraction (DRIVE), Breast Ultrasound 2017 (BUS2017), Brain Tumour Segmentation 2020 (BraTS20) and Kidney Tumour Segmentation 2019 (KiTS19). We compare our loss function performance against six Dice or cross entropy-based loss functions, across 2D binary, 3D binary and 3D multiclass segmentation tasks, demonstrating that our proposed loss function is robust to class imbalance and consistently outperforms the other loss functions. Source code is available at: https://github.com/mlyg/unified-focal-loss",0,Human
"This paper studies how common and important specific arrangements of connections among elements (nodes) are in complex systems. Motifs are recurring configurations of links among nodes; they have received much study focusing on homogenous motifs where all nodes play similar roles. However, most real world networks are heterogeneous with nodes differing by function and other characteristics. Here we present a method for finding heterogeneous motifs and assess their prevalence across various real systems such as biology, society and technology. Analysis shows that these heterogeneous motifs are very common and play key roles in processing information and regulation. Many also contribute to modular organization and signal propagation along with regulatory feedback circuits. We also show that these motifs recur in diverse systems, suggesting they serve as basic building blocks for complex networks. Results have significant implications: by considering functional roles of motifs, we learn about mechanisms underlying system behavior and develop better ways to manage them. Moreover, the approach for detecting heterogeneous motifs works broadly and allows further exploration of relationships between structure and function in complex systems.",1,AI
"This paper investigates how to use two metrics, VMAF and ED, for assessing the quality of high frame rate videos. As high frame rate videos become more popular, it is important to have good ways to assess visual quality. These metrics have worked well for regular video before but there hasn't been much work done on their effectiveness with high frame rate. To test if they perform well, researchers asked 30 people to judge sequences of videos using the Absolute Category Rating method. They then compared their scores to those obtained by VMAF and ED metrics. Using videos of diverse genres and motions, the authors ensured reliability of results. Results show that both VMAF and ED perform reliably; they correlate very well with scores given by people using the AC rating method. VMAF performs slightly better but this difference is not statistically significant. This research improves objective methods for assessing quality of high frame rate videos, which will be helpful for compression and streaming. Findings of this study may also help design new algorithms especially tailored to high frame rate videos.",1,AI
This paper introduces a new way to remove artifacts from geometry based point clouds through compression. The new approach uses a two step process; first downsampling the point cloud using a quad tree method and then removing artifacts. Artifact removal relies on local regression which uses spatial and attribute information of neighboring points to estimate and eliminate those artifacts. Results on test datasets are evaluated and show better performance compared to current best methods in both objective scores and visual quality. The approach is fast and easy to integrate with existing compression frameworks. Results also indicate important advancements for developing better compression techniques for point cloud data.,1,AI
"We introduce a flexible, scalable Bayesian inference framework for nonlinear dynamical systems characterised by distinct and hierarchical variability at the individual, group, and population levels. Our model class is a generalisation of nonlinear mixed-effects (NLME) dynamical systems, the statistical workhorse for many experimental sciences. We cast parameter inference as stochastic optimisation of an end-to-end differentiable, block-conditional variational autoencoder. We specify the dynamics of the data-generating process as an ordinary differential equation (ODE) such that both the ODE and its solver are fully differentiable. This model class is highly flexible: the ODE right-hand sides can be a mixture of user-prescribed or ""white-box"" sub-components and neural network or ""black-box"" sub-components. Using stochastic optimisation, our amortised inference algorithm could seamlessly scale up to massive data collection pipelines (common in labs with robotic automation). Finally, our framework supports interpretability with respect to the underlying dynamics, as well as predictive generalization to unseen combinations of group components (also called ""zero-shot"" learning). We empirically validate our method by predicting the dynamic behaviour of bacteria that were genetically engineered to function as biosensors. Our implementation of the framework, the dataset, and all code to reproduce the experimental results is available at https://www.github.com/Microsoft/vi-hds .",0,Human
"Human pose estimation is a fundamental yet challenging task in computer vision, which aims at localizing human anatomical keypoints. However, unlike human vision that is robust to various data corruptions such as blur and pixelation, current pose estimators are easily confused by these corruptions. This work comprehensively studies and addresses this problem by building rigorous robust benchmarks, termed COCO-C, MPII-C, and OCHuman-C, to evaluate the weaknesses of current advanced pose estimators, and a new algorithm termed AdvMix is proposed to improve their robustness in different corruptions. Our work has several unique benefits. (1) AdvMix is model-agnostic and capable in a wide-spectrum of pose estimation models. (2) AdvMix consists of adversarial augmentation and knowledge distillation. Adversarial augmentation contains two neural network modules that are trained jointly and competitively in an adversarial manner, where a generator network mixes different corrupted images to confuse a pose estimator, improving the robustness of the pose estimator by learning from harder samples. To compensate for the noise patterns by adversarial augmentation, knowledge distillation is applied to transfer clean pose structure knowledge to the target pose estimator. (3) Extensive experiments show that AdvMix significantly increases the robustness of pose estimations across a wide range of corruptions, while maintaining accuracy on clean data in various challenging benchmark datasets.",0,Human
"This paper introduces a new approach for fast and efficient training of multitask models that work across different languages. These models are increasingly used in natural language processing because they can handle multiple tasks together. But currently this training is very time consuming and expensive, making them impractical. We use distributed training on multiple nodes and adaptive optimization methods to speed up the training process and reduce memory use. Specifically, we introduce a new method called task aware gradient accumulation which efficiently allocates computational resources among different tasks during training. We also use adaptive optimizers that adjust learning rates and momentum dynamically according to each task's features. Results on a large dataset show that this new approach outperforms other methods on language modeling, machine translation, and POS tagging tasks and significantly reduces training time and memory usage. Overall, this new approach opens promising directions toward scalable and efficient training of multitask models that have important practical value in real world natural language processing tasks.",1,AI
This paper investigates how autonomous systems and AI can affect stability in the nuclear sector. It looks at how such systems might enhance safety and security and reduce risks of accidents and incidents. Also considered are challenges involved with integrating these systems including ethical issues and cybersecurity risks. Results show that integration of these systems can transform the nuclear sector and contribute toward a stable future. Careful planning and implementation strategies are needed to integrate them responsibly and safely. Collaboration between experts in both nuclear and technology domains is crucial for realizing the full benefits of these advanced technologies towards attaining a stable future.,1,AI
"Manipulating facial expressions is a challenging task due to fine-grained shape changes produced by facial muscles and the lack of input-output pairs for supervised learning. Unlike previous methods using Generative Adversarial Networks (GAN), which rely on cycle-consistency loss or sparse geometry (landmarks) loss for expression synthesis, we propose a novel GAN framework to exploit 3D dense (depth and surface normals) information for expression manipulation. However, a large-scale dataset containing RGB images with expression annotations and their corresponding depth maps is not available. To this end, we propose to use an off-the-shelf state-of-the-art 3D reconstruction model to estimate the depth and create a large-scale RGB-Depth dataset after a manual data clean-up process. We utilise this dataset to minimise the novel depth consistency loss via adversarial learning (note we do not have ground truth depth maps for generated face images) and the depth categorical loss of synthetic data on the discriminator. In addition, to improve the generalisation and lower the bias of the depth parameters, we propose to use a novel confidence regulariser on the discriminator side of the framework. We extensively performed both quantitative and qualitative evaluations on two publicly available challenging facial expression benchmarks: AffectNet and RaFD. Our experiments demonstrate that the proposed method outperforms the competitive baseline and existing arts by a large margin.",0,Human
This paper introduces an efficient computational method that predicts temperature evolution along overhead power lines. The approach blends analytical solutions for heat transfer with numerical weather prediction data to simulate temperatures along the line. Results show this proposed approach saves computing time and can be used in real time for monitoring and control of transmission lines. Results from a case study clearly show accuracy in predicting temperature variation under different weather conditions. Results indicate this new simulation method has great potential to enhance reliability and efficiency of operation for overhead power lines.,1,AI
"In this work, we initiate the study of \emph{smoothed analysis} of population protocols. We consider a population protocol model where an adaptive adversary dictates the interactions between agents, but with probability $p$ every such interaction may change into an interaction between two agents chosen uniformly at random. That is, $p$-fraction of the interactions are random, while $(1-p)$-fraction are adversarial. The aim of our model is to bridge the gap between a uniformly random scheduler (which is too idealistic) and an adversarial scheduler (which is too strict).  We focus on the fundamental problem of leader election in population protocols. We show that, for a population of size $n$, the leader election problem can be solved in $O(p^{-2}n \log^3 n)$ steps with high probability, using $O((\log^2 n) \cdot (\log (n/p)))$ states per agent, for \emph{all} values of $p\leq 1$. Although our result does not match the best known running time of $O(n \log n)$ for the uniformly random scheduler ($p=1$), we are able to present a \emph{smooth transition} between a running time of $O(n \cdot \mathrm{polylog} n)$ for $p=1$ and an infinite running time for the adversarial scheduler ($p=0$), where the problem cannot be solved. The key technical contribution of our work is a novel \emph{phase clock} algorithm for our model. This is a key primitive for much-studied fundamental population protocol algorithms (leader election, majority), and we believe it is of independent interest.",0,Human
This paper investigates using stacked multilayer perceptrons (MLPs) for learning events that vary over time. Events which change over time include traffic patterns or weather changes. Proposed is an approach that trains a sequence of MLPs to learn different stages of dynamic event evolution. Each network's output feeds into the next so the system learns temporal dependency and improves prediction accuracy. Results from a dataset of traffic flow prediction indicate that stacked MLPs perform better than traditional methods like linear regression and support vector regression. Sensitivity tests also explore how performance varies depending on different settings for parameters and architecture. Overall findings suggest this approach works well for learning events that evolve dynamically; stacked MLPs have broad applicability to diverse tasks where events vary with time.,1,AI
"This paper introduces a new way of choosing representatives from large amounts of data using a combination of sparse graphs and geodesic distances on Grassmann manifolds. The main aim is to select a small subset of representative samples that reflect the essence of the whole dataset. To do this, we start by constructing a sparse graph from the data where nodes are data points and edges denote similarity between pairs of points. We then use clustering algorithms to find groups of closely related data points. Finally, we choose a representative element from each group by computing geodesic distances on the Grassmann manifold and picking the one closest to the mean of the group. Results show that our method works better than others in terms of both representing and efficiency. We expect this method to be useful in many tasks like summarizing data, visualizing and classifying large data sets where choosing good representatives is important.",1,AI
"Ramsey's theorem, in the version of Erd\H{o}s and Szekeres, states that every 2-coloring of the edges of the complete graph on {1, 2,...,n} contains a monochromatic clique of order 1/2\log n. In this paper, we consider two well-studied extensions of Ramsey's theorem.  Improving a result of R\""odl, we show that there is a constant $c>0$ such that every 2-coloring of the edges of the complete graph on \{2, 3,...,n\} contains a monochromatic clique S for which the sum of 1/\log i over all vertices i \in S is at least c\log\log\log n. This is tight up to the constant factor c and answers a question of Erd\H{o}s from 1981.  Motivated by a problem in model theory, V\""a\""an\""anen asked whether for every k there is an n such that the following holds. For every permutation \pi of 1,...,k-1, every 2-coloring of the edges of the complete graph on {1, 2, ..., n} contains a monochromatic clique a_1<...<a_k with a_{\pi(1)+1}-a_{\pi(1)}>a_{\pi(2)+1}-a_{\pi(2)}>...>a_{\pi(k-1)+1}-a_{\pi(k-1)}. That is, not only do we want a monochromatic clique, but the differences between consecutive vertices must satisfy a prescribed order. Alon and, independently, Erd\H{o}s, Hajnal and Pach answered this question affirmatively. Alon further conjectured that the true growth rate should be exponential in k. We make progress towards this conjecture, obtaining an upper bound on n which is exponential in a power of k. This improves a result of Shelah, who showed that n is at most double-exponential in k.",0,Human
"Software cost estimation is one of the prerequisite managerial activities carried out at the software development initiation stages and also repeated throughout the whole software life-cycle so that amendments to the total cost are made. In software cost estimation typically, a selection of project attributes is employed to produce effort estimations of the expected human resources to deliver a software product. However, choosing the appropriate project cost drivers in each case requires a lot of experience and knowledge on behalf of the project manager which can only be obtained through years of software engineering practice. A number of studies indicate that popular methods applied in the literature for software cost estimation, such as linear regression, are not robust enough and do not yield accurate predictions. Recently the dual variables Ridge Regression (RR) technique has been used for effort estimation yielding promising results. In this work we show that results may be further improved if an AI method is used to automatically select appropriate project cost drivers (inputs) for the technique. We propose a hybrid approach combining RR with a Genetic Algorithm, the latter evolving the subset of attributes for approximating effort more accurately. The proposed hybrid cost model has been applied on a widely known high-dimensional dataset of software project samples and the results obtained show that accuracy may be increased if redundant attributes are eliminated.",0,Human
"In the last two years, more than 200 papers have been written on how machine learning (ML) systems can fail because of adversarial attacks on the algorithms and data; this number balloons if we were to incorporate papers covering non-adversarial failure modes. The spate of papers has made it difficult for ML practitioners, let alone engineers, lawyers, and policymakers, to keep up with the attacks against and defenses of ML systems. However, as these systems become more pervasive, the need to understand how they fail, whether by the hand of an adversary or due to the inherent design of a system, will only become more pressing. In order to equip software developers, security incident responders, lawyers, and policy makers with a common vernacular to talk about this problem, we developed a framework to classify failures into ""Intentional failures"" where the failure is caused by an active adversary attempting to subvert the system to attain her goals; and ""Unintentional failures"" where the failure is because an ML system produces an inherently unsafe outcome. After developing the initial version of the taxonomy last year, we worked with security and ML teams across Microsoft, 23 external partners, standards organization, and governments to understand how stakeholders would use our framework. Throughout the paper, we attempt to highlight how machine learning failure modes are meaningfully different from traditional software failures from a technology and policy perspective.",0,Human
This paper introduces a deep reinforcement learning method for joint optimization of spectrum and energy efficiency in C-V2X communication networks and also considers security aspects. This new work uses a deep learning algorithm to tackle this complex optimization problem and yields an optimal solution which strikes a balance between spectrum and energy efficiency as well as improving network security. Results from simulation experiments show this new approach is effective and superior compared to traditional optimization approaches. Findings of this research offer important guidance on joint optimization of spectrum and energy efficiency along with security in C-V2X networks.,1,AI
"The visual cue of optical flow plays a major role in the navigation of flying insects, and is increasingly studied for use by small flying robots as well. A major problem is that successful optical flow control seems to require distance estimates, while optical flow is known to provide only the ratio of velocity to distance. In this article, a novel, stability-based strategy is proposed to estimate distances with monocular optical flow and knowledge of the control inputs (efference copies). It is shown analytically that given a fixed control gain, the stability of a constant divergence control loop only depends on the distance to the approached surface. At close distances, the control loop first starts to exhibit self-induced oscillations, eventually leading to instability. The proposed stability-based strategy for estimating distances has two major attractive characteristics. First, self-induced oscillations are easy for the robot to detect and are hardly influenced by wind. Second, the distance can be estimated during a zero divergence maneuver, i.e., around hover. The stability-based strategy is implemented and tested both in simulation and with a Parrot AR drone 2.0. It is shown that it can be used to: (1) trigger a final approach response during a constant divergence landing with fixed gain, (2) estimate the distance in hover, and (3) estimate distances during an entire landing if the robot uses adaptive gain control to continuously stay on the 'edge of oscillation'.",0,Human
"The Fast Fourier Transform (FFT) is an important tool used across many different fields such as signal processing, image processing and scientific simulations. When dealing with larger datasets, computation costs increase and hence efficient parallel algorithms are needed to compute FFT. This paper introduces a new parallel algorithm based on MPI for multidimensional FFT. We use a two stage approach that uses a distributed 1D FFT along with a scheme for block cyclic distribution of data. We exploit the advantages of this cyclic distribution to reduce communication overhead and balance workloads among processors. We use recent features of MPI to improve communication and minimize synchronization overheads. Performance and scalability results were obtained on diverse hardware including large clusters and supercomputers. Experiments showed our algorithm achieves near linear speedup up to thousands of processors and performs better than other parallel FFT libraries for large data sets. Overall we have developed an efficient method which greatly reduces computation time and thus enhances faster and more accurate simulations and analyses in diverse areas.",1,AI
"In this paper, we study the Nash dynamics of strategic interplays of n buyers in a matching market setup by a seller, the market maker. Taking the standard market equilibrium approach, upon receiving submitted bid vectors from the buyers, the market maker will decide on a price vector to clear the market in such a way that each buyer is allocated an item for which he desires the most (a.k.a., a market equilibrium solution). While such equilibrium outcomes are not unique, the market maker chooses one (maxeq) that optimizes its own objective --- revenue maximization. The buyers in turn change bids to their best interests in order to obtain higher utilities in the next round's market equilibrium solution.  This is an (n+1)-person game where buyers place strategic bids to gain the most from the market maker's equilibrium mechanism. The incentives of buyers in deciding their bids and the market maker's choice of using the maxeq mechanism create a wave of Nash dynamics involved in the market. We characterize Nash equilibria in the dynamics in terms of the relationship between maxeq and mineq (i.e., minimum revenue equilibrium), and develop convergence results for Nash dynamics from the maxeq policy to a mineq solution, resulting an outcome equivalent to the truthful VCG mechanism.  Our results imply revenue equivalence between maxeq and mineq, and address the question that why short-term revenue maximization is a poor long run strategy, in a deterministic and dynamic setting.",0,Human
"Learning disentangled representation of data without supervision is an important step towards improving the interpretability of generative models. Despite recent advances in disentangled representation learning, existing approaches often suffer from the trade-off between representation learning and generation performance i.e. improving generation quality sacrifices disentanglement performance). We propose an Information-Distillation Generative Adversarial Network (ID-GAN), a simple yet generic framework that easily incorporates the existing state-of-the-art models for both disentanglement learning and high-fidelity synthesis. Our method learns disentangled representation using VAE-based models, and distills the learned representation with an additional nuisance variable to the separate GAN-based generator for high-fidelity synthesis. To ensure that both generative models are aligned to render the same generative factors, we further constrain the GAN generator to maximize the mutual information between the learned latent code and the output. Despite the simplicity, we show that the proposed method is highly effective, achieving comparable image generation quality to the state-of-the-art methods using the disentangled representation. We also show that the proposed decomposition leads to an efficient and stable model design, and we demonstrate photo-realistic high-resolution image synthesis results (1024x1024 pixels) for the first time using the disentangled representations.",0,Human
"The proven efficacy of learning-based control schemes strongly motivates their application to robotic systems operating in the physical world. However, guaranteeing correct operation during the learning process is currently an unresolved issue, which is of vital importance in safety-critical systems. We propose a general safety framework based on Hamilton-Jacobi reachability methods that can work in conjunction with an arbitrary learning algorithm. The method exploits approximate knowledge of the system dynamics to guarantee constraint satisfaction while minimally interfering with the learning process. We further introduce a Bayesian mechanism that refines the safety analysis as the system acquires new evidence, reducing initial conservativeness when appropriate while strengthening guarantees through real-time validation. The result is a least-restrictive, safety-preserving control law that intervenes only when (a) the computed safety guarantees require it, or (b) confidence in the computed guarantees decays in light of new observations. We prove theoretical safety guarantees combining probabilistic and worst-case analysis and demonstrate the proposed framework experimentally on a quadrotor vehicle. Even though safety analysis is based on a simple point-mass model, the quadrotor successfully arrives at a suitable controller by policy-gradient reinforcement learning without ever crashing, and safely retracts away from a strong external disturbance introduced during flight.",0,Human
"This paper reviews Ultra Reliable Low Latency Communications (URLLC) for Massive Machine Type Communication (MMTC) with the goal of supporting critical MMTC applications. First, it introduces MMTC and its demands which include low latency and high reliability along with scalability at very high levels. Then the paper concentrates on URLLC as a way to fulfill strict requirements for important applications such as autonomous driving and industrial automation. Key ideas and design principles are discussed here including ways to enhance reliability and reduce latency, coding and modulation schemes, and network architecture. Challenges and limits to deploying URLLC in MMTC networks are also addressed and some thoughts about future research directions are presented. In conclusion the paper summarizes results and suggests further research avenues.",1,AI
"This paper introduces Visual Genome as a new platform which combines language and vision through dense annotation from crowd workers. It details how the authors annotate large sets of images and create a database that contains detailed scene descriptions, object and relationship annotations along with question/answer pairs. Quality is assessed by comparing against other datasets and showing capability for different computer vision and NLP tasks. Challenges and future development are discussed; improvements to data quality and expansion of annotations are among them. In conclusion, this paper asserts that Visual Genome offers significant value for research in both fields and could drive progress forward quite a bit.",1,AI
The abandoned part of the web consists of domain names that have expired and stopped being used. This study looks at how people take control of those unused domain names and related internet resources. It explores reasons and methods that hijackers use to acquire control of those abandoned assets and also considers what impact this has on former owners and the internet. Results show that hijacking expired domains is becoming more common and has major effects on internet stability and security. Concluding remarks offer suggestions to policymakers and registrars to deal with this issue and safeguard rights of domain owners.,1,AI
"This paper introduces an improved system for estimating curvature using level set method by combining traditional methods with machine learning. Level set method is commonly used in computer vision, medical imaging and graphics for modeling shapes and segmenting images. Curvature estimation is important here because it is necessary for proper propagation of level set functions. But traditional methods often have shortcomings such as being sensitive to noise and irregular sampling. To address this we propose a hybrid system that blends traditional curvature estimation with machine learning. Learning models are trained on a large set of both synthetic and real images and they learn to predict curvature at every point on evolving interface based on local features. Hybrid system mixes predictions from learning models with results from traditional methods and thus produces more accurate and robust results. Results on different data sets show that this hybrid system performs better compared to traditional ones regarding accuracy and robustness. It is also fast enough to be integrated into existing level set algorithms. In summary this work suggests promising avenues to improve level set method accuracy and robustness using learning models. This system could also be extended to other shape modeling and segmentation tasks relying on curvature estimation.",1,AI
"Writing accurate numerical software is hard because of many sources of unavoidable uncertainties, including finite numerical precision of implementations. We present a programming model where the user writes a program in a real-valued implementation and specification language that explicitly includes different types of uncertainties. We then present a compilation algorithm that generates a conventional implementation that is guaranteed to meet the desired precision with respect to real numbers. Our verification step generates verification conditions that treat different uncertainties in a unified way and encode reasoning about floating-point roundoff errors into reasoning about real numbers. Such verification conditions can be used as a standardized format for verifying the precision and the correctness of numerical programs. Due to their often non-linear nature, precise reasoning about such verification conditions remains difficult. We show that current state-of-the art SMT solvers do not scale well to solving such verification conditions. We propose a new procedure that combines exact SMT solving over reals with approximate and sound affine and interval arithmetic. We show that this approach overcomes scalability limitations of SMT solvers while providing improved precision over affine and interval arithmetic. Using our initial implementation we show the usefullness and effectiveness of our approach on several examples, including those containing non-linear computation.",0,Human
This paper introduces a new framework for counting and localization in crowds. This new framework uses only points and does not rely on regions; previous approaches have relied heavily on region based techniques and have thus made some incorrect assumptions about crowd dynamics and therefore are prone to mistakes. The new framework attempts to resolve this by considering each person as an isolated point and using that data to count and localize them better. Results have been tested using actual data and show promising improvements in accuracy. This research also points out current weaknesses of region based techniques and suggests that an approach using points is a potential way to solve these problems.,1,AI
"This research studies maximum minimum distance of linear codes that allow local repair, aiming to set tight upper limits on this distance. Recent years have seen increased interest in codes that are resilient to faults and storage system design. Maximum minimum distance of a code is an important measure related to correction of errors. This paper carefully examines this distance and relates it to other code parameters. Designing codes also affects their value. Authors use different math tools to derive new bounds on this maximum minimum distance and test tightness of these bounds via numerical experiments. Results from this work give valuable insight into designing codes with better performance for correcting errors.",1,AI
This paper introduces a new method for estimating crowd numbers using fusion of different resolution levels and use of priors across multiple scales. Proposed method combines strengths from fusion of various resolutions and use of priors at different scales to accurately estimate crowd size in a scene. Method uses capture of scenes at different resolutions and fuses information together to generate high resolution images. Further refinement of crowd estimation is achieved by use of priors at multiple scales. Results show this method performs better than current leading methods on different public datasets. This approach promises significant improvement for real world tasks like crowd management and safety monitoring.,1,AI
"We introduce Bee$^+$, a 95-mg four-winged microrobot with improved controllability and open-loop-response characteristics with respect to those exhibited by state-of-the-art two-winged microrobots with the same size and similar weight (i.e., the 75-mg Harvard RoboBee). The key innovation that made possible the development of Bee$^+$ is the introduction of an extremely light (28-mg) pair of twinned unimorph actuators, which enabled the design of a new microrobotic mechanism that flaps four wings independently. A first main advantage of the proposed design, compared to those of two-winged flyers, is that by increasing the number of actuators from two to four, the number of direct control inputs increases from three to four when simple sinusoidal excitations are employed. A second advantage of Bee$^+$ is that its four-wing configuration and flapping mode naturally damp the rotational disturbances that commonly affect the yaw degree of freedom of two-winged microrobots. In addition, the proposed design greatly reduces the complexity of the associated fabrication process compared to those of other microrobots, as the unimorph actuators are fairly easy to build. Lastly, we hypothesize that given the relatively low wing-loading affecting their flapping mechanisms, the life expectancy of Bee$^+$s must be considerably higher than those of the two-winged counterparts. The functionality and basic capabilities of the robot are demonstrated through a set of simple control experiments.",0,Human
"In this paper we investigate the problem of allocating spectrum among radio nodes under SINR requirements. This problem is of special interest in dynamic spectrum access networks where topology and spectral resources differ with time and location. The problem is to determine the number of radio nodes that can transmit simultaneously while still achieving their SINR requirements and then decide which channels these nodes should transmit on. Previous work have shown how this can be done for a large spectrum pool where nodes allocate multiple channels from that pool which renders a linear programming approach feasible when the pool is large enough. In this paper we extend their work by considering arbitrary individual pool sizes and allow nodes to only transmit on one channel. Due to the accumulative nature of interference this problem is a non-convex integer problem which is NP-hard. However, we introduce a constraint transformation that transforms the problem to a binary quadratic constraint problem. Although this problem is still NP-hard, well known heuristic algorithms for solving this problem are known in the literature. We implement a heuristic algorithm based on Lagrange relaxation which bounds the solution value of the heuristic to the optimal value of the constraint transformed problem. Simulation results show that this approach provides solutions within an average gap of 10% of solutions obtained by a genetic algorithm for the original non-convex integer problem.",0,Human
"This paper introduces an efficient sampling method based on polynomial chaos for quantifying uncertainties and analyzing sensitivities. Proposed method uses weighted approximations of Fekete points, known for their ability to distribute points evenly for interpolating polynomials. Using such points allows authors to show that proposed method achieves high accuracy with fewer samples than traditional Monte Carlo methods. Performance of this method is assessed by various numerical tests; results indicate that this approach is promising for efficient sampling in quantification of uncertainties and sensitivities using polynomial chaos.",1,AI
"OdoViz is a reactive web-based tool for 3D visualization and processing of autonomous vehicle datasets designed to support common tasks in visual place recognition research. The system includes functionality for loading, inspecting, visualizing, and processing GPS/INS poses, point clouds and camera images. It supports a number of commonly used driving datasets and can be adapted to load custom datasets with minimal effort. OdoViz's design consists of a slim server to serve the datasets coupled with a rich client frontend. This design supports multiple deployment configurations including single user stand-alone installations, research group installations serving datasets internally across a lab, or publicly accessible web-frontends for providing online interfaces for exploring and interacting with datasets. The tool allows viewing complete vehicle trajectories traversed at multiple different time periods simultaneously, facilitating tasks such as sub-sampling, comparing and finding pose correspondences both across and within sequences. This significantly reduces the effort required in creating subsets of data from existing datasets for machine learning tasks. Further to the above, the system also supports adding custom extensions and plugins to extend the capabilities of the software for other potential data management, visualization and processing tasks. The platform has been open-sourced to promote its use and encourage further contributions from the research community.",0,Human
"Humans and animals are believed to use a very minimal set of trajectories to perform a wide variety of tasks including walking. Our main objective in this paper is two fold 1) Obtain an effective tool to realize these basic motion patterns for quadrupedal walking, called the kinematic motion primitives (kMPs), via trajectories learned from deep reinforcement learning (D-RL) and 2) Realize a set of behaviors, namely trot, walk, gallop and bound from these kinematic motion primitives in our custom four legged robot, called the `Stoch'. D-RL is a data driven approach, which has been shown to be very effective for realizing all kinds of robust locomotion behaviors, both in simulation and in experiment. On the other hand, kMPs are known to capture the underlying structure of walking and yield a set of derived behaviors. We first generate walking gaits from D-RL, which uses policy gradient based approaches. We then analyze the resulting walking by using principal component analysis. We observe that the kMPs extracted from PCA followed a similar pattern irrespective of the type of gaits generated. Leveraging on this underlying structure, we then realize walking in Stoch by a straightforward reconstruction of joint trajectories from kMPs. This type of methodology improves the transferability of these gaits to real hardware, lowers the computational overhead on-board, and also avoids multiple training iterations by generating a set of derived behaviors from a single learned gait.",0,Human
"Depth coding in 3D-HEVC for the multiview video plus depth (MVD) architecture (i) deforms object shapes due to block-level edge-approximation; (ii) misses an opportunity for high compressibility at near-lossless quality by failing to exploit strong homogeneity (clustering tendency) in depth syntax, motion vector components, and residuals at frame-level; and (iii) restricts interactivity and limits responsiveness of independent use of depth information for ""non-viewing"" applications due to texture-depth coding dependency. This paper presents a standalone depth sequence coder, which operates in the lossless to near-lossless quality range while compressing depth data superior to lossy 3D-HEVC. It preserves edges implicitly by limiting quantisation to the spatial-domain and exploits clustering tendency efficiently at frame-level with a novel binary tree based decomposition (BTBD) technique. For mono-view coding of standard MVD test sequences, on average, (i) lossless BTBD achieved $\times 42.2$ compression-ratio and $-60.0\%$ coding gain against the pseudo-lossless 3D-HEVC, using the lowest quantisation parameter $QP = 1$, and (ii) near-lossless BTBD achieved $-79.4\%$ and $6.98$ dB Bj{\o}ntegaard delta bitrate (BD-BR) and distortion (BD-PSNR), respectively, against 3D-HEVC. In view-synthesis applications, decoded depth maps from BTBD rendered superior quality synthetic-views, compared to 3D-HEVC, with $-18.9\%$ depth BD-BR and $0.43$ dB synthetic-texture BD-PSNR on average.",0,Human
"This research looks into what makes summaries good in an educational context and focuses on automatic summarization technology. It aims to assess how important certain elements such as conciseness, relevance and coherence are to summarization and how those elements influence effectiveness in education. The paper also looks at current status of technology related to automatic summarization including its drawbacks and possibilities for enhancement. Results from this study offer insights into how summarization is used effectively in education and future developments of automatic summarization technology.",1,AI
"Entity resolution is a widely studied problem with several proposals to match records across relations. Matching textual content is a widespread task in many applications, such as question answering and search. While recent methods achieve promising results for these two tasks, there is no clear solution for the more general problem of matching textual content and structured data. We introduce a framework that supports this new task in an unsupervised setting for any pair of corpora, being relational tables or text documents. Our method builds a fine-grained graph over the content of the corpora and derives word embeddings to represent the objects to match in a low dimensional space. The learned representation enables effective and efficient matching at different granularity, from relational tuples to text sentences and paragraphs. Our flexible framework can exploit pre-trained resources, but it does not depends on their existence and achieves better quality performance in matching content when the vocabulary is domain specific. We also introduce optimizations in the graph creation process with an ""expand and compress"" approach that first identifies new valid relationships across elements, to improve matching, and then prunes nodes and edges, to reduce the graph size. Experiments on real use cases and public datasets show that our framework produces embeddings that outperform word embeddings and fine-tuned language models both in results' quality and in execution times.",0,Human
"This paper studies how to construct critical graphs geometrically (CGG) in dense sensor networks (WSNs) via a distributed approach. CGGs are important structures in WSNs that reflect spatial relationships among nodes and are widely utilized for many tasks. However, building CGGs in densely populated networks is hard because of large numbers of nodes and small transmission range. Thus, we introduce a new distributed algorithm based on properties of Delaunay triangulation and Gabriel graph. This algorithm iterates in stages and comprises three major phases: selecting nodes, constructing locally and synchronizing globally. Each node chooses neighbor sets according to Delaunay triangulation during selection stage. Nodes then construct local subgraphs with selected neighbors using Gabriel graph in construction phase. At last, merging these local graphs results in final CGG in synchronization phase. Results of extensive simulations show that our algorithm produces high quality CGGs efficiently even in dense networks. Moreover, compared to previous methods, it performs better regarding both construction time and energy usage. Overall, our distributed algorithm offers promising solutions to building CGGs in dense networks and could be useful for localization, routing and other control tasks in WSNs.",1,AI
This paper introduces a new method for interpolation called Neurint. It uses neural ordinary differential equations (ODEs) and takes an approach where it approximates solutions of systems of ODEs by using parameters learned via back propagation. Thus allowing the model to interpolate between any two points within the input space. Experiments are conducted to compare Neurint against traditional methods of interpolation and show Neurint excels in both accuracy and generalization. Results from this research indicate Neurint has great potential as a promising tool for interpolation tasks across different disciplines.,1,AI
"DeepCert is a new approach for certifying robustness of classifiers that recognize images in practical contexts. It tackles the problem of ensuring that such classifiers perform reliably when encountering images in reality which might be noisy or distorted compared to what they were trained on. Using both formal verification and machine learning techniques, Deep Cert gives probabilistic guarantees about the robustness of a given classifier for specified sets of transformations. Results are evaluated against various benchmark datasets and show marked improvement over previous methods that certify robustness. Results indicate feasibility of Deep Cert for enhancing confidence and trustworthiness of image classifiers in practice.",1,AI
This paper introduces a new design of a Multi Objective Evolutionary Algorithm (MOEA) using two different algorithm frameworks. One framework is about designing an MOEA based on decomposition methods; they use decomposition techniques to convert a multi objective optimization problem into a series of subproblems. The other framework considers embedding these decomposition techniques into existing MOEAs to enhance performance. Proposed algorithms are tested on a suite of benchmark problems and compared against top performing MOEAs. Results show that these new algorithms perform better in convergence and diversity; they are able to obtain diverse good quality solutions for optimization problems with multiple objectives. Results from this research offer valuable insight for developing MOEAs and show effectiveness of decomposition MOEAs at solving optimization problems with multiple objectives.,1,AI
"This study performs empirical analysis on machine learning pipeline production and offers new insight into optimization opportunities. It assesses current condition of industrial pipelines and points out common issues and roadblocks. Through various experiments and case studies, researchers show impact of different optimization methods on performance and efficiency. Results show that using a combination of hardware and software optimization approaches like parallel processing and pruning improves both speed and accuracy of machine learning pipelines. Conclusions suggest practical recommendations for improving production pipelines and point to future research needs.",1,AI
"This paper introduces a new method for scene parsing which involves learning features at different scales, using purity trees for hierarchical segmentation of images and using optimal coverages for class assignment. At first we extract features from the image at various levels of scale so that we can capture both local and global context. After that we sort regions based on purity trees so that similar pixels and objects are grouped together. Finally, using optimal coverages we assign each region to a semantic class and this results in high quality scene parsing. Results show our method performs at leading edge compared to benchmarks.",1,AI
"This paper introduces a new way of classifying indoor scenes by using both spatial layout and scale invariant features. Spatial layout captures overall structure of the scene, while scale invariant features extract features which are less sensitive to scale changes. Combining these two approaches leads to a better and more accurate system for classification compared to previous methods. Experiments on different benchmarks show this new method performs better than leading ones.",1,AI
"This paper surveys tensor decomposition techniques and their use cases in signal processing and machine learning. Decomposing tensors is a powerful means to analyze high dimensional data which frequently occurs in real life. The survey begins with a basic introduction of tensors and decomposition methods. It introduces different tensor formats along with the mathematics that support tensor decomposition. Following this introduction, the paper focuses on the main decomposition methods, namely Tucker decomposition, CPD, and PARAFAC2. Each method receives detailed discussion regarding algorithms, benefits and shortcomings. Furthermore, examples of application include tensor completion, regression and clustering. Lastly, the paper reflects on future research prospects and challenges related to this topic. In summary, this paper offers a broad review of tensor decomposition and its relevance to signal processing and learning.",1,AI
"In recent years self supervised learning has received significant attention as an effective way to deal with computer vision tasks without supervision. This paper introduces a new approach to tracking using target aware data synthesis. Proposed method synthesizes realistic training data by blending target object images with different backgrounds. Training data synthesized this way is then used to train a deep neural network which tracks targets in real time video. Network learns to distinguish target from background based on appearance, motion and contextual information. Results are compared against top performing current methods on benchmark datasets and show performance comparable to current methods and superior performance on hard cases. This work highlights that target aware data synthesis for supervised tracking is promising.",1,AI
"We present a strategy grounded in the element removal idea of Bruns and Tortorelli [1] and aimed at reducing computational cost and circumventing potential numerical instabilities of density-based topology optimization. The design variables and the relative densities are both represented on a fixed, uniform finite element grid, and linked through filtering and Heaviside projection. The regions in the analysis domain where the relative density is below a specified threshold are removed from the forward analysis and replaced by fictitious nodal boundary conditions. This brings a progressive cut of the computational cost as the optimization proceeds and helps to mitigate numerical instabilities associated with low-density regions. Removed regions can be readily reintroduced since all the design variables remain active and are modeled in the formal sensitivity analysis. A key feature of the proposed approach is that the Heaviside functions promote material reintroduction along the structural boundaries by amplifying the magnitude of the sensitivities inside the filter reach. Several 2D and 3D structural topology optimization examples are presented, including linear and nonlinear compliance minimization, the design of a force inverter, and frequency and buckling load maximization. The approach is shown to be effective at producing optimized designs equivalent or nearly equivalent to those obtained without the element removal, while providing remarkable computational savings.",0,Human
"Machine learning techniques have enabled robots to learn narrow, yet complex tasks and also perform broad, yet simple skills with a wide variety of objects. However, learning a model that can both perform complex tasks and generalize to previously unseen objects and goals remains a significant challenge. We study this challenge in the context of ""improvisational"" tool use: a robot is presented with novel objects and a user-specified goal (e.g., sweep some clutter into the dustpan), and must figure out, using only raw image observations, how to accomplish the goal using the available objects as tools. We approach this problem by training a model with both a visual and physical understanding of multi-object interactions, and develop a sampling-based optimizer that can leverage these interactions to accomplish tasks. We do so by combining diverse demonstration data with self-supervised interaction data, aiming to leverage the interaction data to build generalizable models and the demonstration data to guide the model-based RL planner to solve complex tasks. Our experiments show that our approach can solve a variety of complex tool use tasks from raw pixel inputs, outperforming both imitation learning and self-supervised learning individually. Furthermore, we show that the robot can perceive and use novel objects as tools, including objects that are not conventional tools, while also choosing dynamically to use or not use tools depending on whether or not they are required.",0,Human
"This paper studies the problem of predicting missing relationships between entities in knowledge graphs through learning their representations. Currently, the majority of existing link prediction models employ simple but intuitive scoring functions and relatively small embedding size so that they could be applied to large-scale knowledge graphs. However, these properties also restrict the ability to learn more expressive and robust features. Therefore, diverging from most of the prior works which focus on designing new objective functions, we propose, DeCom, a simple but effective mechanism to boost the performance of existing link predictors such as DistMult, ComplEx, etc, through extracting more expressive features while preventing overfitting by adding just a few extra parameters. Specifically, embeddings of entities and relationships are first decompressed to a more expressive and robust space by decompressing functions, then knowledge graph embedding models are trained in this new feature space. Experimental results on several benchmark knowledge graphs and advanced link prediction systems demonstrate the generalization and effectiveness of our method. Especially, RESCAL + DeCom achieves state-of-the-art performance on the FB15k-237 benchmark across all evaluation metrics. In addition, we also show that compared with DeCom, explicitly increasing the embedding size significantly increase the number of parameters but could not achieve promising performance improvement.",0,Human
"Trans-dimensional random field language models (TRF LMs) have recently been introduced, where sentences are modeled as a collection of random fields. The TRF approach has been shown to have the advantages of being computationally more efficient in inference than LSTM LMs with close performance and being able to flexibly integrating rich features. In this paper we propose neural TRFs, beyond of the previous discrete TRFs that only use linear potentials with discrete features. The idea is to use nonlinear potentials with continuous features, implemented by neural networks (NNs), in the TRF framework. Neural TRFs combine the advantages of both NNs and TRFs. The benefits of word embedding, nonlinear feature learning and larger context modeling are inherited from the use of NNs. At the same time, the strength of efficient inference by avoiding expensive softmax is preserved. A number of technical contributions, including employing deep convolutional neural networks (CNNs) to define the potentials and incorporating the joint stochastic approximation (JSA) strategy in the training algorithm, are developed in this work, which enable us to successfully train neural TRF LMs. Various LMs are evaluated in terms of speech recognition WERs by rescoring the 1000-best lists of WSJ'92 test data. The results show that neural TRF LMs not only improve over discrete TRF LMs, but also perform slightly better than LSTM LMs with only one fifth of parameters and 16x faster inference efficiency.",0,Human
"The field of analyzing performance is very important and sensitive in particular when it is related to the performance of lecturers in academic institutions. Locating the weak points of lecturers through a system that provides an early warning to notify or reward the lecturers with warned or punished notices will help them to improve their weaknesses, leads to a better quality in the institutions. The current system has major issues in the higher education at Salahaddin University-Erbil (SUE) in Kurdistan-Iraq. These issues are: first, the assessment of lecturers' activities is conducted traditionally via the Quality Assurance Teams at different departments and colleges at the university, second, the outcomes in some cases of lecturers' performance provoke a low level of acceptance among lectures, as these cases are reflected and viewed by some academic communities as unfair cases, and finally, the current system is not accurate and vigorous. In this paper, Particle Swarm Optimization Neural Network is used to assess performance of lecturers in more fruitful way and also to enhance the accuracy of recognition system. Different real and novel data sets are collected from SUE. The prepared datasets preprocessed and important features are then fed as input source to the training and testing phases. Particle Swarm Optimization is used to find the best weights and biases in the training phase of the neural network. The best accuracy rate obtained in the test phase is 98.28 %.",0,Human
"Unless special conditions apply, the attempt to solve ill-conditioned systems of linear equations with standard numerical methods leads to uncontrollably high numerical error. Often, such systems arise from the discretization of operator equations with a large number of discrete variables. In this paper we show that the accuracy can be improved significantly if the equation is transformed before discretization, a process we call full operator preconditioning (FOP). It bears many similarities with traditional preconditioning for iterative methods but, crucially, transformations are applied at the operator level. We show that while condition-number improvements from traditional preconditioning generally do not improve the accuracy of the solution, FOP can. A number of topics in numerical analysis can be interpreted as implicitly employing FOP; we highlight (i) Chebyshev interpolation in polynomial approximation, and (ii) Olver-Townsend's spectral method, both of which produce solutions of dramatically improved accuracy over a naive problem formulation. In addition, we propose a FOP preconditioner based on integration for the solution of fourth-order differential equations with the finite-element method, showing the resulting linear system is well-conditioned regardless of the discretization size, and demonstrate its error-reduction capabilities on several examples. This work shows that FOP can improve accuracy beyond the standard limit for both direct and iterative methods.",0,Human
"MIMO processing plays a central part towards the recent increase in spectral and energy efficiencies of wireless networks. MIMO has grown beyond the original point-to-point channel and nowadays refers to a diverse range of centralized and distributed deployments. The fundamental bottleneck towards enormous spectral and energy efficiency benefits in multiuser MIMO networks lies in a huge demand for accurate channel state information at the transmitter (CSIT). This has become increasingly difficult to satisfy due to the increasing number of antennas and access points in next generation wireless networks relying on dense heterogeneous networks and transmitters equipped with a large number of antennas. CSIT inaccuracy results in a multi-user interference problem that is the primary bottleneck of MIMO wireless networks. Looking backward, the problem has been to strive to apply techniques designed for perfect CSIT to scenarios with imperfect CSIT. In this paper, we depart from this conventional approach and introduce the readers to a promising strategy based on rate-splitting. Rate-splitting relies on the transmission of common and private messages and is shown to provide significant benefits in terms of spectral and energy efficiencies, reliability and CSI feedback overhead reduction over conventional strategies used in LTE-A and exclusively relying on private message transmissions. Open problems, impact on standard specifications and operational challenges are also discussed.",0,Human
"Quantum computation promises significant computational advantages over classical computation for some problems. However, quantum hardware suffers from much higher error rates than in classical hardware. As a result, extensive quantum error correction is required to execute a useful quantum algorithm. The decoder is a key component of the error correction scheme whose role is to identify errors faster than they accumulate in the quantum computer and that must be implemented with minimum hardware resources in order to scale to the regime of practical applications. In this work, we consider surface code error correction, which is the most popular family of error correcting codes for quantum computing, and we design a decoder micro-architecture for the Union-Find decoding algorithm. We propose a three-stage fully pipelined hardware implementation of the decoder that significantly speeds up the decoder. Then, we optimize the amount of decoding hardware required to perform error correction simultaneously over all the logical qubits of the quantum computer. By sharing resources between logical qubits, we obtain a 67% reduction of the number of hardware units and the memory capacity is reduced by 70%. Moreover, we reduce the bandwidth required for the decoding process by a factor at least 30x using low-overhead compression algorithms. Finally, we provide numerical evidence that our optimized micro-architecture can be executed fast enough to correct errors in a quantum computer.",0,Human
"When 5G began its commercialisation journey around 2020, the discussion on the vision of 6G also surfaced. Researchers expect 6G to have higher bandwidth, coverage, reliability, energy efficiency, lower latency, and, more importantly, an integrated ""human-centric"" network system powered by artificial intelligence (AI). Such a 6G network will lead to an excessive number of automated decisions made every second. These decisions can range widely, from network resource allocation to collision avoidance for self-driving cars. However, the risk of losing control over decision-making may increase due to high-speed data-intensive AI decision-making beyond designers and users' comprehension. The promising explainable AI (XAI) methods can mitigate such risks by enhancing the transparency of the black box AI decision-making process. This survey paper highlights the need for XAI towards the upcoming 6G age in every aspect, including 6G technologies (e.g., intelligent radio, zero-touch network management) and 6G use cases (e.g., industry 5.0). Moreover, we summarised the lessons learned from the recent attempts and outlined important research challenges in applying XAI for building 6G systems. This research aligns with goals 9, 11, 16, and 17 of the United Nations Sustainable Development Goals (UN-SDG), promoting innovation and building infrastructure, sustainable and inclusive human settlement, advancing justice and strong institutions, and fostering partnership at the global level.",0,Human
"Smart cities solutions are often monolithically implemented, from sensors data handling through to the provided services. The same challenges are regularly faced by different developers, for every new solution in a new city. Expertise and know-how can be re-used and the effort shared. In this article we present the methodologies to minimize the efforts of implementing new smart city solutions and maximizing the sharing of components. The final target is to have a live technical community of smart city application developers. The results of this activity comes from the implementation of 35 city services in 27 cities between Europe and South Korea. To share efforts, we encourage developers to devise applications using a modular approach. Single-function components that are re-usable by other city services are packaged and published as standalone components, named Atomic Services. We identify 15 atomic services addressing smart city challenges in data analytics, data evaluation, data integration, data validation, and visualization. 38 instances of the atomic services are already operational in several smart city services. We detail in this article, as atomic service examples, some data predictor components. Furthermore, we describe real-world atomic services usage in the scenarios of Santander and three Danish cities. The resulting atomic services also generate a side market for smart city solutions, allowing expertise and know-how to be re-used by different stakeholders.",0,Human
"We present Stable View Synthesis (SVS). Given a set of source images depicting a scene from freely distributed viewpoints, SVS synthesizes new views of the scene. The method operates on a geometric scaffold computed via structure-from-motion and multi-view stereo. Each point on this 3D scaffold is associated with view rays and corresponding feature vectors that encode the appearance of this point in the input images. The core of SVS is view-dependent on-surface feature aggregation, in which directional feature vectors at each 3D point are processed to produce a new feature vector for a ray that maps this point into the new target view. The target view is then rendered by a convolutional network from a tensor of features synthesized in this way for all pixels. The method is composed of differentiable modules and is trained end-to-end. It supports spatially-varying view-dependent importance weighting and feature transformation of source images at each point; spatial and temporal stability due to the smooth dependence of on-surface feature aggregation on the target view; and synthesis of view-dependent effects such as specular reflection. Experimental results demonstrate that SVS outperforms state-of-the-art view synthesis methods both quantitatively and qualitatively on three diverse real-world datasets, achieving unprecedented levels of realism in free-viewpoint video of challenging large-scale scenes. Code is available at https://github.com/intel-isl/StableViewSynthesis",0,Human
"Processing of symbolic sequences represented by mapping of symbolic data into numerical signals is commonly used in various applications. It is a particularly popular approach in genomic and proteomic sequence analysis. Numerous mappings of symbolic sequences have been proposed for various applications. It is unclear however whether the processing of symbolic data provides an artifact of the numerical mapping or is an inherent property of the symbolic data. This issue has been long ignored in the engineering and scientific literature. It is possible that many of the results obtained in symbolic signal processing could be a byproduct of the mapping and might not shed any light on the underlying properties embedded in the data. Moreover, in many applications, conflicting conclusions may arise due to the choice of the mapping used for numerical representation of symbolic data. In this paper, we present a novel framework for the analysis of the equivalence of the mappings used for numerical representation of symbolic data. We present strong and weak equivalence properties and rely on signal correlation to characterize equivalent mappings. We derive theoretical results which establish conditions for consistency among numerical mappings of symbolic data. Furthermore, we introduce an abstract mapping model for symbolic sequences and extend the notion of equivalence to an algebraic framework. Finally, we illustrate our theoretical results by application to DNA sequence analysis.",0,Human
"Vehicular Ad-hoc NETworks (VANETs) are developing at a very fast pace to enable smart transportation in urban cities, by designing some mechanisms for decreasing travel time for commuters by reducing congestion. Inefficient Traffic signals and routing mechanisms are the major factors that contribute to the increase of road congestion. For smoother traffic movement and reducing congestion on the roads, the waiting time at intersections must be reduced and an optimal path should be chosen simultaneously. In this paper, A GPU assisted Preemptive MACO (GMACO-P) algorithm has been proposed to minimize the total travel time of the commuters. GMACO-P is an improvement of MACO-P algorithm that uses the harnessing the power of the GPU to provide faster computations for further minimizing the travel time. The MACO-P algorithm is based on an existing MACO algorithm that avoid the path with the congestion. The MACO-P algorithm reduces the average queue length at intersections by incorporating preemption that ensures less waiting time. In this paper, GMACO-P algorithm is proposed harnessing the power of GPU to improve MACO-P to further reduce the travel time. The GMACO-P algorithm is executed with CUDA toolkit 7.5 using C language and the obtained results were compared with existing Dijkstra, ACO, MACO, MACO-P, parallel implementation of the Dijkstra, ACO and MACO algorithms. Obtained results show the significant reduction in the travel time after using the proposed GMACO-P algorithm.",0,Human
"This research thoroughly investigates bloated dependencies in the Maven ecosystem. We aim to understand how much and what effect such dependencies have on Maven project builds and overall performance. Using both static code analysis and dynamic profiling, we pinpoint and measure this effect. Results indicate that bloated dependencies are common and they greatly reduce build speed and increase system memory usage. Recommendations include better management of dependencies and using lighter alternatives for dependencies that are large. Results offer important guidance to developers, build engineers and managers seeking to optimize Maven build processes and improve project performance.",1,AI
"This paper introduces an approach to domain adaptation using unsupervised learning on time series data. The goal is to address issues related to domain shift and shortage of labeled data. This method uses intrinsic temporal relationships in time series data and adapts to new domains using unsupervised learning. No labeled data from target domain is required. The approach trains an unsupervised autoregressive model on source data; then it synthesizes new samples for target domain using this model. These synthetic samples are then used to train a model for domain adaptation, which minimizes the discrepancy between source and target domains. Results show that this method performs better than other methods on real datasets in terms of performance and robustness to domain shift. Study indicates that unsupervised autoregressive domain adaptation makes good use of temporal relationships within time series and offers promising solutions especially when labeled data is scarce or unavailable.",1,AI
"The human visual perception system has very strong robustness and contextual awareness in a variety of image processing tasks. This robustness and the perception ability of contextual awareness is closely related to the characteristics of multi-task auxiliary learning and subjective attention of the human visual perception system. In order to improve the robustness and contextual awareness of image fusion tasks, we proposed a multi-task auxiliary learning image fusion theory guided by subjective attention. The image fusion theory effectively unifies the subjective task intention and prior knowledge of human brain. In order to achieve our proposed image fusion theory, we first analyze the mechanism of multi-task auxiliary learning, build a multi-task auxiliary learning network. Secondly, based on the human visual attention perception mechanism, we introduce the human visual attention network guided by subjective tasks on the basis of the multi-task auxiliary learning network. The subjective intention is introduced by the subjective attention task model, so that the network can fuse images according to the subjective intention. Finally, in order to verify the superiority of our image fusion theory, we carried out experiments on the combined vision system image data set, and the infrared and visible image data set for experimental verification. The experimental results demonstrate the superiority of our fusion theory over state-of-arts in contextual awareness and robustness.",0,Human
"This paper introduces a new dataset named ""Amigos"" that aims at research on affect, personality and mood for both individuals and groups. Data are collected via recordings of audio and video along with self report questionnaires from diverse participants interacting in varied contexts. Detailed descriptions are given about the dataset including collection procedures, measures for affect, personality and mood together with ethical considerations. The paper also gives some examples of use of this dataset in different fields such as social psychology, affective computing and HCI. Ultimately, Amigos dataset serves as a useful resource for researchers who are interested in conducting research related to affect, personality and mood at individual and social levels.",1,AI
"A color image contains luminance and chrominance components representing the intensity and color information respectively. The objective of the work presented in this paper is to show the significance of incorporating the chrominance information for the task of scene classification. An improved color-to-grayscale image conversion algorithm by effectively incorporating the chrominance information is proposed using color-to-gay structure similarity index (C2G-SSIM) and singular value decomposition (SVD) to improve the perceptual quality of the converted grayscale images. The experimental result analysis based on the image quality assessment for image decolorization called C2G-SSIM and success rate (Cadik and COLOR250 datasets) shows that the proposed image decolorization technique performs better than 8 existing benchmark algorithms for image decolorization. In the second part of the paper, the effectiveness of incorporating the chrominance component in scene classification task is demonstrated using the deep belief network (DBN) based image classification system developed using dense scale invariant feature transform (SIFT) as features. The levels of chrominance information incorporated by the proposed image decolorization technique is confirmed by the improvement in the overall scene classification accuracy . Also, the overall scene classification performance is improved by the combination of models obtained using the proposed and the conventional decolorization methods.",0,Human
"This study introduces a new SLAM approach using stereo cameras for outdoor construction sites that contain big moving objects. A key feature of this method is hierarchical masking which eliminates those moving objects to improve accuracy. This masking process is carried out in several steps; we start with a coarse mask and refine it step by step to get finer masks. Additionally, the paper also proposes a module for distinguishing between stationary and mobile objects and enhances robustness of masking process. Effectiveness of the proposed method is validated with experiments on real outdoor construction sites. Results show the new method performs better than leading visual SLAM approaches in terms of both accuracy and robustness when there are many moving things around.",1,AI
"This paper reports results from a thorough review of routing protocols for Delay Tolerant Networks (DTNs). DTNs are designed to communicate reliably in hard environments such as those found in rescue missions, space exploration, and remote regions where connectivity is poor. The report concentrates on recent advancements in routing; both traditional and newer methods are covered. Features and performance metrics of diverse DTN routing protocols are presented systematically along with advantages and disadvantages. Challenges and open research issues currently facing this domain of routing are identified and recommendations for future work are suggested. Results of this survey are important for researchers, practitioners, and decision makers who want to develop, deploy or use DTN networks.",1,AI
"Thermodynamic RAM (T RAM) is a new computational system which uses principles from thermodynamics to perform computations. This has shown great potential to simulate neuron behavior. In this paper we explore the possibility of using T RAM to simulate processing in the cerebral cortex. We start by outlining the architecture and operation rules of T RAM and comparing them to other neuromorphic computers. Next we report on our implementation of T RAM for processing simulation using a simple model of visual cortex as a case study. Results show that T RAM can replicate some key aspects of processing such as selective attention, receptive fields and binocular rivalry. We also look into different parameters' impact on performance and discuss implications for further development of more complex models of cortical processing. Results suggest that T RAM could prove to be useful in studying mechanisms of cortical processing and may eventually result in better designed systems inspired by biology.",1,AI
"This paper evaluates adaptive pilot pattern usage for CA OFDM systems operating under nonstationary wireless channels. Such channels bring challenges because they vary over time, which adversely impacts performance. To combat this issue, adaptive pilot patterns are suggested as a means to improve tracking of such varying channel conditions. Performance improvement using these adaptive pilot patterns is measured via simulations on channels that vary over time. Results show that using adaptive pilot patterns improves tracking performance of CA OFDM systems, thereby achieving higher throughput and lower error rates. Contributions here include further development of adaptive techniques important for meeting high speed demands in variable environments.",1,AI
"This paper studies a design process for digital microfluidic biochips (DMFBs) aimed at enhancing their tolerance to faults. These chips have great potential for revolutionizing bioanalysis because they allow for miniaturization and automation of complex lab procedures. But due to high levels of complexity and integration into larger systems, DMFBs often suffer from different kinds of faults and errors that can undermine functionality and reliability. To improve fault tolerance, we propose a design process comprising four major steps: modeling faults and causes, assessing impact on performance, synthesizing fault tolerance, and performing fault testing. Modeling faults involves identifying possible faults and causes within designs of DMFBs. Analysis evaluates how faults affect performance and overall function. Synthesis generates designs that reduce identified faults. Finally, testing validates performance and reliability of resulting designs. We demonstrate this approach using a case study involving protein crystallization and other complicated biochemical reactions. We inject faults into the system and test to verify our fault tolerant design; results show this approach effectively improves fault tolerance. Contributions of this work include a comprehensive design process for reliable DMFBs, a demonstration of effectiveness of proposed method, and discussion of limitations and future developments. Proposed framework could serve as a foundation for developing high quality DMFBs for diverse applications in bioanalysis.",1,AI
"An accountable algorithmic transparency report (ATR) should ideally investigate the (a) transparency of the underlying algorithm, and (b) fairness of the algorithmic decisions, and at the same time preserve data subjects' privacy. However, a provably formal study of the impact to data subjects' privacy caused by the utility of releasing an ATR (that investigates transparency and fairness), is yet to be addressed in the literature. The far-fetched benefit of such a study lies in the methodical characterization of privacy-utility trade-offs for release of ATRs in public, and their consequential application-specific impact on the dimensions of society, politics, and economics. In this paper, we first investigate and demonstrate potential privacy hazards brought on by the deployment of transparency and fairness measures in released ATRs. To preserve data subjects' privacy, we then propose a linear-time optimal-privacy scheme, built upon standard linear fractional programming (LFP) theory, for announcing ATRs, subject to constraints controlling the tolerance of privacy perturbation on the utility of transparency schemes. Subsequently, we quantify the privacy-utility trade-offs induced by our scheme, and analyze the impact of privacy perturbation on fairness measures in ATRs. To the best of our knowledge, this is the first analytical work that simultaneously addresses trade-offs between the triad of privacy, utility, and fairness, applicable to algorithmic transparency reports.",0,Human
This paper investigates unsupervised domain adaptation focusing on maximizing determinacy using classifiers. Goal is to train a model on a source domain where labeled data is available and then generalize performance to a target domain where there are no labels. A classifier that leverages decision boundaries learned by two classifiers trained separately on source and target domains is used to improve performance. We propose learning a decision boundary that maximizes consistency between these classifiers so that we reduce discrepancies between source and target domains. Results show this approach outperforms current best methods using experiments on different datasets. Conclusions highlight application areas including computer vision and NLP.,1,AI
"Transmission over multiple frequency bands combined into one logical channel speeds up data transfer for wireless networks. On the other hand, the allocation of multiple channels to a single user decreases the probability of finding a free logical channel for new connections, which may result in a network-wide throughput loss. While this relationship has been studied experimentally, especially in the WLAN configuration, little is known on how to analytically model such phenomena. With the advent of Opportunistic Spectrum Access (OSA) networks, it is even more important to understand the circumstances in which it is beneficial to bond channels occupied by primary users with dynamic duty cycle patterns. In this paper we propose an analytical framework which allows the investigation of the average channel throughput at the medium access control layer for OSA networks with channel bonding enabled. We show that channel bonding is generally beneficial, though the extent of the benefits depend on the features of the OSA network, including OSA network size and the total number of channels available for bonding. In addition, we show that performance benefits can be realized by adaptively changing the number of bonded channels depending on network conditions. Finally, we evaluate channel bonding considering physical layer constraints, i.e. throughput reduction compared to the theoretical throughput of a single virtual channel due to a transmission power limit for any bonding size.",0,Human
This paper studies chemical reactions at diffusion level using molecular communication in a scenario involving bidirectional relaying. Using this method of communication is attractive for nanoscale devices and systems because electromagnetic waves cannot be used. Here we consider a scenario with two nanodevices exchanging information via diffusion of molecules in a medium. Medium compartments represent distinct environments which have different reaction rates. A mathematical framework is developed to model reaction rates of medium and molecular signals and derive analytical expressions for bit error rate (BER) and capacity of the communication system. We apply this framework to assess how medium reaction influences performance of the system. Simulation results show that reaction of medium has a strong effect on BER and capacity and that the proposed framework can be used to improve system performance by tuning reaction rates. Results from this work may prove useful for designing and optimizing systems based on diffusion communication especially in scenarios where chemical reactions in medium play an important role.,1,AI
This paper introduces an approach for adaptive control of manufacturing processes through reinforcement learning that uses learning algorithms directly rather than relying on explicit models of dynamics. It focuses on problems where the time horizon is fixed and evaluates performance by maximizing total rewards across that time frame. Results show that this new approach successfully adapts to changes in dynamics and results in better control performance compared to conventional methods. Results highlight reinforcement learning's promise for real time control of manufacturing processes and suggest avenues for further research.,1,AI
"In the last years, multi-objective evolutionary algorithms (MOEA) have been applied to different software engineering problems where many conflicting objectives have to be optimized simultaneously. In theory, evolutionary algorithms feature a nice property for runtime optimization as they can provide a solution in any execution time. In practice, based on a Darwinian inspired natural selection, these evolutionary algorithms produce many deadborn solutions whose computation results in a computational resources wastage: natural selection is naturally slow. In this paper, we reconsider this founding analogy to accelerate convergence of MOEA, by looking at modern biology studies: artificial selection has been used to achieve an anticipated specific purpose instead of only relying on crossover and natural selection (i.e., Muller et al [18] research on artificial mutation of fruits with X-Ray). Putting aside the analogy with natural selection , the present paper proposes an hyper-heuristic for MOEA algorithms named Sputnik 1 that uses artificial selective mutation to improve the convergence speed of MOEA. Sputnik leverages the past history of mutation efficiency to select the most relevant mutations to perform. We evaluate Sputnik on a cloud-reasoning engine, which drives on-demand provisioning while considering conflicting performance and cost objectives. We have conducted experiments to highlight the significant performance improvement of Sputnik in terms of resolution time.",0,Human
"Machine learning models have been successfully used in many scientific and engineering fields. However, it remains difficult for a model to simultaneously utilize domain knowledge and experimental observation data. The application of knowledge-based symbolic AI represented by an expert system is limited by the expressive ability of the model, and data-driven connectionism AI represented by neural networks is prone to produce predictions that violate physical mechanisms. In order to fully integrate domain knowledge with observations, and make full use of the prior information and the strong fitting ability of neural networks, this study proposes theory-guided hard constraint projection (HCP). This model converts physical constraints, such as governing equations, into a form that is easy to handle through discretization, and then implements hard constraint optimization through projection. Based on rigorous mathematical proofs, theory-guided HCP can ensure that model predictions strictly conform to physical mechanisms in the constraint patch. The performance of the theory-guided HCP is verified by experiments based on the heterogeneous subsurface flow problem. Due to the application of hard constraints, compared with fully connected neural networks and soft constraint models, such as theory-guided neural networks and physics-informed neural networks, theory-guided HCP requires fewer data, and achieves higher prediction accuracy and stronger robustness to noisy observations.",0,Human
"This paper reviews the latest research on social networks in an integrated way, focusing particularly on privacy issues. It looks at recent trends and development in this area and stresses that users increasingly worry about protecting personal information and that companies are under strong pressure to deal with these concerns. The paper also points out major challenges and opportunities for future work. These include the need to understand better what privacy means and how to manage it differently across various network contexts and potential new technology solutions to enhance privacy protection on those sites. Finally, the paper urges further research and cooperation among scholars, industry leaders, and policy makers regarding the important and urgent matter of privacy in social networking sites.",1,AI
"This paper introduces a new approach to tackle the problem of detecting objects in rare categories and slices using targeted active learning and utilizing submodular mutual information. We call this approach Talisman. Talisman aims at improving performance of object detectors through iterative selection of informative samples to label. Using a sub modular function we measure mutual information between labeled and unlabeled samples and use this measure to choose most informative samples for labelling. Our approach also considers target classes and slices and uses them to give priority to choosing samples of those specific rare ones of interest. Performance has been measured on different benchmark datasets and results show effectiveness of Talisman in improving performance on rare categories and slices. Results also indicate our method outperforms many leading active learning approaches particularly when dealing with rare categories and slices. In summary, Talisman offers promising solution for targeted active learning context especially for difficult situation with rare categories and slices.",1,AI
"3D object detection based on point clouds has become more and more popular. Some methods propose localizing 3D objects directly from raw point clouds to avoid information loss. However, these methods come with complex structures and significant computational overhead, limiting its broader application in real-time scenarios. Some methods choose to transform the point cloud data into compact tensors first and leverage off-the-shelf 2D detectors to propose 3D objects, which is much faster and achieves state-of-the-art results. However, because of the inconsistency between 2D and 3D data, we argue that the performance of compact tensor-based 3D detectors is restricted if we use 2D detectors without corresponding modification. Specifically, the distribution of point clouds is uneven, with most points gather on the boundary of objects, while detectors for 2D data always extract features evenly. Motivated by this observation, we propose DENse Feature Indicator (DENFI), a universal module that helps 3D detectors focus on the densest region of the point clouds in a boundary-aware manner. Moreover, DENFI is lightweight and guarantees real-time speed when applied to 3D object detectors. Experiments on KITTI dataset show that DENFI improves the performance of the baseline single-stage detector remarkably, which achieves new state-of-the-art performance among previous 3D detectors, including both two-stage and multi-sensor fusion methods, in terms of mAP with a 34FPS detection speed.",0,Human
"This paper investigates processes, roles and their interrelations within organizational contexts. Its goal is to deepen understanding of dynamic relations among these elements and the effect on performance. By thoroughly reviewing related literature, this paper identifies different process types and roles along with functions they perform. It also looks at interaction among processes and roles including dependencies, conflicts and cooperation. Research method uses both qualitative and quantitative approaches like case studies and surveys together with statistical analyses. Results show that processes and roles are essential for success and effective collaboration matters most. Conclusions offer practical advice for better optimization of processes, roles and interactions for higher performance.",1,AI
"Semantic segmentation of aerial videos has been extensively used for decision making in monitoring environmental changes, urban planning, and disaster management. The reliability of these decision support systems is dependent on the accuracy of the video semantic segmentation algorithms. The existing CNN based video semantic segmentation methods have enhanced the image semantic segmentation methods by incorporating an additional module such as LSTM or optical flow for computing temporal dynamics of the video which is a computational overhead. The proposed research work modifies the CNN architecture by incorporating temporal information to improve the efficiency of video semantic segmentation.  In this work, an enhanced encoder-decoder based CNN architecture (UVid-Net) is proposed for UAV video semantic segmentation. The encoder of the proposed architecture embeds temporal information for temporally consistent labelling. The decoder is enhanced by introducing the feature-refiner module, which aids in accurate localization of the class labels. The proposed UVid-Net architecture for UAV video semantic segmentation is quantitatively evaluated on extended ManipalUAVid dataset. The performance metric mIoU of 0.79 has been observed which is significantly greater than the other state-of-the-art algorithms. Further, the proposed work produced promising results even for the pre-trained model of UVid-Net on urban street scene with fine tuning the final layer on UAV aerial videos.",0,Human
"With growing need for fast connections and good mobile network coverage, drones equipped with base stations are becoming promising means to improve coverage in remote and disaster areas. However, handing over connections from one station to another due to high mobility and limited range remains a key challenge for these drones. This paper proposes a strategy for group handovers for drones. The strategy seeks to minimize latency and maintain smooth connections for multiple users during motion. It uses a proactive approach where the station initiates handover before loss of connection. Also considered is station location and speed along with quality of user link. Results of simulation studies conducted using real urban scenarios showed that proposed strategy reduces latency by up to 50% compared to reactive approach. Throughput improvement and reduction of failure events were also noted. In summary, this study shows effectiveness of proposed strategy for drones in mobile networks; this strategy enhances service quality and connectivity in places with weak coverage. Future research should consider integration with other handover management methods and testing different deployment scenarios.",1,AI
"The development of positioning technologies has resulted in an increasing amount of mobility data being available. While bringing a lot of convenience to people's life, such availability also raises serious concerns about privacy. In this paper, we concentrate on one of the most sensitive information that can be inferred from mobility data, namely social relationships. We propose a novel social relation inference attack that relies on an advanced feature learning technique to automatically summarize users' mobility features. Compared to existing approaches, our attack is able to predict any two individuals' social relation, and it does not require the adversary to have any prior knowledge on existing social relations. These advantages significantly increase the applicability of our attack and the scope of the privacy assessment. Extensive experiments conducted on a large dataset demonstrate that our inference attack is effective, and achieves between 13% to 20% improvement over the best state-of-the-art scheme. We propose three defense mechanisms -- hiding, replacement and generalization -- and evaluate their effectiveness for mitigating the social link privacy risks stemming from mobility data sharing. Our experimental results show that both hiding and replacement mechanisms outperform generalization. Moreover, hiding and replacement achieve a comparable trade-off between utility and privacy, the former preserving better utility and the latter providing better privacy.",0,Human
"Communication or influence networks are probably the most controllable of all factors that are known to impact on the problem-solving capability of task-forces. In the case connections are costly, it is necessary to implement a policy to allocate them to the individuals. Here we use an agent-based model to study how distinct allocation policies affect the performance of a group of agents whose task is to find the global maxima of NK fitness landscapes. Agents cooperate by broadcasting messages informing on their fitness and use this information to imitate the fittest agent in their influence neighborhoods. The larger the influence neighborhood of an agent, the more links, and hence information, the agent receives. We find that the elitist policy in which agents with above-average fitness have their influence neighborhoods amplified, whereas agents with below-average fitness have theirs deflated, is optimal for smooth landscapes, provided the group size is not too small. For rugged landscapes, however, the elitist policy can perform very poorly for certain group sizes. In addition, we find that the egalitarian policy, in which the size of the influence neighborhood is the same for all agents, is optimal for both smooth and rugged landscapes in the case of small groups. The welfarist policy, in which the actions of the elitist policy are reversed, is always suboptimal, i.e., depending on the group size it is outperformed by either the elitist or the egalitarian policies.",0,Human
"Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.",0,Human
This research introduces a general approach to enhance slope limiters on irregular grids that vary in size. Slope limiters are mathematical tools to ensure stability and avoid physically unrealistic results in simulation codes like CFD. This work focuses on how to use these limiters on irregular grids which are very common in engineering and science but pose difficulties for limiter algorithms. Proposed here is an approach using both adaptation techniques for grids and new limiters to improve accuracy and robustness of simulation results on irregular grids. Results indicate that this method effectively overcomes shortcomings of current limiters and provides high quality results on a variety of test problems. Results of this research are anticipated to have significant implications for the development of numerical methods for simulations on irregular grids.,1,AI
"Contextual bandit algorithms are very important now because they excel at balancing exploration and exploitation for sequential decisions. This paper looks into estimation issues related to contextual bandits. We look at different things like choice of reward function, number of arms, context representation choice, and sample size and see how they affect performance. We review existing literature carefully and suggest some future research topics. Results show that performance depends a lot on how well you estimate things; careful design and evaluation of algorithms matters a lot. Insights from this study help practitioners design and deploy better contextual bandit algorithms and serve as a basis for further work in this field.",1,AI
This paper introduces a framework for ranking specific entities based on knowledge base properties. Results from evaluations using two datasets are presented: one dataset uses data about advisors for PhD students and another uses data about medical conditions. Results show that the new ranking framework outperforms traditional methods in both accuracy and relevance. Results have significant implications for bettering overall system quality and performance especially in areas like academic advising and diagnosis of medical conditions. An extended version of this paper contains a detailed analysis of the proposed method and its implementation along with an extensive evaluation of effectiveness using both datasets.,1,AI
"This paper investigates the Conditional Lucas and Kanade (CLK) method which is widely used in computer vision for computing optical flow between frames of videos. We introduce a new development of this method by integrating a spatially variable weight matrix that conditions flow calculation based on local features of images. This development improves the accuracy and robustness of the original Lucas and Kanade (LK) method through taking into account variations in image data like lighting changes and occlusions. Performance evaluation was conducted using different standard benchmarks and results were compared against other leading methods; results showed that the CLK method performs better than LK method and others especially under difficult conditions with high displacements and occlusions. Moreover, we show that CLK works well in practical uses such as visual odometry where precise and reliable estimation of camera movement is necessary. Results indicate that CLK excels other methods both in accuracy and computational efficiency. In summary, our work emphasizes the advantages of the proposed CLK method as an improvement over LK and its promise for diverse practical applications in computer vision.",1,AI
"As real-world images come in varying sizes, the machine learning model is part of a larger system that includes an upstream image scaling algorithm. In this system, the model and the scaling algorithm have become attractive targets for numerous attacks, such as adversarial examples and the recent image-scaling attack. In response to these attacks, researchers have developed defense approaches that are tailored to attacks at each processing stage. As these defenses are developed in isolation, their underlying assumptions may not hold when viewing them from the perspective of an end-to-end machine learning system. Thus, it is necessary to study these attacks and defenses in the context of machine learning systems. In this paper, we investigate the interplay between vulnerabilities of the image scaling procedure and machine learning models in the challenging hard-label black-box setting. We propose a series of novel techniques to make a black-box attack exploit vulnerabilities in scaling algorithms, scaling defenses, and the final machine learning model in an end-to-end manner. Based on this scaling-aware attack, we reveal that most existing scaling defenses are ineffective under threat from downstream models. Moreover, we empirically observe that standard black-box attacks can significantly improve their performance by exploiting the vulnerable scaling procedure. We further demonstrate this problem on a commercial Image Analysis API with transfer-based black-box attacks.",0,Human
"This paper looks at how we can improve load transfer capability in distribution systems through use of genetic algorithms. The main goal is to maximize overall system loadability. Using realistic models of distribution systems, the authors propose an approach using genetic algorithms to solve optimization problems. Results from this method are tested against different load profiles and performance is judged on improved loadability. Results indicate that this genetic algorithm approach successfully improves load transfer and increases loadability of systems. Performance comparisons to other methods are made and results show that using genetic algorithms leads to better improvements in loadability. Results from this work could be useful for distribution system operators to refine load transfer strategy and enhance performance.",1,AI
"In recent years there has grown increasing interest in studying the computational complexity of different variants of combinatorial problems related to voting control in elections. Voting control refers to the task of influencing election outcomes through strategic changes to the votes of some subset of voters. We conduct a detailed analysis of the computational hardness of several variants of this problem. We start by considering the basic problem which aims to minimize changes in votes to secure a desired candidate's victory. We prove this problem is NP hard and hence computationally impractical for large instances. We then look at variants of this problem that include budget limits or restrictions on how many votes a single voter can change. We show these variants are also NP hard and present algorithms for finding approximate solutions. We also investigate computational hardness for different types of election systems like single winner, multiple winners and proportional representation and show complexity depends on system type and propose algorithms to solve the problem under each system. This paper therefore studies thoroughly the computational complexity of voting control variants and gives insight into designing algorithms.",1,AI
This paper studies carefully the reliability of Graph Neural Networks (GNNs) for virtual screening and evaluates their performance on a wide variety of protein ligand pairs. Authors test different top performing GNN models against other common methods and show that they perform comparably or better in terms of accuracy and stability. Results also reveal important factors that influence reliability including choice of hyperparameters and quality of training data. Insights are provided into promising potential for GNNs and utility as reliable tools for new drug discovery.,1,AI
"The off switch game is a straightforward two player game that has been researched in game theory and decision making. In this game, one player selects whether to turn off play or to keep playing while the other can choose to accept or reject that choice. Researchers discuss this game extensively including its mathematical formulation, equilibria and its use in different fields. Authors also introduce new ways to study the game and its variations and show their practical value via case studies and simulation. Results from these studies enhance our understanding of how people make decisions when they can opt out early and have relevance in bargaining and social dynamics among others.",1,AI
"Purpose: Development of a fast and fully automated deep learning pipeline (FatSegNet) to accurately identify, segment, and quantify abdominal adipose tissue on Dixon MRI from the Rhineland Study - a large prospective population-based study. Method: FatSegNet is composed of three stages: (i) consistent localization of the abdominal region using two 2D-Competitive Dense Fully Convolutional Networks (CDFNet), (ii) segmentation of adipose tissue on three views by independent CDFNets, and (iii) view aggregation. FatSegNet is trained with 33 manually annotated subjects, and validated by: 1) comparison of segmentation accuracy against a testingset covering a wide range of body mass index (BMI), 2) test-retest reliability, and 3) robustness in a large cohort study. Results: The CDFNet demonstrates increased robustness compared to traditional deep learning networks. FatSegNet dice score outperforms manual raters on the abdominal visceral adipose tissue (VAT, 0.828 vs. 0.788), and produces comparable results on subcutaneous adipose tissue (SAT, 0.973 vs. 0.982). The pipeline has very small test-retest absolute percentage difference and excellent agreement between scan sessions (VAT: APD = 2.957%, ICC=0.998 and SAT: APD= 3.254%, ICC=0.996). Conclusion: FatSegNet can reliably analyze a 3D Dixon MRI in1 min. It generalizes well to different body shapes, sensitively replicates known VAT and SAT volume effects in a large cohort study, and permits localized analysis of fat compartments.",0,Human
"The problem of estimating sparse eigenvectors of a symmetric matrix attracts a lot of attention in many applications, especially those with high dimensional data set. While classical eigenvectors can be obtained as the solution of a maximization problem, existing approaches formulate this problem by adding a penalty term into the objective function that encourages a sparse solution. However, the resulting methods achieve sparsity at the expense of sacrificing the orthogonality property. In this paper, we develop a new method to estimate dominant sparse eigenvectors without trading off their orthogonality. The problem is highly non-convex and hard to handle. We apply the MM framework where we iteratively maximize a tight lower bound (surrogate function) of the objective function over the Stiefel manifold. The inner maximization problem turns out to be a rectangular Procrustes problem, which has a closed form solution. In addition, we propose a method to improve the covariance estimation problem when its underlying eigenvectors are known to be sparse. We use the eigenvalue decomposition of the covariance matrix to formulate an optimization problem where we impose sparsity on the corresponding eigenvectors. Numerical experiments show that the proposed eigenvector extraction algorithm matches or outperforms existing algorithms in terms of support recovery and explained variance, while the covariance estimation algorithms improve significantly the sample covariance estimator.",0,Human
"This paper looks at reducing interference in Cloud Radio Access Networks (C R ANs) using the splitting of rates combined with decoding of common messages. Splitting rates is a promising way to reduce interference and enhance spectral efficiency in wireless networks, but applying it to C R ANs has been difficult because coordination among Baseband Units (BBUs) and Remote Radio Heads (RR Hs) is complex. We propose joint design of splitting rates and common message decoding at BBU to reduce interference in these networks. We develop a new algorithm to optimize splitting rate parameters and coefficients for common message decoding to minimize total transmitted power and maintain QoS for all users. Results from simulations show significant performance improvement compared to other methods especially under dense and highly interfering conditions. Insights are gained regarding potential use of splitting rates and common message decoding for interference management and this knowledge informs design of future wireless networks.",1,AI
"Current evaluation functions for heuristic planning are expensive to compute. In numerous planning problems these functions provide good guidance to the solution, so they are worth the expense. However, when evaluation functions are misguiding or when planning problems are large enough, lots of node evaluations must be computed, which severely limits the scalability of heuristic planners. In this paper, we present a novel solution for reducing node evaluations in heuristic planning based on machine learning. Particularly, we define the task of learning search control for heuristic planning as a relational classification task, and we use an off-the-shelf relational classification tool to address this learning task. Our relational classification task captures the preferred action to select in the different planning contexts of a specific planning domain. These planning contexts are defined by the set of helpful actions of the current state, the goals remaining to be achieved, and the static predicates of the planning task. This paper shows two methods for guiding the search of a heuristic planner with the learned classifiers. The first one consists of using the resulting classifier as an action policy. The second one consists of applying the classifier to generate lookahead states within a Best First Search algorithm. Experiments over a variety of domains reveal that our heuristic planner using the learned classifiers solves larger problems than state-of-the-art planners.",0,Human
"This research analyses two leading broadband mobile options for South Asia: WiMAX and LTE. The aim is to decide which technology works best for dependable and fast mobile broadband service delivery for the region. We look at current infrastructure for mobile broadband and challenges specific to South Asia like population density and geography. We compare and contrast these technologies in detail: focusing on their technical specs, deployment requirements and performance attributes. Factors compared include coverage, capacity, latency and cost. Results conclude which technology excels for South Asia and takes regional needs into account. This paper will help policymakers and other players in the telecom sector improve mobile broadband connectivity in South Asia.",1,AI
"E-commerce platforms usually display a mixed list of ads and organic items in feed. One key problem is to allocate the limited slots in the feed to maximize the overall revenue as well as improve user experience, which requires a good model for user preference. Instead of modeling the influence of individual items on user behaviors, the arrangement signal models the influence of the arrangement of items and may lead to a better allocation strategy. However, most of previous strategies fail to model such a signal and therefore result in suboptimal performance. In addition, the percentage of ads exposed (PAE) is an important indicator in ads allocation. Excessive PAE hurts user experience while too low PAE reduces platform revenue. Therefore, how to constrain the PAE within a certain range while keeping personalized recommendation under the PAE constraint is a challenge.  In this paper, we propose Cross Deep Q Network (Cross DQN) to extract the crucial arrangement signal by crossing the embeddings of different items and modeling the crossed sequence by multi-channel attention. Besides, we propose an auxiliary loss for batch-level constraint on PAE to tackle the above-mentioned challenge. Our model results in higher revenue and better user experience than state-of-the-art baselines in offline experiments. Moreover, our model demonstrates a significant improvement in the online A/B test and has been fully deployed on Meituan feed to serve more than 300 millions of customers.",0,Human
"This paper looks at connectivity properties of random annuli graphs and the geometric block model. Both these models are frequently used in network science for modeling networks that have underlying geometric structure. Specifically, we focus on determining critical threshold probabilities at which networks go from disconnected to connected. We start by introducing random annuli graphs: they consist of randomly placing points inside an annulus and connecting those points if the Euclidean distance between them meets a certain probability criterion. We prove that connectivity critical threshold depends simply on parameters of the annulus. We then consider the geometric block model: here we split a d dimensional space into blocks and connect points within the same block with a fixed probability. We show that connectivity critical threshold depends on product of block probabilities and block volume. Lastly we compare critical thresholds for connectivity of random annuli graphs and geometric block model and find that critical threshold for model based on blocks is always higher than or equal to that of the annuli graph. Results have major implications for designing and analyzing systems that include networks with embedded geometry like wireless sensor networks and social networks. Overall this work reveals key features of connectivity in random graphs having underlying geometric features.",1,AI
"Recent advances in deep convolutional neural networks (DCNNs) have shown impressive performance improvements on thermal to visible face synthesis and matching problems. However, current DCNN-based synthesis models do not perform well on thermal faces with large pose variations. In order to deal with this problem, heterogeneous face frontalization methods are needed in which a model takes a thermal profile face image and generates a frontal visible face. This is an extremely difficult problem due to the large domain as well as large pose discrepancies between the two modalities. Despite its applications in biometrics and surveillance, this problem is relatively unexplored in the literature. We propose a domain agnostic learning-based generative adversarial network (DAL-GAN) which can synthesize frontal views in the visible domain from thermal faces with pose variations. DAL-GAN consists of a generator with an auxiliary classifier and two discriminators which capture both local and global texture discriminations for better synthesis. A contrastive constraint is enforced in the latent space of the generator with the help of a dual-path training strategy, which improves the feature vector discrimination. Finally, a multi-purpose loss function is utilized to guide the network in synthesizing identity preserving cross-domain frontalization. Extensive experimental results demonstrate that DAL-GAN can generate better quality frontal views compared to the other baseline methods.",0,Human
"Recent results by Alagic and Russell have given some evidence that the Even-Mansour cipher may be secure against quantum adversaries with quantum queries, if considered over other groups than $(\mathbb{Z}/2)^n$. This prompts the question as to whether or not other classical schemes may be generalized to arbitrary groups and whether classical results still apply to those generalized schemes. In this thesis, we generalize the Even-Mansour cipher and the Feistel cipher. We show that Even and Mansour's original notions of secrecy are obtained on a one-key, group variant of the Even-Mansour cipher. We generalize the result by Kilian and Rogaway, that the Even-Mansour cipher is pseudorandom, to super pseudorandomness, also in the one-key, group case. Using a Slide Attack we match the bound found above. After generalizing the Feistel cipher to arbitrary groups we resolve an open problem of Patel, Ramzan, and Sundaram by showing that the 3-round Feistel cipher over an arbitrary group is not super pseudorandom. We generalize a result by Gentry and Ramzan showing that the Even-Mansour cipher can be implemented using the Feistel cipher as the public permutation. In this result, we also consider the one-key case over a group and generalize their bound. Finally, we consider Zhandry's result on quantum pseudorandom permutations, showing that his result may be generalized to hold for arbitrary groups. In this regard, we consider whether certain card shuffles may be generalized as well.",0,Human
"Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.",0,Human
This paper introduces an algorithm designed to scale well for solving large linear programming problems using shared memory. Proposed algorithm takes advantage of parallelism in modern multicore processors and uses efficient data structures and algorithms to improve performance. Algorithm uses dynamic scheduling and execution of tasks on processors. Tasks divide the problem into small pieces and solve them independently using simplex method. Load balancing techniques are used to distribute workloads evenly across processors. Results from a series of experiments using different benchmark problems demonstrate scalability and significant speedup compared to sequential simplex algorithm. Performance improvements include use of advanced optimizations such as lazy updates and columnar data layout. Results suggest that using shared memory parallelism will have a big effect on linear programming. Flexible enough to handle different types of LPs including mixed integer programming and network flow problems. Results of this study indicate that using parallelism is feasible and effective for solving large LP problems and opens up avenues for future research. Overall this work shows promise for advancing research in this domain.,1,AI
"This paper introduces a new way to derive theorems in implicational linear logic through declarative methods. Linear logic is an ideal form of logic for modelling systems because it handles concepts of resource consumption and management very well. But deriving theorems is hard, especially if you're unfamiliar with its proof rules and strategies. To deal with this problem, researchers propose a method for theorem derivation that uses a high level algebraic representation of proofs. The core idea of this method is to use a set of rewrite rules that focus on essential structure of proofs and systematically automate application of such rules until the desired result is achieved. Results indicate this method works well and efficiently at deriving theorems in implication logic and also suggests it could be useful for many different kinds of logics and tasks.",1,AI
"This paper introduces an innovative method for face recognition through transformed shape features. Proposed algorithm uses transformation of facial features into new shape representations as input to a recognition model. Transformation accounts for variability inherent in features such as illumination, pose and expression so as to generate a robust face representation. Results of experiments on diverse public datasets show proposed approach excels current top performance in accuracy and robustness. Results indicate transformed shape feature approach is promising for future research.",1,AI
"This paper introduces a new approach for anomaly detection that uses similarity based on multiple criteria and Pareto depth analysis. We address shortcomings of current methods by using Pareto depth scores to evaluate data points. Pareto depth measures how far away an observation is from other observations which are not dominated by any others; this represents good tradeoffs among different criteria. Using Pareto depth we find anomalies that diverge markedly from these tradeoffs and thereby understand better tradeoffs among different criteria. Performance evaluation using diverse real datasets demonstrates this approach outperforms leading recent anomaly detection methods. This new approach is useful across many areas like finance, health care, and cybersecurity where identifying anomalies is important to ensure system integrity and safety. Results show this research makes a significant contribution to anomaly detection and suggest integration into practical applications.",1,AI
"This paper introduces a new approach to detecting cells free massive multiple input multiple output (MIMO). Cell free massive MIMO is very promising for wireless communications because of high spectral efficiency, but its detection algorithms are often computationally expensive and hard to implement. Distributed expectation propagation (DEP) provides an efficient solution by dividing detection into smaller parts which can be solved simultaneously by different processors. Results show DEP performs similarly or better compared to other recent approaches. Compared to others, DEP uses less computation and memory. Also because it is distributed, scaling up to large networks incurs little overhead. The authors also mention that the DEP technique might be applicable in other contexts including distributed optimization and inference. Overall results indicate DEP is promising both for improvement of performance and efficiency of detection and that it might have broader use in distributed computing too.",1,AI
"The security evaluation for Mail Distribution Systems focuses on certification and reliability of sensitive data between mail servers. The need to certify the information conveyed is a result of known weaknesses in the simple mail transfer protocol (SMTP). The most important consequence of these weaknesses is the possibility to mislead the recipient, which is achieved via spam (especially email spoofing). Email spoofing refers to alterations in the headers and/or the content of the message. Therefore, the authenticity of the message is compromised. Unfortunately, the broken link between certification and reliability of the information is unsolicited email (spam).  Unlike the current practice of estimating the cost of spam, which prompts organizations to purchase and maintain appropriate anti-spam software, our approach offers an alternative perspective of the economic and moral consequences of unsolicited mail. The financial data provided in this paper show that spam is a major contributor to the financial and production cost of an organization, necessitating further attention. Additionally, this paper highlights the importance and severity of the weaknesses of the SMTP protocol, which can be exploited even with the use of simple applications incorporated within most commonly used Operating Systems (e.g. Telnet).  As a consequence of these drawbacks Mail Distribution Systems need to be appropriate configured so as to provide the necessary security services to the users.",0,Human
"In recent years, deep neural network is widely used in machine learning. The multi-class classification problem is a class of important problem in machine learning. However, in order to solve those types of multi-class classification problems effectively, the required network size should have hyper-linear growth with respect to the number of classes. Therefore, it is infeasible to solve the multi-class classification problem using deep neural network when the number of classes are huge. This paper presents a method, so called Label Mapping (LM), to solve this problem by decomposing the original classification problem to several smaller sub-problems which are solvable theoretically. Our method is an ensemble method like error-correcting output codes (ECOC), but it allows base learners to be multi-class classifiers with different number of class labels. We propose two design principles for LM, one is to maximize the number of base classifier which can separate two different classes, and the other is to keep all base learners to be independent as possible in order to reduce the redundant information. Based on these principles, two different LM algorithms are derived using number theory and information theory. Since each base learner can be trained independently, it is easy to scale our method into a large scale training system. Experiments show that our proposed method outperforms the standard one-hot encoding and ECOC significantly in terms of accuracy and model complexity.",0,Human
"This study looks at how groups evolve in running races. Using information gathered from different races we look into changes in size and composition of groups through time along with factors that cause formation and dissolution of these groups. We find that groups typically form at the beginning of a race and that size and composition are influenced by things like pacing strategies, social interaction and environmental conditions. Moreover we observe that as the race goes on, groups tend to dissolve as individual runners set their own pace and strategy. Results show the complex behavior of group dynamics in endurance competitions and they suggest important considerations for designing race courses and training programs.",1,AI
"This paper investigates how to realize better and more efficient embedding learning to tackle the semi-supervised video object segmentation under challenging multi-object scenarios. The state-of-the-art methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times computing resources. To solve the problem, we propose an Associating Objects with Transformers (AOT) approach to match and decode multiple objects uniformly. In detail, AOT employs an identification mechanism to associate multiple targets into the same high-dimensional embedding space. Thus, we can simultaneously process multiple objects' matching and segmentation decoding as efficiently as processing a single object. For sufficiently modeling multi-object association, a Long Short-Term Transformer is designed for constructing hierarchical matching and propagation. We conduct extensive experiments on both multi-object and single-object benchmarks to examine AOT variant networks with different complexities. Particularly, our R50-AOT-L outperforms all the state-of-the-art competitors on three popular benchmarks, i.e., YouTube-VOS (84.1% J&F), DAVIS 2017 (84.9%), and DAVIS 2016 (91.1%), while keeping more than $3\times$ faster multi-object run-time. Meanwhile, our AOT-T can maintain real-time multi-object speed on the above benchmarks. Based on AOT, we ranked 1st in the 3rd Large-scale VOS Challenge.",0,Human
"This paper develops a new geometric understanding of sensitivity analysis for monomials which are commonly used across diverse scientific and engineering disciplines. Based on an innovative interpretation of sensitivity matrix as subspaces of parameter space this work shows that dimensions and orientations of these subspaces contain important information about how sensitive outputs of the model are to variations of input parameters. Specifically, sensitivity is shown to depend on geometric features of the subspaces and amount of overlap among them. Results provide a fresh outlook on sensitivity analysis for monomials and introduce a practical framework for analysis and visualization of sensitivities in high dimensional parameter spaces. Numerical examples are also given and comparison with other approaches is made.",1,AI
"This paper studies throughput performance of wireless powered cognitive radio networks (WPCRNs) via CS and MC methods. The work considers how excess energy from main users is used to power secondary users in WPCRNs and looks at the influence of key parameters including numbers of primary and secondary users, CS compression ratios and MC ranks on system throughput performance. Results are obtained by simulation and compared against current methods. Results indicate that integration of CS and MC methods greatly improves performance and also leads to higher energy efficiency. Results here contribute to further research in this area and offer insight into designing good communications systems.",1,AI
"This paper looks into using machine learning methods to enhance the efficiency of solving Boolean Satisfiability Problems (SAT). SAT is well known as an NP complete problem and is used extensively in many areas such as hardware/software verification, scheduling and cryptography. A new method is proposed here which uses machine learning models to direct the search process of traditional SAT solvers. Results indicate that this new approach greatly reduces solving time for SAT instances when compared to leading solvers. Performance also matches well against standard benchmark sets across a wide range of cases showing effectiveness. This work sheds light on applying machine learning to hard computational problems and opens doors to future development in this domain.",1,AI
This study looks at how different training methods work for different kinds of dialog tasks. They investigate various ways to pretrain and assess their effectiveness on diverse tasks. Results indicate that there isn't one best method; different methods perform better for different kinds of tasks. Authors conclude that careful consideration should be given to task specifics when picking a method for pretraining so that best performance can be achieved.,1,AI
"Several recent studies in privacy-preserving learning have considered the trade-off between utility or risk and the level of differential privacy guaranteed by mechanisms for statistical query processing. In this paper we study this trade-off in private Support Vector Machine (SVM) learning. We present two efficient mechanisms, one for the case of finite-dimensional feature mappings and one for potentially infinite-dimensional feature mappings with translation-invariant kernels. For the case of translation-invariant kernels, the proposed mechanism minimizes regularized empirical risk in a random Reproducing Kernel Hilbert Space whose kernel uniformly approximates the desired kernel with high probability. This technique, borrowed from large-scale learning, allows the mechanism to respond with a finite encoding of the classifier, even when the function class is of infinite VC dimension. Differential privacy is established using a proof technique from algorithmic stability. Utility--the mechanism's response function is pointwise epsilon-close to non-private SVM with probability 1-delta--is proven by appealing to the smoothness of regularized empirical risk minimization with respect to small perturbations to the feature mapping. We conclude with a lower bound on the optimal differential privacy of the SVM. This negative result states that for any delta, no mechanism can be simultaneously (epsilon,delta)-useful and beta-differentially private for small epsilon and small beta.",0,Human
"Background: The learning of genotype-phenotype associations and history of human disease by doing detailed and precise analysis of phenotypic abnormalities can be defined as deep phenotyping. To understand and detect this interaction between phenotype and genotype is a fundamental step when translating precision medicine to clinical practice. The recent advances in the field of machine learning is efficient to predict these interactions between abnormal human phenotypes and genes.  Methods: In this study, we developed a framework to predict links between human phenotype ontology (HPO) and genes. The annotation data from the heterogeneous knowledge resources i.e., orphanet, is used to parse human phenotype-gene associations. To generate the embeddings for the nodes (HPO & genes), an algorithm called node2vec was used. It performs node sampling on this graph based on random walks, then learns features over these sampled nodes to generate embeddings. These embeddings were used to perform the downstream task to predict the presence of the link between these nodes using 5 different supervised machine learning algorithms.  Results: The downstream link prediction task shows that the Gradient Boosting Decision Tree based model (LightGBM) achieved an optimal AUROC 0.904 and AUCPR 0.784. In addition, LightGBM achieved an optimal weighted F1 score of 0.87. Compared to the other 4 methods LightGBM is able to find more accurate interaction/link between human phenotype & gene pairs.",0,Human
Adaptive motion planning is crucial for autonomous systems especially robots. We introduce a new way to plan movement by mixing artificial potential fields with a prior path. We use an algorithm combining these features to avoid problems like local minima and slow convergence that are typical of standard potential field methods. Results from both simulation and real robotic systems show higher performance when compared to conventional potential field approaches; results indicate that using prior paths improves performance of planning algorithms based on potential fields for practical use.,1,AI
