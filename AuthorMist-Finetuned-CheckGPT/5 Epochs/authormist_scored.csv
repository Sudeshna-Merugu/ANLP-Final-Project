text,label,source,detectgpt_score,radar_score,detectgpt_labels,radar_labels
"Since Erdős and Kleitman's seminal work from 1971, there has been ongoing research into matching theory within families of sets. This paper focuses on two key questions: finding maximum matchings and maximal matchings. Maximum matching concerns finding largest possible matchings within given families of sets. We propose a new algorithm that obtains better upper bounds on the number of edges in matchings compared to previous bounds. It is also faster and thus practical for large families of sets. Maximal matching aims to find matchings that cannot be further enlarged. We show this is strongly connected to combinatorial structure of these families and introduce a new measure called maximal matching degree. Using this measure we prove new upper bounds on maximal matching degree and this leads to results about existence of maximal matchings in some classes of set families. Results improve those of Erdős and Kleitman and open up new ways of thinking about matching structures. Applications range broadly across computer science, operations research and other disciplines.",1,AI,0.0009113550186157,0.0097353458404541,Human,LABEL_1
"The field of Artificial Intelligence (AI) and, in particular, the Machine Learning area, counts on a wide range of performance metrics and benchmark data sets to assess the problem-solving effectiveness of its solutions. However, the appearance of research centres, projects or institutions addressing AI solutions from a multidisciplinary and multi-stakeholder perspective suggests a new approach to assessment comprising ethical guidelines, reports or tools and frameworks to help both academia and business to move towards a responsible conceptualisation of AI. They all highlight the relevance of three key aspects: (i) enhancing cooperation among the different stakeholders involved in the design, deployment and use of AI; (ii) promoting multidisciplinary dialogue, including different domains of expertise in this process; and (iii) fostering public engagement to maximise a trusted relation with new technologies and practitioners. In this paper, we introduce the Observatory on Society and Artificial Intelligence (OSAI), an initiative grew out of the project AI4EU aimed at stimulating reflection on a broad spectrum of issues of AI (ethical, legal, social, economic and cultural). In particular, we describe our work in progress around OSAI and suggest how this and similar initiatives can promote a wider appraisal of progress in AI. This will give us the opportunity to present our vision and our modus operandi to enhance the implementation of these three fundamental dimensions.",0,Human,0.0055737495422363,0.1198071837425232,Human,LABEL_1
"In this paper, we present the design, implementation and our year-long maintenance experience of SNSAPI, a Python-based middleware which unifies the interfaces and data structures of heterogeneous Social Networking Services (SNS). Unlike most prior works, our middleware is user-oriented and requires zero infrastructure support. It enables a user to readily conduct online social activities in a programmable, cross-platform fashion while gradually reducing the dependence on centralized Online Social Networks (OSN). More importantly, as the SNSAPI middleware can be used to support decentralized social networking services via conventional communication channels such as RSS or Email, it enables the deployment of Decentralized Social Networks (DSN) in an incremental, ad hoc manner. To demonstrate the viability of such type of DSNs, we have deployed an experimental 6000-node SNSAPI-based DSN on PlanetLab and evaluate its performance by replaying traces of online social activities collected from a mainstream OSN. Our results show that, with only mild resource consumption, the SNSAPI-based DSN can achieve acceptable forwarding latency comparable to that of a centralized OSN. We also develop an analytical model to characterize the trade-offs between resource consumption and message forwarding delay in our DSN. Via 20 parameterized experiments on PlanetLab, we have found that the empirical measurement results match reasonably with the performance predicted by our analytical model.",0,Human,0.0005141496658325,0.0086060166358947,Human,LABEL_1
"Fueled by the rapid development of communication networks and sensors in portable devices, today many mobile users are invited by content providers to sense and send back real-time useful information (e.g., traffic observations and sensor data) to keep the freshness of the providers' content updates. However, due to the sampling cost in sensing and transmission, an individual may not have the incentive to contribute the real-time information to help a content provider reduce the age of information (AoI). Accordingly, we propose dynamic pricing for the provider to offer age-dependent monetary returns and encourage users to sample information at different rates over time. This dynamic pricing design problem needs to balance the monetary payments to users and the AoI evolution over time, and is challenging to solve especially under the incomplete information about users' arrivals and their private sampling costs. For analysis tractability, we linearize the nonlinear AoI evolution in the constrained dynamic programming problem, by approximating the dynamic AoI reduction as a time-average term and solving the approximate dynamic pricing in closed-form. Then, we estimate this approximate term based on Brouwer's fixed-point theorem. Finally, we provide the steady-state analysis of the optimized approximate dynamic pricing scheme for an infinite time horizon, and show that the pricing scheme can be further simplified to an e-optimal version without recursive computing over time.",0,Human,0.0354343056678772,0.0168040990829467,Human,LABEL_1
"This paper introduces Lamassu which is a new technique for secure storage at the host side that enables storage of sensitive data directly on client devices without compromising security. Lamassu secures data through confidentiality, integrity and authenticity while using minimal storage space for encrypted data. It achieves this by using an innovative approach combining authenticated encryption with inline encryption techniques. Thus Lamassu encrypts data as it is created and does not require extra storage space for the encrypted data. Lamassu also ensures that data is secure throughout transmission and storage. Results of this work show security properties such as formal proofs and performance evaluations demonstrating effectiveness in terms of efficiency in storage, speed and security. Authors also detail practical uses of Lamassu in diverse contexts like cloud storage and networking. In summary results of this research indicate that Lamassu performs well as an effective technique for efficient host side storage and has important practical uses across many scenarios.",1,AI,0.001694917678833,0.0985432863235473,Human,LABEL_1
"Successful design of human-in-the-loop control systems requires appropriate models for human decision makers. Whilst most paradigms adopted in the control systems literature hide the (limited) decision capability of humans, in behavioral economics individual decision making and optimization processes are well-known to be affected by perceptual and behavioral biases. Our goal is to enrich control engineering with some insights from behavioral economics research through exposing such biases in control-relevant settings. This paper addresses the following two key questions: 1) How do behavioral biases affect decision making? 2) What is the role played by feedback in human-in-the-loop control systems? Our experimental framework shows how individuals behave when faced with the task of piloting an UAV under risk and uncertainty, paralleling a real-world decision-making scenario. Our findings support the notion of humans in Cyberphysical Systems underlying behavioral biases regardless of -- or even because of -- receiving immediate outcome feedback. We observe substantial shares of drone controllers to act inefficiently through either flying excessively (overconfident) or overly conservatively (underconfident). Furthermore, we observe human-controllers to self-servingly misinterpret random sequences through being subject to a ""hot hand fallacy"". We advise control engineers to mind the human component in order not to compromise technological accomplishments through human issues.",0,Human,0.0004947185516357,0.015541672706604,Human,LABEL_1
"This paper studies radio propagation mechanisms that impact handoffs, air interface design, beam steering, and MIMO for 5G mobile communication systems. Knife edge diffraction (KED) and a creeping wave linear model are shown to predict diffraction loss around typical building objects from 10 to 26 GHz, and human blockage measurements at 73 GHz are shown to fit a double knife-edge diffraction (DKED) model which incorporates antenna gains. Small-scale spatial fading of millimeter wave received signal voltage amplitude is generally Ricean-distributed for both omnidirectional and directional receive antenna patterns under both line-of-sight (LOS) and non-line-of-sight (NLOS) conditions in most cases, although the log-normal distribution fits measured data better for the omnidirectional receive antenna pattern in the NLOS environment. Small-scale spatial autocorrelations of received voltage amplitudes are shown to fit sinusoidal exponential and exponential functions for LOS and NLOS environments, respectively, with small decorrelation distances of 0.27 cm to 13.6 cm (smaller than the size of a handset) that are favorable for spatial multiplexing. Local area measurements using cluster and route scenarios show how the received signal changes as the mobile moves and transitions from LOS to NLOS locations, with reasonably stationary signal levels within clusters. Wideband mmWave power levels are shown to fade from 0.4 dB/ms to 40 dB/s, depending on travel speed and surroundings.",0,Human,0.0005207657814025,0.0062926411628723,Human,LABEL_1
"Let $G=(V,E)$ be an $n$-nodes non-negatively real-weighted undirected graph. In this paper we show how to enrich a {\em single-source shortest-path tree} (SPT) of $G$ with a \emph{sparse} set of \emph{auxiliary} edges selected from $E$, in order to create a structure which tolerates effectively a \emph{path failure} in the SPT. This consists of a simultaneous fault of a set $F$ of at most $f$ adjacent edges along a shortest path emanating from the source, and it is recognized as one of the most frequent disruption in an SPT. We show that, for any integer parameter $k \geq 1$, it is possible to provide a very sparse (i.e., of size $O(kn\cdot f^{1+1/k})$) auxiliary structure that carefully approximates (i.e., within a stretch factor of $(2k-1)(2|F|+1)$) the true shortest paths from the source during the lifetime of the failure. Moreover, we show that our construction can be further refined to get a stretch factor of $3$ and a size of $O(n \log n)$ for the special case $f=2$, and that it can be converted into a very efficient \emph{approximate-distance sensitivity oracle}, that allows to quickly (even in optimal time, if $k=1$) reconstruct the shortest paths (w.r.t. our structure) from the source after a path failure, thus permitting to perform promptly the needed rerouting operations. Our structure compares favorably with previous known solutions, as we discuss in the paper, and moreover it is also very effective in practice, as we assess through a large set of experiments.",0,Human,0.00083589553833,0.0095937848091125,Human,LABEL_1
"The nuclear norm minimization (NNM) is commonly used to approximate the matrix rank by shrinking all singular values equally. However, the singular values have clear physical meanings in many practical problems, and NNM may not be able to faithfully approximate the matrix rank. To alleviate the above-mentioned limitation of NNM, recent studies have suggested that the weighted nuclear norm minimization (WNNM) can achieve a better rank estimation than NNM, which heuristically set the weight being inverse to the singular values. However, it still lacks a rigorous explanation why WNNM is more effective than NMM in various applications. In this paper, we analyze NNM and WNNM from the perspective of group sparse representation (GSR). Concretely, an adaptive dictionary learning method is devised to connect the rank minimization and GSR models. Based on the proposed dictionary, we prove that NNM and WNNM are equivalent to L1-norm minimization and the weighted L1-norm minimization in GSR, respectively. Inspired by enhancing sparsity of the weighted L1-norm minimization in comparison with L1-norm minimization in sparse representation, we thus explain that WNNM is more effective than NMM. By integrating the image nonlocal self-similarity (NSS) prior with the WNNM model, we then apply it to solve the image denoising problem. Experimental results demonstrate that WNNM is more effective than NNM and outperforms several state-of-the-art methods in both objective and perceptual quality.",0,Human,0.0099125504493713,0.0524937510490417,Human,LABEL_1
"As demand grows for continuous monitoring of the environment, there is development of energy efficient sensor nodes which can collect energy directly from their surroundings. This paper introduces EICO; an energy harvesting sensor node that optimizes both energy and information. It includes several new features such as an adaptive power management system that minimizes energy consumption, a sensing module that dynamically adjusts sensing frequency based on environmental activity levels and a machine learning algorithm that forecasts energy availability and adjusts sensing frequency. Results show that EICO has much longer lifetimes and better sensing accuracy compared to other nodes that also harvest energy. Results suggest EICO could revolutionize environmental monitoring by collecting continuous and precise data with very low energy use.",1,AI,0.0006000995635986,0.8556751012802124,Human,LABEL_0
Semi supervised GANs have been very popular for unsupervised learning tasks in vision and NLP but they have not been applied well to regression because it is hard to define a good loss function. This paper introduces a novel approach to extend these GANs to solve regression problems through use of feature contrast loss. This new method effectively uses both labeled and unlabeled data to learn a mapping between input and output spaces. Results on diverse benchmarks demonstrate that this approach outperforms current best methods regarding both accuracy and reliability. This research indicates that these kinds of GANs can perform much better when solving practical problems.,1,AI,0.0120427012443542,0.0753153562545776,Human,LABEL_1
"This paper introduces a new approach to bidirectional updates on detailed medical records through blockchain technology. Typical ways of updating medical information are slow, prone to errors and not transparent. This new method aims to overcome these shortcomings by making use of security and immutability attributes of blockchain to enable fast and transparent updates of medical data. The system architecture includes a blockchain network, smart contract and user interface. Smart contracts define rules for updating medical information including conditions for granting access and types of updates permitted. User interface makes interaction between patients and providers easy and comfortable. Privacy is ensured through fine granular access control allowing patients to grant varying levels of access depending on provider roles and responsibilities. Encryption and decryption are also used to protect sensitive data. Performance and security were evaluated by implementing a prototype and conducting experiments. Results show high efficiency and scalability with ability to process many transactions per second. Security is strong and immutability prevents tampering. Concluding remarks state this blockchain system offers a new solution for efficient and transparent updates of medical records and is suitable for practical use due to its security and performance characteristics.",1,AI,0.0091472268104553,0.0255218744277954,Human,LABEL_1
"This paper introduces a new algorithm to determine optimal pricing for an electricity system in smart microgrid networks. The algorithm uses a framework for solving multiple objectives that takes into consideration economic and technical factors. Economic goals aim to maximize profits of the microgrid operator; technical goals minimize total costs to consumers. The new method uses real time information about demand and supply and adjusts electricity prices accordingly by considering factors like renewable generation capacity, storage capacity and load management strategies. Performance of this algorithm has been assessed through simulation studies and compared to other pricing methods; results show significant improvement in overall network efficiency along with substantial economic benefits for both operator and consumers. This work also advances smart grid technology and supplies a useful means to optimize pricing in microgrid systems.",1,AI,0.0007430911064147,0.2813307046890259,Human,LABEL_1
A concept from game theory called far sighted collusion occurs when multiple players act together to achieve a common goal and sometimes sacrifice short term benefits for long term gains. This idea has been explored in different fields such as economics and psychology and recently also in computer science. In this paper we look at the stable marriage problem from an algorithmic and discrete math point of view. This problem deals with pairing two sets of elements so there are no pairs that would prefer each other over what they are matched with. We study when collusion among players of this problem can be beneficial and show that such collusion can yield more stable solutions. Results from our work shed new light on how far sighted behavior plays a role in matching problems and have significant implications for designing algorithms and mechanisms for practical solutions to the stable marriage problem.,1,AI,0.0564537644386291,0.1825677752494812,Human,LABEL_1
"This study introduces a new deep bilateral learning approach for enhancing images in real time. The approach makes use of deep learning to improve visual quality while preserving essential features like edges and textures. Bilateral filtering is used to enforce smoothness and retain edges, and a deep neural network learns a mapping from low quality to high quality images. Results on a benchmark dataset show that performance of this approach surpasses leading methods regarding both visual quality and computational efficiency. Results from this work will have substantial impact across many different application domains including photography, computer vision and image processing.",1,AI,0.0028239488601684,0.5821061730384827,Human,LABEL_0
"This paper compares empirically the performance of four learning algorithms for collision prediction in a simulation environment: Q learning, SARSA, expected SARSA, and double Q learning. Collision prediction entails predicting the probability of collision between two agents in a two dimensional grid world. Experiments are carried out across a range of hyperparameters and performance metrics like mean squared error and average reward are evaluated for each algorithm. Results indicate that expected SARSA outperforms other methods both in terms of mean squared error and average reward whereas double Q learning performs worst. We also notice that performance of these algorithms varies strongly with hyperparameters and sensitivity to hyperparameters differs among them. Overall this study makes valuable comparisons of learning algorithms for collision prediction tasks; it reveals relative strengths and weaknesses of different algorithms which are useful for both researchers and practitioners working on related prediction tasks and helps select suitable algorithms according to different contexts.",1,AI,0.000339388847351,0.0312851071357727,Human,LABEL_1
"Multi-view detection incorporates multiple camera views to alleviate occlusion in crowded scenes, where the state-of-the-art approaches adopt homography transformations to project multi-view features to the ground plane. However, we find that these 2D transformations do not take into account the object's height, and with this neglection features along the vertical direction of same object are likely not projected onto the same ground plane point, leading to impure ground-plane features. To solve this problem, we propose VFA, voxelized 3D feature aggregation, for feature transformation and aggregation in multi-view detection. Specifically, we voxelize the 3D space, project the voxels onto each camera view, and associate 2D features with these projected voxels. This allows us to identify and then aggregate 2D features along the same vertical line, alleviating projection distortions to a large extent. Additionally, because different kinds of objects (human vs. cattle) have different shapes on the ground plane, we introduce the oriented Gaussian encoding to match such shapes, leading to increased accuracy and efficiency. We perform experiments on multiview 2D detection and multiview 3D detection problems. Results on four datasets (including a newly introduced MultiviewC dataset) show that our system is very competitive compared with the state-of-the-art approaches. %Our code and data will be open-sourced.Code and MultiviewC are released at https://github.com/Robert-Mar/VFA.",0,Human,0.0691710710525512,0.0122885107994079,Human,LABEL_1
"This paper reports on a study aimed at developing an efficient model for real time use of UAVs. As demand increases for UAVs across different sectors, we need high performance and reliable algorithms for real time detection of objects. Proposed method uses a blend of deep learning and computer vision techniques that are fine tuned to perform precise and quick object detection in real time. Comprehensive testing on a standard dataset shows this new model performs better than current leading methods regarding both precision and speed. Results are also verified through varied real world tests such as surveillance and inspection tasks and show efficacy and practical utility. Contribution of this research is significant because it provides an extremely efficient and accurate model for real time UAV work.",1,AI,0.051078975200653,0.4154539108276367,Human,LABEL_1
This paper introduces new methodology for efficient blind compressed sensing used in MRI. Using transform sparsification we can reduce the amount of data to acquire while retaining key features. Use of transform sparsification also reduces acquisition time and lowers cost. Method also comes with convergence guarantees that ensure reconstructed images match the original. Performance improvements are demonstrated by experiments using MRI data. Results show this new approach advances current standards in blind compressed sensing and opens up possibilities for different imaging technologies as well.,1,AI,0.0003823041915893,0.7676999568939209,Human,LABEL_0
"This paper introduces a new approach to Bayesian optimization for combinatorics which uses the Cartesian product of graphs to solve challenges related to high dimensional spaces, discrete elements and lack of separability. We show that our method excels at navigating through the search space and outperforms other methods on a broad set of benchmark problems. In addition, scalability to big data sets is demonstrated and thus the method is applicable to many different real world domains. Results of this work are important contributions to the growing community of Bayesian optimization and have broader relevance to diverse optimization tasks in machine learning and elsewhere.",1,AI,0.0004045367240905,0.6277690529823303,Human,LABEL_0
"Age-Related Macular Degeneration (AMD) is an asymptomatic retinal disease which may result in loss of vision. There is limited access to high-quality relevant retinal images and poor understanding of the features defining sub-classes of this disease. Motivated by recent advances in machine learning we specifically explore the potential of generative modeling, using Generative Adversarial Networks (GANs) and style transferring, to facilitate clinical diagnosis and disease understanding by feature extraction. We design an analytic pipeline which first generates synthetic retinal images from clinical images; a subsequent verification step is applied. In the synthesizing step we merge GANs (DCGANs and WGANs architectures) and style transferring for the image generation, whereas the verified step controls the accuracy of the generated images. We find that the generated images contain sufficient pathological details to facilitate ophthalmologists' task of disease classification and in discovery of disease relevant features. In particular, our system predicts the drusen and geographic atrophy sub-classes of AMD. Furthermore, the performance using CFP images for GANs outperforms the classification based on using only the original clinical dataset. Our results are evaluated using existing classifier of retinal diseases and class activated maps, supporting the predictive power of the synthetic images and their utility for feature extraction. Our code examples are available online.",0,Human,0.0003067255020141,0.0076044797897338,Human,LABEL_1
"This paper introduces a new approach to guided exploration of graphs using labels; users can now fine tune the proportion of labels used during this process. Guided exploration is frequently employed to navigate through big and complicated graphs and labels act as guides directing us towards interesting nodes. But traditional methods usually use a fixed proportion of labels which may not suit every situation. This new method lets users customize the proportion according to their individual preferences and aims. We present an algorithm that computes dynamically the best proportion of labels based on current progress of exploration. The algorithm considers several factors including graph density, distribution of labels and user feedback to select an optimal proportion. In order to test effectiveness we ran experiments using both real data and synthetic data. Results show that this new method excels traditional ones regarding efficiency and accuracy especially when the optimal proportion varies or is not known. We also carried out a usability study which users rated highly. In summary, we propose a flexible and effective way to navigate through big and complicated graphs and this new approach would be valuable for diverse areas such as social network analysis, recommendation systems and bioinformatics.",1,AI,0.0004412531852722,0.014006495475769,Human,LABEL_1
"This paper introduces a new way of doing something called Re-ID by using transformer models. We call this method TransReID. Using attention mechanisms from transformers, we capture important relationships between feature maps and global context. Results show that TransReID outperforms other top methods on some important datasets. We also study the effect of different parts of TransReID such as number of layers and model size and also augmentations. Results help us understand how good this new method is and how it can work well in real life.",1,AI,0.0620703697204589,0.429378867149353,ChatGPT,LABEL_1
"Despite the tremendous success of BitTorrent, its swarming system suffers from a fundamental limitation: lower or no availability of unpopular contents. Recently, Menasche et al. has shown that bundling is a promising solution to mitigate this availability problem; it improves the availability and reduces download times for unpopular contents by combining multiple files into a single swarm. There also have been studies on bundling strategies and performance issues in bundled swarms. In spite of the recent surge of interest in the benefits of and strategies for bundling, there are still little empirical grounding for understanding, describing, and modeling it. This is the first empirical study that measures and analyzes how prevalent contents bundling is in BitTorrent and how peers access the bundled contents, in comparison to the other non-bundled (i.e., single-filed) ones. To our surprise, we found that around 70% of BitTorrent swarms contain multiple files, which indicate that bundling has become widespread for contents sharing. We also show that the amount of bytes shared in bundled swarms is estimated to be around 85% out of all the BitTorrent contents logged in our datasets. Inspired from our findings, we raise and discuss three important research questions in the field of file sharing systems as well as future contents-oriented networking: i) bundling strategies, ii) bundling-aware sharing systems in BitTorrent, and iii) implications on content-oriented networking.",0,Human,0.0011641383171081,0.014218270778656,Human,LABEL_1
This paper introduces a new approach to fast Bayesian calibration of brain tumor models by means of a geometry aware neural solver. Proposed method uses deep learning to solve efficiently the inverse problem that maps MR scans to tumor physical parameters while respecting underlying geometric constraints. The solver is trained using a mix of simulated and real data; results show marked improvement in both accuracy and computational time over traditional optimization methods. Results of this study offer promising solutions for estimating physical tumor properties based on medical images and have potential for use in personalized medicine and radiation therapy planning.,1,AI,0.0003747940063476,0.5067007541656494,Human,LABEL_0
"This paper focuses on security for edge nodes in IoT systems when considering machine learning. It specifically looks into security threats that can occur at edge nodes; these are devices placed close to a network to collect and process data from other IoT devices. With greater use of machine learning in IoT, security has become an increasing concern because machine learning algorithms are susceptible to different kinds of attacks. In this paper we analyze various kinds of attack vectors targeting these edge nodes like inference attacks, evasion attacks and poisoning attacks. We also consider what impact these attacks have on performance of ML algorithms and how they can undermine security of IoT networks. We propose several measures for defense against these attacks such as using secure communication protocols, implementing security features within ML algorithms and using secure hardware and software systems. Authors also highlight challenges for effective implementation of these countermeasures and give recommendations for future research in this field. Overall this paper offers a detailed analysis of security issues related to ML and IoT edge nodes and suggests practical means of protection. Results and suggestions in this work can help practitioners and researchers improve security of IoT networks and safe deployment of ML algorithms in IoT systems.",1,AI,0.0188632011413574,0.1539225578308105,Human,LABEL_1
"Millions of drivers worldwide have enjoyed financial benefits and work schedule flexibility through a ride-sharing economy, but meanwhile they have suffered from the lack of a sense of identity and career achievement. Equipped with social identity and contest theories, financially incentivized team competitions have been an effective instrument to increase drivers' productivity, job satisfaction, and retention, and to improve revenue over cost for ride-sharing platforms. While these competitions are overall effective, the decisive factors behind the treatment effects and how they affect the outcomes of individual drivers have been largely mysterious. In this study, we analyze data collected from more than 500 large-scale team competitions organized by a leading ride-sharing platform, building machine learning models to predict individual treatment effects. Through a careful investigation of features and predictors, we are able to reduce out-sample prediction error by more than 24%. Through interpreting the best-performing models, we discover many novel and actionable insights regarding how to optimize the design and the execution of team competitions on ride-sharing platforms. A simulated analysis demonstrates that by simply changing a few contest design options, the average treatment effect of a real competition is expected to increase by as much as 26%. Our procedure and findings shed light on how to analyze and optimize large-scale online field experiments in general.",0,Human,0.0526017546653747,0.0207727551460266,Human,LABEL_1
This work performs an overall evaluation of feature selection systems for selection of algorithms for black box numerical optimization problems. It considers performance comparison of different algorithm selection methods by their ability to select the best algorithm for each particular problem. Evaluation is performed using a set of benchmark problems and suites of algorithms. Results show that systems based on features do much better at improving performance compared to relying solely on one algorithm. Insights are also gained regarding comparative advantages and disadvantages of various selection methods and the significance of feature engineering. Findings from this research help direct further system development for optimization using black boxes.,1,AI,0.0024722218513488,0.3802425265312195,Human,LABEL_1
"With increasing complexity of software systems, reliability and deployability are becoming critical issues. There has been emerging promise in using record and replay (R&R) approaches to improve system reliability by recording and reproducing execution traces in a controlled environment. However, existing methods face challenges related to scalability, precision and overhead. This technical report introduces an engineering approach for R&R that overcomes these limitations. First, it identifies critical requirements for engineering R&R systems such as support for multi tier systems, efficient collection and storage of data, and low overhead for recording and replay. Then we present our system architecture that uses virtualization and distributed tracing to achieve high scalability and precision. We also introduce new techniques to minimize overhead such as adaptive sampling and selective replay. Experiments conducted on real world applications have demonstrated effectiveness of our approach: our system captures and reproduces complex execution traces accurately and efficiently along with giving insight into system behavior under various conditions. Results also show that this system is useful for identifying and diagnosing performance and reliability problems. In summary, our engineering approach for R&R provides a practical way to improve deployability and reliability of software systems especially for large and complex ones. The system facilitates debugging and testing tasks by software engineers and developers; future research will integrate this R&R system into typical workflow and explore further applications in fields like security and compliance.",1,AI,0.0014085173606872,0.0488162040710449,Human,LABEL_1
"Reinforcement learning (RL) is one of the most important methods for solving decision making problems; agents learn by acting and receiving a reward. In practice however, there is often noise or variation in rewards that can result in poor decisions. This paper looks at reinforcement learning with noisy rewards through two major themes: (1) how reward noise impacts learning and algorithm performance and (2) development of robust algorithms capable of handling noise well. The study considers different types of noise like Gaussian noise and attacks and compares different RL algorithms such as value based and actor critic methods. Results show that noise has a strong effect on learning and algorithm performance and suggests that we need better algorithms to deal with these issues. The paper offers guidance and ideas for further research and applications ranging from robotics to games and autonomous systems.",1,AI,0.014369785785675,0.1953881382942199,Human,LABEL_1
"Current performance-driven building design methods are not widely adopted outside the research field for several reasons that make them difficult to integrate into a typical design process. In the early design phase, in particular, the time-intensity and the cognitive load associated with optimization and form parametrization are incompatible with design exploration, which requires quick iteration. This research introduces a novel method for performance-driven geometry generation that can afford interaction directly in the 3d modeling environment, eliminating the need for explicit parametrization, and is multiple orders faster than the equivalent form optimization. The method uses Machine Learning techniques to train a generative model offline. The generative model learns a distribution of optimal performing geometries and their simulation contexts based on a dataset that addresses the performance(s) of interest. By navigating the generative model's latent space, geometries with the desired characteristics can be quickly generated. A case study is presented, demonstrating the generation of a synthetic dataset and the use of a Variational Autoencoder (VAE) as a generative model for geometries with optimal solar gain. The results show that the VAE-generated geometries perform on average at least as well as the optimized ones, suggesting that the introduced method shows a feasible path towards more intuitive and interactive early-phase performance-driven design assistance.",0,Human,0.0148752927780151,0.0152475237846374,Human,LABEL_1
This paper introduces a new method for spatiotemporal modeling using connected vehicles. It integrates deep learning with generative models and performs effective modeling of complex spatial and temporal relationships among vehicle data. Results show high performance relative to traditional machine learning methods. Results from this study open promising avenues of future research into modeling spatiotemporal data; it also has practical implications for prediction and control of traffic and intelligent system design.,1,AI,0.000308871269226,0.9656372666358948,Human,LABEL_0
"This paper introduces a new way of compressing categorical data in machine learning models using submodular optimization. Many real world applications use this kind of data but their high dimensionality causes great computational burden and storage issues. We use optimization of submodular sets to choose a subset of representative categories which contain most important information from raw data. Specifically, we formulate this as a maximization problem using submodular functions and develop a greedy algorithm for efficient solution selection. We also establish theoretical guarantees for the quality of results generated by our algorithm. Empirical results show that this method outperforms previous methods both in terms of compression rate and accuracy on multiple benchmark datasets. Our approach is easy to incorporate into existing pipelines and thus promises to greatly enhance efficiency and scalability of models dealing with categorical data.",1,AI,0.0005172491073608,0.0141114592552185,Human,LABEL_1
"This paper looks at how to select an energy plan in markets where consumers have retail choice. Authors introduce two competitive algorithms for consumers to decide whether to continue with their current plan or to switch to a different one. One algorithm called ""Stay or Switch"" uses history of energy use and costs to find best strategy. The other called ""Adaptive Stay or Switch"" learns from changes in the market and adjusts plan selection. Performance of these algorithms is evaluated via simulations using real market data; results show both perform better than others and save consumers money.",1,AI,0.0053635239601135,0.3146872520446777,Human,LABEL_1
"Volume transmission is an important neural communication pathway in which neurons in one brain region influence the neurotransmitter concentration in the extracellular space of a distant brain region. In this paper, we apply asymptotic analysis to a stochastic partial differential equation model of volume transmission to calculate the neurotransmitter concentration in the extracellular space. Our model involves the diffusion equation in a three-dimensional domain with interior holes that randomly switch between being either sources or sinks. These holes model nerve varicosities that alternate between releasing and absorbing neurotransmitter, according to when they fire action potentials. In the case that the holes are small, we compute analytically the first two nonzero terms in an asymptotic expansion of the average neurotransmitter concentration. The first term shows that the concentration is spatially constant to leading order and that this constant is independent of many details in the problem. Specifically, this constant first term is independent of the number and location of nerve varicosities, neural firing correlations, and the size and geometry of the extracellular space. The second term shows how these factors affect the concentration at second order. Interestingly, the second term is also spatially constant under some mild assumptions. We verify our asymptotic results by high-order numerical simulation using radial basis function-generated finite differences.",0,Human,0.0358161330223083,0.0069073438644409,Human,LABEL_1
"This paper looks at current status of interoperability of electronic health records (EHR) in hospitals in Tanzania and identifies important issues, challenges, and opportunities that should be addressed to improve interoperability of these systems. Using a thorough review of literature and interviews with healthcare workers and other stakeholders, researchers pinpoint major barriers to interoperability such as lack of standardization, insufficient infrastructure and technology and lack of trained personnel. Researchers also look at potential benefits like better patient care, fewer medical errors and enhanced surveillance of public health. Finally, they give recommendations to policymakers, health care organizations and technology providers on how to improve interoperability through developing standards and guidelines, investing in technology and infrastructure, and implementing professional training programs. Research results offer valuable insights into interoperability of EHRs in Tanzania and are of interest to professionals in health IT.",1,AI,0.066777765750885,0.7646744251251221,ChatGPT,LABEL_0
"This paper introduces a new way to improve security for Industry 4.0 systems using QUDOS, a system based on cloud and edge computing. With the growing use of cloud applications and services, there is now increased demand for secure and fast processing of data. Yet current security solutions have not sufficiently addressed the complicated nature of threats faced today by industry. Therefore, the authors introduce QUDOS—a framework that executes deep neural networks securely. QUDOS leverages benefits from both cloud and edge computing to offer a strong solution for data processing in Industry 4.0. Using a consensus method based on quorums ensures secure and decentralized execution of DNNs across various edge devices and cloud servers. Secure multiparty computation methods are used to protect sensitive data during processing. Performance and security are evaluated through experiments and simulations; results demonstrate high accuracy and low latency. Results also show QUDOS excels compared to leading security solutions. In summary, QUDOS is presented as a promising enhancement for security in Industry 4.0 and offers a strong framework for executing DNNs in cloud and edge computing environments.",1,AI,0.001694917678833,0.0339341163635253,Human,LABEL_1
This paper investigates constellation design optimization for non coherent multiple input multiple output channels and multiple access channels. Objective is to improve performance of data transmission by optimizing constellation designs for use on such channels. A new approach for joint constellation design is introduced which considers both channel state information and pairing information of users. Performance evaluation is carried out using simulation studies and compared against other approaches. Results indicate that this new method significantly improves Bit Error Rate performance especially at high signal to noise ratio levels. The conclusion is that this new design for joint constellation is promising and useful for practical communications systems as well.,1,AI,0.000406801700592,0.1139816641807556,Human,LABEL_1
"Single-frame infrared small target (SIRST) detection aims at separating small targets from clutter backgrounds. With the advances of deep learning, CNN-based methods have yielded promising results in generic object detection due to their powerful modeling capability. However, existing CNN-based methods cannot be directly applied for infrared small targets since pooling layers in their networks could lead to the loss of targets in deep layers. To handle this problem, we propose a dense nested attention network (DNANet) in this paper. Specifically, we design a dense nested interactive module (DNIM) to achieve progressive interaction among high-level and low-level features. With the repeated interaction in DNIM, infrared small targets in deep layers can be maintained. Based on DNIM, we further propose a cascaded channel and spatial attention module (CSAM) to adaptively enhance multi-level features. With our DNANet, contextual information of small targets can be well incorporated and fully exploited by repeated fusion and enhancement. Moreover, we develop an infrared small target dataset (namely, NUDT-SIRST) and propose a set of evaluation metrics to conduct comprehensive performance evaluation. Experiments on both public and our self-developed datasets demonstrate the effectiveness of our method. Compared to other state-of-the-art methods, our method achieves better performance in terms of probability of detection (Pd), false-alarm rate (Fa), and intersection of union (IoU).",0,Human,0.0091643333435058,0.0122416615486145,Human,LABEL_1
"Chatter identification and detection in machining processes has been an active area of research in the past two decades. Part of the challenge in studying chatter is that machining equations that describe its occurrence are often nonlinear delay differential equations. The majority of the available tools for chatter identification rely on defining a metric that captures the characteristics of chatter, and a threshold that signals its occurrence. The difficulty in choosing these parameters can be somewhat alleviated by utilizing machine learning techniques. However, even with a successful classification algorithm, the transferability of typical machine learning methods from one data set to another remains very limited. In this paper we combine supervised machine learning with Topological Data Analysis (TDA) to obtain a descriptor of the process which can detect chatter. The features we use are derived from the persistence diagram of an attractor reconstructed from the time series via Takens embedding. We test the approach using deterministic and stochastic turning models, where the stochasticity is introduced via the cutting coefficient term. Our results show a 97% successful classification rate on the deterministic model labeled by the stability diagram obtained using the spectral element method. The features gleaned from the deterministic model are then utilized for characterization of chatter in a stochastic turning model where there are very limited analysis methods.",0,Human,0.0016263723373413,0.0156663060188293,Human,LABEL_1
"This paper introduces an algorithm that performs well on large networks by exploiting parallel processing capabilities of modern computers. It targets nodes with very high degree, which are problematic when using traditional methods to count triangles. The authors designed their method so that it scales up and processes big data quickly. Experimental results show the proposed method runs faster and better than other methods at triangle counting. Results show effectiveness of parallel processing and open new avenues for designing efficient algorithms for graph analysis.",1,AI,0.0248790979385375,0.9601587057113647,Human,LABEL_0
"This paper studies few shot audio classification, where one tries to categorize sounds into a limited number of classes based on very little labeled data. Focus is on evaluating various methods for dealing with scarcity of labeled data like transfer learning and meta learning. Results show these methods greatly improve performance compared to standard approaches. Performance is also analyzed depending on factors including size of labeled data used, number of classes, and signal complexity. Insights from this study contribute to designing better models for classification tasks which are useful for speech recognition, music analysis and other audio surveillance systems.",1,AI,0.0004203915596008,0.597227931022644,Human,LABEL_0
"Unstructured data from diverse sources, such as social media and aerial imagery, can provide valuable up-to-date information for intelligent situation assessment. Mining these different information sources could bring major benefits to applications such as situation awareness in disaster zones and mapping the spread of diseases. Such applications depend on classifying the situation across a region of interest, which can be depicted as a spatial ""heatmap"". Annotating unstructured data using crowdsourcing or automated classifiers produces individual classifications at sparse locations that typically contain many errors. We propose a novel Bayesian approach that models the relevance, error rates and bias of each information source, enabling us to learn a spatial Gaussian Process classifier by aggregating data from multiple sources with varying reliability and relevance. Our method does not require gold-labelled data and can make predictions at any location in an area of interest given only sparse observations. We show empirically that our approach can handle noisy and biased data sources, and that simultaneously inferring reliability and transferring information between neighbouring reports leads to more accurate predictions. We demonstrate our method on two real-world problems from disaster response, showing how our approach reduces the amount of crowdsourced data required and can be used to generate valuable heatmap visualisations from SMS messages and satellite images.",0,Human,0.0099888443946838,0.0138419866561889,Human,LABEL_1
"This paper proposes and evaluates an approach to study software use using knowledge graphs and focuses on the social sciences. The authors emphasize that understanding tools and methods important for research and that such knowledge graphs offer a way to see the whole picture of software use. They show results from a study which collects and analyzes information from different sources such as academic papers, software repositories and discussion forums. They illustrate how this approach reveals connections between software, researchers and research topics and shows ways to improve software use. The conclusion is that using knowledge graphs is valuable for studying software use and they suggest that this approach can serve as a basis for future work and development.",1,AI,0.1355674862861633,0.989055633544922,Human,LABEL_0
"Knowledge Graphs (KGs) have been integrated in several models of recommendation to augment the informational value of an item by means of its related entities in the graph. Yet, existing datasets only provide explicit ratings on items and no information is provided about user opinions of other (non-recommendable) entities. To overcome this limitation, we introduce a new dataset, called the MindReader, providing explicit user ratings both for items and for KG entities. In this first version, the MindReader dataset provides more than 102 thousands explicit ratings collected from 1,174 real users on both items and entities from a KG in the movie domain. This dataset has been collected through an online interview application that we also release open source. As a demonstration of the importance of this new dataset, we present a comparative study of the effect of the inclusion of ratings on non-item KG entities in a variety of state-of-the-art recommendation models. In particular, we show that most models, whether designed specifically for graph data or not, see improvements in recommendation quality when trained on explicit non-item ratings. Moreover, for some models, we show that non-item ratings can effectively replace item ratings without loss of recommendation quality. This finding, thanks also to an observed greater familiarity of users towards common KG entities than towards long-tail items, motivates the use of KG entities for both warm and cold-start recommendations.",0,Human,0.00274258852005,0.0056986808776855,Human,LABEL_1
This paper puts forward using neural nets as an instrument for analysis of attack patterns and generation of solutions for anticipated threats. Identification of attack patterns is crucial for cybersecurity; recognizing and dealing with such patterns is a significant task for cybersecurity experts. We develop a new method based on neural nets which uses machine learning techniques to detect and understand attack pattern features. Supervised and unsupervised learning is used to extract important features from large datasets of attacks. Neural nets use these features to recognize and classify threats and generate practical solutions for reducing threat impact. Results show the approach successfully analyzes attack patterns and generates actionable results with high precision and efficiency. Proposed model could enhance cybersecurity by offering a broad and automated way to detect and handle security risks.,1,AI,0.0010116696357727,0.7809411883354187,Human,LABEL_0
"This paper looks at how important collisions and collision avoidance are for collective behavior. First, we look at collective behavior among animals and artificial swarms and then discuss various things that influence this behavior such as catastrophic collisions and collision avoidance. We also simulate and conduct experiments to show how collisions and avoidance affect collective behavior. We analyze the results to determine if there is a connection between those factors and the performance of a swarm. Results indicate that avoiding collisions catastrophically and effectively is essential for successful collective behavior and impacts swarm performance significantly.",1,AI,0.115664541721344,0.9504450559616088,ChatGPT,LABEL_0
"Existing color-guided depth super-resolution (DSR) approaches require paired RGB-D data as training samples where the RGB image is used as structural guidance to recover the degraded depth map due to their geometrical similarity. However, the paired data may be limited or expensive to be collected in actual testing environment. Therefore, we explore for the first time to learn the cross-modality knowledge at training stage, where both RGB and depth modalities are available, but test on the target dataset, where only single depth modality exists. Our key idea is to distill the knowledge of scene structural guidance from RGB modality to the single DSR task without changing its network architecture. Specifically, we construct an auxiliary depth estimation (DE) task that takes an RGB image as input to estimate a depth map, and train both DSR task and DE task collaboratively to boost the performance of DSR. Upon this, a cross-task interaction module is proposed to realize bilateral cross task knowledge transfer. First, we design a cross-task distillation scheme that encourages DSR and DE networks to learn from each other in a teacher-student role-exchanging fashion. Then, we advance a structure prediction (SP) task that provides extra structure regularization to help both DSR and DE networks learn more informative structure representations for depth recovery. Extensive experiments demonstrate that our scheme achieves superior performance in comparison with other DSR methods.",0,Human,0.0272164940834045,0.0135819315910339,Human,LABEL_1
This paper introduces a meta transfer objective aimed at learning to separate causal mechanisms. It uses ideas from transfer learning wherein learning from one task improves performance on related tasks that differ. This new objective aims to transfer the skill of separating mechanisms from one problem to another. Results from a set of experiments are reported and compared against existing methods for separating causal mechanisms. Results show the proposed objective performs better in terms of accuracy and general performance; thus it demonstrates effectiveness at improving separation of mechanisms.,1,AI,0.0003019571304321,0.7725815773010254,Human,LABEL_0
"Recently, the problem of inaccurate learning targets in crowd counting draws increasing attention. Inspired by a few pioneering work, we solve this problem by trying to predict the indices of pre-defined interval bins of counts instead of the count values themselves. However, an inappropriate interval setting might make the count error contributions from different intervals extremely imbalanced, leading to inferior counting performance. Therefore, we propose a novel count interval partition criterion called Uniform Error Partition (UEP), which always keeps the expected counting error contributions equal for all intervals to minimize the prediction risk. Then to mitigate the inevitably introduced discretization errors in the count quantization process, we propose another criterion called Mean Count Proxies (MCP). The MCP criterion selects the best count proxy for each interval to represent its count value during inference, making the overall expected discretization error of an image nearly negligible. As far as we are aware, this work is the first to delve into such a classification task and ends up with a promising solution for count interval partition. Following the above two theoretically demonstrated criterions, we propose a simple yet effective model termed Uniform Error Partition Network (UEPNet), which achieves state-of-the-art performance on several challenging datasets. The codes will be available at: https://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.",0,Human,0.0049520134925842,0.0131146907806396,Human,LABEL_1
"Low-precision representation of deep neural networks (DNNs) is critical for efficient deployment of deep learning application on embedded platforms, however, converting the network to low precision degrades its performance. Crucially, networks that are designed for embedded applications usually suffer from increased degradation since they have less redundancy. This is most evident for the ubiquitous MobileNet architecture which requires a costly quantization-aware training cycle to achieve acceptable performance when quantized to 8-bits. In this paper, we trace the source of the degradation in MobileNets to a shift in the mean activation value. This shift is caused by an inherent bias in the quantization process which builds up across layers, shifting all network statistics away from the learned distribution. We show that this phenomenon happens in other architectures as well. We propose a simple remedy - compensating for the quantization induced shift by adding a constant to the additive bias term of each channel. We develop two simple methods for estimating the correction constants - one using iterative evaluation of the quantized network and one where the constants are set using a short training phase. Both methods are fast and require only a small amount of unlabeled data, making them appealing for rapid deployment of neural networks. Using the above methods we are able to match the performance of training-based quantization of MobileNets at a fraction of the cost.",0,Human,0.0005539059638977,0.0080368518829345,Human,LABEL_1
"With the growing need for energy efficient computing, there have been many approaches developed for allocating virtual machines (VMs) in cloud computing environments. This paper introduces a new method to reduce overall busy time for VMs and considers energy use very important. We model this as an optimization problem and design a heuristic algorithm that takes into account the load of VMs and server energy use. This algorithm allocates VMs so that energy consumption is balanced among servers while also minimizing overall busy time. Performance of the proposed algorithm is assessed using both real data and synthetic data. Results demonstrate that the proposed algorithm is very effective at reducing busy time while also substantially lowering energy use. Findings from this work are important for designing energy efficient cloud systems.",1,AI,0.0346887111663818,0.4756600856781006,Human,LABEL_1
"This paper explores connections among Artin automorphisms, cyclotomic function fields and folded list decodable codes. Artin automorphisms are important algebraic functions central to number theory and algebraic geometry; cyclotomic function fields are algebraic extensions of rationals obtained by adding roots of unity. Also, folded list decodable codes are special kinds of error correcting codes with desirable properties and they are extensively used in communication engineering. We show how one can use Artin automorphisms on cyclotomic function fields to produce codes that outperform previous approaches. Results indicate that better understanding of these automorphisms could lead to new advances for code design. Our work has practical implications in cryptography, communication engineering and computer science. Numerical simulations support results and point to advantages of the approach. This study paves new ways for future research and makes an important contribution to number theory, algebraic geometry and coding theory.",1,AI,0.0004196763038635,0.0715619325637817,Human,LABEL_1
"This paper looks into how important traceability is for neuroimaging analysis and offers a way to ensure high quality results. With increasing complexity and diversity in imaging data, there are many difficulties for researchers like potential mistakes and inconsistency in data handling and analysis. The study asserts that having a traceable and transparent process is key to maintain integrity and reliability of results and describes an integrated approach towards achieving that. This approach integrates tools and best practices from software engineering and data management with emphasis on automation, version control and metadata tracking. Detailed evaluation is also given of this solution along with advantages such as higher efficiency, lower error risk and better collaboration sharing results. Results of this work have significant implications for neuroimaging community and support broader efforts to enhance reliability and transparency of research.",1,AI,0.0183995962142944,0.735809326171875,Human,LABEL_0
"Reinforcement Learning is proving a successful tool that can manage urban intersections with a fraction of the effort required to curate traditional traffic controllers. However, literature on the introduction and control of pedestrians to such intersections is scarce. Furthermore, it is unclear what traffic state variables should be used as reward to obtain the best agent performance. This paper robustly evaluates 30 different Reinforcement Learning reward functions for controlling intersections serving pedestrians and vehicles covering the main traffic state variables available via modern vision-based sensors. Some rewards proposed in previous literature solely for vehicular traffic are extended to pedestrians while new ones are introduced. We use a calibrated model in terms of demand, sensors, green times and other operational constraints of a real intersection in Greater Manchester, UK. The assessed rewards can be classified in 5 groups depending on the magnitudes used: queues, waiting time, delay, average speed and throughput in the junction. The performance of different agents, in terms of waiting time, is compared across different demand levels, from normal operation to saturation of traditional adaptive controllers. We find that those rewards maximising the speed of the network obtain the lowest waiting time for vehicles and pedestrians simultaneously, closely followed by queue minimisation, demonstrating better performance than other previously proposed methods.",0,Human,0.0006852149963378,0.0032587647438049,Human,LABEL_1
"The ability to segment teeth precisely from digitized 3D dental models is an essential task in computer-aided orthodontic surgical planning. To date, deep learning based methods have been popularly used to handle this task. State-of-the-art methods directly concatenate the raw attributes of 3D inputs, namely coordinates and normal vectors of mesh cells, to train a single-stream network for fully-automated tooth segmentation. This, however, has the drawback of ignoring the different geometric meanings provided by those raw attributes. This issue might possibly confuse the network in learning discriminative geometric features and result in many isolated false predictions on the dental model. Against this issue, we propose a two-stream graph convolutional network (TSGCNet) to learn multi-view geometric information from different geometric attributes. Our TSGCNet adopts two graph-learning streams, designed in an input-aware fashion, to extract more discriminative high-level geometric representations from coordinates and normal vectors, respectively. These feature representations learned from the designed two different streams are further fused to integrate the multi-view complementary information for the cell-wise dense prediction task. We evaluate our proposed TSGCNet on a real-patient dataset of dental models acquired by 3D intraoral scanners, and experimental results demonstrate that our method significantly outperforms state-of-the-art methods for 3D shape segmentation.",0,Human,0.0016645789146423,0.0098128318786621,Human,LABEL_1
This paper proposes an approach using recurrence relations to analyze effective capacity for retransmission schemes in wireless communication systems. Effective capacity measures maximum achievable data rates with strict delay requirements. Proposed recurrence relation methodology supplies a tool for calculating effective capacity for various types of retransmission protocols such as ARQ and HARP protocols. Results from evaluation via numerical examples and simulation show effectiveness and efficiency of the proposed approach. Results of this work offer important guidance on design of efficient retransmission schemes and serve as a reference for future research related to wireless communications.,1,AI,0.0012319684028625,0.4948703646659851,Human,LABEL_1
"With the increasing interest in the content creation field in multiple sectors such as media, education, and entertainment, there is an increasing trend in the papers that uses AI algorithms to generate content such as images, videos, audio, and text. Generative Adversarial Networks (GANs) in one of the promising models that synthesizes data samples that are similar to real data samples. While the variations of GANs models, in general, have been covered to some extent in several survey papers, to the best of our knowledge, this is among the first survey papers that reviews the state-of-the-art video GANs models. This paper first categorized GANs review papers into general GANs review papers, image GANs review papers, and special field GANs review papers such as anomaly detection, medical imaging, or cybersecurity. The paper then summarizes the main improvements in GANs frameworks that are not initially developed for the video domain but have been adopted in multiple video GANs variations. Then, a comprehensive review of video GANs models is provided under two main divisions according to the presence or non-presence of a condition. The conditional models then further grouped according to the type of condition into audio, text, video, and image. The paper is concluded by highlighting the main challenges and limitations of the current video GANs models. A comprehensive list of datasets, applied loss functions, and evaluation metrics is provided in the supplementary material.",0,Human,0.0324223637580871,0.0334102511405944,Human,LABEL_1
The IEEE CIG 2017 competition on Game Data Mining sought the best way to predict how players behave in video games. This paper describes winning solutions and analyzes them. Winning method used machine learning including boosting gradients and deep neural nets. Results show that winning methods significantly outperform other competitors regarding accuracy of prediction. Results of this study point to potential use of combining machine learning to predict player behavior and emphasize the significance of careful choice of features and design of model structure. Authors also offer suggestions as to what might be improved and future directions for research.,1,AI,0.0021461844444274,0.8119898438453674,Human,LABEL_0
"This paper introduces a new method for understanding causal structures via a semantic framework based on category theory. Category theory is a mathematical discipline used here to define relationships among variables and effects resulting from changes in these variables. The core contribution of this paper is the introduction of a categorical causal model that enables reasoning about causal relations among variables concisely and intuitively. The authors provide various case studies to show how useful this categorical causal model is across diverse fields such as physics, biology and social sciences. Results show that using the semantic framework based on category theory is an effective way to formalize and reason clearly and consistently about causal relationships.",1,AI,0.0048564672470092,0.9856855869293212,Human,LABEL_0
"This paper looks at the concept of liveness within broadcasting networks. Liveness means real time transmission as opposed to prerecorded or on demand materials. We study how this idea has developed along with digital media and growing usage of social media sites. We look at existing literature that covers both theoretical and empirical studies across diverse disciplines. We report our own research on audience perceptions and understanding of liveness through qualitative and quantitative methods. We find that definition of liveness is fluid and depends on various cultural, technical and social factors. People's expectations and interpretations of liveness vary based on content type and context. In conclusion, liveness remains an important feature of broadcasting especially for live events and breaking news. But with ongoing development of digital media, we should reconsider liveness to accommodate newer engagement and interactivity.",1,AI,0.0413095951080322,0.3509371280670166,Human,LABEL_1
"This paper introduces a method of deep learning for better performance of systems that use proxy examples for object recognition. Usually such systems require a lot of proxy samples; this leads to high costs in computation and low scalability. However, we propose an approach called ""Less but Better"" which learns compact representations of proxy samples using deep models of graphs. Using relationships among proxies, this approach learns a lower dimensional feature space capturing discriminative information for each object category using fewer proxies. Results on different benchmarks show that this new approach excels at both accuracy and efficiency compared to previous methods that use many proxies. Overall, this work offers promising directions towards more efficient and scalable systems for object recognition.",1,AI,0.0003890395164489,0.1021476984024047,Human,LABEL_1
"This paper investigates efficient algorithms for solving adjacent quadratic shortest paths and compares different labeling algorithms including A* and Dijkstra. They introduce a new hybrid approach that combines features from both algorithms and experimentally find their proposed hybrid to perform better than other methods in terms of computation speed and solution quality. Results of this research contribute to further development of more efficient algorithms and have practical implications in areas like transportation, logistics, and network design.",1,AI,0.015665590763092,0.9016472101211548,Human,LABEL_0
"CNNs have excelled at performing place recognition over time, particularly when the neural network is optimized for localization in the current environmental conditions. In this paper we investigate the concept of feature map filtering, where, rather than using all the activations within a convolutional tensor, only the most useful activations are used. Since specific feature maps encode different visual features, the objective is to remove feature maps that are detract from the ability to recognize a location across appearance changes. Our key innovation is to filter the feature maps in an early convolutional layer, but then continue to run the network and extract a feature vector using a later layer in the same network. By filtering early visual features and extracting a feature vector from a higher, more viewpoint invariant later layer, we demonstrate improved condition and viewpoint invariance. Our approach requires image pairs for training from the deployment environment, but we show that state-of-the-art performance can regularly be achieved with as little as a single training image pair. An exhaustive experimental analysis is performed to determine the full scope of causality between early layer filtering and late layer extraction. For validity, we use three datasets: Oxford RobotCar, Nordland, and Gardens Point, achieving overall superior performance to NetVLAD. The work provides a number of new avenues for exploring CNN optimizations, without full re-training.",0,Human,0.0064812302589416,0.0084109902381896,Human,LABEL_1
"A new way of doing semantic segmentation called CCNet is presented in this paper. This method uses an attention mechanism that works well on other computer vision tasks. The core of CCNet involves using cross attention to get information from distant pixels. These distant pixels are important for good segmentation. The network structure includes a backbone for feature extraction, an attention module, and a segmentation head. This module focuses on both spatial and channel level to model pixel relationships. Results show that CCNet performs very well on benchmarks and does so with a light network design. Results indicate that proposed cross attention is promising and could work well on other tasks too.",1,AI,0.0099114775657653,0.1396523714065551,Human,LABEL_1
"This paper studies effectiveness of two main methods to solve Vehicle Routing Problems: sequence optimization and set optimization. Sequence optimization means optimizing the order in which stops are made at various customers by each vehicle; set optimization means grouping customers into clusters that different vehicles will visit. Comparisons between these methods are made through conducting a series of computational tests using benchmark instances of VRP. Different problem sizes and configurations were tested using different combinations of methods, and performance is judged by quality of solutions, computation times and scalability. Results indicate that effectiveness of one method over another depends largely on specific features of the problem. For smaller and medium sized instances where demands among customers are similar, sequence optimization performs better; for larger and more complex ones with varied customer demands and multiple depot locations, set optimization excels. However, mixed methods combining both approaches are also effective for some cases. Overall findings provide insight into relative advantages and disadvantages of different methods for VRP and suggest useful guidelines for professionals and researchers in VRP selection.",1,AI,0.0007181763648986,0.0091326236724853,Human,LABEL_1
"Permutations play a role in many different fields such as cryptography and computer science as well as pure mathematics. Relations among permutations are important for practical application; however, discerning those relations is difficult. This paper investigates the testability of such relation among permutations specifically. We concentrate on whether two permutations are identical, inverses, or conjugates to each other by probing them through some fixed set of points. We give efficient algorithms for testing these properties and prove they use only a low number of queries. The algorithms rely on methods from both group theory and graph theory, and their proofs use tools from probabilistic combinatorics and algebraic geometry. We also establish lower bounds on the number of queries needed which shows that our algorithms are nearly optimal. Results of this work impact studies into permutation groups and their uses in computing and cryptography.",1,AI,0.0004968643188476,0.0204313993453979,Human,LABEL_1
"Explanation of machine learning means using algorithms to explain why a model makes predictions. These algorithms offer great benefits but can cause harm if misused. This paper looks at both sides of this issue, focusing on ethical considerations. On the positive side, explanation through algorithmic means increases transparency and responsibility regarding decision making by offering clear reasons behind model predictions. This helps stakeholders to understand and trust decisions made by those models which improves effectiveness and efficiency. Yet there are also negative aspects to consider. If training data has bias, explanations produced from algorithms can perpetuate such bias and promote discrimination. Moreover, explanations generated by algorithms can be misleading so that decisions are wrong because of missing or inaccurate information. Ethical concerns are very high and practitioners must think about these when designing and deploying these algorithms. In conclusion, more research and discussion are needed to realize benefits while reducing harms of explanation via machine learning.",1,AI,0.0020850300788879,0.1737127304077148,Human,LABEL_1
This paper looks at specification methods for bisimulation and modal logic for the π calculus. π calculus is a formal system for modeling dynamic behavior of distributed systems. The paper studies use of bisimulation and modal logic for checking correctness of π calculus models. Authors detail an analysis of algorithm search proof and its implementation within π calculus. Results show effectiveness of using bisimulation and modal logic for checking correctness of π calculus models. Results of this research are significant for designing and implementing verification tools for distributed systems.,1,AI,0.0022817850112915,0.9164092540740968,Human,LABEL_0
"We consider Bayesian optimization of the output of a network of functions, where each function takes as input the output of its parent nodes, and where the network takes significant time to evaluate. Such problems arise, for example, in reinforcement learning, engineering design, and manufacturing. While the standard Bayesian optimization approach observes only the final output, our approach delivers greater query efficiency by leveraging information that the former ignores: intermediate output within the network. This is achieved by modeling the nodes of the network using Gaussian processes and choosing the points to evaluate using, as our acquisition function, the expected improvement computed with respect to the implied posterior on the objective. Although the non-Gaussian nature of this posterior prevents computing our acquisition function in closed form, we show that it can be efficiently maximized via sample average approximation. In addition, we prove that our method is asymptotically consistent, meaning that it finds a globally optimal solution as the number of evaluations grows to infinity, thus generalizing previously known convergence results for the expected improvement. Notably, this holds even though our method might not evaluate the domain densely, instead leveraging problem structure to leave regions unexplored. Finally, we show that our approach dramatically outperforms standard Bayesian optimization methods in several synthetic and real-world problems.",0,Human,0.0020028948783874,0.0078132152557373,Human,LABEL_1
"This paper presents a comprehensive survey of existing authentication and privacy-preserving schemes for 4G and 5G cellular networks. We start by providing an overview of existing surveys that deal with 4G and 5G communications, applications, standardization, and security. Then, we give a classification of threat models in 4G and 5G cellular networks in four categories, including, attacks against privacy, attacks against integrity, attacks against availability, and attacks against authentication. We also provide a classification of countermeasures into three types of categories, including, cryptography methods, humans factors, and intrusion detection methods. The countermeasures and informal and formal security analysis techniques used by the authentication and privacy preserving schemes are summarized in form of tables. Based on the categorization of the authentication and privacy models, we classify these schemes in seven types, including, handover authentication with privacy, mutual authentication with privacy, RFID authentication with privacy, deniable authentication with privacy, authentication with mutual anonymity, authentication and key agreement with privacy, and three-factor authentication with privacy. In addition, we provide a taxonomy and comparison of authentication and privacy-preserving schemes for 4G and 5G cellular networks in form of tables. Based on the current survey, several recommendations for further research are discussed at the end of this paper.",0,Human,0.0319743752479553,0.2980901598930359,Human,LABEL_1
"This paper conducts a quantitative study on cryptocurrency egalitarianism and analyzes how wealth distribution among users relates to egalitarian principles. We look at Bitcoin, Ethereum, and Litecoin, three of the most used crypto currencies. We compute Gini coefficients for each to measure wealth inequality. We also look at wealth concentration among top percentiles and consider different factors such as mining and trading rates. We find that in many cryptos wealth distribution is quite unequal despite frequent claims of decentralization and democracy. We also note however that there are some ways to enhance equity in this emerging financial ecosystem such as addressing wealth concentration and promoting fair mining practices. In sum we illuminate how much crypto aligns with egalitarian ideals and offer insight into strategies for greater wealth equity.",1,AI,0.013231337070465,0.0284029245376586,Human,LABEL_1
This paper develops tight bounds for randomized load balancing under arbitrary network topologies. Load balancing is a basic issue in distributed computing and randomized algorithms have been widely applied to resolve it. Previous work has given loose bounds for performance of such algorithms on arbitrary networks. To close this gap we introduce a new analysis framework that lets us derive tight bounds for expected maximum load of randomized load balancers. We use this framework by means of an innovative application of classic balls and bins problem and bound probability that any bin gets too many balls. Using this result we obtain tight bounds on expected maximum load that apply for any topology. Experimental results show effectiveness of our framework. In particular we show that our framework can accurately predict performance of load balancing algorithms on diverse topologies including real internet topologies. Results from this study are very valuable because they shed light on performance of load balancing on arbitrary topologies and can guide system design and analysis relying on load balancing.,1,AI,0.0013420581817626,0.009866714477539,Human,LABEL_1
"Cycling archetypes are essential concepts in analysis of nuclear fuel cycles. This study reviews all proposed and used archetypes comprehensively. The piece starts by defining what these archetypes are and explaining why they are important for analysis. Then an overview is given of the different archetypes that have been proposed and used including the once through, closed and open archetypes. Strengths and weaknesses of each archetype along with limitations and assumptions to consider are discussed as well. Finally, the results and implications for future research and development of sustainable nuclear energy systems are summarized.",1,AI,0.0109925270080566,0.9930883049964904,Human,LABEL_0
This study introduces an improved method for conducting sealed bidding auctions via multiparty circular quantum key agreement. This proposal aims to solve weaknesses in traditional auctions which are susceptible to threats like bid manipulation and information leakage. By integrating principles of quantum cryptography this proposal offers higher security assurance during auction processing. Multiparty circular key agreement improves security through guaranteed secure channels among all participants along with keeping bids secret. Results from simulation and experiment show that this new protocol performs well and securely; it represents a promising approach towards running auctions securely. Findings from this work have great significance for designing and implementing secure electronic auction systems.,1,AI,0.0003353357315063,0.6790287494659424,Human,LABEL_0
"Compile-time information flow analysis has been a promising technique for protecting confidentiality and integrity of private data. In the last couple of decades, a large number of information flow security tools in the form of run-time execution-monitors or static type systems have been developed for programming languages to analyze information flow security policies. However, existing flow analysis tools lack in precision and usability, which is the primary reason behind not being widely adopted in real application development. In this paper, we propose a compile-time information flow analysis for an imperative program based on a hybrid (mutable + immutable) labelling approach that enables a user to detect information flow-policy breaches and modify the program to overcome violations. We have developed an information flow security analyzer for a dialect of Python language, PyX, called Pifthon using the said approach. The flow-analyzer aids in identifying possible misuse of the information in sequential PyX programs corresponding to a given information flow policy (IFP). Pifthon has distinct advantages like reduced labelling overhead that ameliorates usability, covers a wide range of PyX programs that include termination-and progress-sensitive channels, in contrast to other approaches in the literature. The proposed flow analysis is proved to be sound under the classical non-interference property. Further, case study and experience in the usage of Pifthon are provided.",0,Human,0.0380389690399169,0.0104960203170776,Human,LABEL_1
"This paper presents an extensive review of recent progress regarding subpath queries on compressed graphs. It starts with a short introduction about significance of graph compression and its use cases across different areas. The paper then discusses different methods of compressing graphs such as adjacency lists, matrices, and multilevel decompositions. The main focus here is on subquery algorithms that run on compressed graphs along with consideration of tradeoffs between time and space efficiency. Both exact and approximate algorithms are considered and performance evaluation results on both real world and synthetic data sets are also discussed. Finally, the paper ends by outlining current open questions and avenues for future research in this domain.",1,AI,0.0157650113105773,0.7033182978630066,Human,LABEL_0
"This paper studies ways to resolve privacy conflicts among different users in social media. Because these platforms collect large amounts of personal information from millions of users and are therefore targets for privacy violations, there is now a strong need for good methods to handle conflicts between different parties. The research introduces a framework for handling privacy conflicts which considers privacy preferences of people involved. It integrates ideas from game theory, computational social choice and privacy enhancing technology to find a common solution. Experimental evaluations show that this proposed framework works well and can resolve conflicts realistically within a typical social media environment. Results from this research will help build privacy preserving social media sites and give useful guidance to practitioners and policymakers.",1,AI,0.0013186335563659,0.0816108584403991,Human,LABEL_1
"We study exploration in stochastic multi-armed bandits when we have access to a divisible resource that can be allocated in varying amounts to arm pulls. We focus in particular on the allocation of distributed computing resources, where we may obtain results faster by allocating more resources per pull, but might have reduced throughput due to nonlinear scaling. For example, in simulation-based scientific studies, an expensive simulation can be sped up by running it on multiple cores. This speed-up however, is partly offset by the communication among cores, which results in lower throughput than if fewer cores were allocated per trial to run more trials in parallel. In this paper, we explore these trade-offs in two settings. First, in a fixed confidence setting, we need to find the best arm with a given target success probability as quickly as possible. We propose an algorithm which trades off between information accumulation and throughput and show that the time taken can be upper bounded by the solution of a dynamic program whose inputs are the gaps between the sub-optimal and optimal arms. We also prove a matching hardness result. Second, we present an algorithm for a fixed deadline setting, where we are given a time deadline and need to maximize the probability of finding the best arm. We corroborate our theoretical insights with simulation experiments that show that the algorithms consistently match or outperform baseline algorithms on a variety of problem instances.",0,Human,0.0143886804580688,0.0159766674041748,Human,LABEL_1
"Vulnerable software represents a tremendous threat to modern information systems. Vulnerabilities in widespread applications may be used to spread malware, steal money and conduct target attacks. To address this problem, developers and researchers use different approaches of dynamic and static software analysis; one of these approaches is called fuzzing. Fuzzing is performed by generating and sending potentially malformed data to an application under test. Since first appearance in 1988, fuzzing has evolved a lot, but issues which addressed to effectiveness evaluation have not fully investigated until now. In our research, we propose a novel approach of fuzzing effectiveness evaluation, taking into account semantics of executed code along with a quantitative assessment. For this purpose, we use specific metrics of source code complexity assessment adapted to perform analysis of machine code. We conducted effectiveness evaluation of these metrics on 104 widespread applications with known vulnerabilities. As a result of these experiments, we were able to identify a set of metrics that are more suitable to find bugs. In addition, we conducted separate experiments on 7 applications without known vulnerabilities by using the set of metrics. The experimental results confirmed that proposed approach can be applied to increase performance of the fuzzing. Moreover, the tools helped detect two critical zero day (previously unknown) vulnerabilities in the wide-spread applications.",0,Human,0.0005905628204345,0.0157685279846191,Human,LABEL_1
"Distributed storage systems employ codes to provide resilience to failure of multiple storage disks. Specifically, an $(n, k)$ MDS code stores $k$ symbols in $n$ disks such that the overall system is tolerant to a failure of up to $n-k$ disks. However, access to at least $k$ disks is still required to repair a single erasure. To reduce repair bandwidth, array codes are used where the stored symbols or packets are vectors of length $\ell$. MDS array codes have the potential to repair a single erasure using a fraction $1/(n-k)$ of data stored in the remaining disks. We introduce new methods of analysis which capitalize on the translation of the storage system problem into a geometric problem on a set of operators and subspaces. In particular, we ask the following question: for a given $(n, k)$, what is the minimum vector-length or sub-packetization factor $\ell$ required to achieve this optimal fraction? For \emph{exact recovery} of systematic disks in an MDS code of low redundancy, i.e. $k/n > 1/2$, the best known explicit codes \cite{WTB12} have a sub-packetization factor $\ell$ which is exponential in $k$. It has been conjectured \cite{TWB12} that for a fixed number of parity nodes, it is in fact necessary for $\ell$ to be exponential in $k$. In this paper, we provide a new log-squared converse bound on $k$ for a given $\ell$, and prove that $k \le 2\log_2\ell\left(\log_{\delta}\ell+1\right)$, for an arbitrary number of parity nodes $r = n-k$, where $\delta = r/(r-1)$.",0,Human,0.001428484916687,0.0120809674263,Human,LABEL_1
"This paper introduces a new deep learning model for traffic forecasting called CDGNet. CDGNet is based on dynamic graphs and incorporates both spatial and temporal relationships into traffic data. Specifically, CDGNet uses a dynamic graph convolutional neural network to recognize correlations among different regions within traffic networks and uses a recurrent neural network with an attention mechanism for time pattern recognition. Performance of CDGNet was evaluated using real data and compared against leading models. Results show CDGNet excels in both short term and long term accuracy of traffic forecasting. CDGNet successfully captures complex spatio temporal relationships and generates high performance forecasts efficiently. Overall, this study offers promising approaches to traffic forecasting with broad implications such as improved management of traffic, urban planning and intelligent transportation systems. Contributions include the advancement of smart and efficient transportation systems which mitigate congestion, enhance safety and raise quality of life in cities.",1,AI,0.0011181235313415,0.2896373867988586,Human,LABEL_1
"The Internet of Things (IoT) is growing very fast; billions of devices are now connected to the Internet. As such growth has come, there are many security problems because most IoT devices have limited resources and don't have strong security measures. To combat this situation, a new architecture called OSCAR (Object Security Architecture for IoT) is introduced. OSCAR is a lightweight security design that employs object oriented security methods to defend against different kinds of attacks. This proposed architecture includes three major parts: object modeling, security for objects and communication among objects. Object modeling describes the structure and behavior of IoT objects and their security attributes. Security for objects is designed to protect them against various security threats. Communication specifies protocols and procedures for secure interactions among IoT objects. Performance metrics like memory and processing overheads, latency, and energy consumption are used to evaluate the proposed architecture. Results indicate that OSCAR offers good security performance and low overhead. It also supports services like confidentiality, integrity, authentication and access control. Compared to other existing architectures, OSCAR performs better on security and performance. Finally, OSCAR is implemented on diverse IoT devices including sensors, actuators and gateways showing practicality and effectiveness. In summary, this proposed architecture OSCAR offers lightweight and effective security solutions that can be widely adopted to IoT systems. Using object oriented security methods makes OSCAR adaptable and scalable, dealing with security issues posed by IoT.",1,AI,0.0091980099678039,0.0849575400352478,Human,LABEL_1
This paper introduces a model for classifying different kinds of vulnerabilities in smart contracts using an Attention based Bidirectional LSTM (AWDLSTM) network with an encoder based on natural language processing that has been pre trained. The aim here is to effectively identify various types of vulnerabilities so that decentralized applications can be secure and dependable. Using an encoder from natural language processing to extract features from smart contract code and using the AWDLSTM network for classification are employed by the model. Results of testing demonstrate that the proposed method surpasses previous approaches and offers a promising solution for classification of vulnerabilities in smart contracts.,1,AI,0.0044896602630615,0.7821045517921448,Human,LABEL_0
"Recent advances in artificial intelligence (AI) and machine learning have created a general perception that AI could be used to solve complex problems, and in some situations over-hyped as a tool that can be so easily used. Unfortunately, the barrier to realization of mass adoption of AI on various business domains is too high because most domain experts have no background in AI. Developing AI applications involves multiple phases, namely data preparation, application modeling, and product deployment. The effort of AI research has been spent mostly on new AI models (in the model training stage) to improve the performance of benchmark tasks such as image recognition. Many other factors such as usability, efficiency and security of AI have not been well addressed, and therefore form a barrier to democratizing AI. Further, for many real world applications such as healthcare and autonomous driving, learning via huge amounts of possibility exploration is not feasible since humans are involved. In many complex applications such as healthcare, subject matter experts (e.g. Clinicians) are the ones who appreciate the importance of features that affect health, and their knowledge together with existing knowledge bases are critical to the end results. In this paper, we take a new perspective on developing AI solutions, and present a solution for making AI usable. We hope that this resolution will enable all subject matter experts (eg. Clinicians) to exploit AI like data scientists.",0,Human,0.0071003437042236,0.0215341448783874,Human,LABEL_1
This paper investigates the automatic recognition of security discussion topics in microservice systems. It combines both survey results from industry professionals and experimental results to conduct a thorough analysis of this field. Surveys are conducted to understand how important security is and what difficulties exist for systems based on microservices. Experiments focus on developing and evaluating a machine learning approach to automatically recognize security discussions. Results show insights into current security levels in microservice systems and indicate that automated identification of security discussions is promising for future research and development.,1,AI,0.0247851610183715,0.9668405055999756,Human,LABEL_0
"Several cybersecurity domains, such as ransomware detection, forensics and data analysis, require methods to reliably identify encrypted data fragments. Typically, current approaches employ statistics derived from byte-level distribution, such as entropy estimation, to identify encrypted fragments. However, modern content types use compression techniques which alter data distribution pushing it closer to the uniform distribution. The result is that current approaches exhibit unreliable encryption detection performance when compressed data appears in the dataset. Furthermore, proposed approaches are typically evaluated over few data types and fragment sizes, making it hard to assess their practical applicability. This paper compares existing statistical tests on a large, standardized dataset and shows that current approaches consistently fail to distinguish encrypted and compressed data on both small and large fragment sizes. We address these shortcomings and design EnCoD, a learning-based classifier which can reliably distinguish compressed and encrypted data. We evaluate EnCoD on a dataset of 16 different file types and fragment sizes ranging from 512B to 8KB. Our results highlight that EnCoD outperforms current approaches by a wide margin, with accuracy ranging from ~82 for 512B fragments up to ~92 for 8KB data fragments. Moreover, EnCoD can pinpoint the exact format of a given data fragment, rather than performing only binary classification like previous approaches.",0,Human,0.0005730390548706,0.0217053890228271,Human,LABEL_1
"In system identification, estimating parameters of a model using limited observations results in poor identifiability. To cope with this issue, we propose a new method to simultaneously select and estimate sensitive parameters as key model parameters and fix the remaining parameters to a set of typical values. Our method is formulated as a nonlinear least squares estimator with L1-regularization on the deviation of parameters from a set of typical values. First, we provide consistency and oracle properties of the proposed estimator as a theoretical foundation. Second, we provide a novel approach based on Levenberg-Marquardt optimization to numerically find the solution to the formulated problem. Third, to show the effectiveness, we present an application identifying a biomechanical parametric model of a head position tracking task for 10 human subjects from limited data. In a simulation study, the variances of estimated parameters are decreased by 96.1% as compared to that of the estimated parameters without L1-regularization. In an experimental study, our method improves the model interpretation by reducing the number of parameters to be estimated while maintaining variance accounted for (VAF) at above 82.5%. Moreover, the variances of estimated parameters are reduced by 71.1% as compared to that of the estimated parameters without L1-regularization. Our method is 54 times faster than the standard simplex-based optimization to solve the regularized nonlinear regression.",0,Human,0.0568041205406188,0.016660451889038,Human,LABEL_1
"Conventional RGB-D salient object detection methods aim to leverage depth as complementary information to find the salient regions in both modalities. However, the salient object detection results heavily rely on the quality of captured depth data which sometimes are unavailable. In this work, we make the first attempt to solve the RGB-D salient object detection problem with a novel depth-awareness framework. This framework only relies on RGB data in the testing phase, utilizing captured depth data as supervision for representation learning. To construct our framework as well as achieving accurate salient detection results, we propose a Ubiquitous Target Awareness (UTA) network to solve three important challenges in RGB-D SOD task: 1) a depth awareness module to excavate depth information and to mine ambiguous regions via adaptive depth-error weights, 2) a spatial-aware cross-modal interaction and a channel-aware cross-level interaction, exploiting the low-level boundary cues and amplifying high-level salient channels, and 3) a gated multi-scale predictor module to perceive the object saliency in different contextual scales. Besides its high performance, our proposed UTA network is depth-free for inference and runs in real-time with 43 FPS. Experimental evidence demonstrates that our proposed network not only surpasses the state-of-the-art methods on five public RGB-D SOD benchmarks by a large margin, but also verifies its extensibility on five public RGB SOD benchmarks.",0,Human,0.0003333091735839,0.0196756720542907,Human,LABEL_1
"Properly designed precoders can significantly improve the spectral efficiency of multiple-input multiple-output (MIMO) relay systems. In this paper, we investigate joint source and relay precoding design based on the mean-square-error (MSE) criterion in MIMO two-way relay systems, where two multi-antenna source nodes exchange information via a multi-antenna amplify-and-forward relay node. This problem is non-convex and its optimal solution remains unsolved. Aiming to find an efficient way to solve the problem, we first decouple the primal problem into three tractable sub-problems, and then propose an iterative precoding design algorithm based on alternating optimization. The solution to each sub-problem is optimal and unique, thus the convergence of the iterative algorithm is guaranteed. Secondly, we propose a structured precoding design to lower the computational complexity. The proposed precoding structure is able to parallelize the channels in the multiple access (MAC) phase and broadcast (BC) phase. It thus reduces the precoding design to a simple power allocation problem. Lastly, for the special case where only a single data stream is transmitted from each source node, we present a source-antenna-selection (SAS) based precoding design algorithm. This algorithm selects only one antenna for transmission from each source and thus requires lower signalling overhead. Comprehensive simulation is conducted to evaluate the effectiveness of all the proposed precoding designs.",0,Human,0.0058386921882629,0.0250788331031799,Human,LABEL_1
"In the last decade, scenario-based serious-games have become a main tool for learning new skills and capabilities. An important factor in the development of such systems is the overhead in time, cost and human resources to manually create the content for these scenarios. We focus on how to create content for scenarios in medical, military, commerce and gaming applications where maintaining the integrity and coherence of the content is integral for the system's success. To do so, we present an automatic method for generating content about everyday activities through combining computer science techniques with the crowd. We use the crowd in three basic ways: to capture a database of scenarios of everyday activities, to generate a database of likely replacements for specific events within that scenario, and to evaluate the resulting scenarios. We found that the generated scenarios were rated as reliable and consistent by the crowd when compared to the scenarios that were originally captured. We also compared the generated scenarios to those created by traditional planning techniques. We found that both methods were equally effective in generated reliable and consistent scenarios, yet the main advantages of our approach is that the content we generate is more varied and much easier to create. We have begun integrating this approach within a scenario-based training application for novice investigators within the law enforcement departments to improve their questioning skills.",0,Human,0.0898329019546508,0.2225884795188903,ChatGPT,LABEL_1
This paper introduces a new approach for identifying emission sources in advection diffusion systems by integrating learning algorithms with the Green Function Inverse Method. Using supervised learning we estimate unknown sources using measured concentrations. We apply this inverse method to verify accuracy and to reconstruct unknown parameters. Results indicate that this new method works better than previous methods regarding both accuracy and computational speed; this makes it an attractive new tool for identifying sources in practical advection diffusion systems.,1,AI,0.0003559589385986,0.5776315927505493,Human,LABEL_0
"This paper studies random caching and cooperative transmission together in networks with different characteristics. Authors suggest a new method for better wireless data transmission using caching at intermediary nodes. This approach merges advantages of cooperation and caching and results in better performance and lower latency. Authors perform detailed simulation and evaluation to show effectiveness of their suggestion. Results indicate significant improvement in network performance, specifically in terms of throughput and energy efficiency over traditional methods. Results also show how various factors like number of caches and cache size impact performance. Results are valuable for designing future networks. In summary this paper offers promising solutions for enhancing network performance through integration of caching and cooperative transmission. Results of this research are important for network design and can serve as a basis for developing new technology.",1,AI,0.0149466395378112,0.4869120717048645,Human,LABEL_1
"This paper introduces a new way of designing an artificial neuron that uses superconducting nanowires and reduces power consumption. Proposed neurons consist of loops with junctions made of nanowires that can react differently to different inputs. Using superconductivity, these junctions serve as activation functions and reduce resistance. We have developed a detailed model and performed many simulations to prove their performance features. Results indicate that proposed neurons perform very well and consume much less power compared to previous designs. We also detail a method for integration of many neurons into larger networks and show that the design can scale up to build powerful yet efficient networks. Overall this research opens an exciting path towards creating energy efficient artificial neural networks that are useful for tasks like machine learning and data analysis.",1,AI,0.0218275785446167,0.1502503156661987,Human,LABEL_1
"This paper presents a discrete-time option pricing model that is rooted in Reinforcement Learning (RL), and more specifically in the famous Q-Learning method of RL. We construct a risk-adjusted Markov Decision Process for a discrete-time version of the classical Black-Scholes-Merton (BSM) model, where the option price is an optimal Q-function, while the optimal hedge is a second argument of this optimal Q-function, so that both the price and hedge are parts of the same formula. Pricing is done by learning to dynamically optimize risk-adjusted returns for an option replicating portfolio, as in the Markowitz portfolio theory. Using Q-Learning and related methods, once created in a parametric setting, the model is able to go model-free and learn to price and hedge an option directly from data, and without an explicit model of the world. This suggests that RL may provide efficient data-driven and model-free methods for optimal pricing and hedging of options, once we depart from the academic continuous-time limit, and vice versa, option pricing methods developed in Mathematical Finance may be viewed as special cases of model-based Reinforcement Learning. Further, due to simplicity and tractability of our model which only needs basic linear algebra (plus Monte Carlo simulation, if we work with synthetic data), and its close relation to the original BSM model, we suggest that our model could be used for benchmarking of different RL algorithms for financial trading applications",0,Human,0.0006146430969238,0.0088437795639038,Human,LABEL_1
"We consider a decentralized networked control system (DNCS) consisting of a remote controller and a collection of linear plants, each associated with a local controller. Each local controller directly observes the state of its co-located plant and can inform the remote controller of the plant's state through an unreliable uplink channel. The downlink channels from the remote controller to local controllers were assumed to be perfect. The objective of the local controllers and the remote controller is to cooperatively minimize the infinite horizon time average of expected quadratic cost. The finite horizon version of this problem was solved in our prior work [2]. The optimal strategies in the finite horizon case were shown to be characterized by coupled Riccati recursions. In this paper, we show that if the link failure probabilities are below certain critical thresholds, then the coupled Riccati recursions of the finite horizon solution reach a steady state and the corresponding decentralized strategies are optimal. Above these thresholds, we show that no strategy can achieve finite cost. We exploit a connection between our DNCS Riccati recursions and the coupled Riccati recursions of an auxiliary Markov jump linear system to obtain our results. Our main results in Theorems 1 and 2 explicitly identify the critical thresholds for the link failure probabilities and the optimal decentralized control strategies when all link failure probabilities are below their thresholds.",0,Human,0.0026113390922546,0.0110902190208435,Human,LABEL_1
"We explore the value of weak labels in learning transferable representations for medical images. Compared to hand-labeled datasets, weak or inexact labels can be acquired in large quantities at significantly lower cost and can provide useful training signals for data-hungry models such as deep neural networks. We consider weak labels in the form of pseudo-labels and propose a semi-weakly supervised contrastive learning (SWCL) framework for representation learning using semi-weakly annotated images. Specifically, we train a semi-supervised model to propagate labels from a small dataset consisting of diverse image-level annotations to a large unlabeled dataset. Using the propagated labels, we generate a patch-level dataset for pretraining and formulate a multi-label contrastive learning objective to capture position-specific features encoded in each patch. We empirically validate the transfer learning performance of SWCL on seven public retinal fundus datasets, covering three disease classification tasks and two anatomical structure segmentation tasks. Our experiment results suggest that, under very low data regime, large-scale ImageNet pretraining on improved architecture remains a very strong baseline, and recently proposed self-supervised methods falter in segmentation tasks, possibly due to the strong invariant constraint imposed. Our method surpasses all prior self-supervised methods and standard cross-entropy training, while closing the gaps with ImageNet pretraining.",0,Human,0.0005704760551452,0.0059804916381835,Human,LABEL_1
"The analysis of the structure of musical pieces is a task that remains a challenge for Artificial Intelligence, especially in the field of Deep Learning. It requires prior identification of structural boundaries of the music pieces. This structural boundary analysis has recently been studied with unsupervised methods and \textit{end-to-end} techniques such as Convolutional Neural Networks (CNN) using Mel-Scaled Log-magnitude Spectograms features (MLS), Self-Similarity Matrices (SSM) or Self-Similarity Lag Matrices (SSLM) as inputs and trained with human annotations. Several studies have been published divided into unsupervised and \textit{end-to-end} methods in which pre-processing is done in different ways, using different distance metrics and audio characteristics, so a generalized pre-processing method to compute model inputs is missing. The objective of this work is to establish a general method of pre-processing these inputs by comparing the inputs calculated from different pooling strategies, distance metrics and audio characteristics, also taking into account the computing time to obtain them. We also establish the most effective combination of inputs to be delivered to the CNN in order to establish the most efficient way to extract the limits of the structure of the music pieces. With an adequate combination of input matrices and pooling strategies we obtain a measurement accuracy $F_1$ of 0.411 that outperforms the current one obtained under the same conditions.",0,Human,0.006049633026123,0.0366758108139038,Human,LABEL_1
"A common writing style for statistical results are the recommendations of the American Psychology Association, known as APA-style. However, in practice, writing styles vary as reports are not 100% following APA-style or parameters are not reported despite being mandatory. In addition, the statistics are not reported in isolation but in context of experimental conditions investigated and the general topic. We address these challenges by proposing a flexible pipeline STEREO based on active wrapper induction and unsupervised aspect extraction. We applied our pipeline to the over 100,000 documents in the CORD-19 dataset. It required only 0.25% of the corpus (about 500 documents) to learn statistics extraction rules that cover 95% of the sentences in CORD-19. The statistic extraction has nearly 100% precision on APA-conform and 95% precision on non-APA writing styles. In total, we were able to extract 113k reported statistics, of which only <1% is APA conform. We could extract in 46% the correct conditions from APA-conform reports (30% for non-APA). The best model for topic extraction achieves a precision of 75% on statistics reported in APA style (73% for non-APA conform). We conclude that STEREO is a good foundation for automatic statistic extraction and future developments for scientific paper analysis. Particularly the extraction of non-APA conform reports is important and allows applications such as giving feedback to authors about what is missing and could be changed.",0,Human,0.0045171976089477,0.0059902667999267,Human,LABEL_1
"We introduce a parallel implementation of Ensemble Kalman Filter (EnKF) that uses modifications of Cholesky decomposition. EnKF is a common method for data assimilation using Monte Carlo approaches to estimate states of complicated systems. But its usual implementation can be quite costly in terms of computing power, especially when dealing with large problems. We improve on this by proposing a parallel version of EnKF which exploits modification of Cholesky decomposition to distribute processing among many processors. Cholesky decomposition factors matrices efficiently and computes square roots of positive definite ones. Using this trick, our parallel EnKF reduces computation time substantially while maintaining high accuracy. Performance is assessed through numerical tests and results show substantial speed ups compared to sequential code. Results have wide impact such as in weather prediction, climate modeling and seismic imaging where EnKF processes very large datasets in near real time.",1,AI,0.0005492568016052,0.0070236921310424,Human,LABEL_1
This paper is about fair division issues under heterogenous matroids that appear in diverse allocation scenarios where different resources have different properties and priority levels. We develop a new algorithmic framework that ensures both fairness and efficiency by making use of structural features of matroids and the idea of envy freedom up to one item (EF1). Our approach builds upon existing research on matroid intersections and partitions and is suitable for many agents and indivisible goods. We also conduct a theoretical analysis of the algorithm showing that it attains a constant approximation ratio for social welfare and EF1 envy freedom. Empirical results using synthetic and real data show practical effectiveness of this proposal surpassing current leading algorithms on both objective and evaluation by human subjects. Overall we contribute to literature on fair mechanisms allocation and provide an important tool for dealing with complicated constraints in practice.,1,AI,0.0060938000679016,0.0110787153244018,Human,LABEL_1
"Visual exploration of high-dimensional real-valued datasets is a fundamental task in exploratory data analysis (EDA). Existing methods use predefined criteria to choose the representation of data. There is a lack of methods that (i) elicit from the user what she has learned from the data and (ii) show patterns that she does not know yet. We construct a theoretical model where identified patterns can be input as knowledge to the system. The knowledge syntax here is intuitive, such as ""this set of points forms a cluster"", and requires no knowledge of maths. This background knowledge is used to find a Maximum Entropy distribution of the data, after which the system provides the user data projections in which the data and the Maximum Entropy distribution differ the most, hence showing the user aspects of the data that are maximally informative given the user's current knowledge. We provide an open source EDA system with tailored interactive visualizations to demonstrate these concepts. We study the performance of the system and present use cases on both synthetic and real data. We find that the model and the prototype system allow the user to learn information efficiently from various data sources and the system works sufficiently fast in practice. We conclude that the information theoretic approach to exploratory data analysis where patterns observed by a user are formalized as constraints provides a principled, intuitive, and efficient basis for constructing an EDA system.",0,Human,0.0043015480041503,0.0096309781074523,Human,LABEL_1
"The spectral gap $\gamma$ of a finite, ergodic, and reversible Markov chain is an important parameter measuring the asymptotic rate of convergence. In applications, the transition matrix $P$ may be unknown, yet one sample of the chain up to a fixed time $n$ may be observed. We consider here the problem of estimating $\gamma$ from this data. Let $\pi$ be the stationary distribution of $P$, and $\pi_\star = \min_x \pi(x)$. We show that if $n = \tilde{O}\bigl(\frac{1}{\gamma \pi_\star}\bigr)$, then $\gamma$ can be estimated to within multiplicative constants with high probability. When $\pi$ is uniform on $d$ states, this matches (up to logarithmic correction) a lower bound of $\tilde{\Omega}\bigl(\frac{d}{\gamma}\bigr)$ steps required for precise estimation of $\gamma$. Moreover, we provide the first procedure for computing a fully data-dependent interval, from a single finite-length trajectory of the chain, that traps the mixing time $t_{\text{mix}}$ of the chain at a prescribed confidence level. The interval does not require the knowledge of any parameters of the chain. This stands in contrast to previous approaches, which either only provide point estimates, or require a reset mechanism, or additional prior knowledge. The interval is constructed around the relaxation time $t_{\text{relax}} = 1/\gamma$, which is strongly related to the mixing time, and the width of the interval converges to zero roughly at a $1/\sqrt{n}$ rate, where $n$ is the length of the sample path.",0,Human,0.0126001834869384,0.014972448348999,Human,LABEL_1
"Recommender System research suffers currently from a disconnect between the size of academic data sets and the scale of industrial production systems. In order to bridge that gap we propose to generate more massive user/item interaction data sets by expanding pre-existing public data sets. User/item incidence matrices record interactions between users and items on a given platform as a large sparse matrix whose rows correspond to users and whose columns correspond to items. Our technique expands such matrices to larger numbers of rows (users), columns (items) and non zero values (interactions) while preserving key higher order statistical properties. We adapt the Kronecker Graph Theory to user/item incidence matrices and show that the corresponding fractal expansions preserve the fat-tailed distributions of user engagements, item popularity and singular value spectra of user/item interaction matrices. Preserving such properties is key to building large realistic synthetic data sets which in turn can be employed reliably to benchmark Recommender Systems and the systems employed to train them. We provide algorithms to produce such expansions and apply them to the MovieLens 20 million data set comprising 20 million ratings of 27K movies by 138K users. The resulting expanded data set has 10 billion ratings, 864K items and 2 million users in its smaller version and can be scaled up or down. A larger version features 655 billion ratings, 7 million items and 17 million users.",0,Human,0.0003854036331176,0.0045700073242187,Human,LABEL_1
This paper looks at spectral efficiency when there are correlated desired and interfering signals that follow gamma log normal distribution. It focuses on understanding how correlation affects performance of wireless systems. Results show that performance declines sharply because of high correlation between desired and interfering signals. A theoretical basis is developed for characterizing efficiency under such conditions and insight is gained into balancing correlation against efficiency. This research has big importance for designing and optimizing communication systems and can serve as guidance for better design and robustness.,1,AI,0.0104320645332336,0.691379189491272,Human,LABEL_0
"Today communications are everywhere. Social media, instant messaging apps, and online forums have become widespread; they produce huge amounts of conversational data daily. Observing and analyzing conversations among people has become a useful tool to understand and forecast behavior in these networks. But with larger and larger networks, new methods are needed to analyze conversation data. This paper introduces a framework for representing conversations designed for scalability. It uses graph structure of communication networks to make a compact representation of data that can be quickly processed. We use clustering and sequence modeling to discover topics in conversations and represent them as sequences of topic vectors. This representation reduces data dimensionality and makes overhear algorithms more efficient. We test our framework on a large dataset of forum conversations and compare its performance against baselines. Results show better accuracy and efficiency. Scalability is demonstrated through various size datasets containing millions of messages. Overall, our framework offers a new way to represent conversations for scalability. By exploiting network structure and lowering data dimensionality, our approach allows for more efficient large network analysis.",1,AI,0.004929780960083,0.050891637802124,Human,LABEL_1
"This paper studies pricing problems that are simple yet nearly optimal when dealing with proportional complementarity issues. These complementarities occur frequently in economics where value rises with increased adoption among others. The research concentrates on developing pricing schemes that produce maximum total surplus from trading and that are straightforward and easy to put into practice. It introduces a new approach that integrates ideas of optimal pricing with approximation techniques to derive pricing formulas that are both high quality and practical. Results from simulations indicate that proposed pricing formulas work well, producing near optimum performance while being much easier to use than traditional methods of optimal pricing. Findings also offer significant insights for people who make pricing decisions for goods with proportional complementarity and policy makers.",1,AI,0.0154403448104858,0.0544518828392028,Human,LABEL_1
"This paper introduces a new way to manage congestion in networks using Remote Direct Memory Access (RDMA). Called Dart, this method splits the network into smaller subnets and assigns each subnet to deal with different kinds of traffic. Dart reacts quickly to congestion and reduces latency for important applications. Results show Dart performs better than leading methods at both throughput and fairness; it also maintains low latency for real time apps. Results indicate Dart is promising as a solution for managing congestion in RDMA data center networks.",1,AI,0.0013571381568908,0.5515311360359192,Human,LABEL_0
"This paper studies changes in subjective probabilities of events or propositions over time; we call this belief dynamics. We focus on studying methods for tracking and modeling this change whether at the level of individuals or groups. Methods rely on Bayesian belief networks and probabilistic graphical models. These are established frameworks for modeling uncertainties and updating beliefs. Performance is evaluated by simulation and empirical testing and we also explore key factors influencing accuracy. Results from this work will improve models for real world tasks like decision making, risk assessment, and communication of information.",1,AI,0.0003615617752075,0.1241858005523681,Human,LABEL_1
"Since the 1990s, there have been significant advances in the technology space and the e-Commerce area, leading to an exponential increase in demand for cashless payment solutions. This has led to increased demand for credit cards, bringing along with it the possibility of higher credit defaults and hence higher delinquency rates, over a period of time. The purpose of this research paper is to build a contemporary credit scoring model to forecast credit defaults for unsecured lending (credit cards), by employing machine learning techniques. As much of the customer payments data available to lenders, for forecasting Credit defaults, is imbalanced (skewed), on account of a limited subset of default instances, this poses a challenge for predictive modelling. In this research, this challenge is addressed by deploying Synthetic Minority Oversampling Technique (SMOTE), a proven technique to iron out such imbalances, from a given dataset. On running the research dataset through seven different machine learning models, the results indicate that the Light Gradient Boosting Machine (LGBM) Classifier model outperforms the other six classification techniques. Thus, our research indicates that the LGBM classifier model is better equipped to deliver higher learning speeds, better efficiencies and manage larger data volumes. We expect that deployment of this model will enable better and timely prediction of credit defaults for decision-makers in commercial lending institutions and banks.",0,Human,0.0101863741874694,0.0482518672943115,Human,LABEL_1
This paper introduces a new approach to cooperative verification using collective generation of invariants. The method relies on the idea that many agents working together are able to produce mathematical descriptions of what the system should do. These invariants can be used to verify that the system functions correctly. Results show that this method performs better than previous approaches because it thoroughly explores system behavior and finds errors overlooked by other methods. Results from experiments show effectiveness across diverse practical examples and emphasize that this approach has broad applicability within verification domain.,1,AI,0.0066274404525756,0.8363034725189209,Human,LABEL_0
"There has been a remarkable increase in the data exchange over web and the widespread use of digital media. As a result, multimedia data transfers also had a boost up. The mounting interest with reference to digital watermarking throughout the last decade is certainly due to the increase in the need of copyright protection of digital content. This is also enhanced due to commercial prospective. Applications of video watermarking in copy control, broadcast monitoring, fingerprinting, video authentication, copyright protection etc is immensely rising. The main aspects of information hiding are capacity, security and robustness. Capacity deals with the amount of information that can be hidden. The skill of anyone detecting the information is security and robustness refers to the resistance to modification of the cover content before concealed information is destroyed. Video watermarking algorithms normally prefers robustness. In a robust algorithm it is not possible to eliminate the watermark without rigorous degradation of the cover content. In this paper, we introduce the notion of Video Watermarking and the features required to design a robust watermarked video for a valuable application. We review several algorithms, and introduce frequently used key techniques. The aim of this paper is to focus on the various domains of video watermarking techniques. The majority of the reviewed methods based on video watermarking emphasize on the notion of robustness of the algorithm.",0,Human,0.0004923343658447,0.0278927683830261,Human,LABEL_1
"Symbolic Aggregate Approximation (SAX) has been popular for time series analysis because of its effectiveness and pattern recognition capabilities. However, there are well known limitations when it comes to recognizing segments of trend. This paper proposes modifications to SAX to incorporate such segment trend information to improve performance. Through rigorous testing and evaluation, we show that this modification outperforms traditional SAX at recognizing both large and small trends in time series data. Results from this work contribute to further refinement of high performing time series analysis methods and are significant for practical applications in reality.",1,AI,0.0565589666366577,0.8041934967041016,Human,LABEL_0
"We have designed, fabricated, and successfully tested a prototype mixed-signal, 28x28-binary-input, 10-output, 3-layer neuromorphic network (""MLP perceptron""). It is based on embedded nonvolatile floating-gate cell arrays redesigned from a commercial 180-nm NOR flash memory. The arrays allow precise (~1%) individual tuning of all memory cells, having long-term analog-level retention and low noise. Each array performs a very fast and energy-efficient analog vector-by-matrix multiplication, which is the bottleneck for signal propagation in most neuromorphic networks. All functional components of the prototype circuit, including 2 synaptic arrays with 101,780 floating-gate synaptic cells, 74 analog neurons, and the peripheral circuitry for weight adjustment and I/O operations, have a total area below 1 mm^2. Its testing on the common MNIST benchmark set (at this stage, with a relatively low weight import precision) has shown a classification fidelity of 94.65%, close to the 96.2% obtained in simulation. The classification of one pattern takes less than 1 us time and ~20 nJ energy - both numbers much better than for digital implementations of the same task. Estimates show that this performance may be further improved using a better neuron design and a more advanced memory technology, leading to a >10^2 advantage in speed and a >10^4 advantage in energy efficiency over the state-of-the-art purely digital (GPU and custom) circuits, at classification of large, complex patterns.",0,Human,0.0014996528625488,0.0042587518692016,Human,LABEL_1
This paper introduces a new way of detecting deception attacks in systems that use networks. It uses a sequential detection framework along with watermark embedding. The authors embed a unique watermark into the system inputs and monitor the outputs for any unusual behaviors. Using sequential hypothesis testing they can detect if there are attacks by looking at how well the watermark correlates with the output. Results show this method works very well at finding different kinds of deception attacks with high accuracy but low false alarms. Research here shows promise for making these systems more secure and reliable against harmful attacks.,1,AI,0.0037254095077514,0.8958638310432434,Human,LABEL_0
"This study looks into using Local Interpretability Measure of Explanation with Outlier Detection (LIMEOUT) to improve fairness of machine learning (ML) models. LIMEOUT is a new approach that integrates LIME for explanation of model predictions locally with outlier detection for finding examples far from the training data distribution. Results show that LIMEOUT effectively enhances fairness of ML models by performing well on diverse fairness challenges benchmark datasets. Results indicate that LIMEOUT significantly reduces bias in prediction outcomes. Moreover, this paper shows that LIMEOUT can also serve to provide clear and understandable explanation for decisions made by ML models thereby enhancing credibility and accountability. Overall, the study indicates that LIMEOUT is an important advance towards making ML models both fairer and more transparent and hence mitigates ethical issues that arise with their usage across different sectors.",1,AI,0.0036051869392395,0.2636768221855163,Human,LABEL_1
"We consider the numerical stability of the parameter recovery problem in Linear Structural Equation Model ($\LSEM$) of causal inference. A long line of work starting from Wright (1920) has focused on understanding which sub-classes of $\LSEM$ allow for efficient parameter recovery. Despite decades of study, this question is not yet fully resolved. The goal of this paper is complementary to this line of work; we want to understand the stability of the recovery problem in the cases when efficient recovery is possible. Numerical stability of Pearl's notion of causality was first studied in Schulman and Srivastava (2016) using the concept of condition number where they provide ill-conditioned examples. In this work, we provide a condition number analysis for the $\LSEM$. First we prove that under a sufficient condition, for a certain sub-class of $\LSEM$ that are \emph{bow-free} (Brito and Pearl (2002)), the parameter recovery is stable. We further prove that \emph{randomly} chosen input parameters for this family satisfy the condition with a substantial probability. Hence for this family, on a large subset of parameter space, recovery is numerically stable. Next we construct an example of $\LSEM$ on four vertices with \emph{unbounded} condition number. We then corroborate our theoretical findings via simulations as well as real-world experiments for a sociology application. Finally, we provide a general heuristic for estimating the condition number of any $\LSEM$ instance.",0,Human,0.0008472204208374,0.0151230096817016,Human,LABEL_1
"Cybercrime markets support the development and diffusion of new attack technologies, vulnerability exploits, and malware. Whereas the revenue streams of cyber attackers have been studied multiple times in the literature, no quantitative account currently exists on the economics of attack acquisition and deployment. Yet, this understanding is critical to characterize the production of (traded) exploits, the economy that drives it, and its effects on the overall attack scenario. In this paper we provide an empirical investigation of the economics of vulnerability exploitation, and the effects of market factors on likelihood of exploit. Our data is collected first-handedly from a prominent Russian cybercrime market where the trading of the most active attack tools reported by the security industry happens. Our findings reveal that exploits in the underground are priced similarly or above vulnerabilities in legitimate bug-hunting programs, and that the refresh cycle of exploits is slower than currently often assumed. On the other hand, cybercriminals are becoming faster at introducing selected vulnerabilities, and the market is in clear expansion both in terms of players, traded exploits, and exploit pricing. We then evaluate the effects of these market variables on likelihood of attack realization, and find strong evidence of the correlation between market activity and exploit deployment. We discuss implications on vulnerability metrics, economics, and exploit measurement.",0,Human,0.0177181959152221,0.0062347650527954,Human,LABEL_1
"We consider finite horizon reach-avoid problems for discrete time stochastic systems. Our goal is to construct upper bound functions for the reach-avoid probability by means of tractable convex optimization problems. We achieve this by restricting attention to the span of Gaussian radial basis functions and imposing structural assumptions on the transition kernel of the stochastic processes as well as the target and safe sets of the reach-avoid problem. In particular, we require the kernel to be written as a Gaussian mixture density with each mean of the distribution being affine in the current state and input and the target and safe sets to be written as intersections of quadratic inequalities. Taking advantage of these structural assumptions, we formulate a recursion of semidefinite programs where each step provides an upper bound to the value function of the reach- avoid problem. The upper bounds provide a performance metric to which any suboptimal control policy can be compared, and can themselves be used to construct suboptimal control policies. We illustrate via numerical examples that even if the resulting bounds are conservative, the associated control policies achieve higher reach-avoid probabilities than heuristic controllers for problems of large state-input space dimensions (more than 20). The results presented in this paper, far exceed the limits of current approximation methods for reach-avoid problems in the specific class of stochastic systems considered.",0,Human,0.0023499131202697,0.0100374817848205,Human,LABEL_1
This paper investigates using heterogeneity for robust best arm selection in federated settings. Authors consider the challenge of choosing the best option from multiple arms distributed among many agents which may have different distributions. Proposed solution uses heterogeneity among agents to develop an efficient and robust algorithm for identifying the best arm. Performance of this algorithm is assessed on diverse simulated scenarios; results show it outperforms other methods. Results of this work advance robust algorithms for federated bandit selection which has wide application such as personalized recommendation systems and online advertising.,1,AI,0.0002892613410949,0.5432724952697754,Human,LABEL_0
"This paper introduces a new method to map unordered labeled trees with opposite isomorphisms, called Anti Tai Mapping. It tackles the problem of establishing one to one relations among nodes from different unordered labeled trees while preserving similarity in both structure and labels. Based on tree tai mapping concept, we modify this idea to deal with unordered trees and to ensure anti isomorphism. Performance evaluation of this method has been conducted on different data sets and compared with previous methods; results show high performance levels in terms of accuracy and speed. Results of this research will be useful for a variety of domains including pattern recognition, machine learning and computational biology.",1,AI,0.0004682540893554,0.347160279750824,Human,LABEL_1
This paper introduces a new method for sequence learning using Recurrent Neural Networks along with Semantic Variational Autoencoders (RNN-SVAE). This system combines an encoding and decoding architecture from typical autoencoders together with variational inference and recurrence to enhance learning of sequential data. Learning meaningful latent representations is at core of this approach and allows for high coherence semantic generation of outputs. Results indicate performance improvements over previous sequence models on many text generation tasks and shows capability to capture deep structure and meaning in sequences.,1,AI,0.000853419303894,0.3014401793479919,Human,LABEL_1
"In Federated Learning (FL), a strong global model is collaboratively learned by aggregating the clients' locally trained models. Although this allows no need to access clients' data directly, the global model's convergence often suffers from data heterogeneity. This paper suggests that forgetting could be the bottleneck of global convergence. We observe that fitting on biased local distribution shifts the feature on global distribution and results in forgetting of global knowledge. We consider this phenomenon as an analogy to Continual Learning, which also faces catastrophic forgetting when fitted on the new task distribution. Based on our findings, we hypothesize that tackling down the forgetting in local training relives the data heterogeneity problem. To this end, we propose a simple yet effective framework Federated Local Self-Distillation (FedLSD), which utilizes the global knowledge on locally available data. By following the global perspective on local data, FedLSD encourages the learned features to preserve global knowledge and have consistent views across local models, thus improving convergence without compromising data privacy. Under our framework, we further extend FedLSD to FedLS-NTD, which only considers the not-true class signals to compensate noisy prediction of the global model. We validate that both FedLSD and FedLS-NTD significantly improve the performance in standard FL benchmarks in various setups, especially in the extreme data heterogeneity cases.",0,Human,0.0012367963790893,0.0155835747718811,Human,LABEL_1
Learning and inference of relations within cortical networks is critical for understanding brain functioning. This paper presents a novel computational framework that models this process using deep learning theory. The framework uses artificial neural networks to study large datasets of sensory input and associated output targets. Through training on this dataset the framework learns the relationship between inputs and outputs. Results show that this new framework can reliably infer underlying relationships within cortical networks and opens new avenues for further investigation into brain function. Results also have significant implications for developing new AI systems and advancing knowledge about brain functions.,1,AI,0.0041209459304809,0.9637978076934814,Human,LABEL_0
"This paper investigates ethical issues concerning actuarial risk assessment and intervention methods to reduce forecasted negative results. Traditionally, actuaries assess risks using predictive algorithms to determine which people are at high risk for bad results and then distribute resources accordingly. But there are ethical concerns over discrimination and disparate treatment through this method. The paper contends that reframing this debate to intervene rather than predict results leads to more ethical and fair practice for risk assessment. Interventions should aim to fix underlying causes of risk and increase good results instead of just reducing foreseen bad results. In conclusion, consideration of ethics is important for actuarial risk assessment and dialogue is necessary about proper use of interventions.",1,AI,0.0097585320472717,0.8477910757064819,Human,LABEL_0
This paper investigates patterns of co-location among pollutants and cancers in children through a case study. Data from specific geographic region includes measurements of pollution levels and incidence of childhood cancer. Results describe how to identify correlations between these two things. Results show that there is strong clustering of high levels of certain pollutants alongside high rates of cancer incidence. Results of this work contribute to the ongoing discussion of the relationship between environmental pollution and health and serve as important input for regulators and public health officials when deciding on regulatory actions regarding environment.,1,AI,0.2522869706153869,0.9500982165336608,ChatGPT,LABEL_0
"We propose NormalGAN, a fast adversarial learning-based method to reconstruct the complete and detailed 3D human from a single RGB-D image. Given a single front-view RGB-D image, NormalGAN performs two steps: front-view RGB-D rectification and back-view RGBD inference. The final model was then generated by simply combining the front-view and back-view RGB-D information. However, inferring backview RGB-D image with high-quality geometric details and plausible texture is not trivial. Our key observation is: Normal maps generally encode much more information of 3D surface details than RGB and depth images. Therefore, learning geometric details from normal maps is superior than other representations. In NormalGAN, an adversarial learning framework conditioned by normal maps is introduced, which is used to not only improve the front-view depth denoising performance, but also infer the back-view depth image with surprisingly geometric details. Moreover, for texture recovery, we remove shading information from the front-view RGB image based on the refined normal map, which further improves the quality of the back-view color inference. Results and experiments on both testing data set and real captured data demonstrate the superior performance of our approach. Given a consumer RGB-D sensor, NormalGAN can generate the complete and detailed 3D human reconstruction results in 20 fps, which further enables convenient interactive experiences in telepresence, AR/VR and gaming scenarios.",0,Human,0.0014289617538452,0.0173254609107971,Human,LABEL_1
This paper introduces an approach using deep reinforcement learning (DRL) for caching content in vehicular edge computing networks (VECNs) that use permissioned blockchains. The authors use DRL to select the best caching strategy for each vehicle by taking into account relevant factors like storage capacity and popularity of content along with network conditions. Permissioned blockchains are employed to handle distribution and sharing of cached content among vehicles and also to secure transactions among participants in VECNs. Results from simulations indicate that this DRL based caching approach performs better compared to traditional approaches in terms of hit rates and energy efficiency. Combining DRL with permissioned blockchain promises a promising solution for caching content in VECNs and substantially improves system performance.,1,AI,0.0310034751892089,0.1507904529571533,Human,LABEL_1
"We develop here a computationally effective approach for producing high-quality $\mathcal{H}_\infty$-approximations to large scale linear dynamical systems having multiple inputs and multiple outputs (MIMO). We extend an approach for $\mathcal{H}_\infty$ model reduction introduced by Flagg, Beattie, and Gugercin for the single-input/single-output (SISO) setting, which combined ideas originating in interpolatory $\mathcal{H}_2$-optimal model reduction with complex Chebyshev approximation. Retaining this framework, our approach to the MIMO problem has its principal computational cost dominated by (sparse) linear solves, and so it can remain an effective strategy in many large-scale settings. We are able to avoid computationally demanding $\mathcal{H}_\infty$ norm calculations that are normally required to monitor progress within each optimization cycle through the use of ""data-driven"" rational approximations that are built upon previously computed function samples. Numerical examples are included that illustrate our approach. We produce high fidelity reduced models having consistently better $\mathcal{H}_\infty$ performance than models produced via balanced truncation; these models often are as good as (and occasionally better than) models produced using optimal Hankel norm approximation as well. In all cases considered, the method described here produces reduced models at far lower cost than is possible with either balanced truncation or optimal Hankel norm approximation.",0,Human,0.0004253983497619,0.006705105304718,Human,LABEL_1
"With the explosive growth in the number of pictures taken by smartphones, organizing and searching pictures has become important tasks. To efficiently fulfill these tasks, the key enabler is annotating images with proper keywords, with which keyword-based searching and organizing become available for images. Currently, smartphones usually synchronize photo albums with cloud storage platforms, and have their images annotated with the help of cloud computing. However, the ""offloading-to-cloud"" solution may cause privacy breach, since photos from smart photos contain various sensitive information. For privacy protection, existing research made effort to support cloud-based image annotation on encrypted images by utilizing cryptographic primitives. Nevertheless, for each annotation, it requires the cloud to perform linear checking on the large-scale encrypted dataset with high computational cost. This paper proposes a cloud-assisted privacy-preserving image annotation with randomized kd-forest, namely CPAR. With CPAR, users are able to automatically assign keywords to their images by leveraging the power of cloud with privacy protected. CPAR proposes a novel privacy-preserving randomized kd-forest structure, which significantly improves the annotation performance compared with existing research. Thorough analysis is carried out to demonstrate the security of CPAR. Experimental evaluation on the well-known IAPR TC-12 dataset validates the efficiency and effectiveness of CPAR.",0,Human,0.0187695026397705,0.03981614112854,Human,LABEL_1
This paper introduces FILIP as a new model that fine tunes interaction between language and images for better performance at tasks involving computer vision and natural language processing. FILIP combines a language model with an image recognition model and trains on a large corpus of both textual and visual data. Results show that when compared to previous pre training models FILIP performs much better. This study shows effectiveness of FILIP's approach to jointly training languages and images. Results also suggest that FILIP could prove to be a useful resource for many areas of AI research.,1,AI,0.0025654435157775,0.9529797434806824,Human,LABEL_0
"This paper introduces Augury, an application focused on time series analysis and forecasting of performance metrics of systems and networks. Augury uses advanced machine learning methods like neural nets and time series decomposition to discern trends and anomalies in performance metrics. Augury also supplies a broad range of forecast models that enable system admins to accurately predict future performance metrics. Effectiveness of Augury was validated using real data from different systems and networks. Results show that Augury excels at accuracy, scalability and speed compared to previous solutions. Moreover, Augury offers important clues about root causes of performance problems which may help optimize system and network performance. Augury promises to improve efficiency and reliability of complicated systems and networks and lower costs and effort related to performance monitoring and management.",1,AI,0.0009979605674743,0.2336159348487854,Human,LABEL_1
"Adversarial examples are often cited by neuroscientists and machine learning researchers as an example of how computational models diverge from biological sensory systems. Recent work has proposed adding biologically-inspired components to visual neural networks as a way to improve their adversarial robustness. One surprisingly effective component for reducing adversarial vulnerability is response stochasticity, like that exhibited by biological neurons. Here, using recently developed geometrical techniques from computational neuroscience, we investigate how adversarial perturbations influence the internal representations of standard, adversarially trained, and biologically-inspired stochastic networks. We find distinct geometric signatures for each type of network, revealing different mechanisms for achieving robust representations. Next, we generalize these results to the auditory domain, showing that neural stochasticity also makes auditory models more robust to adversarial perturbations. Geometric analysis of the stochastic networks reveals overlap between representations of clean and adversarially perturbed stimuli, and quantitatively demonstrates that competing geometric effects of stochasticity mediate a tradeoff between adversarial and clean performance. Our results shed light on the strategies of robust perception utilized by adversarially trained and stochastic networks, and help explain how stochasticity may be beneficial to machine and biological computation.",0,Human,0.0019252300262451,0.0179853439331054,Human,LABEL_1
This paper introduces a new method of behavior planning for connected autonomous vehicles by using feedback based deep reinforcement learning. Research goals focus on developing a system capable of learning from prior experience and making timely decisions to ensure road safety and efficiency. Proposed approach uses a combination of deep reinforcement learning and feedback control. Stability and robustness are enhanced by the use of control feedback components. Results show this approach performs much better than traditional planning methods in terms of both safety and efficiency. Results indicate the feasibility of this approach as a practical solution for vehicle behavior planning.,1,AI,0.0284017920494079,0.9714374542236328,Human,LABEL_0
"Regression analysis-based approaches have been widely studied for face recognition (FR) in the past several years. More recently, to better deal with some difficult conditions such as occlusions and illumination, nuclear norm based matrix regression methods have been proposed to characterize the low-rank structure of the error image, which generalize the one-dimensional, pixel-based error model to the two-dimensional structure. These methods, however, are inherently devised for grayscale image based FR and without exploiting the color information which is proved beneficial for FR of color face images. Benefiting from quaternion representation, which is capable of encoding the cross-channel correlation of color images, we propose a novel color FR method by formulating the color FR problem as a nuclear norm based quaternion matrix regression (NQMR). We further develop a more robust model called R-NQMR by using the logarithm of the nuclear norm, instead of the original nuclear norm, which adaptively assigns weights on different singular values, and then extend it to deal with the mixed noise. The proposed models, then, are solved using the effective alternating direction multiplier method (ADMM). Experiments on several public face databases demonstrate the superior performance and efficacy of the proposed approaches for color FR, especially for some difficult conditions (occlusion, illumination and mixed noise) over some state-of-the-art regression analysis-based approaches.",0,Human,0.0016107559204101,0.0142353177070617,Human,LABEL_1
"This paper establishes a lower bound for solving Boolean satisfiability problems using deterministic Turing machines. SAT is an important question in computer science asking whether we can assign true/false values to variables so that a given Boolean formula evaluates to true. Here we show that any algorithm running on such machines will take at least 2 to the power of n divided by a polynomial function of n in time; here n refers to the number of variables in the input formula and polynomial functions are related to n. To prove this result, this paper develops a new method of creating hard SAT instances. This method involves adding redundant clauses in a way such that satisfying assignments have to satisfy a large fraction of them. We show that checking these clauses takes a long time and this results in our lower bound. The significance of this work is that it proves a strong lower bound for SAT on deterministic machines which is a central topic in theoretical computer science. Results imply that efficient deterministic algorithms do not exist for solving SAT and have broader implications for algorithm design and analysis. Concluding remarks mention some promising lines for further research using ideas from this paper.",1,AI,0.004498541355133,0.0158605575561523,Human,LABEL_1
"This paper investigates scheduling algorithms for distributed event analysis in hybrid environments involving both edges and clouds. The goal is to distribute workload evenly between these components to ensure prompt and reliable analysis of events. A hybrid approach of splitting tasks and balancing loads is used dynamically according to resource availability on edges and clouds. Results show that this method distributes processing work effectively, reduces latency and enhances overall performance for event analysis. Results also indicate this research offers practical solutions for analytics in edge cloud environments that are important for applications like IoT and CPS systems.",1,AI,0.0094807147979736,0.4143012762069702,Human,LABEL_1
"Color constancy refers to the human visual system's ability to judge accurately the colors of objects under varying illumination conditions. For this reason, this has been difficult for computer vision systems which have attempted solutions such as using gray world, white patch and shades of gray. Yet those methods are not effective under all circumstances and they demand high computational power. This paper formulates adaptive color constancy as a few shot learning problem. Proposed approach uses meta learning framework to allow the model to quickly adapt to new cameras and light conditions using very little data. Results show the approach excels in both performance and efficiency compared to previous algorithms using diverse datasets. This work introduces a promising avenue for handling adaptive color constancy and could have broad impacts in vision computing and image processing.",1,AI,0.0020031332969665,0.0945183634757995,Human,LABEL_1
"Non-orthogonal multiple access (NOMA) has shown potential for scalable multicast of video data. However, one key drawback for NOMA-based video multicast is the limited number of layers allowed by the embedded successive interference cancellation algorithm, failing to meet satisfaction of heterogeneous receivers. We propose a novel receiver-driven superposed video multicast (Supcast) scheme by integrating Softcast, an analog-like transmission scheme, into the NOMA-based system to achieve high bandwidth efficiency as well as gradual decoding quality proportional to channel conditions at receivers. Although Softcast allows gradual performance by directly transmitting power-scaled transformation coefficients of frames, it suffers performance degradation due to discarding coefficients under insufficient bandwidth and its power allocation strategy cannot be directly applied in NOMA due to interference. In Supcast, coefficients are grouped into chunks, which are basic units for power allocation and superposition scheduling. By bisecting chunks into base-layer chunks and enhanced-layer chunks, the joint power allocation and chunk scheduling is formulated as a distortion minimization problem. A two-stage power allocation strategy and a near-optimal low-complexity algorithm for chunk scheduling based on the matching theory are proposed. Simulation results have shown the advantage of Supcast against Softcast as well as the reference scheme in NOMA under various practical scenarios.",0,Human,0.0009300112724304,0.0092036724090576,Human,LABEL_1
"Accurately modeling human decision-making in security is critical to thinking about when, why, and how to recommend that users adopt certain secure behaviors. In this work, we conduct behavioral economics experiments to model the rationality of end-user security decision-making in a realistic online experimental system simulating a bank account. We ask participants to make a financially impactful security choice, in the face of transparent risks of account compromise and benefits offered by an optional security behavior (two-factor authentication). We measure the cost and utility of adopting the security behavior via measurements of time spent executing the behavior and estimates of the participant's wage. We find that more than 50% of our participants made rational (e.g., utility optimal) decisions, and we find that participants are more likely to behave rationally in the face of higher risk. Additionally, we find that users' decisions can be modeled well as a function of past behavior (anchoring effects), knowledge of costs, and to a lesser extent, users' awareness of risks and context (R2=0.61). We also find evidence of endowment effects, as seen in other areas of economic and psychological decision-science literature, in our digital-security setting. Finally, using our data, we show theoretically that a ""one-size-fits""-all emphasis on security can lead to market losses, but that adoption by a subset of users with higher risks or lower costs can lead to market gains.",0,Human,0.1874265074729919,0.0109138488769531,Human,LABEL_1
"We present here a new approach to infer correlated signals through free energy exploration. This method estimates the free energy landscape for a set of correlated signals and explores that landscape to find the most probable configuration of those signals. We base our method on assuming that free energy is an adequate approximation for system entropy and that exploration of this landscape leads to inferring underlying structures among correlated signals. To validate our approach we use different data sets both synthetic and real and we compare performances against other methods. Results show that free energy exploration outperforms previous approaches in many cases and infers structure effectively. Our approach has broad application in signal processing and machine learning including analysis of complex systems, inference of hidden variables and pattern discovery in high dimensions.",1,AI,0.0004088282585144,0.1162279248237609,Human,LABEL_1
Authors introduce a new beamforming method called Layered Group Sparse Beamforming (LGSB) for green wireless networks which have caching capabilities. This new method addresses the dual challenge of balancing energy efficiency and data rate. LGSB works by classifying users into different levels and assigning sparse beamforming resources to each level. Simulation results indicate LGSB performs much better than traditional methods regarding both energy efficiency and high data rates. Results from this research may help develop more sustainable and efficient wireless networks.,1,AI,0.0019981265068054,0.9784005880355836,Human,LABEL_0
"Wide Area Monitoring Systems (WAMS) utilizing synchrophasor measurements is considered one of the essential parts in smart grids that enable system operators to monitor, operate, and control power systems in wide geographical area. On the other hand, high-speed, reliable and scalable data communication infrastructure is crucial in both construction and operation of WAMS. Universal mobile Telecommunication System (UMTS), the 3G standard for mobile communication networks, was developed to provide high speed data transmission with reliable service performance for mobile users. Therefore, UMTS is considered a promising solution for providing a communication infrastructure for WAMS. 3G based EWAMS (Egyptian wide area Monitoring System) is designed and implemented in Egypt through deployment a number of frequency disturbance recorders (FDRs) devices on a live 220kV/500kV Egyptian grid in cooperation with the Egyptian Electricity Transmission Company (EETC). The developed EWAMS can gather information from 11 FDRs devices which are geographically dispersed throughout the boundary of the Egyptian power grid and to a remote data management center located at Helwan University. The communication performance for the developed EWAMS in terms of communication time delay, throughput, and percentage of wasted bandwidth are studied in this paper. The results showed that the system can achieve successfully the communication requirements needed by various wide area monitoring applications.",0,Human,0.0203261971473693,0.0145095586776733,Human,LABEL_1
"This paper introduces a new detection method for MIMO multiple access channels that combines low complexity with full diversity gain. It exploits structural features of the channel matrix and uses iterative detection to achieve this. Performance is assessed via simulations comparing this proposed method against traditional detection methods using metrics like Bit Error Rate (BER) and computational load. Results show this new scheme achieves full diversity benefits along with better BER performance compared to current methods; meanwhile, its computational load remains very low. Proposed method has promise as an important solution for high speed communication systems which need low computational cost.",1,AI,0.0002968907356262,0.1627326607704162,Human,LABEL_1
This paper investigates coordination policies for robotic swarms. These swarms consist of a large number of simple robots that can cooperate to do complicated things. But it's hard to coordinate them because they're complicated systems and there's no central controller. We suggest using reinforcement learning here. Using this approach we use a central critic and decentralized agents to learn coordination policies. The critic looks at overall swarm state and gives feedback to the agents who then act locally based on observation of local environment. Critic feedback updates actor policies to gradually learn coordination among swarming robots. Performance on benchmarks such as formation control and avoidance of obstacles and cooperation transport is evaluated. Results show that this method learns coordination well; performing high performance and generalizing to new environments too. Overall this work shows promise for reinforcement learning of coordination policies for swarms and establishes a basis for further research. Coordination through this method could result in advanced and efficient swarms that can handle various environments effectively.,1,AI,0.0005068778991699,0.0077010989189147,Human,LABEL_1
"Kernel machines often yield superior predictive performance on various tasks; however, they suffer from severe computational challenges. In this paper, we show how to overcome the important challenge of speeding up kernel machines. In particular, we develop a parallel block minimization framework for solving kernel machines, including kernel SVM and kernel logistic regression. Our framework proceeds by dividing the problem into smaller subproblems by forming a block-diagonal approximation of the Hessian matrix. The subproblems are then solved approximately in parallel. After that, a communication efficient line search procedure is developed to ensure sufficient reduction of the objective function value at each iteration. We prove global linear convergence rate of the proposed method with a wide class of subproblem solvers, and our analysis covers strongly convex and some non-strongly convex functions. We apply our algorithm to solve large-scale kernel SVM problems on distributed systems, and show a significant improvement over existing parallel solvers. As an example, on the covtype dataset with half-a-million samples, our algorithm can obtain an approximate solution with 96% accuracy in 20 seconds using 32 machines, while all the other parallel kernel SVM solvers require more than 2000 seconds to achieve a solution with 95% accuracy. Moreover, our algorithm can scale to very large data sets, such as the kdd algebra dataset with 8 million samples and 20 million features.",0,Human,0.008205235004425,0.0077960491180419,Human,LABEL_1
"This paper looks at why people judge differently when it comes to errors called erasure and undetected errors in digital communication systems. It focuses specifically on why people perceive and appraise these different kinds of errors differently and what makes them do so. The paper starts by defining erasure and undetected errors and explains their importance in digital communication systems. Next, it reviews previous work about how people judge these errors and highlights that there is no consensus among researchers. Methodology includes experiments where people rate the severity of probabilities of erasures and undetected errors in a simulation of digital communication. Both quantitative measures (like severity ratings) and qualitative information (such as estimated probabilities and reasons for judgment) are collected from participants. Results show that people usually judge undetected errors more harshly and they think that they affect trust more strongly. Factors influencing this distinction include system design, users' experience, and personal differences regarding risk perception. Overall this paper deepens our understanding of how we judge errors in digital communication and offers guidance for designers and engineers to build better systems.",1,AI,0.0619198679924011,0.0303288102149963,Human,LABEL_1
"Domain-Specific Languages (DSLs) help practitioners in contributing solutions to challenges of specific domains. The efficient development of user-friendly DSLs suitable for industrial practitioners with little expertise in modelling still is challenging. For such practitioners, who often do not model on a daily basis, there is a need to foster reduction of repetitive modelling tasks and providing simplified visual representations of DSL parts. For industrial language engineers, there is no methodical support for providing such guidelines or documentation as part of reusable language modules. Previous research either addresses the reuse of languages or guidelines for modelling. For the efficient industrial deployment of DSLs, their combination is essential: the efficient engineering of DSLs from reusable modules that feature integrated documentation and guidelines for industrial practitioners. To solve these challenges, we propose a systematic approach for the industrial engineering of DSLs based on the concept of reusable DSL Building Blocks, which rests on several years of experience in the industrial engineering of DSLs and their deployment to various organizations. We investigated our approach via focus group methods consisting of five participants from industry and research qualitatively. Ultimately, DSL Building Blocks support industrial language engineers in developing better usable DSLs and industrial practitioners in more efficiently achieving their modelling.",0,Human,0.0007039308547973,0.0169030427932739,Human,LABEL_1
"This research introduces an integrated approach for computing resource allocation in three layered IoT fog networks that merges Stackelberg games with matching algorithms. The structure includes cloud, fog and edge layers with distinct computing resources. A leader follower relationship is modelled using Stackelberg game interaction among the layers, while matching algorithms optimize resource allocation based on network boundaries and goals. Results from simulation show effectiveness of the suggested approach: high efficiency of resource allocation, reduced latency and better network performance relative to conventional methods. Results also point toward potential for combining game theory with optimization to deal with allocation issues in higher tier IoT networks.",1,AI,0.0004589557647705,0.0585257411003112,Human,LABEL_1
This paper studies structured mapping as a way to efficiently share information among many users through shared communication channels. We look at how structured mappings reduce interference and increase channel capacity. We present a theoretical framework for designing structured mappings that optimize conveying common information against resource requirements. Proposed algorithm uses sparse graph codes for practical implementation. Results from simulations show this method performs better in terms of achievable rates and decoding complexity. Conclusions are that structured mappings offer a promising solution for improving efficiency and channel capacity and have potential application areas including wireless networks and multimedia communications.,1,AI,0.0036802291870117,0.4943184852600097,Human,LABEL_1
"This study introduces a new way to estimate depth without supervision using H-Net, which is an attention based deep neural network and uses epipolar geometry. Depth estimation from pairs of stereo images is very important in computer vision and has applications in robotics and driving autonomously and for 3D reconstruction. This work uses an attention mechanism to learn to focus on important image areas for depth estimation. Epipolar geometry is used to impose geometric constraints on estimated depth. Experimental results show that this method performs better than leading methods on benchmark datasets. Moreover, this method can be straightforwardly adapted to tasks like optical flow estimation and super resolution too. Results indicate that H Net using epipolar geometry promises good performance for unsupervised depth estimation and could have broad application in computer vision.",1,AI,0.0022854804992675,0.0521847009658813,Human,LABEL_1
"This paper introduces Serket which is a new architecture for linking stochastic models into large cognitive systems. The architecture relies on modularity and enables integration of different models into one system. Using strength of stochastic models such as handling uncertainty and making probabilistic predictions while also dealing with their drawbacks like difficulty of integrating diverse models, Serket is evaluated on various tasks including language understanding and generation, decision making and perception. Results show that performance of Serket surpasses previous work concerning accuracy, scalability and interpretability. At the conclusion, this paper discusses implications for further development of advanced cognitive systems and potential applications such as AI and robotics.",1,AI,0.0003230571746826,0.8022837042808533,Human,LABEL_0
This study introduces a new method for classifying artists based on music using Convolutional Recurrent Neural Networks (CRNNs). The system extracts features both temporally and spectrally directly from raw audio signals. This allows capturing fine short and long term patterns in the audio data. Performance is evaluated against large datasets and compared to leading methods; results indicate that the proposed model using CRNNs performs very well in terms of accuracy and speed. Results also show great promise for applying deep learning for artist classification tasks.,1,AI,0.0005587935447692,0.7344936728477478,Human,LABEL_0
We propose a new approach to design faster algorithms that use conditional sampling. Sampling is used here to process large datasets quickly without reading all the data. Using our approach we sample only a small fraction of data to get accurate estimates with high probability of certain functions. This leads to considerable reduction of computation time and preserves accuracy. Results from testing on real data sets have shown that this new method performs better compared to previous methods. This work initiates new research paths into sub linear algorithms and has practical uses in areas like machine learning and data mining.,1,AI,0.0067840218544006,0.8822976350784302,Human,LABEL_0
"Dense Multi-GPU systems have recently gained a lot of attention in the HPC arena. Traditionally, MPI runtimes have been primarily designed for clusters with a large number of nodes. However, with the advent of MPI+CUDA applications and CUDA-Aware MPI runtimes like MVAPICH2 and OpenMPI, it has become important to address efficient communication schemes for such dense Multi-GPU nodes. This coupled with new application workloads brought forward by Deep Learning frameworks like Caffe and Microsoft CNTK pose additional design constraints due to very large message communication of GPU buffers during the training phase. In this context, special-purpose libraries like NVIDIA NCCL have been proposed for GPU-based collective communication on dense GPU systems. In this paper, we propose a pipelined chain (ring) design for the MPI_Bcast collective operation along with an enhanced collective tuning framework in MVAPICH2-GDR that enables efficient intra-/inter-node multi-GPU communication. We present an in-depth performance landscape for the proposed MPI_Bcast schemes along with a comparative analysis of NVIDIA NCCL Broadcast and NCCL-based MPI_Bcast. The proposed designs for MVAPICH2-GDR enable up to 14X and 16.6X improvement, compared to NCCL-based solutions, for intra- and inter-node broadcast latency, respectively. In addition, the proposed designs provide up to 7% improvement over NCCL-based solutions for data parallel training of the VGG network on 128 GPUs using Microsoft CNTK.",0,Human,0.0074261426925659,0.0097254514694213,Human,LABEL_1
"The world is facing a tough situation due to the catastrophic pandemic caused by novel coronavirus (COVID-19). The number people affected by this virus are increasing exponentially day by day and the number has already crossed 6.4 million. As no vaccine has been discovered yet, the early detection of patients and isolation is the only and most effective way to reduce the spread of the virus. Detecting infected persons from chest X-Ray by using Deep Neural Networks, can be applied as a time and laborsaving solution. In this study, we tried to detect Covid-19 by classification of Covid-19, pneumonia and normal chest X-Rays. We used five different Convolutional Pre-Trained Neural Network models (VGG16, VGG19, Xception, InceptionV3 and Resnet50) and compared their performance. VGG16 and VGG19 shows precise performance in classification. Both models can classify between three kinds of X-Rays with an accuracy over 92%. Another part of our study was to find the impact of weather factors (temperature, humidity, sun hour and wind speed) on this pandemic using Decision Tree Regressor. We found that temperature, humidity and sun-hour jointly hold 85.88% impact on escalation of Covid-19 and 91.89% impact on death due to Covid-19 where humidity has 8.09% impact on death. We also tried to predict the death of an individual based on age, gender, country, and location due to COVID-19 using the LogisticRegression, which can predict death of an individual with a model accuracy of 94.40%.",0,Human,0.0305227041244506,0.0178288221359252,Human,LABEL_1
"This paper investigates how to use machine learning algorithms to forecast congestion related to routing in High Level Synthesis of FPGAs. Congestion issues are major impediments in FPGA design and can result in delays and higher costs. Accurately predicting congestion is important for optimizing design parameters and ensuring timely product delivery. We propose a methodology that integrates machine learning models like decision trees and random forests with traditional congestion prediction metrics. We also study how different feature extraction techniques and learning algorithms affect the accuracy of congestion prediction. Experimental results indicate that this approach performs well and surpasses traditional congestion forecasting methods. Results show that techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) significantly enhance performance. Results show this research offers an efficient way to forecast congestion during synthesis; this approach optimizes design parameters and reduces iteration counts, which ultimately reduces development costs and speeds up delivery time. This approach also opens the door for further application in optimization problems specific to FPGA design. This research shows the importance of incorporating machine learning into FPGA design.",1,AI,0.0009031295776367,0.2201382517814636,Human,LABEL_1
"This study introduces a framework for enhancing lexical semantic resources using distributional semantics. We aim to overcome the shortcomings of traditional lexical resources like WordNet and Ontology by incorporating statistical information extracted from large corpora of text. This framework consists of three main parts: data acquisition, feature extraction, and semantic enhancement. Data acquisition retrieves a vast amount of text from diverse sources such as forums, news, and social media. Feature extraction uses algorithms such as word2vec and GloVe to extract word vector representations from acquired text. Finally, we integrate these vectors into existing resources like WordNet and Ontology to increase semantic scope and quality. Performance evaluation on benchmark tasks such as word similarity and analogy shows that this proposed framework greatly enhances performance of current lexical semantic resources. This framework offers practical means for improving lexical semantic resources and could affect many NLP tasks.",1,AI,0.012145459651947,0.0217008590698242,Human,LABEL_1
"This paper introduces an Approximate Dynamic Programming (ADP) method for disaster recovery management. Current methods tend to rely on fixed and rigid plans that fail to suit the unpredictable nature of post disaster scenarios. Proposed ADP uses past disaster experience to adaptively adjust strategies according to evolving conditions of the affected community. This approach blends data driven machine learning techniques with a framework for dynamic optimization to achieve flexible decision making for recovery management. It addresses diverse goals including reducing recovery time, maximizing resource use, and lowering costs while considering uncertainties and constraints of the recovery process. Feedback from community and recovery stakeholders can also enhance learning and ongoing improvement. A hypothetical community hit by a major earthquake serves as a case study. Results show that this new approach produces high quality recovery plans surpassing previous static plans. Furthermore, ADP swiftly adjusts to environmental changes allowing effective and efficient management. Overall, this research shows potential benefits of ADP for disaster recovery management offering decision makers a strong tool for disaster preparation and response. ADP learns from past events, flexibly adapts to changing environments and efficiently balances diverse recovery goals thus creating more resilient and sustainable communities.",1,AI,0.0275794863700866,0.044325828552246,Human,LABEL_1
"This paper looks at status reporting in wireless sensor networks which is not continuous but intermittent. It focuses specifically on joint scheduling of sensing and retransmission operations for high efficiency. Authors suggest a new scheduling algorithm that strikes a balance between sensing quality and energy usage by dynamically changing sensing and retransmission intervals according to network conditions. Results from simulations show effectiveness of this new algorithm compared to conventional scheduling methods. Findings are significant for design and deployment of such networks for diverse applications such as environmental monitoring, industrial automation and healthcare.",1,AI,0.0003610849380493,0.2645736932754516,Human,LABEL_1
"From the simple measurement of tissue attributes in pathology workflow to designing an explainable diagnostic/prognostic AI tool, access to accurate semantic segmentation of tissue regions in histology images is a prerequisite. However, delineating different tissue regions manually is a laborious, time-consuming and costly task that requires expert knowledge. On the other hand, the state-of-the-art automatic deep learning models for semantic segmentation require lots of annotated training data and there are only a limited number of tissue region annotated images publicly available. To obviate this issue in computational pathology projects and collect large-scale region annotations efficiently, we propose an efficient interactive segmentation network that requires minimum input from the user to accurately annotate different tissue types in the histology image. The user is only required to draw a simple squiggle inside each region of interest so it will be used as the guiding signal for the model. To deal with the complex appearance and amorph geometry of different tissue regions we introduce several automatic and minimalistic guiding signal generation techniques that help the model to become robust against the variation in the user input. By experimenting on a dataset of breast cancer images, we show that not only does our proposed method speed up the interactive annotation process, it can also outperform the existing automatic and interactive region segmentation models.",0,Human,0.0009741783142089,0.0260336995124816,Human,LABEL_1
"We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.",0,Human,0.0032489299774169,0.010481059551239,Human,LABEL_1
"With the rapid development of artificial intelligence, conversational bots have became prevalent in mainstream E-commerce platforms, which can provide convenient customer service timely. To satisfy the user, the conversational bots need to understand the user's intention, detect the user's emotion, and extract the key entities from the conversational utterances. However, understanding dialogues is regarded as a very challenging task. Different from common language understanding, utterances in dialogues appear alternately from different roles and are usually organized as hierarchical structures. To facilitate the understanding of dialogues, in this paper, we propose a novel contextual dialogue encoder (i.e. DialogueBERT) based on the popular pre-trained language model BERT. Five self-supervised learning pre-training tasks are devised for learning the particularity of dialouge utterances. Four different input embeddings are integrated to catch the relationship between utterances, including turn embedding, role embedding, token embedding and position embedding. DialogueBERT was pre-trained with 70 million dialogues in real scenario, and then fine-tuned in three different downstream dialogue understanding tasks. Experimental results show that DialogueBERT achieves exciting results with 88.63% accuracy for intent recognition, 94.25% accuracy for emotion recognition and 97.04% F1 score for named entity recognition, which outperforms several strong baselines by a large margin.",0,Human,0.0008324384689331,0.0113722681999206,Human,LABEL_1
"Fishing is a very popular pastime but anglers frequently encounter difficulties related to finding and catching fish. This research set out to investigate these difficulties and propose ways to solve them. We interviewed some experienced anglers to find out about their experiences, challenges, and preferred means of locating and catching fish. Results indicate that angling challenges are mainly about restricted access to spots, poor water quality and dwindling fish populations. To deal with those issues, the study suggests making access to fishing sites better, taking conservation steps to safeguard both fish and their habitat and educating anglers on best practices for sustainable fishing. Findings from this research could guide management and policy to support sustainable use of fishing resources and enhance the overall enjoyment of fishing among anglers.",1,AI,0.0283156633377075,0.7492654919624329,ChatGPT,LABEL_0
"We study the communication complexity of combinatorial auctions via interpolation mechanisms that interpolate between non-truthful and truthful protocols. Specifically, an interpolation mechanism has two phases. In the first phase, the bidders participate in some non-truthful protocol whose output is itself a truthful protocol. In the second phase, the bidders participate in the truthful protocol selected during phase one. Note that virtually all existing auctions have either a non-existent first phase (and are therefore truthful mechanisms), or a non-existent second phase (and are therefore just traditional protocols, analyzed via the Price of Anarchy/Stability).  The goal of this paper is to understand the benefits of interpolation mechanisms versus truthful mechanisms or traditional protocols, and develop the necessary tools to formally study them. Interestingly, we exhibit settings where interpolation mechanisms greatly outperform the optimal traditional and truthful protocols. Yet, we also exhibit settings where interpolation mechanisms are provably no better than truthful ones. Finally, we apply our new machinery to prove that the recent single-bid mechanism of Devanur et. al.~\cite{DevanurMSW15} (the only pre-existing interpolation mechanism in the literature) achieves the optimal price of anarchy among a wide class of protocols, a claim that simply can't be addressed by appealing just to machinery from communication complexity or the study of truthful mechanisms.",0,Human,0.0003069639205932,0.0118976235389709,Human,LABEL_1
"This paper introduces a new method for spreading dynamics optimization via message passing. Based on a blend of graph theory and probability theory, this method accurately models how information or influence spreads through networks. Authors show effectiveness of this approach using simulations and experiments on diverse network structures; they show substantial performance gains over other approaches. Results from this study have wide relevance including for such areas as social network analysis, marketing and network design.",1,AI,0.000283658504486,0.8778740763664246,Human,LABEL_0
"Multi-step manipulation tasks in unstructured environments are extremely challenging for a robot to learn. Such tasks interlace high-level reasoning that consists of the expected states that can be attained to achieve an overall task and low-level reasoning that decides what actions will yield these states. We propose a model-free deep reinforcement learning method to learn multi-step manipulation tasks. We introduce a Robotic Manipulation Network (RoManNet), which is a vision-based model architecture, to learn the action-value functions and predict manipulation action candidates. We define a Task Progress based Gaussian (TPG) reward function that computes the reward based on actions that lead to successful motion primitives and progress towards the overall task goal. To balance the ratio of exploration/exploitation, we introduce a Loss Adjusted Exploration (LAE) policy that determines actions from the action candidates according to the Boltzmann distribution of loss estimates. We demonstrate the effectiveness of our approach by training RoManNet to learn several challenging multi-step robotic manipulation tasks in both simulation and real-world. Experimental results show that our method outperforms the existing methods and achieves state-of-the-art performance in terms of success rate and action efficiency. The ablation studies show that TPG and LAE are especially beneficial for tasks like multiple block stacking. Code is available at: https://github.com/skumra/romannet",0,Human,0.0126186609268188,0.0113353729248046,Human,LABEL_1
"A sound Decision-Making (DM) process is key to the successful governance of software projects. In many Open Source Software Development (OSSD) communities, DM processes lie buried amongst vast amounts of publicly available data. Hidden within this data lie the rationale for decisions that led to the evolution and maintenance of software products. While there have been some efforts to extract DM processes from publicly available data, the rationale behind how the decisions are made have seldom been explored. Extracting the rationale for these decisions can facilitate transparency (by making them known), and also promote accountability on the part of decision-makers. This work bridges this gap by means of a large-scale study that unearths the rationale behind decisions from Python development email archives comprising about 1.5 million emails. This paper makes two main contributions. First, it makes a knowledge contribution by unearthing and presenting the rationale behind decisions made. Second, it makes a methodological contribution by presenting a heuristics-based rationale extraction system called Rationale Miner that employs multiple heuristics, and follows a data-driven, bottom-up approach to infer the rationale behind specific decisions (e.g., whether a new module is implemented based on core developer consensus or benevolent dictator's pronouncement). Our approach can be applied to extract rationale in other OSSD communities that have similar governance structures.",0,Human,0.0347530245780944,0.0339339971542358,Human,LABEL_1
"Attempt to fully discover the temporal diversity and chronological characteristics for self-supervised video representation learning, this work takes advantage of the temporal dependencies within videos and further proposes a novel self-supervised method named Temporal Contrastive Graph Learning (TCGL). In contrast to the existing methods that ignore modeling elaborate temporal dependencies, our TCGL roots in a hybrid graph contrastive learning strategy to jointly regard the inter-snippet and intra-snippet temporal dependencies as self-supervision signals for temporal representation learning. To model multi-scale temporal dependencies, our TCGL integrates the prior knowledge about the frame and snippet orders into graph structures, i.e., the intra-/inter- snippet temporal contrastive graphs. By randomly removing edges and masking nodes of the intra-snippet graphs or inter-snippet graphs, our TCGL can generate different correlated graph views. Then, specific contrastive learning modules are designed to maximize the agreement between nodes in different views. To adaptively learn the global context representation and recalibrate the channel-wise features, we introduce an adaptive video snippet order prediction module, which leverages the relational knowledge among video snippets to predict the actual snippet orders. Experimental results demonstrate the superiority of our TCGL over the state-of-the-art methods on large-scale action recognition and video retrieval benchmarks.",0,Human,0.0010850429534912,0.0206387639045715,Human,LABEL_1
"This paper introduces a new heterogeneous graph embedding framework named MTHetGNN for forecasting multivariate time series. By integrating strengths of Graph Neural Networks (GNN) and Attention Mechanisms (AM), this framework models intricate interrelationships among multiple time series along with their dependencies. This framework uses a heterogeneous graph to represent relationships between different types of time series and applies graph convolutional neural networks (GCN) to learn node representations within the graph. Attention mechanism is used to capture significance of each node during forecasting. Performance of this framework is evaluated on different tasks and compared favorably against leading methods; results indicate superior performance in both accuracy and computational efficiency. Results show that this research offers new approaches to tackle challenges in forecasting multivariate time series and suggests promising application domains including finance, energy and transportation.",1,AI,0.0004224181175231,0.0925959944725036,Human,LABEL_1
"This paper introduces SCSGuard as a novel deep learning approach for identifying frauds in Ethereum smart contracts. Fraud is an important issue because it can result in huge financial loss for users. Previous methods for fraud detection rely either on rule sets or static analysis. In contrast, SCSGuard employs deep learning to identify unusual features in contract bytecode. For evaluation, we collect a large dataset of known frauds and normal contracts and train the model via supervised learning. Performance comparison against leading methods shows that SCSGuard outperforms them both in accuracy and recall. Factors contributing to its success include selection of neural network architecture, quality and size of the training data, and adaptation through transfer learning to different types of frauds. We also point out limitations and suggest future work such as enhancing model interpretability and studying adversarial examples to test robustness. Overall results indicate great promise for using deep learning for contract fraud detection and offer insights into system design and evaluation. Results also have implications for strengthening security and trustworthiness in the Ethereum ecosystem and we hope that further research will follow along these lines.",1,AI,0.0348092317581176,0.0126087069511413,Human,LABEL_1
"This paper aims to present a comprehensive study on modeling belief in dynamic systems. It consists of two parts; the first part lays out foundational elements of the subject matter. The authors start by defining the concept of belief and emphasize its significance within dynamic systems. They review existing literature comprehensively and examine different approaches within the field. Next they introduce a framework for modeling beliefs in dynamic systems which includes key elements such as representation of beliefs, updating beliefs, and evaluation of beliefs. This framework also addresses roles of uncertainty along with methods for measuring it. Authors then address challenges in modeling beliefs in dynamic systems and suggest future research directions. Concluding the first part, this paper provides a solid base for understanding belief modeling in systems that change over time. Authors provide a broad literature review and a useful framework for researchers and practitioners. This paper is a key resource for those who are interested in studying beliefs in dynamic systems.",1,AI,0.0013217329978942,0.7205440998077393,Human,LABEL_0
"Public speaking is an important aspect of human communication and interaction. The majority of computational work on public speaking concentrates on analyzing the spoken content, and the verbal behavior of the speakers. While the success of public speaking largely depends on the content of the talk, and the verbal behavior, non-verbal (visual) cues, such as gestures and physical appearance also play a significant role. This paper investigates the importance of visual cues by estimating their contribution towards predicting the popularity of a public lecture. For this purpose, we constructed a large database of more than $1800$ TED talk videos. As a measure of popularity of the TED talks, we leverage the corresponding (online) viewers' ratings from YouTube. Visual cues related to facial and physical appearance, facial expressions, and pose variations are extracted from the video frames using convolutional neural network (CNN) models. Thereafter, an attention-based long short-term memory (LSTM) network is proposed to predict the video popularity from the sequence of visual features. The proposed network achieves state-of-the-art prediction accuracy indicating that visual cues alone contain highly predictive information about the popularity of a talk. Furthermore, our network learns a human-like attention mechanism, which is particularly useful for interpretability, i.e. how attention varies with time, and across different visual cues by indicating their relative importance.",0,Human,0.0037084817886352,0.0591240525245666,Human,LABEL_1
"This paper does a review of bibliometrics to study research trends and patterns of the digital revolution in agriculture. It looks at studies published between 2011 and 2021 using databases such as Scopus, Web of Science and Google Scholar. Focus of the analysis is growth of research in this area, top cited researchers and papers, and major themes and trends. Results show steady growth of research over recent decade with special focus on precision agriculture and sensor technologies along with big data analytics. Most cited researchers and papers tend to be from developed countries showing a large gap in research from developing countries. Results point to further work needed especially in developing countries to deal with challenges and opportunities of digital agricultural revolution.",1,AI,0.0173335075378417,0.0927790999412536,Human,LABEL_1
This paper looks into the use of new words and phrases on Facebook. It collected lots of data from posts and comments for analysis and identified the most frequent ones along with their usage patterns. Results show that on Facebook such terms are mainly used to convey emotion and to be funny and to express feeling of belonging. Findings also indicate that young people use these new terms more frequently and that they spread quickly because of high connectivity. This study brings light to the role of social media in shaping language and underscores importance of tracking development of new terms in this context.,1,AI,0.0798956155776977,0.9328190088272096,Human,LABEL_0
"Reinforcement learning has lead to considerable break-throughs in diverse areas such as robotics, games and many others. But the application to RL in complex real-world decision making problems remains limited. Many problems in operations management (inventory and revenue management, for example) are characterized by large action spaces and stochastic system dynamics. These characteristics make the problem considerably harder to solve for existing RL methods that rely on enumeration techniques to solve per step action problems. To resolve these issues, we develop Programmable Actor Reinforcement Learning (PARL), a policy iteration method that uses techniques from integer programming and sample average approximation. Analytically, we show that the for a given critic, the learned policy in each iteration converges to the optimal policy as the underlying samples of the uncertainty go to infinity. Practically, we show that a properly selected discretization of the underlying uncertain distribution can yield near optimal actor policy even with very few samples from the underlying uncertainty. We then apply our algorithm to real-world inventory management problems with complex supply chain structures and show that PARL outperforms state-of-the-art RL and inventory optimization methods in these settings. We find that PARL outperforms commonly used base stock heuristic by 44.7% and the best performing RL method by up to 12.1% on average across different supply chain environments.",0,Human,0.0147672295570373,0.0107756257057189,Human,LABEL_1
"Methods for neural network hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of model configurations. In this paper, we show that standard frequentist regression models can predict the final performance of partially trained model configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We empirically show that our performance prediction models are much more effective than prominent Bayesian counterparts, are simpler to implement, and are faster to train. Our models can predict final performance in both visual classification and language modeling domains, are effective for predicting performance of drastically varying model architectures, and can even generalize between model classes. Using these prediction models, we also propose an early stopping method for hyperparameter optimization and meta-modeling, which obtains a speedup of a factor up to 6x in both hyperparameter optimization and meta-modeling. Finally, we empirically show that our early stopping method can be seamlessly incorporated into both reinforcement learning-based architecture selection algorithms and bandit based search methods. Through extensive experimentation, we empirically show our performance prediction models and early stopping algorithm are state-of-the-art in terms of prediction accuracy and speedup achieved while still identifying the optimal model configurations.",0,Human,0.0050010681152343,0.0163951516151428,Human,LABEL_1
"This paper introduces an innovative approach for both classification and semantic segmentation using structured networks which use only binary weights and activations. Compared to networks with floating point weights and activations, this new method reduces memory and computational load and maintains high accuracy. Results show that this specific structure improves classification performance. This work also extends this approach to segmentation tasks and demonstrates great improvements over current methods. Experiments conducted on benchmark datasets validate effectiveness of this new method. Overall, this structured binary neural network offers a promising way to achieve accurate classification and segmentation at low computational cost.",1,AI,0.0003787279129028,0.3968622088432312,Human,LABEL_1
"Photo-realistic modeling and rendering of fuzzy objects with complex opacity are critical for numerous immersive VR/AR applications, but it suffers from strong view-dependent brightness, color. In this paper, we propose a novel scheme to generate opacity radiance fields with a convolutional neural renderer for fuzzy objects, which is the first to combine both explicit opacity supervision and convolutional mechanism into the neural radiance field framework so as to enable high-quality appearance and global consistent alpha mattes generation in arbitrary novel views. More specifically, we propose an efficient sampling strategy along with both the camera rays and image plane, which enables efficient radiance field sampling and learning in a patch-wise manner, as well as a novel volumetric feature integration scheme that generates per-patch hybrid feature embeddings to reconstruct the view-consistent fine-detailed appearance and opacity output. We further adopt a patch-wise adversarial training scheme to preserve both high-frequency appearance and opacity details in a self-supervised framework. We also introduce an effective multi-view image capture system to capture high-quality color and alpha maps for challenging fuzzy objects. Extensive experiments on existing and our new challenging fuzzy object dataset demonstrate that our method achieves photo-realistic, globally consistent, and fined detailed appearance and opacity free-viewpoint rendering for various fuzzy objects.",0,Human,0.000832200050354,0.0160477757453918,Human,LABEL_1
"Data structures for efficient sampling from a set of weighted items are an important building block of many applications. However, few parallel solutions are known. We close many of these gaps both for shared-memory and distributed-memory machines. We give efficient, fast, and practicable parallel algorithms for building data structures that support sampling single items (alias tables, compressed data structures). This also yields a simplified and more space-efficient sequential algorithm for alias table construction. Our approaches to sampling $k$ out of $n$ items with/without replacement and to subset (Poisson) sampling are output-sensitive, i.e., the sampling algorithms use work linear in the number of different samples. This is also interesting in the sequential case. Weighted random permutation can be done by sorting appropriate random deviates. We show that this is possible with linear work using a nonlinear transformation of these deviates. Finally, we give a communication-efficient, highly scalable approach to (weighted and unweighted) reservoir sampling. This algorithm is based on a fully distributed model of streaming algorithms that might be of independent interest. Experiments for alias tables and sampling with replacement show near linear speedups both for construction and queries using up to 158 threads of shared-memory machines. An experimental evaluation of distributed weighted reservoir sampling on up to 256 nodes (5120 cores) also shows good speedups.",0,Human,0.0022984147071838,0.0060163140296936,Human,LABEL_1
"Recently, barrier function-based safe reinforcement learning (RL) with the actor-critic structure for continuous control tasks has received increasing attention. It is still challenging to learn a near-optimal control policy with safety and convergence guarantees. Also, few works have addressed the safe RL algorithm design under time-varying safety constraints. This paper proposes a model-based safe RL algorithm for optimal control of nonlinear systems with time-varying state and control constraints. In the proposed approach, we construct a novel barrier-based control policy structure that can guarantee control safety. A multi-step policy evaluation mechanism is proposed to predict the policy's safety risk under time-varying safety constraints and guide the policy to update safely. Theoretical results on stability and robustness are proven. Also, the convergence of the actor-critic learning algorithm is analyzed. The performance of the proposed algorithm outperforms several state-of-the-art RL algorithms in the simulated Safety Gym environment. Furthermore, the approach is applied to the integrated path following and collision avoidance problem for two real-world intelligent vehicles. A differential-drive vehicle and an Ackermann-drive one are used to verify the offline deployment performance and the online learning performance, respectively. Our approach shows an impressive sim-to-real transfer capability and a satisfactory online control performance in the experiment.",0,Human,0.003082275390625,0.0146851539611816,Human,LABEL_1
"In quasi-proportional auctions, each bidder receives a fraction of the allocation equal to the weight of their bid divided by the sum of weights of all bids, where each bid's weight is determined by a weight function. We study the relationship between the weight function, bidders' private values, number of bidders, and the seller's revenue in equilibrium. It has been shown that if one bidder has a much higher private value than the others, then a nearly flat weight function maximizes revenue. Essentially, threatening the bidder who has the highest valuation with having to share the allocation maximizes the revenue. We show that as bidder private values approach parity, steeper weight functions maximize revenue by making the quasi-proportional auction more like a winner-take-all auction. We also show that steeper weight functions maximize revenue as the number of bidders increases. For flatter weight functions, there is known to be a unique pure-strategy Nash equilibrium. We show that a pure-strategy Nash equilibrium also exists for steeper weight functions, and we give lower bounds for bids at an equilibrium. For a special case that includes the two-bidder auction, we show that the pure-strategy Nash equilibrium is unique, and we show how to compute the revenue at equilibrium. We also show that selecting a weight function based on private value ratios and number of bidders is necessary for a quasi-proportional auction to produce more revenue than a second-price auction.",0,Human,0.3218096494674682,0.0114166736602783,Human,LABEL_1
"Frequency estimation is crucial for many applications in signal processing. This paper introduces a new approach to estimating frequencies with one sided errors, a type of estimation where the estimated value must fall within an acceptable upper or lower limit of the actual value. Our proposed method uses a modified version of periodogram together with a statistical hypothesis test. Results compare our proposed approach favorably against previous methods regarding accuracy, robustness, and computational costs. We show that our method performs better under different scenarios. Proposed method is useful for researchers and practitioners in signal processing where high quality frequency estimation with one-sided limits is essential.",1,AI,0.001240849494934,0.8548856377601624,Human,LABEL_0
"The existence of tactile afferents sensitive to slip-related mechanical transients in the human hand augments the robustness of grasping through secondary force modulation protocols. Despite this knowledge and the fact that tactile-based slip detection has been researched for decades, robust slip detection is still not an out-of-the-box capability for any commercially available tactile sensor. This research seeks to bridge this gap with a comprehensive study addressing several aspects of slip detection. Key developments include a systematic data collection process yielding millions of sensory data points, the generalized conversion of multivariate-to-univariate sensor output, an insightful spectral analysis of the univariate sensor outputs, and the application of Long Short-Term Memory (LSTM) neural networks on the univariate signals to produce robust slip detectors from three commercially available sensors capable of tactile sensing. The sensing elements underlying these sensors vary in quantity, spatial arrangement, and mechanics, leveraging principles in electro-mechanical resistance, optics, and hydro-acoustics. Critically, slip detection performance of the tactile technologies is quantified through a measurement methodology that unveils the effects of data window size, sampling rate, material type, slip speed, and sensor manufacturing variability. Results indicate that the investigated commercial tactile sensors are inherently capable of high-quality slip detection.",0,Human,0.0012072324752807,0.0179987549781799,Human,LABEL_1
"Egocentric action anticipation is the task of predicting the future actions a camera wearer will likely perform based on past video observations. While in a real-world system it is fundamental to output such predictions before the action begins, past works have not generally paid attention to model runtime during evaluation. Indeed, current evaluation schemes assume that predictions can be made offline, and hence that computational resources are not limited. In contrast, in this paper, we propose a ``streaming'' egocentric action anticipation evaluation protocol which explicitly considers model runtime for performance assessment, assuming that predictions will be available only after the current video segment is processed, which depends on the processing time of a method. Following the proposed evaluation scheme, we benchmark different state-of-the-art approaches for egocentric action anticipation on two popular datasets. Our analysis shows that models with a smaller runtime tend to outperform heavier models in the considered streaming scenario, thus changing the rankings generally observed in standard offline evaluations. Based on this observation, we propose a lightweight action anticipation model consisting in a simple feed-forward 3D CNN, which we propose to optimize using knowledge distillation techniques and a custom loss. The results show that the proposed approach outperforms prior art in the streaming scenario, also in combination with other lightweight models.",0,Human,0.0004416108131408,0.0075652599334716,Human,LABEL_1
"In recent years, 5G technology has become important for wireless communications systems and delivers high speed data transfers along with low latency. With growing demand for 5G services in high population areas, there is a need for effective ways to select radio access technologies (RATs) for these networks. This paper examines key factors that affect RAT selection for dense 5G wireless networks including network architecture, density, available spectrum and user needs. A new approach using machine learning algorithms is proposed which selects the best RAT for a given situation based on current network conditions. Metrics such as network traffic, interference and coverage are used to make rational decisions about RAT selection. Results of extensive simulations and experiments show this approach performs better than traditional methods in terms of network performance and user satisfaction. Results of this research shed light on challenges of selection of RAT for dense 5G networks and suggest useful solutions for operators and network designers to optimize networks and enhance user experience. Expected impact of this approach on network development and advancement of wireless communication systems is large.",1,AI,0.009615421295166,0.1478659510612487,Human,LABEL_1
"Quora is a popular Q&A site which provides users with the ability to tag questions with multiple relevant topics which helps to attract quality answers. These topics are not predefined but user-defined conventions and it is not so rare to have multiple such conventions present in the Quora ecosystem describing exactly the same concept. In almost all such cases, users (or Quora moderators) manually merge the topic pair into one of the either topics, thus selecting one of the competing conventions. An important application for the site therefore is to identify such competing conventions early enough that should merge in future. In this paper, we propose a two-step approach that uniquely combines the anomaly detection and the supervised classification frameworks to predict whether two topics from among millions of topic pairs are indeed competing conventions, and should merge, achieving an F-score of 0.711. We also develop a model to predict the direction of the topic merge, i.e., the winning convention, achieving an F-score of 0.898. Our system is also able to predict ~ 25% of the correct case of merges within the first month of the merge and ~ 40% of the cases within a year. This is an encouraging result since Quora users on average take 936 days to identify such a correct merge. Human judgment experiments show that our system is able to predict almost all the correct cases that humans can predict plus 37.24% correct cases which the humans are not able to identify at all.",0,Human,0.0003896355628967,0.0056017041206359,Human,LABEL_1
"This paper investigates how two kinds of tiles arranged randomly on a lattice create complex structures with emergent features. Tiles are squares with four different edge types which can be either adhesive or not. When tiles are placed randomly, they form different structures including aperiodic tilings and quasi crystals. Using both computer simulation and mathematical modeling, researchers study how parameters like concentration of tiles and temperature affect resultant structures. Results show promise for creating novel materials with special features through this kind of random assembly; contributions are made to the developing field of self assembling systems.",1,AI,0.000583529472351,0.3608161211013794,Human,LABEL_1
"This paper looks at scaling up acid transactions by partitioning operations. Acid refers to atomicity, consistency, isolation and durability which are essential for transaction security and reliability. As databases grow larger and more complex, it becomes hard to maintain these qualities. This paper suggests using operation partitioning as a method to scale up acid transactions and thus improving performance and reliability. Methodology involves implementation of partitioning in a real application and evaluation of results for different metrics. Results show that partitioning operations is promising; it increases performance and reliability while maintaining fundamental Acid features.",1,AI,0.0003350973129272,0.8639453053474426,Human,LABEL_0
"Graph neural networks (GNN) have been proven to be mature enough for handling graph-structured data on node-level graph representation learning tasks. However, the graph pooling technique for learning expressive graph-level representation is critical yet still challenging. Existing pooling methods either struggle to capture the local substructure or fail to effectively utilize high-order dependency, thus diminishing the expression capability. In this paper we propose HAP, a hierarchical graph-level representation learning framework, which is adaptively sensitive to graph structures, i.e., HAP clusters local substructures incorporating with high-order dependencies. HAP utilizes a novel cross-level attention mechanism MOA to naturally focus more on close neighborhood while effectively capture higher-order dependency that may contain crucial information. It also learns a global graph content GCont that extracts the graph pattern properties to make the pre- and post-coarsening graph content maintain stable, thus providing global guidance in graph coarsening. This novel innovation also facilitates generalization across graphs with the same form of features. Extensive experiments on fourteen datasets show that HAP significantly outperforms twelve popular graph pooling methods on graph classification task with an maximum accuracy improvement of 22.79%, and exceeds the performance of state-of-the-art graph matching and graph similarity learning algorithms by over 3.5% and 16.7%.",0,Human,0.0003226995468139,0.0148646235466003,Human,LABEL_1
"This paper introduces a new method for optimizing trajectories of cooperative swarms of UAVs operating at dual frequencies so that performance improves. This new method makes use of the specific advantages of dual frequency UAVs which can work concurrently in different frequency bands to boost both communication and sensing abilities. This paper formulates the trajectory optimization as a multi objective optimization problem considering various goals like reducing total time spent on mission, reducing energy consumption by UAVs and increasing quality of communication and sensing links among UAVs. Genetic algorithms are used to search for trajectories that optimize these goals. Simulation results presented show effectiveness of this method to significantly reduce mission times and energy consumption while also improving quality of communication and sensing links among UAVs. Practical significance and future research directions are also discussed in this paper. Overall this research advances the body of knowledge about coordinated UAV swarms and offers a promising approach for trajectory optimization for dual frequency UAVs in practice.",1,AI,0.0008338689804077,0.0358015894889831,Human,LABEL_1
"A fundamental problem in neuroscience is to characterize the dynamics of spiking from the neurons in a circuit that is involved in learning about a stimulus or a contingency. A key limitation of current methods to analyze neural spiking data is the need to collapse neural activity over time or trials, which may cause the loss of information pertinent to understanding the function of a neuron or circuit. We introduce a new method that can determine not only the trial-to-trial dynamics that accompany the learning of a contingency by a neuron, but also the latency of this learning with respect to the onset of a conditioned stimulus. The backbone of the method is a separable two-dimensional (2D) random field (RF) model of neural spike rasters, in which the joint conditional intensity function of a neuron over time and trials depends on two latent Markovian state sequences that evolve separately but in parallel. Classical tools to estimate state-space models cannot be applied readily to our 2D separable RF model. We develop efficient statistical and computational tools to estimate the parameters of the separable 2D RF model. We apply these to data collected from neurons in the pre-frontal cortex (PFC) in an experiment designed to characterize the neural underpinnings of the associative learning of fear in mice. Overall, the separable 2D RF model provides a detailed, interpretable, characterization of the dynamics of neural spiking that accompany the learning of a contingency.",0,Human,0.064685046672821,0.0108647346496582,Human,LABEL_1
"Heuristics for iterative optimization (IOHs) are a class of algorithms widely employed to deal with complex issues across different domains. Performance and scalability are key to IOH success; therefore benchmarking and profiling are important means for assessing and enhancing performance. We introduce here iohprofiler as a new benchmark and profiling tool specifically designed to meet demands of this algorithm class. iohprofiler supplies a broad range of benchmark problems, performance measures and profiling tools to facilitate researchers and practitioners to assess and enhance performance and scalability of IOHs. The paper introduces the design and implementation of iohprofiler and reports experimental results showing effectiveness at benchmarking and profiling different IOHs. Results show advantages of using iohprofiler for performance evaluation and improvement and thus it becomes very useful for researchers and practitioners working on this field.",1,AI,0.0003914237022399,0.0921660661697387,Human,LABEL_1
"Most distributed-memory bulk-synchronous parallel programs in HPC assume that compute resources are available continuously and homogeneously across the allocated set of compute nodes. However, long one-off delays on individual processes can cause global disturbances, so-called idle waves, by rippling through the system. This process is mainly governed by the communication topology of the underlying parallel code. This paper makes significant contributions to the understanding of idle wave dynamics. We study the propagation mechanisms of idle waves across the ranks of MPI-parallel programs. We present a validated analytic model for their propagation velocity with respect to communication parameters and topology, with a special emphasis on sparse communication patterns. We study the interaction of idle waves with MPI collectives and show that, depending on the implementation, a collective may be transparent to the wave. Finally we analyze two mechanisms of idle wave decay: topological decay, which is rooted in differences in communication characteristics among parts of the system, and noise-induced decay, which is caused by system or application noise. We show that noise-induced decay is largely independent of noise characteristics but depends only on the overall noise power. An analytic expression for idle wave decay rate with respect to noise power is derived. For model validation we use microbenchmarks and stencil algorithms on three different supercomputing platforms.",0,Human,0.0094901919364929,0.0091083645820617,Human,LABEL_1
"As YouTube becomes more popular as a place to share content, there are growing concerns regarding risks to children due to exposure to harmful or inappropriate material. This paper focuses on detection, characterization and analysis of unsafe content and people who promote such content on YouTube. We performed a large study of the Kidstube dataset containing more than 100,000 videos to find dangerous content that could harm children including violence, sex, and misleading information. We also looked at behavior of those who take advantage of children's vulnerabilities including deceptive ads and manipulation of search algorithms. Results reveal trends about prevalence of unsafe content on YouTube and tactics used by promoters to reach younger audiences. They also show possible impact on children's welfare. Results emphasize importance of better detection and removal of harmful content along with better regulation of ads to safeguard children against harm.",1,AI,0.0289597511291503,0.1321865320205688,ChatGPT,LABEL_1
"Subgraph queries also known as subgraph isomorphism search is a fundamental problem in querying graph-like structured data. It consists to enumerate the subgraphs of a data graph that match a query graph. This problem arises in many real-world applications related to query processing or pattern recognition such as computer vision, social network analysis, bioinformatic and big data analytic. Subgraph isomorphism search knows a lot of investigations and solutions mainly because of its importance and use but also because of its NP-completeness. Existing solutions use filtering mechanisms and optimise the order within witch the query vertices are matched on the data vertices to obtain acceptable processing times. However, existing approaches are iterative and generate several intermediate results. They also require that the data graph is loaded in main memory and consequently are not adapted to large graphs that do not fit into memory or are accessed by streams. To tackle this problem, we propose a new approach based on concepts widely different from existing works. Our approach distills the semantic and topological information that surround a vertex into a simple integer. This simple vertex encoding that can be computed and updated incrementally reduces considerably intermediate results and avoid to load the entire data graph into main memory. We evaluate our approach on several real-word datasets. The experimental results show that our approach is efficient and scalable.",0,Human,0.0007336735725402,0.013934850692749,Human,LABEL_1
This paper looks at how political discussions on Twitter have changed from 2014 to 2019. Using data from Twitter we look at trends in political discussions and identify key shifts. Results show that Twitter is becoming more important for political talk as more people are joining these conversations. We also find that political discussions on Twitter have become more divided among participants and have grown more confrontational. Results suggest this reflects larger changes in politics where more people use social media to express views and get involved politically. Research shows we need to keep an eye on changes in political discussions online and continue studying what these changes mean for discourse and democracy.,1,AI,0.0009484887123107,0.9732198119163512,ChatGPT,LABEL_0
"This paper introduces an innovative approach to decoding in communications systems called SRGRAND - Symbol Reliability Guided Random Additive Noise Decoding. Conventional methods for decoding rely entirely on channel state information (CSI) to operate. However, in real communication systems, CSI is usually unreliable or stale leading to degraded performance. Proposed SRGRAND uses not just CSI but also information about the reliability of symbols. Calculated using statistical properties of transmitted symbols and channel noise, this information gives insight into the quality of each individual symbol. This extra information is used to direct decoding and reduces errors. Simulation results show that SRGRAND performs better than traditional decoding methods and provides stronger performance in practical systems.",1,AI,0.0041703581809997,0.1259177923202514,Human,LABEL_1
"We present a path planning framework that takes into account the human's safety perception in the presence of a flying robot. The framework addresses two objectives: (i) estimation of the uncertain parameters of the proposed safety perception model based on test data collected using Virtual Reality (VR) testbed, and (ii) offline optimal control computation using the estimated safety perception model. Due to the unknown factors in the human tests data, it is not suitable to use standard regression techniques that minimize the mean squared error (MSE). We propose to use a Hidden Markov model (HMM) approach where human's attention is considered as a hidden state to infer whether the data samples are relevant to learn the safety perception model. The HMM approach improved log-likelihood over the standard least squares solution. For path planning, we use Bernstein polynomials for discretization, as the resulting path remains within the convex hull of the control points, providing guarantees for deconfliction with obstacles at low computational cost. An example of optimal trajectory generation using the learned human model is presented. The optimal trajectory generated using the proposed model results in reasonable safety distance from the human. In contrast, the paths generated using the standard regression model have undesirable shapes due to overfitting. The example demonstrates that the HMM approach has robustness to the unknown factors compared to the standard MSE model.",0,Human,0.0168509483337402,0.021777868270874,Human,LABEL_1
"This paper investigates hardness and approximation of probabilistic p center problem under stress. Probabilistic p center is a familiar problem in OR and CS which seeks to find the best locations for p facilities in a network so that maximum distance to nearest facility is reduced. Stress here refers to conditions or external forces affecting decisions. This study first introduces probabilistic p center problem and some related variants like classic and stress sensitive ones. Next, it analyzes computational complexity and proves NP hardness via reduction from well known NP hard problems. Algorithms are presented for approximation of this problem under stress along with performance analysis and numerical results showing effectiveness. Results conclude the paper and suggest further research directions. Overall findings shed light on hardness and approximations of this problem under stress and serve as valuable references for researchers and practitioners in OR and CS.",1,AI,0.0003323554992675,0.0181100368499755,Human,LABEL_1
"This paper investigates use of self supervised learning approaches to enhance representation learning from tabular data. Typical methods for learning features from tables require predefined features or fixed embeddings which often do not reflect underlying structure. We introduce an alternative approach that makes use of intrinsic structure and relationships within the table to learn more useful feature representations. Specifically, we define two supervised learning tasks targeting prediction of missing values and reconstruction of corrupted tables. Results show that this method performs better compared to current methods on real world data sets. Results suggest that supervised learning self supervised offers promise for improvement in representation learning of tabular data and also other downstream tasks such as classification and regression.",1,AI,0.0003602504730224,0.0279985070228576,Human,LABEL_1
"Unsupervised domain adaptation (UDA) aims to solve the problem of knowledge transfer from labeled source domain to unlabeled target domain. Recently, many domain adaptation (DA) methods use centroid to align the local distribution of different domains, that is, to align different classes. This improves the effect of domain adaptation, but domain differences exist not only between classes, but also between samples. This work rethinks what is the alignment between different domains, and studies how to achieve the real alignment between different domains. Previous DA methods only considered one distribution feature of aligned samples, such as full distribution or local distribution. In addition to aligning the global distribution, the real domain adaptation should also align the meso distribution and the micro distribution. Therefore, this study propose a double classifier method based on high confidence label (DCP). By aligning the centroid and the distribution between centroid and sample of different classifiers, the meso and micro distribution alignment of different domains is realized. In addition, in order to reduce the chain error caused by error marking, This study propose a high confidence marking method to reduce the marking error. To verify its versatility, this study evaluates DCP on digital recognition and target recognition data sets. The results show that our method achieves state-of-the-art results on most of the current domain adaptation benchmark datasets.",0,Human,0.0107233524322509,0.0861432552337646,Human,LABEL_1
This paper introduces a Newton type method for switching system optimal control that exploits system structure to increase computational efficiency. Results compare favorably against other methods and show greater accuracy and faster convergence. Results indicate effectiveness of exploiting structure using Newton type method and suggest promising directions for future research into this problem.,1,AI,0.0011324882507324,0.9903358221054076,Human,LABEL_0
"In Knowledge Representation, it is crucial that knowledge engineers have a good understanding of the formal expressions that they write. What formal expressions state intuitively about the domain of discourse is studied in the theory of the informal semantics of a logic. In this paper we study the informal semantics of Answer Set Programming. The roots of answer set programming lie in the language of Extended Logic Programming, which was introduced initially as an epistemic logic for default and autoepistemic reasoning. In 1999, the seminal papers on answer set programming proposed to use this logic for a different purpose, namely, to model and solve search problems. Currently, the language is used primarily in this new role. However, the original epistemic intuitions lose their explanatory relevance in this new context. How answer set programs are connected to the specifications of problems they model is more easily explained in a classical Tarskian semantics, in which models correspond to possible worlds, rather than to belief states of an epistemic agent. In this paper, we develop a new theory of the informal semantics of answer set programming, which is formulated in the Tarskian setting and based on Frege's compositionality principle. It differs substantially from the earlier epistemic theory of informal semantics, providing a different view on the meaning of the connectives in answer set programming and on its relation to other logics, in particular classical logic.",0,Human,0.00202077627182,0.0674028396606445,Human,LABEL_1
"The application of reinforcement learning algorithms onto real life problems always bears the challenge of filtering the environmental state out of raw sensor readings. While most approaches use heuristics, biology suggests that there must exist an unsupervised method to construct such filters automatically. Besides the extraction of environmental states, the filters have to represent them in a fashion that support modern reinforcement algorithms. Many popular algorithms use a linear architecture, so one should aim at filters that have good approximation properties in combination with linear functions. This thesis wants to propose the unsupervised method slow feature analysis (SFA) for this task. Presented with a random sequence of sensor readings, SFA learns a set of filters. With growing model complexity and training examples, the filters converge against trigonometric polynomial functions. These are known to possess excellent approximation capabilities and should therfore support the reinforcement algorithms well. We evaluate this claim on a robot. The task is to learn a navigational control in a simple environment using the least square policy iteration (LSPI) algorithm. The only accessible sensor is a head mounted video camera, but without meaningful filtering, video images are not suited as LSPI input. We will show that filters learned by SFA, based on a random walk video of the robot, allow the learned control to navigate successfully in ca. 80% of the test trials.",0,Human,0.0008768439292907,0.006392478942871,Human,LABEL_1
"This paper introduces an approach based on deep learning for detecting attacks like DDoS and DoS in IoT networks. Using residual networks (ResNet) as a core component, the method learns features from traffic data and distinguishes between normal behavior and attacks. A large dataset containing various attack types and normal traffic is used to train the ResNet model. Results show that using this model leads to high accuracy for detection and outperforms baselines. Results also show the approach is effective against zero days; these are new types of attacks that avoid detection via signatures. Additionally, the false positive rate is very low which prevents unnecessary disruptions due to false alarms. Results show overall effectiveness of deep learning approaches for detecting attacks in IoT networks and emphasize usefulness of ResNets.",1,AI,0.0004219412803649,0.1217344999313354,Human,LABEL_1
"The Ethics and Society Review of Artificial Intelligence (ESR) is a detailed review of the moral and ethical implications of AI research. It looks closely at current issues in AI ethics and analyzes different ethical frameworks and principles that have been suggested for AI development. Challenges and limits of those frameworks are also discussed. The ESR also considers how different kinds of social and political institutions shape AI ethics; this includes government agencies, businesses, and nongovernmental groups. Finally the paper suggests some recommendations for future research such as increased cooperation among AI researchers, ethicists and society and proactivity towards ethical issues that take into account societal impacts of AI.",1,AI,0.0017161965370178,0.9430156350135804,Human,LABEL_0
"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",0,Human,0.0010554194450378,0.0136981010437011,Human,LABEL_1
"This paper investigates filtering with consensus for a network of agents; they aim to estimate a common signal from noisy data distributed among them. An algorithm is proposed that each agent uses to update its estimate by averaging local data with estimates from neighbors. There is a fusion step as well where results combine to get an overall estimate of the signal. Performance analysis is done including conditions for convergence to consensus estimates. Bounds are also derived for error in this estimation. Simulations show effectiveness under noise and dynamics and applications include sensor networks, distributed control and multiagent systems where distributed estimation of common signals is needed.",1,AI,0.0003144741058349,0.0087749361991882,Human,LABEL_1
"Randomization-based Machine Learning methods for prediction are currently a hot topic in Artificial Intelligence, due to their excellent performance in many prediction problems, with a bounded computation time. The application of randomization-based approaches to renewable energy prediction problems has been massive in the last few years, including many different types of randomization-based approaches, their hybridization with other techniques and also the description of new versions of classical randomization-based algorithms, including deep and ensemble approaches. In this paper we review the most important characteristics of randomization-based machine learning approaches and their application to renewable energy prediction problems. We describe the most important methods and algorithms of this family of modeling methods, and perform a critical literature review, examining prediction problems related to solar, wind, marine/ocean and hydro-power renewable sources. We support our critical analysis with an extensive experimental study, comprising real-world problems related to solar, wind and hydro-power energy, where randomization-based algorithms are found to achieve superior results at a significantly lower computational cost than other modeling counterparts. We end our survey with a prospect of the most important challenges and research directions that remain open this field, along with an outlook motivating further research efforts in this exciting research field.",0,Human,0.0020609498023986,0.0146410465240478,Human,LABEL_1
"Using the lattice Boltzmann method (LBM), a computational tool for fluid dynamics simulations, this paper presents an innovative way of massively parallelizing this method using uneven grids. While these irregular grids can flexibly handle complex shapes better than regular ones, they have proven difficult to parallelize efficiently. By leveraging the inherent parallelism in LBM we develop an approach that distributes simulations among different processors. Implementation uses hybrid MPI and OpenMP to balance workload and communication overhead. Performance evaluation shows strong scaling across various test cases and compares favorably against other parallel implementations; results indicate that the developed algorithm scales well and achieves high speedup. This work demonstrates feasibility for large scale CFD simulations.",1,AI,0.0004230141639709,0.0391111373901367,Human,LABEL_1
"We present theory and algorithms for the computation of probability-weighted ""keep-out"" sets to assure probabilistically safe navigation in the presence of multiple rigid body obstacles with stochastic dynamics. Our forward stochastic reachability-based approach characterizes the stochasticity of the future obstacle states in a grid-free and recursion-free manner, using Fourier transforms and computational geometry. We consider discrete-time Markovian switched systems with affine parameter-varying stochastic subsystems (DMSP) as the obstacle dynamics, which includes Markov jump affine systems and discrete-time affine parameter-varying stochastic systems (DPV). We define a probabilistic occupancy function, to describe the probability that a given state is occupied by a rigid body obstacle with stochastic dynamics at a given time; keep-out sets are the super-level sets of this occupancy function. We provide sufficient conditions that ensure convexity and compactness of these keep-out sets for DPV obstacle dynamics. We also propose two computationally efficient algorithms to overapproximate the keep-out sets --- a tight polytopic approximation using projections, and an overapproximation using Minkowski sum. For DMSP obstacle dynamics, we compute a union of convex and compact sets that covers the potentially non-convex keep-out set. Numerical simulations show the efficacy of the proposed algorithms for a modified version of the classical unicycle dynamics, modeled as a DMSP.",0,Human,0.0007476210594177,0.0080956816673278,Human,LABEL_1
"Ride sharing services have become popular in most big cities all over the world. In Nigeria, ORide is one of the main ride sharing services that offer convenient transportation to people commuting. Driver anonymity is a longstanding problem which worries people's safety and trust. This paper presents an empirical study into user perceptions regarding anonymity of drivers and related safety implications. We surveyed 500 users in Lagos, Nigeria and asked them what they think about anonymity and what influences their choice to use this service. Results showed that many users are worried about anonymity and want more information like driver names and pictures. Factors influencing this concern include previous experiences with the service, gender and age. Also, our research indicates risks of anonymity including trouble recognizing drivers during accidents or security incidents. From our results we recommend ways for ORide and other companies to reduce anonymity risks while still protecting users' safety and privacy. Our study adds to the growing literature on ride sharing services in developing countries and highlights the importance of finding balance between privacy and safety for such services.",1,AI,0.0218993425369262,0.0576333999633789,Human,LABEL_1
"This paper introduces a new way to model epistemic uncertainty via Bayesian confidence calibration. Uncertainty due to lack of knowledge about a system often leads to large prediction errors. Bayesian methods are frequently used to model such uncertainty, but these methods tend to overestimate uncertainty when the model is misspecified or when data is scarce. To solve this problem, we present a new approach based on calibrated confidence estimation. Specifically, we introduce a calibration function that adjusts uncertainty estimates made by Bayesian models based on reliability of data and level of specification mismatch. We show effectiveness using both synthetic and real data and compare against popular methods for uncertainty modeling. Results indicate that our method produces better and more reliable uncertainty estimates that are useful for decisions under uncertainty. Finally we discuss possible application areas for this method including finance, engineering and healthcare.",1,AI,0.0080338716506958,0.0461267828941345,Human,LABEL_1
"This paper introduces a new approach to clustering streaming data using the idea of evolving density mountains. It uses peak density notion to track changes in clusters over time and identify patterns of those peaks. This method is superior because it clusters in a more responsive way compared to typical methods which usually presuppose static cluster structures. Performance was tested using both artificial and real data sets; results indicate this approach excels at accuracy, efficiency and scalability. Results from this research advance clustering techniques for streams and suggest applicability to diverse fields like online market segmentation and fraud detection.",1,AI,0.0003223419189453,0.7722762227058411,Human,LABEL_0
"This paper studies packing and covering properties of subspace codes for error correction using random linear network coding. We study the relationship between minimum distance and maximal rate of such codes and construct new codes with optimal packing and covering properties. Specifically, we suggest a method to construct codes having large minimum distance which optimize the balance between minimum distance and maximal rate. We also look into the covering radius of these codes and show that this optimization is achieved in certain ranges. Results are important for designing practical codes that work well for error correction through random linear network coding.",1,AI,0.0216756463050842,0.5825530886650085,Human,LABEL_0
This paper studies when Bayesian prediction strategies work well for any loss function and for any given set of symbols. It aims to find criteria for when these strategies perform optimally and to develop a theoretical basis for proving optimality. First the authors introduce Bayesian prediction and describe its basic ideas. Then they specify what kind of loss function and set of symbols to use and explain how to apply them in prediction. Results are then stated including a theorem specifying sufficient conditions for optimality of Bayesian prediction strategy. Proof uses both math analysis and simulation results. At last conclusions are drawn and directions for further research are highlighted.,1,AI,0.0068031549453735,0.0449874401092529,Human,LABEL_1
"This paper introduces a new motion planning algorithm called e$ \mathbf{^3} $mop. This algorithm integrates heuristically guided motion primitives pruning and path optimization and has sparse band structure. This algorithm tackles high dimensional planning problems with complex constraints such as those found in robotics and autonomous systems. e$ \mathbf{^3} $mop generates a set of primitives using heuristic guidance to reduce search space; this set of primitives is then pruned by means of a new cost function that considers both costs associated with the primitive and expected future costs. This pruning step greatly reduces the number of primitives to consider, thus substantially reducing computational time. Remaining primitives are then used to build a sparse band graph capturing connectivity among them. Using a new path optimization technique that exploits the sparse band structure, this graph is optimized. Results from experiments show that e$ \mathbf{^3} $mop excels in efficiency and quality compared to current best algorithms. Scalability is also demonstrated, rendering it suitable for large scale applications. In summary, e$ \mathbf{^3} $mop marks an important advancement in motion planning, with great potential impact across robotics and other domains.",1,AI,0.0003708600997924,0.0162268877029418,Human,LABEL_1
"This paper introduces a new method to improve security and robustness of communication systems for unmanned aerial vehicles (UAVs) by using reconfigurable intelligent surfaces (RISs). The introduction begins by highlighting current issues such as interference, jamming attacks, and poor reception of wireless signals. Then, it describes how RISs are promising solutions to those problems. Authors suggest a secure and robust system for UAV communications that uses RISs to modify wireless channel conditions in real time. Performance evaluation of the proposed system uses both simulation and experiment data; this shows notable improvements such as better signal quality and higher reliability along with enhanced security compared to typical UAV communication systems. Results of this research offer fresh perspectives on the significance of RISs as a core technology for secure and reliable UAV communication.",1,AI,0.0018666386604309,0.2762356996536255,Human,LABEL_1
"This paper introduces a method for estimating subgraph frequency from data sampled from ego-centric views, both with attributes and without. The approach uses the fact that subgraph frequencies can be estimated by multiplying the frequencies of subgraphs found in sample data by a ratio between total subgraphs in the full graph and those in the sample. Results show that this method performs well in terms of accuracy when compared to synthetic and real data. Results also suggest that this approach has utility in diverse applications including mining graph patterns and conducting network analysis.",1,AI,0.0316129922866821,0.943215012550354,Human,LABEL_0
"This paper introduces a new method of Bayesian optimization which takes advantage of specific domain knowledge for optimizing performance of the ATRIAS biped, a bipedal robot. Bayesian optimization is an effective tool for finding solutions for difficult problems, but it can perform even better when we take into account relevant background information. In this study, we use Bayesian optimization using prior beliefs about parameters affecting performance of ATRIAS. These prior beliefs are used as priors in the optimization algorithm. Results of simulation experiments show that this new method requires fewer function evaluations than others to find optimal solutions. This method also reveals important aspects of how the ATRIAS biped behaves by identifying key parameters and their interactions. Results indicate that integrating domain knowledge improves optimization efficiency and deeper understanding of considered systems. This research advances the literature on Bayesian optimization and shows promise for optimizing complex systems such as bipeds. It also emphasizes importance of incorporating domain knowledge to enhance performance of optimization algorithms.",1,AI,0.0021529197692871,0.2389121055603027,Human,LABEL_1
"This paper introduces a new method called Zoom SVD for extracting important features at specified time intervals from very large data sets. This method is intended to be both quick and low memory usage, making it useful for studying high numbers of observations in series of time. Zoom SVD relies on extending the Singular Value Decomposition (SVD) algorithm so as to efficiently compute major modes of variation within the data. It also uses a zooming mechanism which lets users focus on particular intervals of interest and computes SVD just on those parts of data. Performance is tested using diverse real datasets and results show that this method outperforms other SVD approaches by being much quicker and saving more memory while still performing equally or better at finding relevant features. Overall, this study presents promising means for processing time series data broadly across disciplines.",1,AI,0.0014833807945251,0.0162214636802673,Human,LABEL_1
"The datasets of face recognition contain an enormous number of identities and instances. However, conventional methods have difficulty in reflecting the entire distribution of the datasets because a mini-batch of small size contains only a small portion of all identities. To overcome this difficulty, we propose a novel method called BroadFace, which is a learning process to consider a massive set of identities, comprehensively. In BroadFace, a linear classifier learns optimal decision boundaries among identities from a large number of embedding vectors accumulated over past iterations. By referring more instances at once, the optimality of the classifier is naturally increased on the entire datasets. Thus, the encoder is also globally optimized by referring the weight matrix of the classifier. Moreover, we propose a novel compensation method to increase the number of referenced instances in the training stage. BroadFace can be easily applied on many existing methods to accelerate a learning process and obtain a significant improvement in accuracy without extra computational burden at inference stage. We perform extensive ablation studies and experiments on various datasets to show the effectiveness of BroadFace, and also empirically prove the validity of our compensation method. BroadFace achieves the state-of-the-art results with significant improvements on nine datasets in 1:1 face verification and 1:N face identification tasks, and is also effective in image retrieval.",0,Human,0.054614245891571,0.0150445103645324,Human,LABEL_1
"Temporal pattern mining is important across finance, health care, and engineering because they generate lots of time series data regularly. Conventional methods tend to be slow and expensive for dealing with big data. Mutual information has been widely used to measure dependency among variables and has been successful at reducing data dimensionality. This paper introduces an efficient way to mine temporal patterns from large data sets by utilizing mutual information. The method involves three steps: preprocessing data, feature selection based on mutual information and mining temporal patterns. Preprocessing removes irrelevant data and normalizes the data. Feature selection selects important features by using mutual information. Finally, prefix span algorithm is modified to extract frequent temporal patterns from selected features. Results show effectiveness on different real datasets and compare favorably against leading methods. Compared to other methods, our method performs better in terms of both speed and accuracy and deals well with high dimensional data. This new method can be used broadly for diverse applications such as finance, health and engineering to quickly extract valuable patterns from large time series data.",1,AI,0.0394847393035888,0.0161805748939514,Human,LABEL_1
"Fog radio access networks (F-RANs), which consist of a cloud and multiple edge nodes (ENs) connected via fronthaul links, have been regarded as promising network architectures. The F-RAN entails a joint optimization of cloud and edge computing as well as fronthaul interactions, which is challenging for traditional optimization techniques. This paper proposes a Cloud-Enabled Cooperation-Inspired Learning (CECIL) framework, a structural deep learning mechanism for handling a generic F-RAN optimization problem. The proposed solution mimics cloud-aided cooperative optimization policies by including centralized computing at the cloud, distributed decision at the ENs, and their uplink-downlink fronthaul interactions. A group of deep neural networks (DNNs) are employed for characterizing computations of the cloud and ENs. The forwardpass of the DNNs is carefully designed such that the impacts of the practical fronthaul links, such as channel noise and signling overheads, can be included in a training step. As a result, operations of the cloud and ENs can be jointly trained in an end-to-end manner, whereas their real-time inferences are carried out in a decentralized manner by means of the fronthaul coordination. To facilitate fronthaul cooperation among multiple ENs, the optimal fronthaul multiple access schemes are designed. Training algorithms robust to practical fronthaul impairments are also presented. Numerical results validate the effectiveness of the proposed approaches.",0,Human,0.0043035745620727,0.0104832053184509,Human,LABEL_1
"This paper investigates the effectiveness of hexaconv, a new kind of neural network using hexagonal filters rather than square ones. Results show that this architecture performs similarly or better compared to regular convolutional networks when classifying images. Compared to regular networks, hexaconv also uses fewer parameters and computing resources. The conclusion drawn from these results is that hexaconv is promising and potentially useful especially for classification tasks that use hexagonal data such as satellite imagery or cell phone networks.",1,AI,0.0240857005119323,0.8573983311653137,Human,LABEL_0
"This paper introduces a new way to answer questions about images by using dependency trees to combine information from both visuals and language. This new method uses graphs based on dependencies among words in the question and also incorporates features of the image. Through reasoning using these elements, the proposed technique performs better than top methods on benchmark tasks for visual question answering. Results show that this system also explains its results clearly by using visualizations of dependency trees. Overall this research points toward new ways to develop systems that are transparent and can communicate their decisions to humans.",1,AI,0.0970118641853332,0.9401931166648864,Human,LABEL_0
"The goal of this paper is to establish which practical routing schemes for wireless networks are most suitable for wideband systems in the power-limited regime, which is, for example, a practically relevant mode of operation for the analysis of ultrawideband (UWB) mesh networks. For this purpose, we study the tradeoff between energy efficiency and spectral efficiency (known as the power-bandwidth tradeoff) in a wideband linear multihop network in which transmissions employ orthogonal frequency-division multiplexing (OFDM) modulation and are affected by quasi-static, frequency-selective fading. Considering open-loop (fixed-rate) and closed-loop (rate-adaptive) multihop relaying techniques, we characterize the impact of routing with spatial reuse on the statistical properties of the end-to-end conditional mutual information (conditioned on the specific values of the channel fading parameters and therefore treated as a random variable) and on the energy and spectral efficiency measures of the wideband regime. Our analysis particularly deals with the convergence of these end-to-end performance measures in the case of large number of hops, i.e., the phenomenon first observed in \cite{Oyman06b} and named as ``multihop diversity''. Our results demonstrate the realizability of the multihop diversity advantages in the case of routing with spatial reuse for wideband OFDM systems under wireless channel effects such as path-loss and quasi-static frequency-selective multipath fading.",0,Human,0.0003451108932495,0.011586844921112,Human,LABEL_1
"Recent years have seen growing concerns about security threats posed by adversarial attacks targeting deep neural networks. This paper introduces a new strategy for defense that leverages information about the shapes of inputs. Our solution involves preprocessing input data to retain its shape and then feeding it into a trained deep network that can recognize those shapes. Results from our experiments show that this method works well against different types of attacks, both white box and black box, and significantly outperforms traditional defenses in terms of overall accuracy. Results indicate that including information about shape is effective for defense against such attacks; this research suggests that this approach merits further study as a promising path forward.",1,AI,0.0021040439605712,0.9269304871559144,Human,LABEL_0
"Automatic segmentation methods are an important advancement in medical image analysis. Machine learning techniques, and deep neural networks in particular, are the state-of-the-art for most medical image segmentation tasks. Issues with class imbalance pose a significant challenge in medical datasets, with lesions often occupying a considerably smaller volume relative to the background. Loss functions used in the training of deep learning algorithms differ in their robustness to class imbalance, with direct consequences for model convergence. The most commonly used loss functions for segmentation are based on either the cross entropy loss, Dice loss or a combination of the two. We propose the Unified Focal loss, a new hierarchical framework that generalises Dice and cross entropy-based losses for handling class imbalance. We evaluate our proposed loss function on five publicly available, class imbalanced medical imaging datasets: CVC-ClinicDB, Digital Retinal Images for Vessel Extraction (DRIVE), Breast Ultrasound 2017 (BUS2017), Brain Tumour Segmentation 2020 (BraTS20) and Kidney Tumour Segmentation 2019 (KiTS19). We compare our loss function performance against six Dice or cross entropy-based loss functions, across 2D binary, 3D binary and 3D multiclass segmentation tasks, demonstrating that our proposed loss function is robust to class imbalance and consistently outperforms the other loss functions. Source code is available at: https://github.com/mlyg/unified-focal-loss",0,Human,0.0012080073356628,0.0123078227043151,Human,LABEL_1
"This paper investigates how common and what roles heterogeneous motifs play within complex systems. Motifs are specific connections among system components, and they have been widely studied in homogeneous motifs in which all components have the same function. But most real world networks are heterogeneous; nodes have diverse functions and characteristics. Here we introduce a method for finding heterogeneous motifs and study their prevalence in diverse real systems such as biology, society and technology. Analysis shows that heterogeneous motifs appear much more frequently than homogeneous ones in many systems. They are crucial for processing and regulating information. We also find that many motifs are related to modular organization, propagation of signals and feedback loops. We also show that the same motifs appear in many systems suggesting that they are fundamental building blocks of complex networks. Results have major implications for designing and understanding complex systems. Considering the function of heterogeneous motifs leads to insight into mechanisms that underlie system behavior and developing better strategies for manipulation and control of systems. Moreover this method for finding motifs works on a broad range of networks thereby enhancing understanding of the intricate connection between structure and function in complex systems.",1,AI,0.0008530616760253,0.0922174453735351,Human,LABEL_1
"This paper investigates use of Video Multimethod Assessment Fusion (VMAF) and Entropy Differences (ED) metrics for quality assessment of high frame rate (HFR) videos. As HFR videos become increasingly popular, it becomes important to develop reliable metrics to assess visual quality of such videos. VMAF and ED have been widely used in assessing quality at regular frame rates, but their suitability for HFR videos has not been well studied yet. Researchers conduct subjective tests using 30 participants with ACR method to evaluate HFR video sequences and compare subjective scores to objective scores obtained from VMAF and ED metrics. Results of diverse HFR videos with different genres and motions are analyzed to assure reliability of findings. Results show that both VMAF and ED metrics correlate well with subjective scores obtained through ACR. VMAF performs slightly better than ED but significance difference is not notable. Results contribute to development of metrics for objectively assessing visual quality of HFR videos and suggest promising applications like video compression and streaming. Results also provide guidance for development of new quality assessment algorithms specifically designed for HFR videos.",1,AI,0.0099067091941833,0.0110329985618591,Human,LABEL_1
This paper introduces a new method for removing artifacts from attributes of geometry point clouds through compression. The method uses a two step process: downsampling of point cloud data using a quadtree method and then removal of artifacts. The artifact removal uses local regression and takes advantage of spatial and attribute information of nearby points to estimate and remove artifacts. Results are compared to benchmarks and show improved performance based on both quantitative and qualitative metrics; the processing is efficient and easy to integrate into existing frameworks. Results of this research suggest that new high performance compression techniques are developed for point cloud data.,1,AI,0.0006272196769714,0.9029917120933532,Human,LABEL_0
"We introduce a flexible, scalable Bayesian inference framework for nonlinear dynamical systems characterised by distinct and hierarchical variability at the individual, group, and population levels. Our model class is a generalisation of nonlinear mixed-effects (NLME) dynamical systems, the statistical workhorse for many experimental sciences. We cast parameter inference as stochastic optimisation of an end-to-end differentiable, block-conditional variational autoencoder. We specify the dynamics of the data-generating process as an ordinary differential equation (ODE) such that both the ODE and its solver are fully differentiable. This model class is highly flexible: the ODE right-hand sides can be a mixture of user-prescribed or ""white-box"" sub-components and neural network or ""black-box"" sub-components. Using stochastic optimisation, our amortised inference algorithm could seamlessly scale up to massive data collection pipelines (common in labs with robotic automation). Finally, our framework supports interpretability with respect to the underlying dynamics, as well as predictive generalization to unseen combinations of group components (also called ""zero-shot"" learning). We empirically validate our method by predicting the dynamic behaviour of bacteria that were genetically engineered to function as biosensors. Our implementation of the framework, the dataset, and all code to reproduce the experimental results is available at https://www.github.com/Microsoft/vi-hds .",0,Human,0.0002908110618591,0.0066174268722534,Human,LABEL_1
"Human pose estimation is a fundamental yet challenging task in computer vision, which aims at localizing human anatomical keypoints. However, unlike human vision that is robust to various data corruptions such as blur and pixelation, current pose estimators are easily confused by these corruptions. This work comprehensively studies and addresses this problem by building rigorous robust benchmarks, termed COCO-C, MPII-C, and OCHuman-C, to evaluate the weaknesses of current advanced pose estimators, and a new algorithm termed AdvMix is proposed to improve their robustness in different corruptions. Our work has several unique benefits. (1) AdvMix is model-agnostic and capable in a wide-spectrum of pose estimation models. (2) AdvMix consists of adversarial augmentation and knowledge distillation. Adversarial augmentation contains two neural network modules that are trained jointly and competitively in an adversarial manner, where a generator network mixes different corrupted images to confuse a pose estimator, improving the robustness of the pose estimator by learning from harder samples. To compensate for the noise patterns by adversarial augmentation, knowledge distillation is applied to transfer clean pose structure knowledge to the target pose estimator. (3) Extensive experiments show that AdvMix significantly increases the robustness of pose estimations across a wide range of corruptions, while maintaining accuracy on clean data in various challenging benchmark datasets.",0,Human,0.0013949275016784,0.0096025466918945,Human,LABEL_1
"This paper introduces a new method for efficient and scalable training of models that handle multiple languages and tasks together. Recent research has shown these models to be very useful in NLP because they can process different languages simultaneously. However, training them can take a long time and consume a lot of computing resources, thus making them impractical. To counter this limitation, this new method makes use of distributed training across multiple nodes along with adaptive optimization techniques to decrease time needed for training and lower memory use. Task awareness through gradient accumulation is introduced as a new technique where resources are better balanced among different tasks during training. Furthermore, an adaptive optimizer is used that modifies learning rate and momentum values according to task characteristics dynamically. Results using a large dataset show high performance on various tasks including language modeling, translation, and POS tagging. Our method reduces training times and memory use substantially and promises a way forward for scaling up and improving efficiency of multitask models that handle multiple languages. Implications for real world NLP work are very promising.",1,AI,0.0216812491416931,0.0268272757530212,Human,LABEL_1
"This paper looks at how autonomous systems and AI can affect stability in the nuclear sector. It investigates what role such tech might play in making things safer and securing facilities better by reducing risk of accidents and events. Also, this study considers challenges related to integrating this technology; this includes ethical issues and cyber security risks. Results indicate that using autonomous systems and AI could fundamentally change the nuclear sector and contribute to a secure future. Yet careful planning and strategy implementation is essential to ensure these technologies are used responsibly and safely. Collaboration among specialists in nuclear matters and technology is key to realizing these high tech innovations for a secure future.",1,AI,0.0084692239761352,0.9444098472595216,Human,LABEL_0
"Manipulating facial expressions is a challenging task due to fine-grained shape changes produced by facial muscles and the lack of input-output pairs for supervised learning. Unlike previous methods using Generative Adversarial Networks (GAN), which rely on cycle-consistency loss or sparse geometry (landmarks) loss for expression synthesis, we propose a novel GAN framework to exploit 3D dense (depth and surface normals) information for expression manipulation. However, a large-scale dataset containing RGB images with expression annotations and their corresponding depth maps is not available. To this end, we propose to use an off-the-shelf state-of-the-art 3D reconstruction model to estimate the depth and create a large-scale RGB-Depth dataset after a manual data clean-up process. We utilise this dataset to minimise the novel depth consistency loss via adversarial learning (note we do not have ground truth depth maps for generated face images) and the depth categorical loss of synthetic data on the discriminator. In addition, to improve the generalisation and lower the bias of the depth parameters, we propose to use a novel confidence regulariser on the discriminator side of the framework. We extensively performed both quantitative and qualitative evaluations on two publicly available challenging facial expression benchmarks: AffectNet and RaFD. Our experiments demonstrate that the proposed method outperforms the competitive baseline and existing arts by a large margin.",0,Human,0.0101063847541809,0.0091433525085449,Human,LABEL_1
This paper introduces a fast simulation technique that predicts temperature changes along overhead power lines. This method uses analytical solutions for heat transfer coupled with numerical weather predictions (NWP) to simulate temperature distributions along these lines very accurately. Proposed simulation method is efficient enough to work well in real time for monitoring and control of lines. Results of a demonstration case show that method works effectively at different weather conditions and clearly demonstrates that this new simulation method has great promise for enhancing reliability and efficiency of operation for overhead power lines.,1,AI,0.0203197598457336,0.8674413561820984,Human,LABEL_0
"In this work, we initiate the study of \emph{smoothed analysis} of population protocols. We consider a population protocol model where an adaptive adversary dictates the interactions between agents, but with probability $p$ every such interaction may change into an interaction between two agents chosen uniformly at random. That is, $p$-fraction of the interactions are random, while $(1-p)$-fraction are adversarial. The aim of our model is to bridge the gap between a uniformly random scheduler (which is too idealistic) and an adversarial scheduler (which is too strict).  We focus on the fundamental problem of leader election in population protocols. We show that, for a population of size $n$, the leader election problem can be solved in $O(p^{-2}n \log^3 n)$ steps with high probability, using $O((\log^2 n) \cdot (\log (n/p)))$ states per agent, for \emph{all} values of $p\leq 1$. Although our result does not match the best known running time of $O(n \log n)$ for the uniformly random scheduler ($p=1$), we are able to present a \emph{smooth transition} between a running time of $O(n \cdot \mathrm{polylog} n)$ for $p=1$ and an infinite running time for the adversarial scheduler ($p=0$), where the problem cannot be solved. The key technical contribution of our work is a novel \emph{phase clock} algorithm for our model. This is a key primitive for much-studied fundamental population protocol algorithms (leader election, majority), and we believe it is of independent interest.",0,Human,0.0023869276046752,0.02432382106781,Human,LABEL_1
"This study looks at using stacked multilayer perceptrons (MLPs) to learn dynamic classes of events. Events here are those that vary over time such as traffic patterns or changes in weather. Proposed is an approach which trains a sequence of MLPs for learning different stages of event dynamics; the output of each layer feeds into the next. This stacking allows capturing dependency in time for events and thus improves accuracy of predictions. Performance is evaluated on a traffic flow dataset with goal to predict traffic flow for the following hour. Results indicate stacked MLPs perform better compared to traditional approaches like linear regression and support vector regression. Sensitivity to hyper parameters and architectural decisions is also tested experimentally. In sum, this work suggests a promising way to learn from changing events and stacked MLPs are applicable across many domains where events fluctuate over time.",1,AI,0.0005460381507873,0.0263088941574096,Human,LABEL_1
"This paper presents a new method for selecting representatives in large data sets that combines sparse graphs and geodesic distances on Grassmann manifolds. The aim is to pick a small set of representative samples that embody key features of a large dataset so that we can study and draw inferences more efficiently. First, this method creates a sparse graph based on data; nodes are data points and edges indicate similarities between pairs of points. After clustering high similarity points using graph clustering, representatives are chosen by calculating geodesic distances between pairs of data points and picking the one nearest to the cluster mean on the Grassmann manifold. Results on various real data sets show that this method outperforms other representative selection methods regarding both representativeness and efficiency. The method has broad applicability including summarizing data, visualizing data and classification tasks which all require careful selection of representative samples from large datasets.",1,AI,0.0003496408462524,0.0284021496772766,Human,LABEL_1
"Ramsey's theorem, in the version of Erd\H{o}s and Szekeres, states that every 2-coloring of the edges of the complete graph on {1, 2,...,n} contains a monochromatic clique of order 1/2\log n. In this paper, we consider two well-studied extensions of Ramsey's theorem.  Improving a result of R\""odl, we show that there is a constant $c>0$ such that every 2-coloring of the edges of the complete graph on \{2, 3,...,n\} contains a monochromatic clique S for which the sum of 1/\log i over all vertices i \in S is at least c\log\log\log n. This is tight up to the constant factor c and answers a question of Erd\H{o}s from 1981.  Motivated by a problem in model theory, V\""a\""an\""anen asked whether for every k there is an n such that the following holds. For every permutation \pi of 1,...,k-1, every 2-coloring of the edges of the complete graph on {1, 2, ..., n} contains a monochromatic clique a_1<...<a_k with a_{\pi(1)+1}-a_{\pi(1)}>a_{\pi(2)+1}-a_{\pi(2)}>...>a_{\pi(k-1)+1}-a_{\pi(k-1)}. That is, not only do we want a monochromatic clique, but the differences between consecutive vertices must satisfy a prescribed order. Alon and, independently, Erd\H{o}s, Hajnal and Pach answered this question affirmatively. Alon further conjectured that the true growth rate should be exponential in k. We make progress towards this conjecture, obtaining an upper bound on n which is exponential in a power of k. This improves a result of Shelah, who showed that n is at most double-exponential in k.",0,Human,0.0006994009017944,0.0134778618812561,Human,LABEL_1
"Software cost estimation is one of the prerequisite managerial activities carried out at the software development initiation stages and also repeated throughout the whole software life-cycle so that amendments to the total cost are made. In software cost estimation typically, a selection of project attributes is employed to produce effort estimations of the expected human resources to deliver a software product. However, choosing the appropriate project cost drivers in each case requires a lot of experience and knowledge on behalf of the project manager which can only be obtained through years of software engineering practice. A number of studies indicate that popular methods applied in the literature for software cost estimation, such as linear regression, are not robust enough and do not yield accurate predictions. Recently the dual variables Ridge Regression (RR) technique has been used for effort estimation yielding promising results. In this work we show that results may be further improved if an AI method is used to automatically select appropriate project cost drivers (inputs) for the technique. We propose a hybrid approach combining RR with a Genetic Algorithm, the latter evolving the subset of attributes for approximating effort more accurately. The proposed hybrid cost model has been applied on a widely known high-dimensional dataset of software project samples and the results obtained show that accuracy may be increased if redundant attributes are eliminated.",0,Human,0.0005609393119812,0.0096178650856018,Human,LABEL_1
"In the last two years, more than 200 papers have been written on how machine learning (ML) systems can fail because of adversarial attacks on the algorithms and data; this number balloons if we were to incorporate papers covering non-adversarial failure modes. The spate of papers has made it difficult for ML practitioners, let alone engineers, lawyers, and policymakers, to keep up with the attacks against and defenses of ML systems. However, as these systems become more pervasive, the need to understand how they fail, whether by the hand of an adversary or due to the inherent design of a system, will only become more pressing. In order to equip software developers, security incident responders, lawyers, and policy makers with a common vernacular to talk about this problem, we developed a framework to classify failures into ""Intentional failures"" where the failure is caused by an active adversary attempting to subvert the system to attain her goals; and ""Unintentional failures"" where the failure is because an ML system produces an inherently unsafe outcome. After developing the initial version of the taxonomy last year, we worked with security and ML teams across Microsoft, 23 external partners, standards organization, and governments to understand how stakeholders would use our framework. Throughout the paper, we attempt to highlight how machine learning failure modes are meaningfully different from traditional software failures from a technology and policy perspective.",0,Human,0.0012615323066711,0.0456386804580688,Human,LABEL_1
This paper introduces a deep reinforcement learning approach for joint optimization of spectrum and energy efficiency alongside security in C-V2X communication networks. Novel use of deep reinforcement learning is employed here to address complex optimization problems; resulting in high quality solutions which balance spectrum and energy efficiency and strengthen network security. Results from simulation experiments show the proposed method performs better than previous optimization approaches. Insights regarding joint optimization with security are also provided by this research.,1,AI,0.000278890132904,0.4441578984260559,Human,LABEL_1
"The visual cue of optical flow plays a major role in the navigation of flying insects, and is increasingly studied for use by small flying robots as well. A major problem is that successful optical flow control seems to require distance estimates, while optical flow is known to provide only the ratio of velocity to distance. In this article, a novel, stability-based strategy is proposed to estimate distances with monocular optical flow and knowledge of the control inputs (efference copies). It is shown analytically that given a fixed control gain, the stability of a constant divergence control loop only depends on the distance to the approached surface. At close distances, the control loop first starts to exhibit self-induced oscillations, eventually leading to instability. The proposed stability-based strategy for estimating distances has two major attractive characteristics. First, self-induced oscillations are easy for the robot to detect and are hardly influenced by wind. Second, the distance can be estimated during a zero divergence maneuver, i.e., around hover. The stability-based strategy is implemented and tested both in simulation and with a Parrot AR drone 2.0. It is shown that it can be used to: (1) trigger a final approach response during a constant divergence landing with fixed gain, (2) estimate the distance in hover, and (3) estimate distances during an entire landing if the robot uses adaptive gain control to continuously stay on the 'edge of oscillation'.",0,Human,0.0009573101997375,0.0073654055595397,Human,LABEL_1
"The fast Fourier transform (FFT) is a key tool in diverse areas like science and engineering; examples include processing signals and images and doing simulations. As data volume grows, so does the computational cost for computing FFTs. Thus, development of efficient parallel algorithms for FFTs is important. This paper introduces a new parallel algorithm for multidimensional FFT using MPI on high performance computers. A two stage approach is proposed that uses distributed one dimensional FFT alongside block cyclic distribution. We exploit properties of cyclic distribution to minimize communication burden and balance load among processors. We also employ recent features of MPI to optimize communication and lower synchronization overhead. Performance and scalability of this algorithm are demonstrated on different hardware setups such as large cluster and supercomputers. Results show that the algorithm attains nearly linear speedup up to thousands of processors and outperforms existing parallel FFT libraries for big data. Our method is very effective and significantly reduces computation time for multidimensional FFT; this will improve simulation speed and data analysis accuracy across many disciplines.",1,AI,0.0003897547721862,0.010406494140625,Human,LABEL_1
"In this paper, we study the Nash dynamics of strategic interplays of n buyers in a matching market setup by a seller, the market maker. Taking the standard market equilibrium approach, upon receiving submitted bid vectors from the buyers, the market maker will decide on a price vector to clear the market in such a way that each buyer is allocated an item for which he desires the most (a.k.a., a market equilibrium solution). While such equilibrium outcomes are not unique, the market maker chooses one (maxeq) that optimizes its own objective --- revenue maximization. The buyers in turn change bids to their best interests in order to obtain higher utilities in the next round's market equilibrium solution.  This is an (n+1)-person game where buyers place strategic bids to gain the most from the market maker's equilibrium mechanism. The incentives of buyers in deciding their bids and the market maker's choice of using the maxeq mechanism create a wave of Nash dynamics involved in the market. We characterize Nash equilibria in the dynamics in terms of the relationship between maxeq and mineq (i.e., minimum revenue equilibrium), and develop convergence results for Nash dynamics from the maxeq policy to a mineq solution, resulting an outcome equivalent to the truthful VCG mechanism.  Our results imply revenue equivalence between maxeq and mineq, and address the question that why short-term revenue maximization is a poor long run strategy, in a deterministic and dynamic setting.",0,Human,0.0003045797348022,0.0146560072898864,Human,LABEL_1
"Learning disentangled representation of data without supervision is an important step towards improving the interpretability of generative models. Despite recent advances in disentangled representation learning, existing approaches often suffer from the trade-off between representation learning and generation performance i.e. improving generation quality sacrifices disentanglement performance). We propose an Information-Distillation Generative Adversarial Network (ID-GAN), a simple yet generic framework that easily incorporates the existing state-of-the-art models for both disentanglement learning and high-fidelity synthesis. Our method learns disentangled representation using VAE-based models, and distills the learned representation with an additional nuisance variable to the separate GAN-based generator for high-fidelity synthesis. To ensure that both generative models are aligned to render the same generative factors, we further constrain the GAN generator to maximize the mutual information between the learned latent code and the output. Despite the simplicity, we show that the proposed method is highly effective, achieving comparable image generation quality to the state-of-the-art methods using the disentangled representation. We also show that the proposed decomposition leads to an efficient and stable model design, and we demonstrate photo-realistic high-resolution image synthesis results (1024x1024 pixels) for the first time using the disentangled representations.",0,Human,0.0008397102355957,0.0402503013610839,Human,LABEL_1
"The proven efficacy of learning-based control schemes strongly motivates their application to robotic systems operating in the physical world. However, guaranteeing correct operation during the learning process is currently an unresolved issue, which is of vital importance in safety-critical systems. We propose a general safety framework based on Hamilton-Jacobi reachability methods that can work in conjunction with an arbitrary learning algorithm. The method exploits approximate knowledge of the system dynamics to guarantee constraint satisfaction while minimally interfering with the learning process. We further introduce a Bayesian mechanism that refines the safety analysis as the system acquires new evidence, reducing initial conservativeness when appropriate while strengthening guarantees through real-time validation. The result is a least-restrictive, safety-preserving control law that intervenes only when (a) the computed safety guarantees require it, or (b) confidence in the computed guarantees decays in light of new observations. We prove theoretical safety guarantees combining probabilistic and worst-case analysis and demonstrate the proposed framework experimentally on a quadrotor vehicle. Even though safety analysis is based on a simple point-mass model, the quadrotor successfully arrives at a suitable controller by policy-gradient reinforcement learning without ever crashing, and safely retracts away from a strong external disturbance introduced during flight.",0,Human,0.0004345774650573,0.0049462914466857,Human,LABEL_1
"This paper reviews Ultra Reliable Low Latency Communications (URLLC) targeting critical applications of Massive Machine Type Communications (MMTC). It starts by introducing the basics of MMTC along with important demands like low latency and high reliability. Then it concentrates on how URLLC serves to satisfy very stringent demands of important MMTC tasks like autonomous driving and industrial automation. The authors discuss key features and design principles of URLLC including reliability and latency improvements, coding and modulation techniques, and network architecture. Challenges in deploying URLLC in MMTC systems are discussed and there is insight into future research directions in this field. Finally the authors summarize results and suggest future lines of research.",1,AI,0.0042369365692138,0.2993248701095581,Human,LABEL_1
A new platform called Visual Genome is introduced in this paper which couples language and vision by using dense annotations from crowd workers. This paper describes how they annotate and create a very large set of images with fine scene descriptions and object and relationship annotations along with question answer pairs. Quality of the data is evaluated by comparison against other datasets and demonstrated usefulness for different vision and NLP tasks. Challenges and future work are discussed as well including quality improvements and expansion of annotation scope. Concluding remarks state that this platform provides great benefit for research in both vision and NLP and has strong potential to move the field forward.,1,AI,0.0003769397735595,0.6887810826301575,Human,LABEL_0
Expired domain names are referred to as the neglected side of the internet. This paper looks into the practice of taking over these domain names along with the related online resources. We investigate reasons and tactics used by those who take over these abandoned assets and the possible consequences for previous owners and the internet at large. Results from our research show that hijacking of such domains is becoming a growing concern; it poses a significant risk to internet stability and security. We end with suggestions for policymakers and registrars to deal with this issue and safeguard domain owner rights.,1,AI,0.0449755787849426,0.9708057045936584,Human,LABEL_0
"This paper suggests an integration of hybrid inference systems for better estimation of curvature using machine learning. Level set method is very common for modeling shapes and segmentation in computer vision, medicine and graphics. This method relies heavily on accurate estimation of curvature of evolving interfaces and this is critical for proper propagation of level set functions. Traditional methods for curvature estimation have some drawbacks like sensitivity to noise and uneven sampling. Therefore we propose a system that combines traditional methods with a learning model based on machine learning. This model has been trained using large datasets of both synthetically generated and real images. It uses local image features to predict curvature at every point on the evolving boundary. This hybrid system integrates predictions from the machine learning model along with results from traditional curvature estimation and produces more precise and reliable estimates of curvature. Results are evaluated on different data sets and compared against traditional methods. Results show that this hybrid system performs better than traditional methods regarding precision and reliability. Also this proposed system runs efficiently and is easy to integrate into current level set algorithms. Our research introduces a promising path towards improving both accuracy and reliability of level set methods using machine learning based approaches. System integration into other shape modeling and segmentation tasks that depend on curvature estimation could also be possible.",1,AI,0.0007116198539733,0.0590527057647705,Human,LABEL_1
"Writing accurate numerical software is hard because of many sources of unavoidable uncertainties, including finite numerical precision of implementations. We present a programming model where the user writes a program in a real-valued implementation and specification language that explicitly includes different types of uncertainties. We then present a compilation algorithm that generates a conventional implementation that is guaranteed to meet the desired precision with respect to real numbers. Our verification step generates verification conditions that treat different uncertainties in a unified way and encode reasoning about floating-point roundoff errors into reasoning about real numbers. Such verification conditions can be used as a standardized format for verifying the precision and the correctness of numerical programs. Due to their often non-linear nature, precise reasoning about such verification conditions remains difficult. We show that current state-of-the art SMT solvers do not scale well to solving such verification conditions. We propose a new procedure that combines exact SMT solving over reals with approximate and sound affine and interval arithmetic. We show that this approach overcomes scalability limitations of SMT solvers while providing improved precision over affine and interval arithmetic. Using our initial implementation we show the usefullness and effectiveness of our approach on several examples, including those containing non-linear computation.",0,Human,0.0183885693550109,0.027758777141571,Human,LABEL_1
"This paper presents a new framework that counts and locates people in crowds using points directly rather than regions. Current approaches assume fixed behaviors for crowds but they are often inaccurate; the new framework avoids such assumptions and treats each person as an individual point. Results from real data have demonstrated improved accuracy. By taking a point approach instead of relying on regions, this work opens a new line of thought on how to better count and locate people in crowds.",1,AI,0.0016630887985229,0.9576334953308104,Human,LABEL_0
"This paper studies the maximum minimal distance of linear codes that allow local repair and aims to find tight bounds on this distance. Recent interest has focused on such codes because they are important for designing fault tolerant storage systems. Maximum minimal distance for a code refers to the largest minimum distance that any subcode of that code can have; this is an important parameter related to the correction of errors by codes. The paper carefully examines this distance along with relationships to other code parameters and effects of code design on its value. Using various mathematical tools, the authors derive new bounds on this maximum minimal distance and assess their tightness using simulation evaluations. Results of this research offer valuable insights into improving code performance for linear codes that allow local repairs.",1,AI,0.0011401176452636,0.3570294380187988,Human,LABEL_1
This paper introduces a new way of counting crowds by combining multi resolution fusion and priors at multiple scales. The new method makes use of both benefits of these approaches to accurately estimate the number of people present in an environment. The method works by taking scenes at various resolutions and fusing data together to produce high resolution images. Then priors at different scales refine the estimation of crowd counts. Performance of this new method has been tested on some public datasets and results show that it surpasses leading crowd counting methods. This new approach could greatly improve performance for real world tasks like crowd management and safety supervision.,1,AI,0.0325652956962585,0.8499945402145386,Human,LABEL_0
"We introduce Bee$^+$, a 95-mg four-winged microrobot with improved controllability and open-loop-response characteristics with respect to those exhibited by state-of-the-art two-winged microrobots with the same size and similar weight (i.e., the 75-mg Harvard RoboBee). The key innovation that made possible the development of Bee$^+$ is the introduction of an extremely light (28-mg) pair of twinned unimorph actuators, which enabled the design of a new microrobotic mechanism that flaps four wings independently. A first main advantage of the proposed design, compared to those of two-winged flyers, is that by increasing the number of actuators from two to four, the number of direct control inputs increases from three to four when simple sinusoidal excitations are employed. A second advantage of Bee$^+$ is that its four-wing configuration and flapping mode naturally damp the rotational disturbances that commonly affect the yaw degree of freedom of two-winged microrobots. In addition, the proposed design greatly reduces the complexity of the associated fabrication process compared to those of other microrobots, as the unimorph actuators are fairly easy to build. Lastly, we hypothesize that given the relatively low wing-loading affecting their flapping mechanisms, the life expectancy of Bee$^+$s must be considerably higher than those of the two-winged counterparts. The functionality and basic capabilities of the robot are demonstrated through a set of simple control experiments.",0,Human,0.0487728118896484,0.0070830583572387,Human,LABEL_1
"In this paper we investigate the problem of allocating spectrum among radio nodes under SINR requirements. This problem is of special interest in dynamic spectrum access networks where topology and spectral resources differ with time and location. The problem is to determine the number of radio nodes that can transmit simultaneously while still achieving their SINR requirements and then decide which channels these nodes should transmit on. Previous work have shown how this can be done for a large spectrum pool where nodes allocate multiple channels from that pool which renders a linear programming approach feasible when the pool is large enough. In this paper we extend their work by considering arbitrary individual pool sizes and allow nodes to only transmit on one channel. Due to the accumulative nature of interference this problem is a non-convex integer problem which is NP-hard. However, we introduce a constraint transformation that transforms the problem to a binary quadratic constraint problem. Although this problem is still NP-hard, well known heuristic algorithms for solving this problem are known in the literature. We implement a heuristic algorithm based on Lagrange relaxation which bounds the solution value of the heuristic to the optimal value of the constraint transformed problem. Simulation results show that this approach provides solutions within an average gap of 10% of solutions obtained by a genetic algorithm for the original non-convex integer problem.",0,Human,0.0008294582366943,0.0105590224266052,Human,LABEL_1
This paper introduces an approach for efficient sampling using polynomial chaos for quantifying uncertainties and analyzing sensitivities. Authors use weighted approximate Fekete points which are known for their strong performance at producing good distributions of sampling points for interpolation of polynomials. They show that this new method achieves high accuracy using much fewer samples than traditional Monte Carlo methods. Performance is tested by means of several numerical experiments; results indicate this new method performs well as an efficient sampling strategy for quantifying uncertainties and sensitivity analysis using polynomial chaos.,1,AI,0.000333547592163,0.3146884441375732,Human,LABEL_1
"OdoViz is a reactive web-based tool for 3D visualization and processing of autonomous vehicle datasets designed to support common tasks in visual place recognition research. The system includes functionality for loading, inspecting, visualizing, and processing GPS/INS poses, point clouds and camera images. It supports a number of commonly used driving datasets and can be adapted to load custom datasets with minimal effort. OdoViz's design consists of a slim server to serve the datasets coupled with a rich client frontend. This design supports multiple deployment configurations including single user stand-alone installations, research group installations serving datasets internally across a lab, or publicly accessible web-frontends for providing online interfaces for exploring and interacting with datasets. The tool allows viewing complete vehicle trajectories traversed at multiple different time periods simultaneously, facilitating tasks such as sub-sampling, comparing and finding pose correspondences both across and within sequences. This significantly reduces the effort required in creating subsets of data from existing datasets for machine learning tasks. Further to the above, the system also supports adding custom extensions and plugins to extend the capabilities of the software for other potential data management, visualization and processing tasks. The platform has been open-sourced to promote its use and encourage further contributions from the research community.",0,Human,0.0021691918373107,0.0144270062446594,Human,LABEL_1
"Humans and animals are believed to use a very minimal set of trajectories to perform a wide variety of tasks including walking. Our main objective in this paper is two fold 1) Obtain an effective tool to realize these basic motion patterns for quadrupedal walking, called the kinematic motion primitives (kMPs), via trajectories learned from deep reinforcement learning (D-RL) and 2) Realize a set of behaviors, namely trot, walk, gallop and bound from these kinematic motion primitives in our custom four legged robot, called the `Stoch'. D-RL is a data driven approach, which has been shown to be very effective for realizing all kinds of robust locomotion behaviors, both in simulation and in experiment. On the other hand, kMPs are known to capture the underlying structure of walking and yield a set of derived behaviors. We first generate walking gaits from D-RL, which uses policy gradient based approaches. We then analyze the resulting walking by using principal component analysis. We observe that the kMPs extracted from PCA followed a similar pattern irrespective of the type of gaits generated. Leveraging on this underlying structure, we then realize walking in Stoch by a straightforward reconstruction of joint trajectories from kMPs. This type of methodology improves the transferability of these gaits to real hardware, lowers the computational overhead on-board, and also avoids multiple training iterations by generating a set of derived behaviors from a single learned gait.",0,Human,0.0133094787597656,0.0072748661041259,Human,LABEL_1
"Depth coding in 3D-HEVC for the multiview video plus depth (MVD) architecture (i) deforms object shapes due to block-level edge-approximation; (ii) misses an opportunity for high compressibility at near-lossless quality by failing to exploit strong homogeneity (clustering tendency) in depth syntax, motion vector components, and residuals at frame-level; and (iii) restricts interactivity and limits responsiveness of independent use of depth information for ""non-viewing"" applications due to texture-depth coding dependency. This paper presents a standalone depth sequence coder, which operates in the lossless to near-lossless quality range while compressing depth data superior to lossy 3D-HEVC. It preserves edges implicitly by limiting quantisation to the spatial-domain and exploits clustering tendency efficiently at frame-level with a novel binary tree based decomposition (BTBD) technique. For mono-view coding of standard MVD test sequences, on average, (i) lossless BTBD achieved $\times 42.2$ compression-ratio and $-60.0\%$ coding gain against the pseudo-lossless 3D-HEVC, using the lowest quantisation parameter $QP = 1$, and (ii) near-lossless BTBD achieved $-79.4\%$ and $6.98$ dB Bj{\o}ntegaard delta bitrate (BD-BR) and distortion (BD-PSNR), respectively, against 3D-HEVC. In view-synthesis applications, decoded depth maps from BTBD rendered superior quality synthetic-views, compared to 3D-HEVC, with $-18.9\%$ depth BD-BR and $0.43$ dB synthetic-texture BD-PSNR on average.",0,Human,0.0005042552947998,0.0066130757331848,Human,LABEL_1
This paper looks at what makes a good summary relevant to education and considers what aspects of this are important for automatic summarization technology. Its goal is to find out which key features like conciseness and coherence matter most and how those features influence effectiveness when summaries are used in education. Results will illuminate how well summaries work in education and suggest ways to improve automatic summarization technology.,1,AI,0.028399646282196,0.99356609582901,Human,LABEL_0
"Entity resolution is a widely studied problem with several proposals to match records across relations. Matching textual content is a widespread task in many applications, such as question answering and search. While recent methods achieve promising results for these two tasks, there is no clear solution for the more general problem of matching textual content and structured data. We introduce a framework that supports this new task in an unsupervised setting for any pair of corpora, being relational tables or text documents. Our method builds a fine-grained graph over the content of the corpora and derives word embeddings to represent the objects to match in a low dimensional space. The learned representation enables effective and efficient matching at different granularity, from relational tuples to text sentences and paragraphs. Our flexible framework can exploit pre-trained resources, but it does not depends on their existence and achieves better quality performance in matching content when the vocabulary is domain specific. We also introduce optimizations in the graph creation process with an ""expand and compress"" approach that first identifies new valid relationships across elements, to improve matching, and then prunes nodes and edges, to reduce the graph size. Experiments on real use cases and public datasets show that our framework produces embeddings that outperform word embeddings and fine-tuned language models both in results' quality and in execution times.",0,Human,0.0095657110214233,0.0091612935066223,Human,LABEL_1
"This paper looks at how to create a Critical Geometric Graph (CGG) via a distributed method in dense Wireless Sensor Networks (WSNs). CGG is important in WSNs and useful for many applications because it describes relationships among sensors. But making CGG in dense networks is difficult because there are lots of sensors and they can communicate only short distances. Therefore, we propose a new distributed algorithm that uses properties of Delaunay triangulation and Gabriel graph. This algorithm iterates and has three major phases: selecting nodes, local construction and global synchronization. Each node selects candidates neighbors based on triangulation in selecting phase. Local construction phase lets each node use selected neighbors and build local subgraphs with Gabriel graph. Finally in synchronization phase, local graphs are merged into final CGG. Results show that the proposed algorithm makes accurate and efficient CGG even in dense networks. Performance comparison also shows it is faster and uses less power. Overall, this distributed algorithm provides a good way to make CGG in dense WSNs, which is important for tasks such as localization, routing and coverage control.",1,AI,0.2815569639205932,0.0067883729934692,Human,LABEL_1
"This paper introduces a new method called Neurint that interpolates using neural ordinary differential equations (ODEs). Using a parametric approach, Neurint approximates solutions to systems of ODEs and learns parameters via backpropagation. Thus allowing the model to interpolate between any points within the input space. Experiments comparing Neurint against other traditional methods show higher performance and better generalization. Results suggest Neurint is a promising tool for interpolation tasks in ML and other areas.",1,AI,0.0012962818145751,0.4875460267066955,Human,LABEL_1
"DeepCert introduces a new way of checking how well neural networks recognize images under realistic conditions. The goal is to ensure that these networks perform reliably in practice, handling images that might have been corrupted, partially covered or changed from what they were trained on. Using a blend of formal verification and learning, DeepCert provides probabilistic guarantees regarding robustness for specific classifiers with respect to certain transformation sets. Results show this approach performs much better compared to previous work when verifying robustness. Results also suggest DeepCert is viable for increasing confidence and trust in classification systems used practically.",1,AI,0.0016060471534729,0.6015737652778625,Human,LABEL_0
This research develops new algorithms for solving multi objective optimization problems using decomposition methods. It introduces two different designs of algorithms using different frameworks. One focuses on developing an MOEA based on decomposition techniques; these techniques convert a complex problem into simpler ones. The other framework integrates decomposition methods into existing MOEAs to enhance them. Algorithms are tested against a set of benchmark problems and compared with top performing MOEAs. Results indicate they converge and spread better and produce good sets of diverse solutions. This work gives important guidance on MOEA design and shows effectiveness of MOEAs using decomposition methods.,1,AI,0.0007834434509277,0.2139464616775512,Human,LABEL_1
"This paper examines empirical performance of machine learning pipelines used in industry and discusses ways to optimize them. It analyzes current status of pipelines and points out recurring difficulties and performance limitations. By performing experiments and case studies, authors illustrate how different optimization strategies affect pipeline performance and efficiency. Results show that combining hardware and software optimizations such as parallel processing and model pruning significantly enhance speed and quality of pipelines. Conclusions include specific recommendations for optimization and emphasize ongoing research necessary in this field.",1,AI,0.0192785263061523,0.6855141520500183,Human,LABEL_0
"Scene parsing is an important task in computer vision. The aim is to segment and classify objects within images or videos. This paper introduces a new method for scene parsing that leverages multiscale feature learning, purity trees, and optimal coverings. First, we extract features of an image at different levels of scale to capture both local and global context. We then use purity trees to hierarchically partition the image into regions by clustering similar pixels and objects together. Finally, we assign these regions to semantic classes via optimal covering; this results in comprehensive scene segmentation. Experiments on benchmark datasets show that our approach excels at performance for scene parsing.",1,AI,0.0004844665527343,0.1177653670310974,Human,LABEL_1
This paper introduces a new method for classifying scenes indoors using both spatial layout representation and scale invariant feature extraction. Spatial layout captures overall scene structure while scale invariant features extract features which resist variation in scale. By integrating both representations we obtain a more robust and accurate classifier. Experiments on various benchmark sets show effectiveness of this approach outperforming leading methods.,1,AI,0.0003070831298828,0.8778860569000244,Human,LABEL_0
"This paper surveys the decomposition of tensors and its application in signal processing and machine learning. Decomposition of tensors is a strong method to analyze high dimensional data that frequently occurs in practical situations. Initially, this paper introduces tensors and tensor decomposition briefly, discussing different formats and the mathematical basis of decomposition. Following this, the paper details key decomposition methods like Tucker decomposition, CPD, and PARAFAC2; describing algorithms, pros and cons for each one. Also, the paper reviews various applications, including tensor completion, regression and clustering. Finally, concluding remarks and open questions are presented. Overall, this paper provides a broad overview of decomposition of tensors and related use cases.",1,AI,0.0012803077697753,0.317346453666687,Human,LABEL_1
"Recently self supervised learning has gained strong attention as a powerful approach to deal with computer vision tasks without supervision. In this paper we introduce a new way to do target tracking using synthesis of data aware of targets. We combine target object images with various backgrounds to synthesize realistic training data and use this data to train deep networks to track targets in real time video. The trained network learns to discriminate target from other elements by using appearance, motion and contextual information. Results are evaluated against different benchmark datasets and compared to current top methods. Results show that our method performs competitively and excels on difficult situations compared to previous ones. This work suggests that synthesizing data aware of targets is a promising path towards unsupervised visual tracking.",1,AI,0.0030331015586853,0.0854065418243408,Human,LABEL_1
"We present a strategy grounded in the element removal idea of Bruns and Tortorelli [1] and aimed at reducing computational cost and circumventing potential numerical instabilities of density-based topology optimization. The design variables and the relative densities are both represented on a fixed, uniform finite element grid, and linked through filtering and Heaviside projection. The regions in the analysis domain where the relative density is below a specified threshold are removed from the forward analysis and replaced by fictitious nodal boundary conditions. This brings a progressive cut of the computational cost as the optimization proceeds and helps to mitigate numerical instabilities associated with low-density regions. Removed regions can be readily reintroduced since all the design variables remain active and are modeled in the formal sensitivity analysis. A key feature of the proposed approach is that the Heaviside functions promote material reintroduction along the structural boundaries by amplifying the magnitude of the sensitivities inside the filter reach. Several 2D and 3D structural topology optimization examples are presented, including linear and nonlinear compliance minimization, the design of a force inverter, and frequency and buckling load maximization. The approach is shown to be effective at producing optimized designs equivalent or nearly equivalent to those obtained without the element removal, while providing remarkable computational savings.",0,Human,0.0021579265594482,0.0091642141342163,Human,LABEL_1
"Machine learning techniques have enabled robots to learn narrow, yet complex tasks and also perform broad, yet simple skills with a wide variety of objects. However, learning a model that can both perform complex tasks and generalize to previously unseen objects and goals remains a significant challenge. We study this challenge in the context of ""improvisational"" tool use: a robot is presented with novel objects and a user-specified goal (e.g., sweep some clutter into the dustpan), and must figure out, using only raw image observations, how to accomplish the goal using the available objects as tools. We approach this problem by training a model with both a visual and physical understanding of multi-object interactions, and develop a sampling-based optimizer that can leverage these interactions to accomplish tasks. We do so by combining diverse demonstration data with self-supervised interaction data, aiming to leverage the interaction data to build generalizable models and the demonstration data to guide the model-based RL planner to solve complex tasks. Our experiments show that our approach can solve a variety of complex tool use tasks from raw pixel inputs, outperforming both imitation learning and self-supervised learning individually. Furthermore, we show that the robot can perceive and use novel objects as tools, including objects that are not conventional tools, while also choosing dynamically to use or not use tools depending on whether or not they are required.",0,Human,0.2780386209487915,0.0235134959220886,Human,LABEL_1
"This paper studies the problem of predicting missing relationships between entities in knowledge graphs through learning their representations. Currently, the majority of existing link prediction models employ simple but intuitive scoring functions and relatively small embedding size so that they could be applied to large-scale knowledge graphs. However, these properties also restrict the ability to learn more expressive and robust features. Therefore, diverging from most of the prior works which focus on designing new objective functions, we propose, DeCom, a simple but effective mechanism to boost the performance of existing link predictors such as DistMult, ComplEx, etc, through extracting more expressive features while preventing overfitting by adding just a few extra parameters. Specifically, embeddings of entities and relationships are first decompressed to a more expressive and robust space by decompressing functions, then knowledge graph embedding models are trained in this new feature space. Experimental results on several benchmark knowledge graphs and advanced link prediction systems demonstrate the generalization and effectiveness of our method. Especially, RESCAL + DeCom achieves state-of-the-art performance on the FB15k-237 benchmark across all evaluation metrics. In addition, we also show that compared with DeCom, explicitly increasing the embedding size significantly increase the number of parameters but could not achieve promising performance improvement.",0,Human,0.0003516674041748,0.0103999376296997,Human,LABEL_1
"Trans-dimensional random field language models (TRF LMs) have recently been introduced, where sentences are modeled as a collection of random fields. The TRF approach has been shown to have the advantages of being computationally more efficient in inference than LSTM LMs with close performance and being able to flexibly integrating rich features. In this paper we propose neural TRFs, beyond of the previous discrete TRFs that only use linear potentials with discrete features. The idea is to use nonlinear potentials with continuous features, implemented by neural networks (NNs), in the TRF framework. Neural TRFs combine the advantages of both NNs and TRFs. The benefits of word embedding, nonlinear feature learning and larger context modeling are inherited from the use of NNs. At the same time, the strength of efficient inference by avoiding expensive softmax is preserved. A number of technical contributions, including employing deep convolutional neural networks (CNNs) to define the potentials and incorporating the joint stochastic approximation (JSA) strategy in the training algorithm, are developed in this work, which enable us to successfully train neural TRF LMs. Various LMs are evaluated in terms of speech recognition WERs by rescoring the 1000-best lists of WSJ'92 test data. The results show that neural TRF LMs not only improve over discrete TRF LMs, but also perform slightly better than LSTM LMs with only one fifth of parameters and 16x faster inference efficiency.",0,Human,0.004159390926361,0.0127218961715698,Human,LABEL_1
"The field of analyzing performance is very important and sensitive in particular when it is related to the performance of lecturers in academic institutions. Locating the weak points of lecturers through a system that provides an early warning to notify or reward the lecturers with warned or punished notices will help them to improve their weaknesses, leads to a better quality in the institutions. The current system has major issues in the higher education at Salahaddin University-Erbil (SUE) in Kurdistan-Iraq. These issues are: first, the assessment of lecturers' activities is conducted traditionally via the Quality Assurance Teams at different departments and colleges at the university, second, the outcomes in some cases of lecturers' performance provoke a low level of acceptance among lectures, as these cases are reflected and viewed by some academic communities as unfair cases, and finally, the current system is not accurate and vigorous. In this paper, Particle Swarm Optimization Neural Network is used to assess performance of lecturers in more fruitful way and also to enhance the accuracy of recognition system. Different real and novel data sets are collected from SUE. The prepared datasets preprocessed and important features are then fed as input source to the training and testing phases. Particle Swarm Optimization is used to find the best weights and biases in the training phase of the neural network. The best accuracy rate obtained in the test phase is 98.28 %.",0,Human,0.0289433598518371,0.0330556035041809,Human,LABEL_1
"Unless special conditions apply, the attempt to solve ill-conditioned systems of linear equations with standard numerical methods leads to uncontrollably high numerical error. Often, such systems arise from the discretization of operator equations with a large number of discrete variables. In this paper we show that the accuracy can be improved significantly if the equation is transformed before discretization, a process we call full operator preconditioning (FOP). It bears many similarities with traditional preconditioning for iterative methods but, crucially, transformations are applied at the operator level. We show that while condition-number improvements from traditional preconditioning generally do not improve the accuracy of the solution, FOP can. A number of topics in numerical analysis can be interpreted as implicitly employing FOP; we highlight (i) Chebyshev interpolation in polynomial approximation, and (ii) Olver-Townsend's spectral method, both of which produce solutions of dramatically improved accuracy over a naive problem formulation. In addition, we propose a FOP preconditioner based on integration for the solution of fourth-order differential equations with the finite-element method, showing the resulting linear system is well-conditioned regardless of the discretization size, and demonstrate its error-reduction capabilities on several examples. This work shows that FOP can improve accuracy beyond the standard limit for both direct and iterative methods.",0,Human,0.0005007982254028,0.0051105022430419,Human,LABEL_1
"MIMO processing plays a central part towards the recent increase in spectral and energy efficiencies of wireless networks. MIMO has grown beyond the original point-to-point channel and nowadays refers to a diverse range of centralized and distributed deployments. The fundamental bottleneck towards enormous spectral and energy efficiency benefits in multiuser MIMO networks lies in a huge demand for accurate channel state information at the transmitter (CSIT). This has become increasingly difficult to satisfy due to the increasing number of antennas and access points in next generation wireless networks relying on dense heterogeneous networks and transmitters equipped with a large number of antennas. CSIT inaccuracy results in a multi-user interference problem that is the primary bottleneck of MIMO wireless networks. Looking backward, the problem has been to strive to apply techniques designed for perfect CSIT to scenarios with imperfect CSIT. In this paper, we depart from this conventional approach and introduce the readers to a promising strategy based on rate-splitting. Rate-splitting relies on the transmission of common and private messages and is shown to provide significant benefits in terms of spectral and energy efficiencies, reliability and CSI feedback overhead reduction over conventional strategies used in LTE-A and exclusively relying on private message transmissions. Open problems, impact on standard specifications and operational challenges are also discussed.",0,Human,0.0008787512779235,0.0090417861938476,Human,LABEL_1
"Quantum computation promises significant computational advantages over classical computation for some problems. However, quantum hardware suffers from much higher error rates than in classical hardware. As a result, extensive quantum error correction is required to execute a useful quantum algorithm. The decoder is a key component of the error correction scheme whose role is to identify errors faster than they accumulate in the quantum computer and that must be implemented with minimum hardware resources in order to scale to the regime of practical applications. In this work, we consider surface code error correction, which is the most popular family of error correcting codes for quantum computing, and we design a decoder micro-architecture for the Union-Find decoding algorithm. We propose a three-stage fully pipelined hardware implementation of the decoder that significantly speeds up the decoder. Then, we optimize the amount of decoding hardware required to perform error correction simultaneously over all the logical qubits of the quantum computer. By sharing resources between logical qubits, we obtain a 67% reduction of the number of hardware units and the memory capacity is reduced by 70%. Moreover, we reduce the bandwidth required for the decoding process by a factor at least 30x using low-overhead compression algorithms. Finally, we provide numerical evidence that our optimized micro-architecture can be executed fast enough to correct errors in a quantum computer.",0,Human,0.3396959900856018,0.0155256986618042,Human,LABEL_1
"When 5G began its commercialisation journey around 2020, the discussion on the vision of 6G also surfaced. Researchers expect 6G to have higher bandwidth, coverage, reliability, energy efficiency, lower latency, and, more importantly, an integrated ""human-centric"" network system powered by artificial intelligence (AI). Such a 6G network will lead to an excessive number of automated decisions made every second. These decisions can range widely, from network resource allocation to collision avoidance for self-driving cars. However, the risk of losing control over decision-making may increase due to high-speed data-intensive AI decision-making beyond designers and users' comprehension. The promising explainable AI (XAI) methods can mitigate such risks by enhancing the transparency of the black box AI decision-making process. This survey paper highlights the need for XAI towards the upcoming 6G age in every aspect, including 6G technologies (e.g., intelligent radio, zero-touch network management) and 6G use cases (e.g., industry 5.0). Moreover, we summarised the lessons learned from the recent attempts and outlined important research challenges in applying XAI for building 6G systems. This research aligns with goals 9, 11, 16, and 17 of the United Nations Sustainable Development Goals (UN-SDG), promoting innovation and building infrastructure, sustainable and inclusive human settlement, advancing justice and strong institutions, and fostering partnership at the global level.",0,Human,0.0023713707923889,0.0258296728134155,Human,LABEL_1
"Smart cities solutions are often monolithically implemented, from sensors data handling through to the provided services. The same challenges are regularly faced by different developers, for every new solution in a new city. Expertise and know-how can be re-used and the effort shared. In this article we present the methodologies to minimize the efforts of implementing new smart city solutions and maximizing the sharing of components. The final target is to have a live technical community of smart city application developers. The results of this activity comes from the implementation of 35 city services in 27 cities between Europe and South Korea. To share efforts, we encourage developers to devise applications using a modular approach. Single-function components that are re-usable by other city services are packaged and published as standalone components, named Atomic Services. We identify 15 atomic services addressing smart city challenges in data analytics, data evaluation, data integration, data validation, and visualization. 38 instances of the atomic services are already operational in several smart city services. We detail in this article, as atomic service examples, some data predictor components. Furthermore, we describe real-world atomic services usage in the scenarios of Santander and three Danish cities. The resulting atomic services also generate a side market for smart city solutions, allowing expertise and know-how to be re-used by different stakeholders.",0,Human,0.0012449026107788,0.0077313780784606,Human,LABEL_1
"We present Stable View Synthesis (SVS). Given a set of source images depicting a scene from freely distributed viewpoints, SVS synthesizes new views of the scene. The method operates on a geometric scaffold computed via structure-from-motion and multi-view stereo. Each point on this 3D scaffold is associated with view rays and corresponding feature vectors that encode the appearance of this point in the input images. The core of SVS is view-dependent on-surface feature aggregation, in which directional feature vectors at each 3D point are processed to produce a new feature vector for a ray that maps this point into the new target view. The target view is then rendered by a convolutional network from a tensor of features synthesized in this way for all pixels. The method is composed of differentiable modules and is trained end-to-end. It supports spatially-varying view-dependent importance weighting and feature transformation of source images at each point; spatial and temporal stability due to the smooth dependence of on-surface feature aggregation on the target view; and synthesis of view-dependent effects such as specular reflection. Experimental results demonstrate that SVS outperforms state-of-the-art view synthesis methods both quantitatively and qualitatively on three diverse real-world datasets, achieving unprecedented levels of realism in free-viewpoint video of challenging large-scale scenes. Code is available at https://github.com/intel-isl/StableViewSynthesis",0,Human,0.0003359317779541,0.0095846652984619,Human,LABEL_1
"Processing of symbolic sequences represented by mapping of symbolic data into numerical signals is commonly used in various applications. It is a particularly popular approach in genomic and proteomic sequence analysis. Numerous mappings of symbolic sequences have been proposed for various applications. It is unclear however whether the processing of symbolic data provides an artifact of the numerical mapping or is an inherent property of the symbolic data. This issue has been long ignored in the engineering and scientific literature. It is possible that many of the results obtained in symbolic signal processing could be a byproduct of the mapping and might not shed any light on the underlying properties embedded in the data. Moreover, in many applications, conflicting conclusions may arise due to the choice of the mapping used for numerical representation of symbolic data. In this paper, we present a novel framework for the analysis of the equivalence of the mappings used for numerical representation of symbolic data. We present strong and weak equivalence properties and rely on signal correlation to characterize equivalent mappings. We derive theoretical results which establish conditions for consistency among numerical mappings of symbolic data. Furthermore, we introduce an abstract mapping model for symbolic sequences and extend the notion of equivalence to an algebraic framework. Finally, we illustrate our theoretical results by application to DNA sequence analysis.",0,Human,0.0039772987365722,0.2350441813468933,Human,LABEL_1
"Vehicular Ad-hoc NETworks (VANETs) are developing at a very fast pace to enable smart transportation in urban cities, by designing some mechanisms for decreasing travel time for commuters by reducing congestion. Inefficient Traffic signals and routing mechanisms are the major factors that contribute to the increase of road congestion. For smoother traffic movement and reducing congestion on the roads, the waiting time at intersections must be reduced and an optimal path should be chosen simultaneously. In this paper, A GPU assisted Preemptive MACO (GMACO-P) algorithm has been proposed to minimize the total travel time of the commuters. GMACO-P is an improvement of MACO-P algorithm that uses the harnessing the power of the GPU to provide faster computations for further minimizing the travel time. The MACO-P algorithm is based on an existing MACO algorithm that avoid the path with the congestion. The MACO-P algorithm reduces the average queue length at intersections by incorporating preemption that ensures less waiting time. In this paper, GMACO-P algorithm is proposed harnessing the power of GPU to improve MACO-P to further reduce the travel time. The GMACO-P algorithm is executed with CUDA toolkit 7.5 using C language and the obtained results were compared with existing Dijkstra, ACO, MACO, MACO-P, parallel implementation of the Dijkstra, ACO and MACO algorithms. Obtained results show the significant reduction in the travel time after using the proposed GMACO-P algorithm.",0,Human,0.2352405190467834,0.9547268152236938,Human,LABEL_0
"This paper reports a comprehensive study on bloated dependencies within the Maven ecosystem. Aimed at understanding the degree and effect of bloated dependencies on Maven project builds and performance, this research uses both static code analysis and dynamic profiling to identify and measure these dependencies. Results indicate that bloated dependencies are frequent among Maven projects and have significant effects on slowing down build times and increasing build system memory usage. Recommendations for reducing these dependencies' influence are given such as using dependency management methods and adopting lighter alternatives. The findings contribute greatly to developers, build engineers, and managers aiming to optimize Maven build processes and enhance project performance.",1,AI,0.0216680169105529,0.7241329550743103,Human,LABEL_0
This paper introduces an approach based on supervised learning that aims to address domain shift and lack of labeled data specifically for time series data. The method uses learned temporal dependencies within time series and adapts to new domains via unsupervised learning and uses few if any labels from target domain. Learning begins by training an autoregressive model on the source domain and generating synthetic samples using this model for target domain. Using these synthetic samples the authors train a domain adaptation model to reduce the gap between source and target domains. Results on two real datasets show that this method performs better than current best practices in both accuracy and resistance to shift among domains. Results suggest that leveraging temporal dependencies in time series data using unsupervised learning is promising for adapting to domains where labeled data is scarce or absent.,1,AI,0.0030984878540039,0.0183576345443725,Human,LABEL_1
"The human visual perception system has very strong robustness and contextual awareness in a variety of image processing tasks. This robustness and the perception ability of contextual awareness is closely related to the characteristics of multi-task auxiliary learning and subjective attention of the human visual perception system. In order to improve the robustness and contextual awareness of image fusion tasks, we proposed a multi-task auxiliary learning image fusion theory guided by subjective attention. The image fusion theory effectively unifies the subjective task intention and prior knowledge of human brain. In order to achieve our proposed image fusion theory, we first analyze the mechanism of multi-task auxiliary learning, build a multi-task auxiliary learning network. Secondly, based on the human visual attention perception mechanism, we introduce the human visual attention network guided by subjective tasks on the basis of the multi-task auxiliary learning network. The subjective intention is introduced by the subjective attention task model, so that the network can fuse images according to the subjective intention. Finally, in order to verify the superiority of our image fusion theory, we carried out experiments on the combined vision system image data set, and the infrared and visible image data set for experimental verification. The experimental results demonstrate the superiority of our fusion theory over state-of-arts in contextual awareness and robustness.",0,Human,0.0029840469360351,0.1007590293884277,Human,LABEL_1
"This paper introduces a new dataset called 'Amigos' which focuses on research into emotions, personalities and moods at both individual and group level. Data includes audio and video recordings alongside self reports. Procedures for collecting data are described along with measures of affect, personality and mood. Ethical concerns are also addressed. Results show how this data set is useful for research in psychology of sociality, affective computing and HCI among others. In summary, Amigos dataset is very important for those who want to study emotions, personality and mood across both individual and social levels.",1,AI,0.0004506707191467,0.9457679390907288,Human,LABEL_0
"A color image contains luminance and chrominance components representing the intensity and color information respectively. The objective of the work presented in this paper is to show the significance of incorporating the chrominance information for the task of scene classification. An improved color-to-grayscale image conversion algorithm by effectively incorporating the chrominance information is proposed using color-to-gay structure similarity index (C2G-SSIM) and singular value decomposition (SVD) to improve the perceptual quality of the converted grayscale images. The experimental result analysis based on the image quality assessment for image decolorization called C2G-SSIM and success rate (Cadik and COLOR250 datasets) shows that the proposed image decolorization technique performs better than 8 existing benchmark algorithms for image decolorization. In the second part of the paper, the effectiveness of incorporating the chrominance component in scene classification task is demonstrated using the deep belief network (DBN) based image classification system developed using dense scale invariant feature transform (SIFT) as features. The levels of chrominance information incorporated by the proposed image decolorization technique is confirmed by the improvement in the overall scene classification accuracy . Also, the overall scene classification performance is improved by the combination of models obtained using the proposed and the conventional decolorization methods.",0,Human,0.0002564787864685,0.083591341972351,Human,LABEL_1
This study introduces a new approach to simultaneous localization and mapping using stereo cameras for outdoor construction sites where there are large moving things. Proposed here is an approach which uses hierarchical masking to ignore dynamic objects and improve precision. Masking occurs through different stages starting with a coarse mask and refining it further to get a better mask. Also introduced is a module that classifies object states to distinguish static objects and enhance robustness of masking. Effectiveness of this method is shown by extensive testing on real outdoor construction site scenarios; results show performance advantage over current top methods in terms of accuracy and robustness regarding big moving stuff.,1,AI,0.0002948641777038,0.0257013440132141,Human,LABEL_1
"This paper surveys recent advances in routing protocols for Delay Tolerant Networks (DTN). DTNs are used in situations where communications are difficult and conditions change rapidly, like during emergencies, space missions, or in remote places lacking reliable connectivity. The survey covers both old and new methods of routing. Results provide a clear picture of main features and performance metrics of different routing protocols; these include advantages and disadvantages. Authors also pinpoint present challenges and open questions about DTN routing and propose ways forward. Results from this study will prove useful for researchers, developers and others who wish to design, deploy or utilize DTN networks.",1,AI,0.0080288648605346,0.3258983492851257,Human,LABEL_1
"This paper investigates Thermodynamic RAM (T-RAM), a new computational framework that uses thermodynamics principles to perform calculations. It has been promising for modeling biological neurons. We here look at the potential of T-RAM to simulate cortical processing. First we introduce the architecture and working principles of T-RAM and compare them to other neuromorphic computing systems. Then we implement T-RAM for cortical processing simulation with a simple model of visual cortex as test subject. Results show that T RAM replicates important features of cortical processing such as selective attention, receptive fields and occlusion. We also study how different parameters affect performance and consider implications for developing more advanced models of cortical processing. Results suggest that T RAM could be a useful tool to study mechanisms of cortical processing and potentially lead to development of more efficient and inspired computational systems.",1,AI,0.0153522491455078,0.3964667916297912,Human,LABEL_1
This paper looks at using adaptive pilot patterns for CA OFDM systems operating in channels that vary over time. Time varying channel conditions are problematic for CA OFDM because they result in poor performance. Adaptive pilot patterns are proposed as means to improve tracking of these time variations. Effectiveness is measured by simulations on channels that also vary. Results show improved tracking performance and better throughput and bit error rate. Contribute to developing adaptive techniques important for high speed wireless communications under dynamic conditions.,1,AI,0.0028198957443237,0.605132520198822,Human,LABEL_0
"This paper looks at how to design digital microfluidic biochips (DMFBs), aiming to improve their resistance to faults. DMFBs promise major advances in bioanalysis by allowing for automation and miniaturization of complex lab procedures. Yet DMFB designs are complex and integrate many components which can result in many different types of faults and errors. This can reduce their reliability. Therefore we propose a design flow that includes four main stages: fault modeling, fault analysis, synthesis of fault tolerance and fault tolerance testing. In fault modeling we identify likely faults and determine their root causes. Fault analysis measures the effects of these faults on performance. During synthesis of fault tolerance we create designs that reduce known faults. Finally during fault tolerance testing we evaluate if the resulting design functions reliably. We demonstrate this design flow using an example of designing a chip for crystallization of proteins, involving complex biochemical reactions. Fault injection tests confirm the new design is robust and effective. Contributions of this work include a comprehensive design flow for DMFBs that is illustrated through a case study, along with consideration of potential limits and future directions. This design flow serves as a foundation for developing high reliability and robustness into diverse bioanalysis applications.",1,AI,0.0018154978752136,0.0128327012062072,Human,LABEL_1
"An accountable algorithmic transparency report (ATR) should ideally investigate the (a) transparency of the underlying algorithm, and (b) fairness of the algorithmic decisions, and at the same time preserve data subjects' privacy. However, a provably formal study of the impact to data subjects' privacy caused by the utility of releasing an ATR (that investigates transparency and fairness), is yet to be addressed in the literature. The far-fetched benefit of such a study lies in the methodical characterization of privacy-utility trade-offs for release of ATRs in public, and their consequential application-specific impact on the dimensions of society, politics, and economics. In this paper, we first investigate and demonstrate potential privacy hazards brought on by the deployment of transparency and fairness measures in released ATRs. To preserve data subjects' privacy, we then propose a linear-time optimal-privacy scheme, built upon standard linear fractional programming (LFP) theory, for announcing ATRs, subject to constraints controlling the tolerance of privacy perturbation on the utility of transparency schemes. Subsequently, we quantify the privacy-utility trade-offs induced by our scheme, and analyze the impact of privacy perturbation on fairness measures in ATRs. To the best of our knowledge, this is the first analytical work that simultaneously addresses trade-offs between the triad of privacy, utility, and fairness, applicable to algorithmic transparency reports.",0,Human,0.0011932849884033,0.0217503905296325,Human,LABEL_1
This paper focuses on exploring the unsupervised domain adaptation problem with respect to maximizing the determinacy of binary classifiers. The aim of unsupervised domain adaptation is to learn a model using labeled data from a source domain and apply this model to a target domain where no labels are available. Determinacy maximization using binary classifiers is a method that uses decision boundaries learned from training classifiers separately on source and target domains to improve performance. An approach is introduced here that learns a boundary that maximizes agreement between classifiers. This reduces discrepancy between source and target domains. Results are presented for different datasets to show effectiveness of the proposed approach relative to leading methods. Concluding remarks discuss possible applications including areas like computer vision and NLP.,1,AI,0.0015330910682678,0.0203567147254943,Human,LABEL_1
"Transmission over multiple frequency bands combined into one logical channel speeds up data transfer for wireless networks. On the other hand, the allocation of multiple channels to a single user decreases the probability of finding a free logical channel for new connections, which may result in a network-wide throughput loss. While this relationship has been studied experimentally, especially in the WLAN configuration, little is known on how to analytically model such phenomena. With the advent of Opportunistic Spectrum Access (OSA) networks, it is even more important to understand the circumstances in which it is beneficial to bond channels occupied by primary users with dynamic duty cycle patterns. In this paper we propose an analytical framework which allows the investigation of the average channel throughput at the medium access control layer for OSA networks with channel bonding enabled. We show that channel bonding is generally beneficial, though the extent of the benefits depend on the features of the OSA network, including OSA network size and the total number of channels available for bonding. In addition, we show that performance benefits can be realized by adaptively changing the number of bonded channels depending on network conditions. Finally, we evaluate channel bonding considering physical layer constraints, i.e. throughput reduction compared to the theoretical throughput of a single virtual channel due to a transmission power limit for any bonding size.",0,Human,0.0047098398208618,0.0077577829360961,Human,LABEL_1
This paper studies diffusion based molecular communication using relaying between two nanomachines. Diffusion is considered a promising way for communications at very small scales where electromagnetic waves don't work well. In this study we consider relaying in which two nanomachines exchange information via diffusion of molecules in a medium. We model the medium as a compartmental system where each compartment represents distinct environments with different reaction rates. We develop a mathematical framework to model reaction rates within the medium and signal molecules and derive analytical expressions for bit error rate (BER) and capacity of the communication system. Using this proposed framework we investigate how reaction of medium affects system performance. Simulation results show that medium reaction has a big impact on BER and capacity and the framework allows us to optimize performance by tuning reaction rates. Results from this study may be helpful for designing and optimizing systems using diffusion communication especially when chemical reactions of medium matter.,1,AI,0.014475405216217,0.0281758904457092,Human,LABEL_1
This paper introduces a model free approach for adaptive optimal control for manufacturing processes using reinforcement learning. We use reinforcement learning algorithms to optimize control policies directly and without assuming any specific model for system dynamics. We evaluate performance on tasks with a fixed time horizon; we aim to maximize total rewards. Results show that the approach is successful at adapting to changing dynamics and results in better control performance compared to traditional methods which rely on models. Results also indicate potential for real time control of manufacturing and suggest directions for further research.,1,AI,0.0005612969398498,0.4136999249458313,Human,LABEL_1
"In the last years, multi-objective evolutionary algorithms (MOEA) have been applied to different software engineering problems where many conflicting objectives have to be optimized simultaneously. In theory, evolutionary algorithms feature a nice property for runtime optimization as they can provide a solution in any execution time. In practice, based on a Darwinian inspired natural selection, these evolutionary algorithms produce many deadborn solutions whose computation results in a computational resources wastage: natural selection is naturally slow. In this paper, we reconsider this founding analogy to accelerate convergence of MOEA, by looking at modern biology studies: artificial selection has been used to achieve an anticipated specific purpose instead of only relying on crossover and natural selection (i.e., Muller et al [18] research on artificial mutation of fruits with X-Ray). Putting aside the analogy with natural selection , the present paper proposes an hyper-heuristic for MOEA algorithms named Sputnik 1 that uses artificial selective mutation to improve the convergence speed of MOEA. Sputnik leverages the past history of mutation efficiency to select the most relevant mutations to perform. We evaluate Sputnik on a cloud-reasoning engine, which drives on-demand provisioning while considering conflicting performance and cost objectives. We have conducted experiments to highlight the significant performance improvement of Sputnik in terms of resolution time.",0,Human,0.0002500414848327,0.0136182308197021,Human,LABEL_1
"Machine learning models have been successfully used in many scientific and engineering fields. However, it remains difficult for a model to simultaneously utilize domain knowledge and experimental observation data. The application of knowledge-based symbolic AI represented by an expert system is limited by the expressive ability of the model, and data-driven connectionism AI represented by neural networks is prone to produce predictions that violate physical mechanisms. In order to fully integrate domain knowledge with observations, and make full use of the prior information and the strong fitting ability of neural networks, this study proposes theory-guided hard constraint projection (HCP). This model converts physical constraints, such as governing equations, into a form that is easy to handle through discretization, and then implements hard constraint optimization through projection. Based on rigorous mathematical proofs, theory-guided HCP can ensure that model predictions strictly conform to physical mechanisms in the constraint patch. The performance of the theory-guided HCP is verified by experiments based on the heterogeneous subsurface flow problem. Due to the application of hard constraints, compared with fully connected neural networks and soft constraint models, such as theory-guided neural networks and physics-informed neural networks, theory-guided HCP requires fewer data, and achieves higher prediction accuracy and stronger robustness to noisy observations.",0,Human,0.0063522458076477,0.0343681573867797,Human,LABEL_1
"This paper gives a detailed review of recent work related to privacy issues in social networks; it highlights current trends and developments and focuses especially on user concerns about personal data security and high expectations from network operators regarding privacy protection. Key points for future research are also emphasized. These include clarifying what privacy means and how to manage it differently across various types of social networks, and exploring new technologies and methods to enhance privacy protection on such sites. The paper ends by calling for continued research and cooperation among scholars, practitioners, and policy makers to solve this complicated and important matter of privacy in social networking.",1,AI,0.0025737881660461,0.9671131372451782,Human,LABEL_0
"This paper introduces a new way to solve the problem of detecting objects when some types are rare and slices appear rarely. We use targeted active learning and use submodular mutual information. Our solution is called Talisman. It selects important samples iteratively to label aiming at improving detector performance on rare types and slices. Talisman uses a sub modular function to measure mutual information among labeled and unlabeled samples; it uses this measure to choose samples that are most informative. Also this method takes into account target classes and slices, so that the system can give priority to selecting samples from those interesting rare classes and slices. Results show that Talisman improves detector performance on rare types and slices compared to other benchmark datasets. Results also show that our approach performs better than many leading active learning methods especially in scenarios where such rare types and slices appear. Overall, Talisman framework shows promise as a promising approach to targeted active learning for object detection, especially dealing with rare types and slices.",1,AI,0.0006773471832275,0.0413216352462768,Human,LABEL_1
"3D object detection based on point clouds has become more and more popular. Some methods propose localizing 3D objects directly from raw point clouds to avoid information loss. However, these methods come with complex structures and significant computational overhead, limiting its broader application in real-time scenarios. Some methods choose to transform the point cloud data into compact tensors first and leverage off-the-shelf 2D detectors to propose 3D objects, which is much faster and achieves state-of-the-art results. However, because of the inconsistency between 2D and 3D data, we argue that the performance of compact tensor-based 3D detectors is restricted if we use 2D detectors without corresponding modification. Specifically, the distribution of point clouds is uneven, with most points gather on the boundary of objects, while detectors for 2D data always extract features evenly. Motivated by this observation, we propose DENse Feature Indicator (DENFI), a universal module that helps 3D detectors focus on the densest region of the point clouds in a boundary-aware manner. Moreover, DENFI is lightweight and guarantees real-time speed when applied to 3D object detectors. Experiments on KITTI dataset show that DENFI improves the performance of the baseline single-stage detector remarkably, which achieves new state-of-the-art performance among previous 3D detectors, including both two-stage and multi-sensor fusion methods, in terms of mAP with a 34FPS detection speed.",0,Human,0.044005274772644,0.0129317045211792,Human,LABEL_1
"This paper looks into processes and roles and how they interact within organizational contexts. Its goal is to understand better how these things work together and how that impacts overall performance. By reviewing relevant literature carefully, this paper identifies different kinds of processes and roles and the tasks they do. Also it looks at different interactions between processes and roles like dependencies, conflicts, and cooperation. Methods used include both qualitative and quantitative approaches such as case studies and surveys along with statistical analysis. Results show that processes and roles are key drivers of success and that good interaction is important. In conclusion the paper gives some practical advice for organizations to improve process, roles and interactions for better performance.",1,AI,0.014736533164978,0.5259174108505249,Human,LABEL_0
"Semantic segmentation of aerial videos has been extensively used for decision making in monitoring environmental changes, urban planning, and disaster management. The reliability of these decision support systems is dependent on the accuracy of the video semantic segmentation algorithms. The existing CNN based video semantic segmentation methods have enhanced the image semantic segmentation methods by incorporating an additional module such as LSTM or optical flow for computing temporal dynamics of the video which is a computational overhead. The proposed research work modifies the CNN architecture by incorporating temporal information to improve the efficiency of video semantic segmentation.  In this work, an enhanced encoder-decoder based CNN architecture (UVid-Net) is proposed for UAV video semantic segmentation. The encoder of the proposed architecture embeds temporal information for temporally consistent labelling. The decoder is enhanced by introducing the feature-refiner module, which aids in accurate localization of the class labels. The proposed UVid-Net architecture for UAV video semantic segmentation is quantitatively evaluated on extended ManipalUAVid dataset. The performance metric mIoU of 0.79 has been observed which is significantly greater than the other state-of-the-art algorithms. Further, the proposed work produced promising results even for the pre-trained model of UVid-Net on urban street scene with fine tuning the final layer on UAV aerial videos.",0,Human,0.0003128051757812,0.0462632179260253,Human,LABEL_1
"As there is growing need for fast internet access and robust mobile network coverage, drones with built-in cell stations are emerging as promising solutions to boost network coverage in remote and disaster zones. But managing handovers, i.e., transferring user connections to new stations, is difficult because these drones move quickly and have small coverage. This paper introduces a strategy for managing handovers for stations mounted on drones. Our strategy minimizes latency and ensures smooth connections for multiple users when such station moves. It uses an active rather than passive approach where the station initiates the handover before losing user contact. We also consider the position and speed of the station and quality of communication links between user and station. Performance of the proposed method was evaluated through simulations of a realistic urban environment. Results showed that latency was reduced by up to 50% compared to traditional reactive approaches. Throughput also improved and failure rates decreased. Overall, our research shows that this strategy works well and enhances service quality and connectivity for users in low network coverage regions. Future work might integrate this strategy with other methods and assess its efficacy under various deployment conditions.",1,AI,0.0035126209259033,0.0092169046401977,Human,LABEL_1
"The development of positioning technologies has resulted in an increasing amount of mobility data being available. While bringing a lot of convenience to people's life, such availability also raises serious concerns about privacy. In this paper, we concentrate on one of the most sensitive information that can be inferred from mobility data, namely social relationships. We propose a novel social relation inference attack that relies on an advanced feature learning technique to automatically summarize users' mobility features. Compared to existing approaches, our attack is able to predict any two individuals' social relation, and it does not require the adversary to have any prior knowledge on existing social relations. These advantages significantly increase the applicability of our attack and the scope of the privacy assessment. Extensive experiments conducted on a large dataset demonstrate that our inference attack is effective, and achieves between 13% to 20% improvement over the best state-of-the-art scheme. We propose three defense mechanisms -- hiding, replacement and generalization -- and evaluate their effectiveness for mitigating the social link privacy risks stemming from mobility data sharing. Our experimental results show that both hiding and replacement mechanisms outperform generalization. Moreover, hiding and replacement achieve a comparable trade-off between utility and privacy, the former preserving better utility and the latter providing better privacy.",0,Human,0.0053274035453796,0.024213433265686,Human,LABEL_1
"Communication or influence networks are probably the most controllable of all factors that are known to impact on the problem-solving capability of task-forces. In the case connections are costly, it is necessary to implement a policy to allocate them to the individuals. Here we use an agent-based model to study how distinct allocation policies affect the performance of a group of agents whose task is to find the global maxima of NK fitness landscapes. Agents cooperate by broadcasting messages informing on their fitness and use this information to imitate the fittest agent in their influence neighborhoods. The larger the influence neighborhood of an agent, the more links, and hence information, the agent receives. We find that the elitist policy in which agents with above-average fitness have their influence neighborhoods amplified, whereas agents with below-average fitness have theirs deflated, is optimal for smooth landscapes, provided the group size is not too small. For rugged landscapes, however, the elitist policy can perform very poorly for certain group sizes. In addition, we find that the egalitarian policy, in which the size of the influence neighborhood is the same for all agents, is optimal for both smooth and rugged landscapes in the case of small groups. The welfarist policy, in which the actions of the elitist policy are reversed, is always suboptimal, i.e., depending on the group size it is outperformed by either the elitist or the egalitarian policies.",0,Human,0.0110354423522949,0.0160175561904907,Human,LABEL_1
"Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.",0,Human,0.0289512276649475,0.0108025670051574,Human,LABEL_1
"This paper proposes a new way to enhance slope limiters for use on grids that vary in size and direction. Slope limiters help stop unphysical results from occurring during computer simulations like those of fluid dynamics. Previous work has mostly concentrated on using these limiters on regular grids; this study looks at them on irregular grids too. Using a mix of adaptation techniques and new limiters, the new approach improves simulation accuracy and reliability. Results show that this new approach overcomes shortcomings of previous limiters and works well on a variety of tests. Study outcomes should be important for further development of simulation methods for irregular grids.",1,AI,0.0005097389221191,0.1625093221664428,Human,LABEL_1
"Contextual bandits are an approach that has been receiving increased attention recently because they excel at balancing exploration and exploitation when dealing sequentially with decision problems. This paper focuses on considering estimation issues in contextual bandits; we look at how different factors such as reward selection, number of arms, context representation choices, and sample size affect algorithm performance. We review current literature comprehensively and suggest some future research directions. Results show that performance of contextual bandits varies depending on estimation issues and emphasize the importance of careful design and evaluation of these algorithms. Insights learned here can help practitioners to design and deploy contextual bandits effectively and serve as a basis for further research.",1,AI,0.0003703832626342,0.9346587657928468,Human,LABEL_0
"This paper introduces a framework for specific ranking of knowledge base attributes. The method was tested using two case studies: one based on data about advisors for doctoral students and another based on medical conditions. Results show this specific ranking framework performs better than previous methods in terms of accuracy and relevance. Results suggest benefits for enhancing quality and performance of systems that rely on knowledge bases, especially those related to academic advising and diagnosis of diseases. Full version of the paper includes further analysis of the proposed approach including detailed description of implementation and thorough evaluation against these two case studies.",1,AI,0.007437527179718,0.7761310935020447,Human,LABEL_0
This paper studies the Conditional Lucas & Kanade (CLK) algorithm which is a common technique in computer vision for computing optical flow from one frame to another in a video sequence. We introduce a new enhancement of the original Lucas and Kanade (LK) algorithm by including a spatially varying weight matrix. This modification allows us to compute optical flow conditioned on local features and thus enhance both accuracy and robustness to changes in lighting and occlusion. Performance evaluation was done using standard benchmark datasets and comparisons were made against top algorithms for optical flow. Results show that this enhanced CLK algorithm performs better than the original LK algorithm and competing methods especially in hard cases with large displacements and occlusions. We also show practical benefits of the CLK algorithm in real life such as visual odometry where reliable estimation of camera motion is important. Results indicate that this algorithm excels both in accuracy and computational efficiency. Overall we conclude that the enhanced CLK algorithm is an effective improvement over the original LK algorithm and has promise to advance computer vision in diverse practical tasks.,1,AI,0.0004293322563171,0.0211663842201232,Human,LABEL_1
"As real-world images come in varying sizes, the machine learning model is part of a larger system that includes an upstream image scaling algorithm. In this system, the model and the scaling algorithm have become attractive targets for numerous attacks, such as adversarial examples and the recent image-scaling attack. In response to these attacks, researchers have developed defense approaches that are tailored to attacks at each processing stage. As these defenses are developed in isolation, their underlying assumptions may not hold when viewing them from the perspective of an end-to-end machine learning system. Thus, it is necessary to study these attacks and defenses in the context of machine learning systems. In this paper, we investigate the interplay between vulnerabilities of the image scaling procedure and machine learning models in the challenging hard-label black-box setting. We propose a series of novel techniques to make a black-box attack exploit vulnerabilities in scaling algorithms, scaling defenses, and the final machine learning model in an end-to-end manner. Based on this scaling-aware attack, we reveal that most existing scaling defenses are ineffective under threat from downstream models. Moreover, we empirically observe that standard black-box attacks can significantly improve their performance by exploiting the vulnerable scaling procedure. We further demonstrate this problem on a commercial Image Analysis API with transfer-based black-box attacks.",0,Human,0.2970966696739197,0.028498351573944,Human,LABEL_1
This paper investigates how genetic algorithms can be used to enhance loadability in distribution systems. The purpose is to maximize transfer of load among feeders and thereby overall system performance. Using a realistic system model the authors propose an approach using genetic algorithms to solve the optimization problem. Performance of this method is tested on different load profiles and results indicate that genetic algorithm methods work well for load transfer and increase loadability of the system. Comparisons with other approaches also show that genetic algorithm methods outperform others in terms of improved loadability. Results of this research can help distribution system operators to develop better transfer strategies and enhance performance.,1,AI,0.0052582025527954,0.8387126922607422,Human,LABEL_0
"In recent years there has been increasing interest in studying computational complexity of different versions of combinatorial voter control problems in elections. Combinatorial voter control refers to the issue of strategically changing votes of some subset of voters to influence the result of an election. This paper analyzes the computational complexity of many variants of this problem comprehensively. Initially we look at the standard voter control problem, where the aim is to minimize the number of vote changes needed to secure a desired winner. We show this problem is NP hard, meaning it is computationally impractical for large instances of the problem. Then we investigate variants which consider limits like budget and restriction on number of changes per voter. We find that these variants are also hard and provide approximation algorithms for solving them. Finally we look at the problem under different election systems such as single winner, multi winner and proportional representation. We show the problem's hardness depends on the system and develop algorithms for each. Overall this paper provides an analysis of the computational complexity of combinatorial voting control problems and gives insight into designing algorithms for these problems.",1,AI,0.0037322640419006,0.0307794213294982,Human,LABEL_1
This research evaluates performance of Graph Neural Networks (GNNs) in virtual screening by evaluating binding affinity predictions of small molecules against protein targets. Authors test multiple top GNNs on diverse complexes and compare performances with other standard methods. Results show that GNNs perform comparably or better than usual methods with high accuracy and stability. Authors also identify important aspects affecting prediction reliability like selection of hyper parameters and quality of training data. Findings from this research point out potential usefulness of GNNs for screening and highlight them as valuable tools for drug discovery.,1,AI,0.0007329583168029,0.1740978360176086,Human,LABEL_1
"This paper offers a broad review of the literature on the Off Switch Game which has been studied within the context of game theory and decision theory. The game features two players; one player can turn off play while the other can accept or reject this choice. Results include a detailed description of the game's mathematical formulation and equilibrium states, along with discussion of application areas such as bargaining and negotiation. We also present new analytical methods for studying the game and its variants. Case studies and simulation results show their practical utility. Results from this research improve understanding of how people make decisions when they have the option to terminate interaction early and are relevant to many fields.",1,AI,0.0011826157569885,0.3439799547195434,Human,LABEL_1
"Purpose: Development of a fast and fully automated deep learning pipeline (FatSegNet) to accurately identify, segment, and quantify abdominal adipose tissue on Dixon MRI from the Rhineland Study - a large prospective population-based study. Method: FatSegNet is composed of three stages: (i) consistent localization of the abdominal region using two 2D-Competitive Dense Fully Convolutional Networks (CDFNet), (ii) segmentation of adipose tissue on three views by independent CDFNets, and (iii) view aggregation. FatSegNet is trained with 33 manually annotated subjects, and validated by: 1) comparison of segmentation accuracy against a testingset covering a wide range of body mass index (BMI), 2) test-retest reliability, and 3) robustness in a large cohort study. Results: The CDFNet demonstrates increased robustness compared to traditional deep learning networks. FatSegNet dice score outperforms manual raters on the abdominal visceral adipose tissue (VAT, 0.828 vs. 0.788), and produces comparable results on subcutaneous adipose tissue (SAT, 0.973 vs. 0.982). The pipeline has very small test-retest absolute percentage difference and excellent agreement between scan sessions (VAT: APD = 2.957%, ICC=0.998 and SAT: APD= 3.254%, ICC=0.996). Conclusion: FatSegNet can reliably analyze a 3D Dixon MRI in1 min. It generalizes well to different body shapes, sensitively replicates known VAT and SAT volume effects in a large cohort study, and permits localized analysis of fat compartments.",0,Human,0.002503216266632,0.0066465139389038,Human,LABEL_1
"The problem of estimating sparse eigenvectors of a symmetric matrix attracts a lot of attention in many applications, especially those with high dimensional data set. While classical eigenvectors can be obtained as the solution of a maximization problem, existing approaches formulate this problem by adding a penalty term into the objective function that encourages a sparse solution. However, the resulting methods achieve sparsity at the expense of sacrificing the orthogonality property. In this paper, we develop a new method to estimate dominant sparse eigenvectors without trading off their orthogonality. The problem is highly non-convex and hard to handle. We apply the MM framework where we iteratively maximize a tight lower bound (surrogate function) of the objective function over the Stiefel manifold. The inner maximization problem turns out to be a rectangular Procrustes problem, which has a closed form solution. In addition, we propose a method to improve the covariance estimation problem when its underlying eigenvectors are known to be sparse. We use the eigenvalue decomposition of the covariance matrix to formulate an optimization problem where we impose sparsity on the corresponding eigenvectors. Numerical experiments show that the proposed eigenvector extraction algorithm matches or outperforms existing algorithms in terms of support recovery and explained variance, while the covariance estimation algorithms improve significantly the sample covariance estimator.",0,Human,0.0309419631958007,0.0184413790702819,Human,LABEL_1
This paper studies interference reduction methods in Cloud Radio Access Networks (C RANs) combining Rate Splitting (RS) with Common Message Decoding (CMD). RS is an attractive way to reduce interference and increase spectral efficiency but applying it to C RANs has been challenging because coordination among Baseband Units (BBUs) and Remote Radio Heads (RRHs) is difficult. This paper proposes a joint design of RS and CMD at the BBU to mitigate interference in C RANs. We develop a new algorithm to optimize splitting parameters and CMD coefficients to minimize total transmitted power and ensure service quality for all users. Results from simulation show that this method performs significantly better than previous interference mitigation approaches especially under dense and interference prone conditions. Insights are gained about potential use of RS and CMD for interference management in C RANs and these findings could guide design of future wireless systems.,1,AI,0.0203369259834289,0.0186893939971923,Human,LABEL_1
"Current evaluation functions for heuristic planning are expensive to compute. In numerous planning problems these functions provide good guidance to the solution, so they are worth the expense. However, when evaluation functions are misguiding or when planning problems are large enough, lots of node evaluations must be computed, which severely limits the scalability of heuristic planners. In this paper, we present a novel solution for reducing node evaluations in heuristic planning based on machine learning. Particularly, we define the task of learning search control for heuristic planning as a relational classification task, and we use an off-the-shelf relational classification tool to address this learning task. Our relational classification task captures the preferred action to select in the different planning contexts of a specific planning domain. These planning contexts are defined by the set of helpful actions of the current state, the goals remaining to be achieved, and the static predicates of the planning task. This paper shows two methods for guiding the search of a heuristic planner with the learned classifiers. The first one consists of using the resulting classifier as an action policy. The second one consists of applying the classifier to generate lookahead states within a Best First Search algorithm. Experiments over a variety of domains reveal that our heuristic planner using the learned classifiers solves larger problems than state-of-the-art planners.",0,Human,0.0039746761322021,0.0093169212341308,Human,LABEL_1
"This paper analyzes two leading solutions for mobile broadband in South Asia: WiMAX and LTE. It seeks to discern which technology offers reliable and high speeds. We look at the current infrastructure for mobile broadband in South Asia and specific challenges like population density and geography. An in depth look at these technologies is given: including technical specs, deployment needs and performance features. Results of this study compare both technologies in terms of factors such as coverage, capacity, latency and cost. Conclusions are drawn about best technology to meet South Asian needs. Findings from this paper will be helpful to policy makers, mobile operators and others in telecom industry aiming to enhance broadband connectivity in South Asia.",1,AI,0.0126187801361083,0.2831340432167053,Human,LABEL_1
"E-commerce platforms usually display a mixed list of ads and organic items in feed. One key problem is to allocate the limited slots in the feed to maximize the overall revenue as well as improve user experience, which requires a good model for user preference. Instead of modeling the influence of individual items on user behaviors, the arrangement signal models the influence of the arrangement of items and may lead to a better allocation strategy. However, most of previous strategies fail to model such a signal and therefore result in suboptimal performance. In addition, the percentage of ads exposed (PAE) is an important indicator in ads allocation. Excessive PAE hurts user experience while too low PAE reduces platform revenue. Therefore, how to constrain the PAE within a certain range while keeping personalized recommendation under the PAE constraint is a challenge.  In this paper, we propose Cross Deep Q Network (Cross DQN) to extract the crucial arrangement signal by crossing the embeddings of different items and modeling the crossed sequence by multi-channel attention. Besides, we propose an auxiliary loss for batch-level constraint on PAE to tackle the above-mentioned challenge. Our model results in higher revenue and better user experience than state-of-the-art baselines in offline experiments. Moreover, our model demonstrates a significant improvement in the online A/B test and has been fully deployed on Meituan feed to serve more than 300 millions of customers.",0,Human,0.0004950761795043,0.0120363235473632,Human,LABEL_1
"This study looks at connectivity properties of random annulus graphs and geometric block models. These models are frequently employed in network science to represent networks with inherent geometric structures. Specifically, we focus on critical threshold for connectivity: where networks go from disconnected to connected. Initially we define random annulus graphs, which are random graphs formed by linking points within an annulus with connection probability depending on Euclidean distances between points. We prove that critical threshold for connectivity depends on simple functions of annulus parameters. Next we study geometric block model which generates random graphs by dividing d dimensional space into blocks and linking points within the same block with some probability. Results show that critical threshold of connectivity depends on product of block probabilities and volume of blocks. Finally we compare thresholds for connectivity in random annulus graph versus geometric block model. Results show that connectivity threshold in block model is larger than or equal to that of annulus model. This result matters a lot to design and analysis of systems like wireless sensor networks and social networks. In sum, this paper offers key insight into connectivity properties of random graphs with inherent geometric features.",1,AI,0.0006213188171386,0.0126212239265441,Human,LABEL_1
"Recent advances in deep convolutional neural networks (DCNNs) have shown impressive performance improvements on thermal to visible face synthesis and matching problems. However, current DCNN-based synthesis models do not perform well on thermal faces with large pose variations. In order to deal with this problem, heterogeneous face frontalization methods are needed in which a model takes a thermal profile face image and generates a frontal visible face. This is an extremely difficult problem due to the large domain as well as large pose discrepancies between the two modalities. Despite its applications in biometrics and surveillance, this problem is relatively unexplored in the literature. We propose a domain agnostic learning-based generative adversarial network (DAL-GAN) which can synthesize frontal views in the visible domain from thermal faces with pose variations. DAL-GAN consists of a generator with an auxiliary classifier and two discriminators which capture both local and global texture discriminations for better synthesis. A contrastive constraint is enforced in the latent space of the generator with the help of a dual-path training strategy, which improves the feature vector discrimination. Finally, a multi-purpose loss function is utilized to guide the network in synthesizing identity preserving cross-domain frontalization. Extensive experimental results demonstrate that DAL-GAN can generate better quality frontal views compared to the other baseline methods.",0,Human,0.0004806518554687,0.0144493579864501,Human,LABEL_1
"Recent results by Alagic and Russell have given some evidence that the Even-Mansour cipher may be secure against quantum adversaries with quantum queries, if considered over other groups than $(\mathbb{Z}/2)^n$. This prompts the question as to whether or not other classical schemes may be generalized to arbitrary groups and whether classical results still apply to those generalized schemes. In this thesis, we generalize the Even-Mansour cipher and the Feistel cipher. We show that Even and Mansour's original notions of secrecy are obtained on a one-key, group variant of the Even-Mansour cipher. We generalize the result by Kilian and Rogaway, that the Even-Mansour cipher is pseudorandom, to super pseudorandomness, also in the one-key, group case. Using a Slide Attack we match the bound found above. After generalizing the Feistel cipher to arbitrary groups we resolve an open problem of Patel, Ramzan, and Sundaram by showing that the 3-round Feistel cipher over an arbitrary group is not super pseudorandom. We generalize a result by Gentry and Ramzan showing that the Even-Mansour cipher can be implemented using the Feistel cipher as the public permutation. In this result, we also consider the one-key case over a group and generalize their bound. Finally, we consider Zhandry's result on quantum pseudorandom permutations, showing that his result may be generalized to hold for arbitrary groups. In this regard, we consider whether certain card shuffles may be generalized as well.",0,Human,0.0115564465522766,0.0139947533607482,Human,LABEL_1
"Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.",0,Human,0.0004130005836486,0.0045312643051147,Human,LABEL_1
This paper introduces an algorithm for solving large linear programs efficiently through use of shared memory parallel processing. The new algorithm makes use of inherent parallelism in modern processors and uses efficient data structures and methods to improve performance. It utilizes dynamic scheduling of tasks based on a task parallelism model. Problems are divided into smaller tasks and solved separately using the simplex method. Load balancing is used to distribute work evenly among processors. Performance was tested on diverse benchmark problems of differing size and nature. Results show that the algorithm scales well and outperforms sequential simplex. Further performance enhancement is made through advanced optimization techniques such as lazy updates and column layout. Results indicate that this approach will have a strong impact on LP and particularly on large scale optimization problems. The algorithm is very flexible and easily extensible to other types of LP like mixed integer programming and network flow. Overall results show feasibility and efficacy of shared memory parallelism for large scale LP problems opening up avenues for future research.,1,AI,0.0187890529632568,0.0302628874778747,Human,LABEL_1
"This paper introduces a new way to derive theorems in linear logic that uses a declarative approach. Linear logic is a kind of logic very useful for studying computation because it handles resource usage naturally. However, deriving theorems from linear logic poses a challenge to those unfamiliar with its rules and strategies. In order to address this difficulty, authors suggest an approach to deriving theorems that relies on a high level algebraic representation of proofs. The main idea behind their method is to use rewrite rules capturing core structural elements of proofs and systematically applying them in an automated fashion until a theorem is obtained. Results indicate that this method works well and efficiently for deriving theorems in implication logic and also suggests its applicability to other logics and related tasks.",1,AI,0.1948464512825012,0.2865338325500488,Human,LABEL_1
"This paper introduces a new method for face recognition using transformed shape features. Proposed method transforms facial features into a new shape representation which serves as input for a recognition model. Transformation considers variability inherent in features like illumination, pose and expression to produce robust face representation. Results obtained from experiments on various public datasets show this new approach performs better compared to leading methods in accuracy and robustness. Results indicate that transformation of shape features holds promise for future research into face recognition.",1,AI,0.0003244876861572,0.9326002597808838,Human,LABEL_0
"This paper introduces a new method for anomaly detection using multiple similarity criteria and Pareto depth analysis. It tackles shortcomings in current methods by looking at many criteria and examining scores of Pareto depth. Pareto depth measures how far away a data point is from other points that represent good tradeoffs among different attributes. Through Pareto depth analysis this new method finds anomalies that stand out significantly from those good tradeoffs and clarifies the tradeoffs among different attributes. Performance of this new method is evaluated on various real data sets and results show higher performance compared to leading methods. This approach works in diverse fields such as finance, healthcare, and cybersecurity and thus detection of anomalies is critical to system integrity and security. Results of this study contribute to anomaly detection research and this new method could be incorporated into practical applications.",1,AI,0.0060757994651794,0.1006479859352111,Human,LABEL_1
"This paper introduces a new approach to detect cells in large MIMO systems using distributed expectation propagation (DEP). Large MIMO systems promise high spectral efficiency in wireless networks but detection algorithms are often complex and hard to implement. DEP solves this issue by decomposing detection into smaller tasks which multiple processing elements can work on simultaneously. Performance evaluation through simulations compares this approach favorably to current best methods. Results indicate DEP performs comparably or better while being less computationally intensive and consuming less memory. In addition, because it distributes processing load, this approach scales easily to large networks with low overhead. Discussion also considers potential use beyond cell detection including distributed optimization and inference. Results show DEP is promising both for improving performance and efficiency in cell detection and potentially in other distributed computing contexts.",1,AI,0.0004920959472656,0.0097233057022094,Human,LABEL_1
"The security evaluation for Mail Distribution Systems focuses on certification and reliability of sensitive data between mail servers. The need to certify the information conveyed is a result of known weaknesses in the simple mail transfer protocol (SMTP). The most important consequence of these weaknesses is the possibility to mislead the recipient, which is achieved via spam (especially email spoofing). Email spoofing refers to alterations in the headers and/or the content of the message. Therefore, the authenticity of the message is compromised. Unfortunately, the broken link between certification and reliability of the information is unsolicited email (spam).  Unlike the current practice of estimating the cost of spam, which prompts organizations to purchase and maintain appropriate anti-spam software, our approach offers an alternative perspective of the economic and moral consequences of unsolicited mail. The financial data provided in this paper show that spam is a major contributor to the financial and production cost of an organization, necessitating further attention. Additionally, this paper highlights the importance and severity of the weaknesses of the SMTP protocol, which can be exploited even with the use of simple applications incorporated within most commonly used Operating Systems (e.g. Telnet).  As a consequence of these drawbacks Mail Distribution Systems need to be appropriate configured so as to provide the necessary security services to the users.",0,Human,0.0005041360855102,0.1543916463851928,Human,LABEL_1
"In recent years, deep neural network is widely used in machine learning. The multi-class classification problem is a class of important problem in machine learning. However, in order to solve those types of multi-class classification problems effectively, the required network size should have hyper-linear growth with respect to the number of classes. Therefore, it is infeasible to solve the multi-class classification problem using deep neural network when the number of classes are huge. This paper presents a method, so called Label Mapping (LM), to solve this problem by decomposing the original classification problem to several smaller sub-problems which are solvable theoretically. Our method is an ensemble method like error-correcting output codes (ECOC), but it allows base learners to be multi-class classifiers with different number of class labels. We propose two design principles for LM, one is to maximize the number of base classifier which can separate two different classes, and the other is to keep all base learners to be independent as possible in order to reduce the redundant information. Based on these principles, two different LM algorithms are derived using number theory and information theory. Since each base learner can be trained independently, it is easy to scale our method into a large scale training system. Experiments show that our proposed method outperforms the standard one-hot encoding and ECOC significantly in terms of accuracy and model complexity.",0,Human,0.0041705965995788,0.0104844570159912,Human,LABEL_1
"This paper studies how groups evolve during running races. We use data from different races to look at changes in group size and composition through time and at what causes groups to be formed and dissolved. We find that groups typically form quickly at the start of a race; they are shaped by pacing strategies, social interactions and environmental conditions. We also see that groups disintegrate as a race continues and runners adopt independent paces. Results indicate important dynamics among group behaviors in long events and suggest implications for designing course layouts and training plans.",1,AI,0.0712155699729919,0.3161104917526245,Human,LABEL_1
"This paper investigates how to realize better and more efficient embedding learning to tackle the semi-supervised video object segmentation under challenging multi-object scenarios. The state-of-the-art methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times computing resources. To solve the problem, we propose an Associating Objects with Transformers (AOT) approach to match and decode multiple objects uniformly. In detail, AOT employs an identification mechanism to associate multiple targets into the same high-dimensional embedding space. Thus, we can simultaneously process multiple objects' matching and segmentation decoding as efficiently as processing a single object. For sufficiently modeling multi-object association, a Long Short-Term Transformer is designed for constructing hierarchical matching and propagation. We conduct extensive experiments on both multi-object and single-object benchmarks to examine AOT variant networks with different complexities. Particularly, our R50-AOT-L outperforms all the state-of-the-art competitors on three popular benchmarks, i.e., YouTube-VOS (84.1% J&F), DAVIS 2017 (84.9%), and DAVIS 2016 (91.1%), while keeping more than $3\times$ faster multi-object run-time. Meanwhile, our AOT-T can maintain real-time multi-object speed on the above benchmarks. Based on AOT, we ranked 1st in the 3rd Large-scale VOS Challenge.",0,Human,0.0007118582725524,0.0089026689529418,Human,LABEL_1
"This paper characterizes sensitivity analysis for monomials through a geometric lens. Monomials are common in many disciplines such as science and engineering. Our approach interprets sensitivity matrix as sets of linear subspaces within the space of model parameters; these subspaces are shown to carry important information about how sensitive model outputs are to changes in input parameters. Specifically, dimensions and orientations of these subspaces are significant. Results indicate sensitivity depends on geometry of subspaces and their intersections. New results provide new ways to analyze and visualize sensitivities for this type of model in high dimensional spaces. Numerical examples illustrate the approach and compare favorably against previous methods.",1,AI,0.0003522634506225,0.2799472212791443,Human,LABEL_1
"This paper studies performance metrics for wireless powered cognitive radio networks (WPCRNs) using CS and MC methods. It focuses on using extra energy from main users to energize secondary users. It also looks at performance changes due to different system parameters such as the number of primary and secondary users, compression ratios for CS and MC ranks. Simulations are used to test proposed approaches and compare them against other methods. Results indicate that combining CS and MC methods substantially improves system performance and increases efficiency compared to usual means. Results from this study could be very useful for further research on WPCRNs and contribute to designing good communication systems.",1,AI,0.063778817653656,0.3917953372001648,Human,LABEL_1
"This study looks at the potential of applying machine learning techniques to enhance efficiency of solving Boolean Satisfiability Problems (SAT). SAT is a common problem known to be NP complete and is frequently used in diverse areas like hardware and software verification, scheduling, and cryptography. Authors propose a new method that uses machine learning models to direct the search process of current SAT solvers. Results indicate that proposed method greatly reduces solving time compared to leading solvers. Performance on different benchmark data sets shows comparable results as well; thus showing effectiveness under practical conditions. Research findings also shed light on application of machine learning to solving difficult computational tasks and suggest potential advancement possibilities.",1,AI,0.0004301667213439,0.440742015838623,Human,LABEL_1
"This paper looks at how ""different things work for different people"" applies to different kinds of dialogue tasks by investigating different pre training methods. They test effectiveness of these methods on various dialogue tasks and find that there is no best fit method that works equally well for everything. Different methods work better depending on what kind of task you have. Researchers conclude that careful consideration of specific task characteristics is necessary when selecting a method of pre training to get top performance.",1,AI,0.0923068523406982,0.9373956322669984,Human,LABEL_0
"Several recent studies in privacy-preserving learning have considered the trade-off between utility or risk and the level of differential privacy guaranteed by mechanisms for statistical query processing. In this paper we study this trade-off in private Support Vector Machine (SVM) learning. We present two efficient mechanisms, one for the case of finite-dimensional feature mappings and one for potentially infinite-dimensional feature mappings with translation-invariant kernels. For the case of translation-invariant kernels, the proposed mechanism minimizes regularized empirical risk in a random Reproducing Kernel Hilbert Space whose kernel uniformly approximates the desired kernel with high probability. This technique, borrowed from large-scale learning, allows the mechanism to respond with a finite encoding of the classifier, even when the function class is of infinite VC dimension. Differential privacy is established using a proof technique from algorithmic stability. Utility--the mechanism's response function is pointwise epsilon-close to non-private SVM with probability 1-delta--is proven by appealing to the smoothness of regularized empirical risk minimization with respect to small perturbations to the feature mapping. We conclude with a lower bound on the optimal differential privacy of the SVM. This negative result states that for any delta, no mechanism can be simultaneously (epsilon,delta)-useful and beta-differentially private for small epsilon and small beta.",0,Human,0.0003432035446166,0.0077517032623291,Human,LABEL_1
"Background: The learning of genotype-phenotype associations and history of human disease by doing detailed and precise analysis of phenotypic abnormalities can be defined as deep phenotyping. To understand and detect this interaction between phenotype and genotype is a fundamental step when translating precision medicine to clinical practice. The recent advances in the field of machine learning is efficient to predict these interactions between abnormal human phenotypes and genes.  Methods: In this study, we developed a framework to predict links between human phenotype ontology (HPO) and genes. The annotation data from the heterogeneous knowledge resources i.e., orphanet, is used to parse human phenotype-gene associations. To generate the embeddings for the nodes (HPO & genes), an algorithm called node2vec was used. It performs node sampling on this graph based on random walks, then learns features over these sampled nodes to generate embeddings. These embeddings were used to perform the downstream task to predict the presence of the link between these nodes using 5 different supervised machine learning algorithms.  Results: The downstream link prediction task shows that the Gradient Boosting Decision Tree based model (LightGBM) achieved an optimal AUROC 0.904 and AUCPR 0.784. In addition, LightGBM achieved an optimal weighted F1 score of 0.87. Compared to the other 4 methods LightGBM is able to find more accurate interaction/link between human phenotype & gene pairs.",0,Human,0.0008894801139831,0.0964446067810058,Human,LABEL_1
Adaptive motion planning is important for autonomous systems especially robots. We present a new approach to motion planning using artificial potentials and prior paths. This method combines an algorithm based on potentials with a previous path to overcome limitations of traditional potential fields such as local minima and slow convergence. Performance is compared using simulations and real robot experiments and this new approach shows better performance compared to traditional potential field approaches. Results show that using prior paths can improve performance of planning algorithms based on potentials and make them more fit for practical use.,1,AI,0.0247805118560791,0.7674775123596191,Human,LABEL_0
